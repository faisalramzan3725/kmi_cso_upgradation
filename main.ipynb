{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script to extract cso_concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 14612 labels to CSV: c:\\Users\\Faisal Ramzan\\Desktop\\kmi_project_cso\\cso_label\\cso_label_counts.csv\n",
      "Extracted 14612 unique CSO concepts.\n",
      "Total concepts extracted: 14612\n",
      "\n",
      "Sample Concepts (first 10):\n",
      "- computer science\n",
      "- automated pattern recognition\n",
      "- subtraction technique\n",
      "- nonrigid registration\n",
      "- non-rigid registration\n",
      "- manifold learning\n",
      "- nonlinear dimensionality reduction\n",
      "- locality preserving projections\n",
      "- locality preserving projection\n",
      "- gait recognition\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import required libraries\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the current directory to Python path\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "# Step 2: Import the cso_script function\n",
    "from cso_script import extract_cso_concepts\n",
    "\n",
    "# Step 3: Set the file path to your TTL file\n",
    "ttl_path = os.path.join(os.getcwd(), \"cso_label\", \"CSO\", \"CSO.3.4.1.ttl\")\n",
    "\n",
    "# Step 4: Set the output CSV path\n",
    "output_csv = os.path.join(os.getcwd(), \"cso_label\", \"cso_label_counts.csv\")\n",
    "\n",
    "# Step 5: Extract concepts\n",
    "try:\n",
    "    cso_concepts = extract_cso_concepts(ttl_path, save_csv=True, csv_path=output_csv)\n",
    "    \n",
    "    # Step 6: Preview results\n",
    "    print(f\"Total concepts extracted: {len(cso_concepts)}\")\n",
    "    print(\"\\nSample Concepts (first 10):\")\n",
    "    for concept in cso_concepts[:10]:\n",
    "        print(f\"- {concept}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running main script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Faisal\n",
      "[nltk_data]     Ramzan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing first 1000 rows of the dataset\n",
      "\n",
      "Processing completed in 1891.58 seconds\n",
      "\n",
      "Sample of processed titles and abstracts:\n",
      "\n",
      "First 2 processed titles:\n",
      "0    a new approach of 3d watermarking based on ima...\n",
      "1    attractor neural_networks with activitydepende...\n",
      "Name: title_processed, dtype: object\n",
      "\n",
      "First 2 processed abstracts:\n",
      "0    in this paper a robust 3d triangular mesh wate...\n",
      "1    we studied an autoassociative neural_network w...\n",
      "Name: abstract_processed, dtype: object\n",
      "\n",
      "Processed data saved to: c:\\Users\\Faisal Ramzan\\Desktop\\kmi_project_cso\\paper_dataset\\processed_title_abstract_v2.csv\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from concept_find_replace import process_dataset\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Set paths for input files\n",
    "dataset_path = os.path.join(os.getcwd(), \"paper_dataset\", \"paper_dataset.csv\")\n",
    "cso_path = os.path.join(os.getcwd(), \"cso_label\", \"cso_label_counts.csv\")\n",
    "\n",
    "# Set path for output file\n",
    "save_path = os.path.join(os.getcwd(), \"paper_dataset\", \"processed_title_abstract_v2.csv\")\n",
    "\n",
    "# Set sample size (use None for full dataset)\n",
    "sample_size = 1000  # Change this to any number you want, or None for full dataset\n",
    "\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Read the dataset with sampling\n",
    "    if sample_size:\n",
    "        df = pd.read_csv(dataset_path, nrows=sample_size)\n",
    "        print(f\"Processing first {sample_size} rows of the dataset\")\n",
    "    else:\n",
    "        df = pd.read_csv(dataset_path)\n",
    "        print(\"Processing entire dataset\")\n",
    "    \n",
    "    # Save the sampled dataset temporarily\n",
    "    temp_dataset = \"temp_dataset.csv\"\n",
    "    df.to_csv(temp_dataset, index=False)\n",
    "    \n",
    "    # Process the sampled dataset\n",
    "    processed_df = process_dataset(\n",
    "        dataset_path=temp_dataset,\n",
    "        cso_path=cso_path,\n",
    "        save_path=save_path\n",
    "    )\n",
    "    \n",
    "    # Clean up temporary file\n",
    "    if os.path.exists(temp_dataset):\n",
    "        os.remove(temp_dataset)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nProcessing completed in {processing_time:.2f} seconds\")\n",
    "    print(\"\\nSample of processed titles and abstracts:\")\n",
    "    print(\"\\nFirst 2 processed titles:\")\n",
    "    print(processed_df['title_processed'].head(2))\n",
    "    print(\"\\nFirst 2 processed abstracts:\")\n",
    "    print(processed_df['abstract_processed'].head(2))\n",
    "    \n",
    "    print(f\"\\nProcessed data saved to: {save_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing first 2 rows of the dataset\n",
      "\n",
      "Processing completed in 2.05 seconds\n",
      "\n",
      "=== Processed Text Results ===\n",
      "\n",
      "First 2 processed titles:\n",
      "0    a new approach of 3d watermarking based on ima...\n",
      "1    attractor neural_networks with activitydepende...\n",
      "Name: title_processed, dtype: object\n",
      "\n",
      "First 2 processed abstracts:\n",
      "0    in this paper a robust 3d triangular mesh wate...\n",
      "1    we studied an autoassociative neural_network w...\n",
      "Name: abstract_processed, dtype: object\n",
      "\n",
      "=== N-gram Results ===\n",
      "\n",
      "Sample tokenized text:\n",
      "0    [a, new, approach, of, 3d, watermarking, based...\n",
      "Name: tokens, dtype: object\n",
      "\n",
      "Sample trigrams:\n",
      "\n",
      "Document 1 trigrams:\n",
      "['a', 'new', 'approach', 'of', '3d', 'watermarking', 'based_on', 'image_segmentation', 'in', 'this', 'paper', 'a', 'robust', '3d', 'triangular', 'mesh', 'watermarking', 'algorithm', 'based_on', '3d', 'segmentation', 'is', 'proposed', 'in', 'this', 'algorithm', 'three', 'classes', 'of', 'watermarking', 'are', 'combined', 'first', 'we', 'segment', 'the', 'original', 'image', 'to', 'many', 'different', 'regions', 'then', 'we', 'mark', 'every', 'type', 'of', 'region', 'with', 'the', 'corresponding', 'algorithm', 'based_on', 'their', 'curvature', 'value', 'the', 'experiments', 'show', 'that', 'our', 'watermarking', 'is', 'robust', 'against', 'numerous', 'attacks', 'including', 'rst', 'transformations', 'smoothing', 'additive', 'random', 'noise', 'cropping', 'simplification', 'and', 'remeshing']\n",
      "\n",
      "Document 2 trigrams:\n",
      "['attractor', 'neural_networks', 'with', 'activitydependent', 'synapses', 'the', 'role', 'of', 'synaptic', 'facilitation', 'we', 'studied', 'an', 'autoassociative', 'neural_network', 'with', 'dynamic', 'synapses', 'which', 'include', 'a', 'facilitating', 'mechanism', 'we', 'have', 'developed', 'a', 'general', 'meanfield', 'framework', 'to', 'study', 'the', 'relevance', 'of', 'the', 'different', 'parameters', 'defining', 'the', 'dynamics', 'of', 'the', 'synapses', 'and', 'their', 'influence', 'on', 'the', 'collective', 'properties', 'of', 'the_network', 'depending', 'on', 'these', 'parameters', 'the_network', 'shows', 'different', 'types', 'of', 'behaviour', 'including', 'a', 'retrieval', 'phase', 'an', 'oscillatory', 'regime', 'and', 'a', 'nonretrieval', 'phase', 'in', 'the', 'oscillatory', 'phase', 'the_network', 'activity', 'continously', 'jumps', 'between', 'the_stored', 'patterns', 'compared', 'with', 'other', 'activitydependent', 'mechanisms', 'such', 'as', 'synaptic', 'depression', 'synaptic', 'facilitation', 'enhances', 'the_network', 'ability', 'to', 'switch', 'among', 'the_stored', 'patterns', 'and', 'therefore', 'its', 'adaptation', 'to', 'external', 'stimuli', 'a', 'detailed', 'analysis', 'of', 'our', 'system', 'reflects', 'an', 'efficientmore', 'rapid', 'and', 'with', 'lesser', 'errorsnetwork', 'access', 'to', 'the_stored', 'information', 'with', 'stronger', 'facilitation', 'we', 'also', 'present', 'a', 'set', 'of', 'monte', 'carlo', 'simulations', 'confirming', 'our', 'analytical_results']\n",
      "\n",
      "Processed data saved to: c:\\Users\\Faisal Ramzan\\Desktop\\kmi_project_cso\\paper_dataset\\processed_title_abstract_v2.csv\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from concept_find_replace import process_dataset\n",
    "\n",
    "\n",
    "# Set paths for input files\n",
    "dataset_path = os.path.join(os.getcwd(), \"paper_dataset\", \"paper_dataset.csv\")\n",
    "cso_path = os.path.join(os.getcwd(), \"cso_label\", \"cso_label_counts.csv\")\n",
    "\n",
    "# Set path for output file\n",
    "save_path = os.path.join(os.getcwd(), \"paper_dataset\", \"processed_title_abstract_v2.csv\")\n",
    "\n",
    "# Set sample size (use None for full dataset)\n",
    "sample_size = 2  # Change this to any number you want, or None for full dataset\n",
    "\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Read the dataset with sampling\n",
    "    if sample_size:\n",
    "        df = pd.read_csv(dataset_path, nrows=sample_size)\n",
    "        print(f\"Processing first {sample_size} rows of the dataset\")\n",
    "    else:\n",
    "        df = pd.read_csv(dataset_path)\n",
    "        print(\"Processing entire dataset\")\n",
    "    \n",
    "    # Save the sampled dataset temporarily\n",
    "    temp_dataset = \"temp_dataset.csv\"\n",
    "    df.to_csv(temp_dataset, index=False)\n",
    "    \n",
    "    # Process the sampled dataset\n",
    "    processed_df = process_dataset(\n",
    "        dataset_path=temp_dataset,\n",
    "        cso_path=cso_path,\n",
    "        save_path=save_path\n",
    "    )\n",
    "    \n",
    "    # Clean up temporary file\n",
    "    if os.path.exists(temp_dataset):\n",
    "        os.remove(temp_dataset)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nProcessing completed in {processing_time:.2f} seconds\")\n",
    "    \n",
    "    print(\"\\n=== Processed Text Results ===\")\n",
    "    print(\"\\nFirst 2 processed titles:\")\n",
    "    print(processed_df['title_processed'].head(2))\n",
    "    print(\"\\nFirst 2 processed abstracts:\")\n",
    "    print(processed_df['abstract_processed'].head(2))\n",
    "    \n",
    "    print(\"\\n=== N-gram Results ===\")\n",
    "    if 'tokens' in processed_df.columns:\n",
    "        print(\"\\nSample tokenized text:\")\n",
    "        print(processed_df['tokens'].head(1))\n",
    "    \n",
    "    if 'trigrams' in processed_df.columns:\n",
    "        print(\"\\nSample trigrams:\")\n",
    "        for idx, trigram in enumerate(processed_df['trigrams'].head(2)):\n",
    "            print(f\"\\nDocument {idx + 1} trigrams:\")\n",
    "            print(trigram)\n",
    "    \n",
    "    print(f\"\\nProcessed data saved to: {save_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'c:\\\\Users\\\\Faisal Ramzan\\\\Desktop\\\\kmi_project_cso\\\\paper_dataset\\\\processed_title_abstract_v2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPermissionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m model_output = \u001b[33m\"\u001b[39m\u001b[33mword2vec_cso.model\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Step 1: Process and get trigrams\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m df = \u001b[43mprocess_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcso_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprocessed_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Step 2: Train Word2Vec\u001b[39;00m\n\u001b[32m     24\u001b[39m model = train_and_save_word2vec(processed_output, model_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Faisal Ramzan\\Desktop\\kmi_project_cso\\concept_find_replace.py:65\u001b[39m, in \u001b[36mprocess_dataset\u001b[39m\u001b[34m(dataset_path, cso_path, save_path)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\generic.py:3967\u001b[39m, in \u001b[36mNDFrame.to_csv\u001b[39m\u001b[34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[39m\n\u001b[32m   3956\u001b[39m df = \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.to_frame()\n\u001b[32m   3958\u001b[39m formatter = DataFrameFormatter(\n\u001b[32m   3959\u001b[39m     frame=df,\n\u001b[32m   3960\u001b[39m     header=header,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3964\u001b[39m     decimal=decimal,\n\u001b[32m   3965\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3967\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3968\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3969\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3970\u001b[39m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3972\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3973\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3974\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3975\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3976\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3977\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3978\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3979\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3980\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3981\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3982\u001b[39m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3983\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3984\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[39m, in \u001b[36mDataFrameRenderer.to_csv\u001b[39m\u001b[34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[39m\n\u001b[32m    993\u001b[39m     created_buffer = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    995\u001b[39m csv_formatter = CSVFormatter(\n\u001b[32m    996\u001b[39m     path_or_buf=path_or_buf,\n\u001b[32m    997\u001b[39m     lineterminator=lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1012\u001b[39m     formatter=\u001b[38;5;28mself\u001b[39m.fmt,\n\u001b[32m   1013\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m \u001b[43mcsv_formatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[32m   1017\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[39m, in \u001b[36mCSVFormatter.save\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03mCreate the writer & save.\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[32m    259\u001b[39m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;28mself\u001b[39m.writer = csvlib.writer(\n\u001b[32m    261\u001b[39m         handles.handle,\n\u001b[32m    262\u001b[39m         lineterminator=\u001b[38;5;28mself\u001b[39m.lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m    267\u001b[39m         quotechar=\u001b[38;5;28mself\u001b[39m.quotechar,\n\u001b[32m    268\u001b[39m     )\n\u001b[32m    270\u001b[39m     \u001b[38;5;28mself\u001b[39m._save()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m    876\u001b[39m             encoding=ioargs.encoding,\n\u001b[32m    877\u001b[39m             errors=errors,\n\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mPermissionError\u001b[39m: [Errno 13] Permission denied: 'c:\\\\Users\\\\Faisal Ramzan\\\\Desktop\\\\kmi_project_cso\\\\paper_dataset\\\\processed_title_abstract_v2.csv'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(os.getcwd())\n",
    "from concept_find_replace import process_dataset\n",
    "from train_word2vec import train_and_save_word2vec\n",
    "\n",
    "# Set file paths\n",
    "dataset_path = os.path.join(os.getcwd(), \"paper_dataset\", \"paper_dataset.csv\")\n",
    "cso_path = os.path.join(os.getcwd(), \"cso_label\", \"cso_label_counts.csv\")\n",
    "\n",
    "# Set path for output file\n",
    "processed_output = os.path.join(os.getcwd(), \"paper_dataset\", \"processed_title_abstract_v2.csv\")\n",
    "\n",
    "\n",
    "model_output = \"word2vec_cso.model\"\n",
    "\n",
    "# Step 1: Process and get trigrams\n",
    "df = process_dataset(dataset_path, cso_path, save_path=processed_output)\n",
    "\n",
    "# Step 2: Train Word2Vec\n",
    "model = train_and_save_word2vec(processed_output, model_output)\n",
    "\n",
    "# Step 3: Preview (optional)\n",
    "print(\"\\n Sample Output:\")\n",
    "for i, row in df.head(5).iterrows():\n",
    "    print(f\"\\n Paper {i+1} Title:\\n{row['title_processed']}\")\n",
    "    print(f\"\\n Abstract:\\n{row['abstract_processed']}\")\n",
    "    print(f\"\\n Trigrams:\\n{row['trigrams']}\")\n",
    "    print('-'*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'train_word2vec'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m      2\u001b[39m sys.path.append(\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# Ensure current directory is in path\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtrain_word2vec\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_and_save_word2vec\n\u001b[32m      5\u001b[39m trigrams_csv_path = \u001b[33m'\u001b[39m\u001b[33mpaper_dataset/processed_title_abstract_v2.csv\u001b[39m\u001b[33m'\u001b[39m  \u001b[38;5;66;03m# Update if your path is different\u001b[39;00m\n\u001b[32m      6\u001b[39m model_save_path = \u001b[33m'\u001b[39m\u001b[33mword2vec_cso.model\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'train_word2vec'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('.')  # Ensure current directory is in path\n",
    "from train_word2vec import train_and_save_word2vec\n",
    "\n",
    "trigrams_csv_path = 'paper_dataset/processed_title_abstract_v2.csv'  # Update if your path is different\n",
    "model_save_path = 'word2vec_cso.model'\n",
    "\n",
    "# Train and save the Word2Vec model\n",
    "model = train_and_save_word2vec(trigrams_csv_path, model_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
