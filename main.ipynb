{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script to extract cso_concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 14612 labels to CSV: c:\\Users\\Faisal Ramzan\\Desktop\\kmi_project_cso\\cso_label\\cso_label_counts.csv\n",
      "Extracted 14612 unique CSO concepts.\n",
      "Total concepts extracted: 14612\n",
      "\n",
      "Sample Concepts (first 10):\n",
      "- computer science\n",
      "- automated pattern recognition\n",
      "- subtraction technique\n",
      "- nonrigid registration\n",
      "- non-rigid registration\n",
      "- manifold learning\n",
      "- nonlinear dimensionality reduction\n",
      "- locality preserving projections\n",
      "- locality preserving projection\n",
      "- gait recognition\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import required libraries\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the current directory to Python path\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "# Step 2: Import the cso_script function\n",
    "from cso_script import extract_cso_concepts\n",
    "\n",
    "# Step 3: Set the file path to your TTL file\n",
    "ttl_path = os.path.join(os.getcwd(), \"cso_label\", \"CSO\", \"CSO.3.4.1.ttl\")\n",
    "\n",
    "# Step 4: Set the output CSV path\n",
    "output_csv = os.path.join(os.getcwd(), \"cso_label\", \"cso_label_counts.csv\")\n",
    "\n",
    "# Step 5: Extract concepts\n",
    "try:\n",
    "    cso_concepts = extract_cso_concepts(ttl_path, save_csv=True, csv_path=output_csv)\n",
    "    \n",
    "    # Step 6: Preview results\n",
    "    print(f\"Total concepts extracted: {len(cso_concepts)}\")\n",
    "    print(\"\\nSample Concepts (first 10):\")\n",
    "    for concept in cso_concepts[:10]:\n",
    "        print(f\"- {concept}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running main script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Faisal\n",
      "[nltk_data]     Ramzan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing first 1000 rows of the dataset\n",
      "\n",
      "Processing completed in 1891.58 seconds\n",
      "\n",
      "Sample of processed titles and abstracts:\n",
      "\n",
      "First 2 processed titles:\n",
      "0    a new approach of 3d watermarking based on ima...\n",
      "1    attractor neural_networks with activitydepende...\n",
      "Name: title_processed, dtype: object\n",
      "\n",
      "First 2 processed abstracts:\n",
      "0    in this paper a robust 3d triangular mesh wate...\n",
      "1    we studied an autoassociative neural_network w...\n",
      "Name: abstract_processed, dtype: object\n",
      "\n",
      "Processed data saved to: c:\\Users\\Faisal Ramzan\\Desktop\\kmi_project_cso\\paper_dataset\\processed_title_abstract_v2.csv\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from concept_find_replace import process_dataset\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Set paths for input files\n",
    "dataset_path = os.path.join(os.getcwd(), \"paper_dataset\", \"paper_dataset.csv\")\n",
    "cso_path = os.path.join(os.getcwd(), \"cso_label\", \"cso_label_counts.csv\")\n",
    "\n",
    "# Set path for output file\n",
    "save_path = os.path.join(os.getcwd(), \"paper_dataset\", \"processed_title_abstract_v2.csv\")\n",
    "\n",
    "# Set sample size (use None for full dataset)\n",
    "sample_size = 1000  # Change this to any number you want, or None for full dataset\n",
    "\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Read the dataset with sampling\n",
    "    if sample_size:\n",
    "        df = pd.read_csv(dataset_path, nrows=sample_size)\n",
    "        print(f\"Processing first {sample_size} rows of the dataset\")\n",
    "    else:\n",
    "        df = pd.read_csv(dataset_path)\n",
    "        print(\"Processing entire dataset\")\n",
    "    \n",
    "    # Save the sampled dataset temporarily\n",
    "    temp_dataset = \"temp_dataset.csv\"\n",
    "    df.to_csv(temp_dataset, index=False)\n",
    "    \n",
    "    # Process the sampled dataset\n",
    "    processed_df = process_dataset(\n",
    "        dataset_path=temp_dataset,\n",
    "        cso_path=cso_path,\n",
    "        save_path=save_path\n",
    "    )\n",
    "    \n",
    "    # Clean up temporary file\n",
    "    if os.path.exists(temp_dataset):\n",
    "        os.remove(temp_dataset)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nProcessing completed in {processing_time:.2f} seconds\")\n",
    "    print(\"\\nSample of processed titles and abstracts:\")\n",
    "    print(\"\\nFirst 2 processed titles:\")\n",
    "    print(processed_df['title_processed'].head(2))\n",
    "    print(\"\\nFirst 2 processed abstracts:\")\n",
    "    print(processed_df['abstract_processed'].head(2))\n",
    "    \n",
    "    print(f\"\\nProcessed data saved to: {save_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing first 2 rows of the dataset\n",
      "\n",
      "Processing completed in 2.05 seconds\n",
      "\n",
      "=== Processed Text Results ===\n",
      "\n",
      "First 2 processed titles:\n",
      "0    a new approach of 3d watermarking based on ima...\n",
      "1    attractor neural_networks with activitydepende...\n",
      "Name: title_processed, dtype: object\n",
      "\n",
      "First 2 processed abstracts:\n",
      "0    in this paper a robust 3d triangular mesh wate...\n",
      "1    we studied an autoassociative neural_network w...\n",
      "Name: abstract_processed, dtype: object\n",
      "\n",
      "=== N-gram Results ===\n",
      "\n",
      "Sample tokenized text:\n",
      "0    [a, new, approach, of, 3d, watermarking, based...\n",
      "Name: tokens, dtype: object\n",
      "\n",
      "Sample trigrams:\n",
      "\n",
      "Document 1 trigrams:\n",
      "['a', 'new', 'approach', 'of', '3d', 'watermarking', 'based_on', 'image_segmentation', 'in', 'this', 'paper', 'a', 'robust', '3d', 'triangular', 'mesh', 'watermarking', 'algorithm', 'based_on', '3d', 'segmentation', 'is', 'proposed', 'in', 'this', 'algorithm', 'three', 'classes', 'of', 'watermarking', 'are', 'combined', 'first', 'we', 'segment', 'the', 'original', 'image', 'to', 'many', 'different', 'regions', 'then', 'we', 'mark', 'every', 'type', 'of', 'region', 'with', 'the', 'corresponding', 'algorithm', 'based_on', 'their', 'curvature', 'value', 'the', 'experiments', 'show', 'that', 'our', 'watermarking', 'is', 'robust', 'against', 'numerous', 'attacks', 'including', 'rst', 'transformations', 'smoothing', 'additive', 'random', 'noise', 'cropping', 'simplification', 'and', 'remeshing']\n",
      "\n",
      "Document 2 trigrams:\n",
      "['attractor', 'neural_networks', 'with', 'activitydependent', 'synapses', 'the', 'role', 'of', 'synaptic', 'facilitation', 'we', 'studied', 'an', 'autoassociative', 'neural_network', 'with', 'dynamic', 'synapses', 'which', 'include', 'a', 'facilitating', 'mechanism', 'we', 'have', 'developed', 'a', 'general', 'meanfield', 'framework', 'to', 'study', 'the', 'relevance', 'of', 'the', 'different', 'parameters', 'defining', 'the', 'dynamics', 'of', 'the', 'synapses', 'and', 'their', 'influence', 'on', 'the', 'collective', 'properties', 'of', 'the_network', 'depending', 'on', 'these', 'parameters', 'the_network', 'shows', 'different', 'types', 'of', 'behaviour', 'including', 'a', 'retrieval', 'phase', 'an', 'oscillatory', 'regime', 'and', 'a', 'nonretrieval', 'phase', 'in', 'the', 'oscillatory', 'phase', 'the_network', 'activity', 'continously', 'jumps', 'between', 'the_stored', 'patterns', 'compared', 'with', 'other', 'activitydependent', 'mechanisms', 'such', 'as', 'synaptic', 'depression', 'synaptic', 'facilitation', 'enhances', 'the_network', 'ability', 'to', 'switch', 'among', 'the_stored', 'patterns', 'and', 'therefore', 'its', 'adaptation', 'to', 'external', 'stimuli', 'a', 'detailed', 'analysis', 'of', 'our', 'system', 'reflects', 'an', 'efficientmore', 'rapid', 'and', 'with', 'lesser', 'errorsnetwork', 'access', 'to', 'the_stored', 'information', 'with', 'stronger', 'facilitation', 'we', 'also', 'present', 'a', 'set', 'of', 'monte', 'carlo', 'simulations', 'confirming', 'our', 'analytical_results']\n",
      "\n",
      "Processed data saved to: c:\\Users\\Faisal Ramzan\\Desktop\\kmi_project_cso\\paper_dataset\\processed_title_abstract_v2.csv\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from concept_find_replace import process_dataset\n",
    "\n",
    "\n",
    "# Set paths for input files\n",
    "dataset_path = os.path.join(os.getcwd(), \"paper_dataset\", \"paper_dataset.csv\")\n",
    "cso_path = os.path.join(os.getcwd(), \"cso_label\", \"cso_label_counts.csv\")\n",
    "\n",
    "# Set path for output file\n",
    "save_path = os.path.join(os.getcwd(), \"paper_dataset\", \"processed_title_abstract_v2.csv\")\n",
    "\n",
    "# Set sample size (use None for full dataset)\n",
    "sample_size = 2  # Change this to any number you want, or None for full dataset\n",
    "\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Read the dataset with sampling\n",
    "    if sample_size:\n",
    "        df = pd.read_csv(dataset_path, nrows=sample_size)\n",
    "        print(f\"Processing first {sample_size} rows of the dataset\")\n",
    "    else:\n",
    "        df = pd.read_csv(dataset_path)\n",
    "        print(\"Processing entire dataset\")\n",
    "    \n",
    "    # Save the sampled dataset temporarily\n",
    "    temp_dataset = \"temp_dataset.csv\"\n",
    "    df.to_csv(temp_dataset, index=False)\n",
    "    \n",
    "    # Process the sampled dataset\n",
    "    processed_df = process_dataset(\n",
    "        dataset_path=temp_dataset,\n",
    "        cso_path=cso_path,\n",
    "        save_path=save_path\n",
    "    )\n",
    "    \n",
    "    # Clean up temporary file\n",
    "    if os.path.exists(temp_dataset):\n",
    "        os.remove(temp_dataset)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nProcessing completed in {processing_time:.2f} seconds\")\n",
    "    \n",
    "    print(\"\\n=== Processed Text Results ===\")\n",
    "    print(\"\\nFirst 2 processed titles:\")\n",
    "    print(processed_df['title_processed'].head(2))\n",
    "    print(\"\\nFirst 2 processed abstracts:\")\n",
    "    print(processed_df['abstract_processed'].head(2))\n",
    "    \n",
    "    print(\"\\n=== N-gram Results ===\")\n",
    "    if 'tokens' in processed_df.columns:\n",
    "        print(\"\\nSample tokenized text:\")\n",
    "        print(processed_df['tokens'].head(1))\n",
    "    \n",
    "    if 'trigrams' in processed_df.columns:\n",
    "        print(\"\\nSample trigrams:\")\n",
    "        for idx, trigram in enumerate(processed_df['trigrams'].head(2)):\n",
    "            print(f\"\\nDocument {idx + 1} trigrams:\")\n",
    "            print(trigram)\n",
    "    \n",
    "    print(f\"\\nProcessed data saved to: {save_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.getcwd())\n",
    "from concept_find_replace import process_dataset\n",
    "from train_word2vec import train_and_save_word2vec\n",
    "\n",
    "# Set file paths\n",
    "dataset_path = os.path.join(os.getcwd(), \"paper_dataset\", \"paper_dataset.csv\")\n",
    "cso_path = os.path.join(os.getcwd(), \"cso_label\", \"cso_label_counts.csv\")\n",
    "\n",
    "# Set path for output file\n",
    "processed_output = os.path.join(os.getcwd(), \"paper_dataset\", \"processed_title_abstract_v2.csv\")\n",
    "\n",
    "\n",
    "model_output = \"word2vec_cso.model\"\n",
    "\n",
    "# Step 1: Process and get trigrams\n",
    "df = process_dataset(dataset_path, cso_path, save_path=processed_output)\n",
    "\n",
    "# Step 2: Train Word2Vec\n",
    "model = train_and_save_word2vec(processed_output, model_output)\n",
    "\n",
    "# Step 3: Preview (optional)\n",
    "print(\"\\n Sample Output:\")\n",
    "for i, row in df.head(5).iterrows():\n",
    "    print(f\"\\n Paper {i+1} Title:\\n{row['title_processed']}\")\n",
    "    print(f\"\\n Abstract:\\n{row['abstract_processed']}\")\n",
    "    print(f\"\\n Trigrams:\\n{row['trigrams']}\")\n",
    "    print('-'*100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
