title_processed,abstract_processed
a new approach of 3d watermarking based on image_segmentation,in this paper a robust 3d triangular mesh watermarking algorithm based on 3d segmentation is proposed in this algorithm three classes of watermarking are combined first we segment the original image to many different regions then we mark every type of region with the corresponding algorithm based on their curvature value the experiments show that our watermarking is robust against numerous attacks including rst transformations smoothing additive random noise cropping simplification and remeshing
attractor neural_networks with activitydependent synapses the role of synaptic facilitation,we studied an autoassociative neural_network with dynamic synapses which include a facilitating mechanism we have developed a general meanfield framework to study the relevance of the different parameters defining the dynamics of the synapses and their influence on the collective properties of the network depending on these parameters the network shows different types of behaviour including a retrieval phase an oscillatory regime and a nonretrieval phase in the oscillatory phase the network activity continously jumps between the stored patterns compared with other activitydependent mechanisms such as synaptic depression synaptic facilitation enhances the network ability to switch among the stored patterns and therefore its adaptation to external stimuli a detailed analysis of our system reflects an efficientmore rapid and with lesser errorsnetwork access to the stored information with stronger facilitation we also present a set of monte carlo simulations confirming our analytical_results
a characterization of balanced episturmian sequences,it is wellknown that sturmian sequences are the non ultimately periodic sequences that are balanced over a 2letter alphabet they are also characterized by their complexity they have exactly n1 distinct factors of length n a natural generalization of sturmian sequences is the set of infinite episturmian sequences these sequences are not necessarily balanced over a kletter alphabet nor are they necessarily aperiodic in this paper we characterize balanced episturmian sequences periodic or not and prove fraenkels conjecture for the special case of episturmian sequences it appears that balanced episturmian sequences are all ultimately periodic and they can be classified in 3 families
exploring the space of a human action,one of the fundamental challenges of recognizing actions is accounting for the variability that arises when arbitrary cameras capture humans performing actions in this paper we explicitly identify three important sources of variability 1 viewpoint 2 execution rate and 3 anthropometry of actors and propose a model of human_actions that allows us to investigate all three our hypothesis is that the variability associated with the execution of an action can be closely approximated by a linear combination of action bases in joint spatiotemporal space we demonstrate that such a model bounds the rank of a matrix of image measurements and that this bound can be used to achieve recognition of actions based only on imaged data a test employing principal angles between subspaces that is robust to statistical fluctuations in measurement data is presented to find the membership of an instance of an action the algorithm is applied to recognize several actions and promising results have been obtained
generalized upper bounds on the minimum distance of psk block_codes,this paper generalizes previous optimal upper bounds on the minimum euclidean distance for phase_shift_keying psk block_codes that are explicit in three parameters alphabet size block length a 
applying bcmp multiclass queueing_networks for the performance evaluation of hierarchical and modular software_systems,queueing_networks with multiple classes of customers play a fundamental role for evaluating the performance of both software and hardware architectures the main strength of productform models in particular of bcmp queueing_networks is that they combine a flexible formalism with efficient analysis techniques and solution algorithms in this paper we provide an algorithm that starting from a highlevel description of a system and from the definition of its components in terms of interacting subsystems computes a multipleclass and multiplechain bcmp queueing_network we believe that the strength of this approach is twofold first the modeller deals with simplified models which are defined in a modular and hierarchical way hence we can carry on sensitivity analysis that may easily include structural changes and not only on the time parameters second maintaining the productform property allows one to derive the average system performance indices very efficiently the paper also discusses the 
a pushpull classc cmos vco,a cmos oscillator employing differential transistor pairs working in classc in pushpull configuration is presented the oscillator exhibits the same advantages enjoyed by complementary topologies on oscillators based on a single differential pair while yielding a substantial power consumption reduction thanks to the classc operation the phasenoise performance and the fundamental conditions required to keep the transistors working in classc are analyzed in detail it is shown that for an optimal performance both nmos and pmos transistors should not be pushed into the deep triode region by the instantaneous resonator voltage and a simple circuit solution is proposed to accommodate a large oscillation swing a 018 m cmos prototype of the voltagecontrolled oscillator displays an oscillation_frequency from 609 to 750 ghz the phase_noise at 2mhz offset is below 120 dbchz with a power dissipation of 22 mw for a stateoftheart figureofmerit ranging from 189 to 191 dbchz
on computability of pattern_recognition problems,in statistical setting of the pattern_recognition problem the number of examples required to approximate an unknown labelling function is linear in the vc dimension of the target learning class in this work we consider the question whether such bounds exist if consider only computable pattern_recognition methods assuming that the unknown labelling function is also computable we find that in this case the number of examples required for a computable method to approximate the labelling function not only is not linear but grows faster in the vc dimension of the class than any computable function no time or space constraints are put on the predictors or target functions the only resource we consider is the training examplesrnrnthe task of pattern_recognition is considered in conjunction with another learning problem  data_compression an impossibility result for the task of data_compression allows us to estimate the sample complexity for pattern_recognition
manipulating biological and mechanical microobjects using ligamicrofabricated endeffectors,we first discuss some general aspects of micromanipulation and possible different approaches then we present new results in the micromanipulation of mechanical and biological objects the apparatus we use is a purposely developed workstation comprising macro and micromanipulators the most innovative component of the workstation is a microgripper fabricated using liga technology and actuated by piezoelectric actuators we describe the design fabrication and performance of a few prototypes of liga microgrippers results are presented which demonstrate the ability of the system to manipulate effectively both micromechanical and biological microobjects
a novel injection_locked rotary traveling wave oscillator,
an abundance of invariant polynomials satisfying the riemann hypothesis,in 1999 iwan duursma defined the zeta function for a linear_code as a generating function of its hamming_weight enumerator it can also be defined for other homogeneous polynomials not corresponding to existing codes if the homogeneous polynomial is invariant under the macwilliams transform then its zeta function satisfies a functional equation and we can formulate an analogue of the riemann hypothesis as far as existing codes are concerned the riemann hypothesis is believed to be closely related to the extremal property in this article we show there are abundant polynomials invariant by the macwilliams transform which satisfy the riemann hypothesis the proof is carried out by explicit construction of such polynomials to prove the riemann hypothesis for a certain class of invariant polynomials we establish an analogue of the enestromkakeya theorem
entity resolution with iterative blocking,entity resolution er is the problem of identifying which records in a database refer to the same realworld entity an exhaustive er process involves computing the similarities between pairs of records which can be very expensive for large datasets various blocking techniques can be used to enhance the performance of er by dividing the records into blocks in multiple ways and only comparing records within the same block however most blocking techniques process blocks separately and do not exploit the results of other blocks in this paper we propose an  iterative blocking framework  where the er results of blocks are reflected to subsequently processed blocks blocks are now iteratively processed until no block contains any more matching records compared to simple blocking iterative blocking may achieve higher accuracy because reflecting the er results of blocks to other blocks may generate additional record matches iterative blocking may also be more efficient because processing a block now saves the processing time for other blocks we implement a scalable iterative blocking system and demonstrate that iterative blocking can be more accurate and efficient than blocking for large datasets
evaluating the accuracy of java profilers,performance analysts profile their programs to find methods that are worth optimizing the hot methods this paper shows that four commonlyused java profilers  xprof  hprof  jprofile and yourkit  often disagree on the identity of the hot methods if two profilers disagree at least one must be incorrect thus there is a good chance that a profiler will mislead a performance analyst into wasting time optimizing a cold method with little or no performance improvement   this paper uses causality analysis to evaluate profilers and to gain insight into the source of their incorrectness it shows that these profilers all violate a fundamental requirement for sampling based profilers to be correct a samplingbased profilermust collect samples randomly   we show that a proofofconcept profiler which collects samples randomly does not suffer from the above problems specifically we show using a number of case studies that our profiler correctly identifies methods that are important to optimize in some cases other profilers report that these methods are cold and thus not worth optimizing
robust groupofpicture architecture for video_transmission over errorprone channels,in motioncompensated videocoding schemes such as mpeg an i frame is normally followed by several p frames and possibly b frames in a groupofpicture gop in errorprone environments errors happening in the previous frames in a gop may propagate to all the following frames until the next i frame which is the beginning of the next gop in this paper we propose a novel gop structure for robust transmission of mpeg video bitstream by selecting the optimal position of the i frame in a gop robustness can be achieved without reducing any coding_efficiency experimental results demonstrate the robustness of the proposed gop structure
useful computations need useful numbers,most of us have taken the exact rational and approximate numbers in our computer algebra systems for granted for a long time not thinking to ask if they could be significantly better with exact rational arithmetic and adjustableprecision floatingpoint arithmetic to precision limited only by the total computer memory or our patience what more could we want for such numbers it turns out that there is much more that can be done that permits us to obtain exact results more often more intelligible results approximate results guaranteed to have requested error bounds and recovery of exact results from approximate ones
fast content aware image retargeting,this paper addresses the problem of retargeting namely adapting large source_images for effective viewing at a smaller size with possible applications to pdas or dynamic page layouts instead of extracting regions of interest for retargeting the uninteresting parts are removed from the scene in shai avidan and shamir a 2007 this is done by computing the rgb variance within nonoverlapping 3times3 blocks and removing the block path with minimal variance cost using dynamic programming it is shown that transformation to cielab space is more effective for visual interpretation of image content the implementations are shown to be much faster than the seam carving approach of shai avidan and shamir a 2007 schemes are also presented for speeding up the seam carving scheme itself
scaddar an efficient randomized technique to reorganize continuous media blocks,scalable storage architectures allow for the addition of disks to increase storage capacity andor bandwidth in its general form disk scaling also refers to disk removals when either capacity needs to be conserved or old disk drives are retired assuming random placement of blocks on multiple nodes of a continuous media server our optimization objective is to redistribute a minimum number of media blocks after disk scaling this objective should be met under two restrictions first uniform distribution and hence a balanced load should be ensured after redistribution second the redistributed blocks should be retrieved at the normal mode of operation in one disk access and through low complexity computation we propose a technique that meets the objective while we prove that it also satisfies both restrictions the scaddar approach is based on using a series of remap functions which can derive the location of a new block using only its original location as a basis
importance sampling in markovian settings,rare event simulation for stochastic_models of complex systems is still a great challenge even for markovian models we review results in importance sampling for markov_chains provide new viewpoints and insights and we pose some future research directions
identification of coldinduced genes in cereal crops and arabidopsis through comparative analysis of multiple est sets,freezing tolerance in plants is obtained during a period of low nonfreezing temperatures before the winter sets on through a biological process known as cold acclimation cold is one of the major stress factors that limits the growth productivity and distribution of plants and understanding the mechanism of cold tolerance is therefore important for crop improvement expressed sequence tags est analysis is a powerful economical and timeefficient way of assembling information on the transcriptome to date several est sets have been generated from coldinduced cdna libraries from several different plant species in this study we utilize the variation in the frequency of ests sampled from different coldstressed plant libraries in order to identify genes preferentially expressed in cold in comparison to a number of control sets the species included in the comparative study are oat avena sativa barley hordeum vulgare wheat triticum aestivum rice oryza sativa and arabidopsis thaliana however in order to get comparable gene expression estimates across multiple species and data sets we choose to compare the expression of tentative ortholog groups togs instead of single genes as in the normal procedure we consider togs as preferentially expressed if they are detected as differentially expressed by a test statistic and upregulated in comparison to all control sets andor uniquely expressed during cold stress ie not present in any of the control sets the result of this analysis revealed a diverse representation of genes in the different species in addition the derived togs mainly represent genes that are longterm highly or moderately expressed in response to cold andor other stresses
realtime distributed_computing,this position paper concerns itself with realtime safety critical distributed_systems it presents a computational model that is appropriate for this type of application and architecture it then defines a resource allocations scheme based upon fixed_priority_scheduling such a scheme has the advantage over purely static schedules of supporting greater levels of flexibility and nondeterminism whilst still providing static guarantees of necessary timing behaviour ie endtoend deadlines through the systems priority based communication_protocols are investigated with possible future techniques reviewed
on the impact of using volume as an independent variable for the solution of pt fluidphase equilibrium with equations of state,a b s t r a c t the constant pressuretemperature pt flash plays an important role in the modelling of fluidphase behaviour and its solution is especially challenging for equations of state in which the volume is expressed as an implicit function of the pressure we explore the relative merits of solving the pt flash in two ensembles mole numbers pressure and temperature in which each freeenergy evaluation requires the use of a numerical solver and mole numbers volume and temperature in which a direct evaluation of the freeenergy is possible we examine the performance of two algorithms held helmholtz free energy lagrangian dual introduced in pereira et al 2012 and gild gibbs free energy lagrangian dual introduced here for the fluidphase equilibria of 8 mixtures comprising up to 10 components using two equations of state while the reliability of both algorithms is comparable the computational cost of held is consistently lower this difference becomes increasingly pronounced as the number of components is increased  2014 the authors published by elsevier ltd this is an open_access article under the cc by license
semantic processing of natural_language_queries in the ontonl framework,the ontonl framework provides an architecture and reusable_components for automating as much as possible the building of natural_language_interfaces to information_systems in addition to the syntactic_analysis components ontonl has semantic_analysis components which exploit domain_ontologies to provide better disambiguation of the user input we present in this paper the algorithms used for semantic processing of the natural_language_queries as well as an ontologydriven semantic_relatedness measure developed for this purpose we also present extensive evaluation results with different ontologies using human subjects
a comparison of linear keyword and restricted natural_language data base interfaces for novice users,this study compares a linear keyword language interface and a restricted natural_language_interface for data retrieval by a novice user the comparison focuses on the effect of different data base interfaces on user performance as measured by query correctness and query writing time in a query writing task across varying query types and training levels to accomplish this objective a laboratory experiment was conducted using a splitplot factorial design using two betweensubjects factors and one withinsubjects factor the results indicate that the restricted natural_language subjects performed significantly better than the linear keyword language subjects in terms of both query correctness and query writing time
reactive virtual positionbased routing in wireless_sensor_networks,virtual positionbased routing_protocols have many attractive characteristics for wireless_sensor_networks typically such protocols use a proactive scheme for updating routing tables because sensor_networks can have very low data rate sending periodic beacons to update routing tables can be very expensive instead reactive approaches might be more appropriate in such scenarios manetinspired reactive routing_protocols do not scale well because of the effort in the order of on for each routing_information update in this paper we present reactive virtual cord protocol rvcp a datacentric reactive virtual position based routing_protocol for use in sensor_networks route_discovery is directed towards the destination and hence there is no need to flood the entire network to discover a route our approach is based on virtual cord protocol vcp an efficient virtual relative position based routing_protocol that also provides support for data_management as known from typical distributed_hash_table dht services to minimize the endtoend delay and energy consumption we used adaptive techniques for the development of rvcp
a note on the complexity of scheduling coupled tasks on a single processor,this paper considers a problem of coupled task_scheduling on one processor where all processing times are equal to 1 the gap has exact length h precedence constraints are strict and the criterion is to minimise the schedule length this problem is introduced eg in systems controlling radar operations we show that the general problem is nphard
novel weightingdelaybased stability_criteria for recurrent_neural_networks with timevarying delay,in this paper a weightingdelaybased method is developed for the study of the stability_problem of a class of recurrent_neural_networks rnns with timevarying delay different from previous results the delay interval 0 dt is divided into some variable subintervals by employing weighting delays thus new delaydependent stability_criteria for rnns with timevarying delay are derived by applying this weightingdelay method which are less conservative than previous results the proposed stability_criteria depend on the positions of weighting delays in the interval 0 dt which can be denoted by the weightingdelay parameters different weightingdelay parameters lead to different stability margins for a given system thus a solution based on optimization_methods is further given to calculate the optimal weightingdelay parameters several examples are provided to verify the effectiveness of the proposed criteria
crosslanguage information_retrieval,search for information is no longer exclusively limited within the native language of the user but is more and more extended to other languages this gives rise to the problem of crosslanguage information_retrieval clir whose goal is to find relevant information written in a different language to a query in addition to the problems of monolingual information_retrieval ir translation is the key problem in clir one should translate either the query or the documents from a language to another however this translation problem is not identical to fulltext machine_translation mt the goal is not to produce a humanreadable translation but a translation suitable for finding relevant documents specific translation methods are thus required the goal of this book is to provide a comprehensive description of the specifi c problems arising in clir the solutions proposed in this area as well as the remaining problems the book starts with a general description of the monolingual ir and clir problems different classes of approaches to translation are then presented approaches using an mt system dictionarybased translation and approaches based on parallel and comparable corpora in addition the typical retrieval_effectiveness using different approaches is compared it will be shown that translation approaches specifically designed for clir can rival and outperform highquality mt systems finally the book offers a look into the future that draws a strong parallel between query_expansion in monolingual ir and query_translation in clir suggesting that many approaches developed in monolingual ir can be adapted to clir the book can be used as an introduction to clir advanced readers can also find more technical details and discussions about the remaining research challenges in the future it is suitable to new researchers who intend to carry out research on clir
best case energy analysis of localized euclidean minimum_spanning_tree based multicasting in ad hoc and sensor_networks,i consider the known localized multicast_protocol msteam and derive the energy consumed by the multicast_tree constructed by this protocol in the best case moreover i show that the length of multicast links connecting into a multicast branch can not be bounded from above for typical wireless_networks where links have a limited communication range however i can show that asymptotically the relation between the derived best case energy consumption of msteam and a known lower bound on multicast energy consumption is limited by a factor of 2
dqainf an algorithm for automatic integration of infinite oscillating tails,we describe an automatic quadrature routine which is specifically designed for real functions having a certain type of infinite oscillating tails the algorithm is designed to integrate a vector function over an infinite interval a fortran implementation of the algorithm is included
an interoperability framework for paneuropean egovernment services pegs,interoperability between public administrations receives nowadays a lot of attention also in the european union interworking is high on the priority list but the challenges to achieve the european administrative space is enormous many research projects are undertaken especially in the domain of semantic_interoperability many of these efforts seem to start from a technical solution rather than from an actual business problem by taking a narrow view on the problem space they only promise limited support for the many challenges in the domain of interoperability and innovation of egovernment services in this paper we present a business driven approach that looks promising in enabling entire classes of interoperability solutions
performance optimization of interferencelimited multihop_networks,the performance of a multihop_wireless_network is typically affected by the interference caused by transmissions in the same network in a statistical fading environment the interference effects become harder to predict information sources in a multihop_wireless_network can improve throughput and delay performance of data_streams by implementing interferenceaware packet injection mechanisms forcing packets to wait at the head of queues and coordinating packet injections among different sources enable effective control of copacket interference in this paper throughput and delay performance in interferencelimited multihop_networks is analyzed using nonlinear probabilistic hopping models waiting times which jointly optimize throughput and delay performances are derived optimal coordinated injection strategies are also investigated as functions of the number of information sources and their separations the resulting analysis demonstrates the interaction of performance constraints and achievable capacity in a wireless multihop network
action reaction learning automatic visual_analysis and synthesis of interactive behaviour,we propose actionreaction learning as an approach for analyzing and synthesizing human_behaviour this paradigm uncovers causal mappings between past and future events or between an action and its reaction by observing time sequences we apply this method to analyze human interaction and to subsequently synthesize human_behaviour using a time series of perceptual measurements a system automatically discovers correlations between past gestures from one human participant action and a subsequent gesture reaction from another participant a probabilistic_model is trained from data of the human interaction using a novel estimation technique conditional expectation maximization cem the estimation uses general bounding and maximization to monotonically find the maximum conditional likelihood solution the learning system drives a graphical interactive character which probabilistically predicts a likely response to a users behaviour and performs it interactively thus after analyzing human interaction in a pair of participants the system is able to replace one of them and interact with a single remaining user
circuit delay models and their exact computation using timed boolean_functions,we propose a general circuit delay model that unifies all previous delay models eg floating viability and transition delays and models introduced in this paper eg delays by sequences of vectors and minimum delays then we formulate the computation of the exact circuit delays under both bounded and unbounded gate delay models as a mixed boolean linear programming using a new formulation technique called timed boolean_function next we compute the exact delays of combinational_circuits for transition delay and delay by sequences of vectors we show that delays by sequences of vectors and floating or viability delays are invariant under both bounded and unbounded gate delay models finally we address the effect of gate delay lower bounds on delays of circuits we demonstrate the effectiveness of the method by giving exact delay results for all iscas benchmark_circuits except c6188
an anisotropic evolution formulation applied in 2d unwrapping of discontinuous phase surfaces,in this paper a new method to reconstruct piecewise continuous phase estimates using inphase and quadrature components acquired from interferometry measurements is derived and discussed the method based on the concept of anisotropic evolution formulations is shown to be far less noise sensitive than similar methods operating on modulomapped data ie traditional phase_unwrapping methods the method is able to produce reliable phase estimates from data containing complex sheared structures in combination with high noise content without relying on userdefined weights
a score function of splitting band for twoband speech model,twoband speech model which assumes lower band is a quasiperiodic component and upper band is a nonperiodic component is widely used due to its natural and simple framework in this paper a score function is defined for splitting lower and upper band of twoband speech model and estimation_method of bandsplitting frequency which is the boundary of the two bands is proposed the score function is calculated for each harmonic frequency using the normalized autocorrelation function of the time signal corresponding to the each subband divided by the given frequency by using the score function tracking technique is applied to the bandsplitting frequency_estimation procedure to reflect the continuity between neighboring frames experimental tests confirm that the proposed score function is effective for estimation of the bandsplitting frequency and produces better results compared with the previous other methods
the reliability study of the single hydraulic prop based on finite_element_analysis,in allusion to the reliability which exists in the parametric design and optimizing process of the single hydraulic prop this paper presents the new method in comparison with the traditional calculating and checking method for the reliability the geometry model of the hydraulic prop is built firstly based on the 3d software then analyzed and optimized by the finite element softwareansys results show that the method presented for the reliability is efficient and accurate
the programmable compiler,
fuzzy capacitated locationallocation problem with minimum risk criteria,based on credibility theory a new class of twostage minimum risk locationallocation model is first proposed then we deal with the approximation of the location and allocation problem after that a hybrid_algorithm which integrates the approximation approach neural_network and simulated_annealing is designed to solve the proposed locationallocation problem and a numerical_example is provided to test the effectiveness of the hybrid_algorithm
towards end user service_composition,the popularity of service_oriented_computing soc brings a large number of distributed wellencapsulated and reusable services all over internet and makes it possible to create valueadded services by means of service_composition current composition styles are too professional to those end users when building their own applications actually the end user would prefer rapidly discovering the bestofbreed services to assemble as well as visually personalizing the presentation to enjoy rich experiences we propose an end user service_composition approach for reducing the composition complexity and difficulty from the end user perspective in our approach similar candidate services are aggregated together as a unified resource whose wide qos spectrum can be easily manipulated by the end users to satisfy their requirements then they can personalize the services and the composition occurs only at the presentation layer the main contributions of the approach are i enabling the end users to personalize the composite application with more powerful presentation ii supporting the end users to dynamically customize the service_composition in terms of qos iii alleviating the end users from the timeconsuming task of selecting service to compose
optimal detection of functional connectivity from highdimensional eeg synchrony data,article i nfo computing phaselocking values between eeg_signals is a popular method for quantifying functional connectivity however this method involves largescale highresolution datasets which impose a serious multiple testing problem standard multiple testing methods fail to exploit the information from the complex dependence structure that varies across hypotheses in spectral temporal and spatial dimensions and result in a severe loss of power they tend to control the false positives at the cost of hiding true positives we introduce a new approach called optimal discovery procedure odp for identifying synchrony that is statistically significant odp maximizes the number of true positives for a given number of false positives and thus offers a theoretical optimum for detecting significant synchrony in a multiple testing situation we demonstrate the utility of this method with plv data obtained from a visual search study we also present simulation analysis to confirm the validity and relevance of using odp in comparison with the standard fdr method for given configurations of true synchrony we also compare the effectiveness of odp with our previously published investigation of hierarchical fdr method singh and phillips 2010
starbase v20 decoding mirnacerna mirnancrna and proteinrna interaction networks from largescale clipseq data,although micrornas mirnas other noncoding rnas ncrnas eg lncrnas pseudogenes and circrnas and competing endogenous rnas cernas have been implicated in cellfate determination and in various human diseases surprisingly little is known about the regulatory interaction networks among the multiple classes of rnas in this study we developed starbase v20 httpstarbasesysu educn to systematically identify the rnarna and proteinrna interaction networks from 108 clipseq parclip hitsclip iclip clash data sets generated by 37 independent studies by analyzing millions of rnabinding protein binding sites we identified  9000 mirnacircrna 16 000 mirnapseudogene and 285 000 proteinrna regulatory relationships moreover starbase v20 has been updated to provide the most comprehensive clipseq experimentally supported mirnamrna and mirnalncrna interaction networks to date we identified  10 000 cerna pairs from clipsupported mirna target sites by combining 13 functional genomic annotations we developed mirfunction and cernafunction web_servers to predict the function of mirnas and other ncrnas from the mirnamediated regulatory networks finally we developed interactive web implementations to provide visualization analysis and downloading of the aforementioned largescale data sets this study will greatly expand our understanding of ncrna functions and their coordinated regulatory networks
a low_phase_noise 100mhz silicon baw reference oscillator,the paper presents a temperature compensated 100mhz reference oscillator based on a capacitive silicon bulk acoustic wave baw resonator interfaced with a cmos amplifier the resonator is optimized for high quality factor 92000 and low impedance the cmos ic comprises of a transimpedance amplifier to sustain oscillations and an oven control mechanism for temperature control a phase_noise floor of 136dbchz was measured for the oscillator and the temperature drift of frequency was measured to be 56ppm over 100c
performance bounds for mlsd reception of ofdm_signals in fast fading,for ofdm_systems in fast fading it is difficult to obtain a closed_form expression for the ofdm symbol error probability thus tight analytical bounds on actual performance are extremely useful for performance prediction and verification additionally the structure of the bound provides insight into system behavior a new expurgated 2dimensional union bound is proposed and applied to an ofdm system in fast fading this bound becomes extremely tight as the rate of fading l increases performance comparison of the new bound to a lower bound on a known expurgated union bound demonstrates a gain of up to 15 db at pe  10 2 for channel implicit diversity_order of l  4 a new simulated upper bound that uses a reduced state vector in a modified trellis search algorithm and a simulated lower bound that assumes limited knowledge of the ici are also presented both bounds are derived from a timevarying finite_state_machine model of the received signal performance results for these bounds are extremely tight for small to large values of n where n is the number of ofdm_signal tones also the reduction in computational complexity achieved for n  512 is from 2 512  to 2 6  for both bounds
creating gisbased spatial interaction models for retail centres in jeddah city,spatial interaction models are used today in facilities planning research for predicting and for allocating flows of demand between origin and destination areas based on the attractiveness of each facility and based on the distance between facilities and demand areas these models have been adapted to a wide range of application areas including predicting flows of people to shops offices schools and hospitals the aim of this paper is to use gis for producing spatial interaction models for two retail centres jeddah city saudi arabia these models are created using arcgis software and using the interaction function which is available within the network_analysis module to produce these models detailed geodatabase was created that covers location of retail centres the capacity of each centre the size of centres demand at the study area and road network coverage for jeddah city the created models can be used by city planners for identifying areas of the city that are poorly served by existing retail centres in addition these models can be used to define the impacts of expanding retail supply and or retail demand at the study area
surface models of tube trees,this paper describes a new method for generating surfaces of branching tubular structures with given centerlines and radii as the centerlines are not straight lines the crosssections are not parallel and wellknown algorithms for surface tiling from parallel crosssections cannot be used nonparallel crosssections can be tiled by means of the maximaldisc interpolation method special methods for branchingstructures modeling by means of convolution surfaces produce excellent results but these methods are more complex than our approach the proposed method tiles nonparallel circular crosssections and constructs a topologicallycorrect surface mesh the method is not artifactfree but it is fast and simple the surface mesh serves as a data representation of a vessel tree suitable for realtime virtual_reality operation planning and operation support within a medical application proposed method extracts a classical polygonal representation which can be used in common surfaceoriented graphic accelerators
nodetoset disjointpath routing in metacube,the metacube interconnection_network introduced a few years ago has some very interesting properties it has a short diameter similar to the hypercube and its degree is much lower than that of a hypercube of the same size in this paper we describe an efficient_algorithm for finding disjoint_paths between one source node and at most mk target nodes in a metacube mck m excluding mc1 mc22 mc32 and mc33 we show that we can find mk disjoint_paths between the source node and the mk targets of length at most metacube diameter plus k4 with time complexity of order of metacube degree times its diameter
systematic error in the organization of physical action,current views of the control of complex purposeful movements acknowledge that organizational processes must reconcile multiple concerns the central priority is of course accomplishing the actors goal but in specifying the manner in which this occurs the action plan must accommodate such factors as the interaction of mechanical forces associated with the motion of a multilinked system classical mechanics and in many cases intrinsic bias toward preferred movement patterns characterized by socalled coordination dynamics the most familiar example of the latter is the symmetry constraint where spatial trajectories andor temporal landmarks eg reversal points of concurrentlymoving body segments limbs digits etc exhibit mutual attraction the natural coordination tendencies that emerge through these constraints can facilitate or hinder motor control depending on the degree of congruency with the desired movement pattern motor control theorists have long recognized the role of classical mechanics in theories of movement organization but an appreciation of the importance of intrinsic interlimb bias has been gained only recently although detailed descriptions of temporal coordination dynamics have been provided systematic attempts to identify additional salient dimensions of interlimb constraint have been lacking we develop and implement here a novel method for examining this problem by exploiting two robust principles of psychomotor behavior the symmetry constraint and the twothirds power law empirical evidence is provided that the relative spatial patterns of concurrently moving limbs are naturally constrained in much the same manner as previously identified temporal constraints and further that apparent velocity interference is an indirect secondary consequence of primary spatial assimilation the theoretical implications of spatial interference are elaborated with respect to movement organization and motor learning the need to carefully consider the appropriate dimensions with which to characterize coordination dynamics is also discussed  2001 cognitive_science
modeling the performance of evolutionary_algorithms on the root identification problem a case study with pbil and chc algorithms,the availability of a model to measure the performance of evolutionary_algorithms is very important especially when these algorithms are applied to solve problems with high computational requirements that model would compute an index of the quality of the solution reached by the algorithm as a function of runtime conversely if we fix an index of quality for the solution the model would give the number of iterations to be expected in this work we develop a statistical_model to describe the performance of pbil and chc evolutionary_algorithms applied to solve the root identification problem this problem is basic in constraintbased geometric parametric modeling as an instance of general constraintsatisfaction problems the performance model is empirically validated over a benchmark with very large search spaces
towards a probabilistic calculus for mobile_ad_hoc_networks,in this paper we present a probabilistic calculus for formally modeling and reasoning about mobile_ad_hoc_networks manets with unreliable connections and mobility of nodes in our calculus a manet node can locally broadcast messages to a group of nodes within its physical transmission_range the group probability is also introduced since two distinct nodes within different groups should receive messages from the same sender with different possibilities our calculus naturally captures essential features of manets ie local broadcast mobility and probability moreover we give a formal operational_semantics of the calculus in terms of the labeled transition_system and define the notion of open bisimulation finally we illustrate our calculus with a toy example
slow motion replay of video sequences using fractal zooming,slow motion replay is a special effect used in the video entertainment field it consists in a presentation of a video scene at a rate display lower than the original already consolidated as a commercial feature of analog video players today slow motion is likely to be extended to the digital environment purpose of this paper is to present a technique combining fractals i f s and wavelets to obtain a subjectively pleasant zoom and slow motion of digital_video sequences active scene detection and post processing techniques are used to reduce computational cost and improve visual_quality respectively this study shows that the proposed technique produces better results than the state of the art techniques based either on data replication or classical interpolation
joint semiblind frequency_offset and channel_estimation for multiuser mimoofdm uplink,a semiblind method is proposed for simultaneously estimating the carrier_frequency_offsets cfos and channels of an uplink multiuser multipleinput multipleoutput orthogonal frequencydivision multiplexing mimoofdm system by incorporating the cfos into the transmitted symbols and channels the mimoofdm with cfo is remodeled into an mimoofdm without cfo the known blind method for channel_estimation zeng and ng in 2004 y h zeng and t s ng ldquoa semiblind channel_estimation_method for multiuser multiantenna ofdm systemsrdquo ieee trans signal process vol 52 no 5 pp 14191429 may 2004 is then directly used for the remodeled system to obtain the shaped channels with an ambiguity matrix a pilot ofdm block for each user is then exploited to resolve the cfos and the ambiguity matrix two dedicated pilot designs periodical and consecutive pilots are discussed based on each pilot design and the estimated shaped channels two methods are proposed to estimate the cfos as a result based on the secondorder statistics sos of the received signal and one pilot ofdm block the cfos and channels are found simultaneously finally a fast equalization method is given to recover the signals corrupted by the cfos
the cluster density of a distributed clustering_algorithm in ad_hoc_networks,given is a wireless multihop network whose nodes are randomly distributed according to a homogeneous poisson_point_process of density spl rho in nodes per unit area the network employs basagnis distributed mobilityadaptive clustering dmac algorithm to achieve a selforganizing network structure we show that the cluster density ie the expected number of cluster_heads per unit area is spl rhosub c spl rhospl divide1spl muspl divide2 where spl mu denotes the expected number of neighbors of a node consequently a clusterhead is expected to incorporate half of its neighboring nodes into its cluster this result also holds in a scenario with mobile_nodes and serves as a bound for inhomogeneous spatial node distributions
faulttolerant ring embedding in faulty arrangement graphs,the arrangement graph asub nk which is a generalization of the star_graph nt1 presents more flexibility than the star_graph in adjusting the major design parameters number of nodes degree and diameter previously the arrangement graph has proven hamiltonian in this paper we further show that the arrangement graph remains hamiltonian even if it is faulty let fsub e and fsub v denote the numbers of edge faults and vertex faults respectively we show that asub nk is hamiltonian when 1 k2 and nkspl ges4 or kspl ges3 and nkspl ges4k2 and fsub espl lesknk21 or 2 kspl ges2 nkspl ges2k2 and fsub espl lesknk31 or 3 kspl ges2 nkspl ges3 and fsub 3spl lesk
the local structure of a bipartite distanceregular graph,in this paper we consider a bipartite distanceregular graph  x e with diameter d 3 we investigate the local structure of  focusing on those vertices with distance at most 2 from a given vertex x to do this we consider a subalgebra rrx ofmat0307a0xgif xc where 0307a1xgifx denotes the set of vertices in x at distance 2 from x r is generated by matrices a 0307a2xgif j and 0307a3xgif d defined as follows for all y z 0307a4xgif x the yz entry of a is 1 if y z are at distance 2 and 0 otherwise the y zentry of 0307a5xgif j equals 1 and the yz entry of 0307a6xgif d equals the number of vertices of x adjacent to each ofx  y and z we show that r is commutative and semisimple with dimension at least 2 we assume thatdimr is one of 2 3 or 4 and explore the combinatorial implications of this we are motivated by the fact that if  has a qpolynomial structure thendimr 4
prerequesites for symbiotic brainmachine interfaces,recent advancements in the neuroscience and engineering of brainmachine interfaces are providing a blueprint for how new coadaptive designs based on reinforcement_learning change the nature of a users ability to accomplish tasks that were not possible using static methodologies by designing adaptive controls and artificial_intelligence into the neural interface computers can become active assistants in goaldirected behavior and further enhance human performance this paper presents a set of minimal prerequisites that enable a cooperative symbiosis and dialogue between biological and artificial systems
computation and analysis of natural compliance in fixturing and grasping arrangements,this paper computes and analyzes the natural compliance of fixturing and grasping arrangements traditionally linearspring contact_models have been used to determine the natural compliance of multiple contact arrangements however these models are not supported by experiments or elasticity theory we derive a closedform formula for the stiffness matrix of multiple contact arrangements that admits a variety of nonlinear contact_models including the welljustified hertz model the stiffness matrix formula depends on the geometrical and material properties of the contacting bodies and on the initial loading at the contacts we use the formula to analyze the relative influence of first and secondorder geometrical effects on the stability of multiple contact arrangements secondorder effects ie curvature effects are often practically beneficial and sometimes lead to significant grasp stabilization however in some contact arrangements curvature has a dominant destabilizing influence such contact arrangements are deemed stable under an allrigid body model but in fact are unstable when the natural compliance of the contacting bodies is taken into account we also consider the combined influence of curvature and contact preloading on stability contrary to conventional wisdom under certain curvature conditions higher preloading can increase rather than decrease grasp stability finally we use the stiffness matrix formula to investigate the impact of different choices of contact model on the assessment of the stability of multiple contact arrangements while the linearspring model and the more realistic hertz model usually lead to the same stability conclusions in some cases the two models lead to different stability results
adapting information theoretic clustering to binary images,we consider the problem of finding points_of_interest along local curves of binary images information theoretic vector_quantization is a clustering_algorithm that shifts cluster_centers towards the modes of principal curves of a data set its runtime characteristics however do not allow for efficient processing of many data_points in this paper we show how to solve this problem when dealing with data on a 2d lattice borrowing concepts from signal_processing we adapt information theoretic clustering to the quantization of binary images and gain significant speedup
construction of robust prognostic predictors by using projective adaptive resonance theory as a gene filtering method,motivation for establishing prognostic predictors of various diseases using dna microarray analysis technology it is desired to find selectively significant genes for constructing the prognostic model and it is also necessary to eliminate nonspecific genes or genes with error before constructing the modelrnrnresults we applied projective adaptive resonance theory part to gene screening for dna microarray data genes selected by part were subjected to our fnnsweep modeling method for the construction of a cancer class prediction model the model performance was evaluated through comparison with a conventional screening signaltonoise s2n method or nearest shrunken centroids nsc method the fnnsweep predictor with part screening could discriminate classes of acute leukemia in blinded data with 971 accuracy and classes of lung cancer with 900 accuracy while the predictor with s2n was only 853 and 700 or the predictor with nsc was 882 and 900 respectively the results have proven that part was superior for gene screeningrnrnavailability the software is available upon request from the authorsrnrncontact hondanubionagoyauacjp
the relationship between minimum entropy control and risksensitive control for timevarying systems,the connection between minimum entropy control and risksensitive control for linear timevarying systems is investigated for timeinvariant systems the entropy functional and the linear exponential quadratic gaussian cost are the same in this paper it is shown that this is not true for general time varying systems it does hold however when the system admits a statespace representation
a novel hybrid parallelprefix adder architecture with efficient timingarea characteristic,twooperand binary addition is the most widely used arithmetic operation in modern datapath designs to improve the efficiency of this operation it is desirable to use an adder with good performance and area tradeoff characteristics this paper presents an efficient carrylookahead adder architecture based on the parallelprefix computation graph in our proposed method we define the notion of triplecarryoperator which computes the generate and propagate signals for a merged block which combines three adjacent blocks we use this in conjunction with the classic approach of the carryoperator to compute the generate and propagate signals for a merged block combining two adjacent blocks the timingdriven nature of the proposed design reduces the depth of the adder in addition we use a ripplecarry type of structure in the nontiming critical portion of the parallelprefix computation network these techniques help produce a good timingarea tradeoff characteristic the experimental results indicate that our proposed adder is significantly faster than the popular brentkung adder with some area_overhead on the adder hand the proposed adder also shows marginally faster performance than the fast koggestone adder with significant area savings
control of doublyfed induction generator system using pidnns,an intelligent_control standalone doublyfed induction generator dfig system using proportionalintegralderivative neural_network pidnn is proposed in this study this system can be applied as a standalone power supply system or as the emergency power system when the electricity grid fails for all subsynchronous synchronous and supersynchronous conditions the rotor side converter is controlled using the fieldoriented control to produce threephase stator voltages with constant magnitude and frequency at different rotor speeds moreover the stator side converter which is also controlled using fieldoriented control is primarily implemented to maintain the magnitude of the dclink voltage furthermore the intelligent pidnn controller is proposed for both the rotor and stator side converters to improve the transient and steadystate responses of the dfig system for different operating conditions both the network structure and online_learning algorithm are introduced in detail finally the feasibility of the proposed control scheme is verified through experimentation
passwordbased tripartite key exchange protocol with forward secrecy,a tripartite authenticated_key_agreement protocol is designed for three entities to communicate securely over an open network particularly with a shared key passwordauthenticated key exchange pake allows the participants to share a session_key using a human memorable password only in this paper a passwordbased authenticated tripartite key exchange protocol3pake is presented in the_standard_model the security of the protocol is reduced to thedecisional bilinear diffiehellman dbdh problem and the protocol provides not only the properties of forward secrecy but also resistance against known key attacks the proposed protocol is more efficient than the similar protocols in terms of both communication and computation
evaluation of highaltitude balloons as a learning_technology,the utility of highaltitude balloons as a learning_technology to facilitate education in several disciplines is considered in this paper the role that a highaltitude balloon can play as a learning_technology is discussed and its utility in this role is considered the need for a formal design framework for highaltitude ballooning is also discussed a framework for the assessment of highaltitude ballooning in supporting an undergraduatelevel course is evaluated and an assessment using this framework is conducted the paper concludes with a discussion of techniques that can be used to broaden access to highaltitude ballooning in education
automatic discovery of action taxonomies from multiple_views,we present a new method for segmenting actions into primitives and classifying them into a hierarchy of action classes our scheme learns action classes in an unsupervised manner using examples recorded by multiple cameras segmentation and clustering of action classes is based on a recently proposed motion descriptor which can be extracted efficiently from reconstructed volume sequences because our representation is independent of viewpoint it results in segmentation and classification_methods which are surprisingly efficient and robust our new method can be used as the first step in a semisupervised action_recognition system that will automatically break down training examples of people performing sequences of actions into primitive actions that can be discriminatingly classified and assembled into highlevel recognizers
drvis a database of human diseaserelated viral integration sites,viral integration plays an important role in the development of malignant diseases viruses differ in preferred integration site and flanking sequence viral integration sites vis have been found next to oncogenes and common fragile sites understanding the typical dna features near vis is useful for the identification of potential oncogenes prediction of malignant disease development and assessing the probability of malignant transformation in gene therapy therefore we have built a database of human diseaserelated vis drvis httpwwwscbitorgdbmidrvis to collect and maintain human diseaserelated vis data including characteristics of the malignant disease chromosome region genomic position and viralhost junction sequence the current build of drvis covers about 600 natural vis of 5 oncogenic viruses representing 11 diseases among them about 200 vis have viralhost junction sequence
research on internet marketing relationship model,the present models about internet marketing to a certain extent have some limits which cannot systematically reveals the process and characteristics of internet marketing based on the theories of traditional marketing and internet marketing the paper builds the model of internet marketing relationship that describes consumers purchase decision process and firms internet marketing process and correlation among consumer firm and bank and logistics firm through systematically analyzing the two processes the model reveals intrinsic characteristics and essences of internet marketing that are all different from traditional marketing the model provides a researchful platform for the researchers and a fundamental basis for further researching internet marketing
expanded rectangles a new vlsi data structure,a data structure derived from corner stitching which allows efficient representation of vlsi layouts is presented while each entry in the expanded rectangle database is larger than the corresponding cornerstitched entry generally fewer entries are required to represent the same vlsi layout the data structure has two important features first the vlsi_design is represented as a slicing structure in which each slice contains a portion of the solid material and second corner stitches are used to provide twodimensional nearness information initial measurements indicate that expanded rectangles is a viable data structure for use in a complete vlsi layout system 
predicting opponent actions by observation,in competitive domains the knowledge about the opponent can give players a clear advantage this idea lead us in the past to propose an approach to acquire models of opponents based only on the observation of their inputoutput behavior if opponent outputs could be accessed directly a model can be constructed by feeding a machine_learning method with traces of the opponent however that is not the case in the robocup domain to overcome this problem in this paper we present a three phases approach to model lowlevel behavior of individual opponent agents first we build a classifier to label opponent actions based on observation second our agent observes an opponent and labels its actions using the previous classifier from these observations a model is constructed to predict the opponent actions finally the agent uses the model to anticipate opponent reactions in this paper we have presented a proofofprinciple of our approach termed ombo opponent modeling based on observation so that a striker agent can anticipate a goalie results show that scores are significantly higher using the acquired opponents model of actions
web_accessibility compliance of government web sites in korea,this paper introduces korean web_accessibility activities such as relational laws ordinances policies guidelines it also presents analytical result of the investigation on webcontents accessibilities of the 39 korean government agencies the result shows that only one agency provides web_contents satisfying all the minimum requirements while 97 of the agencies does not satisfy all the minimum requirements unfortunately 6 agencies do not satisfy any
freeviewpoint video sequences a new challenge for objective quality metrics,freeviewpoint television is expected to create a more natural and interactive viewing experience by providing the ability to interactively change the viewpoint to enjoy a 3d scene to render new virtual viewpoints freeviewpoint systems rely on view_synthesis however it is known that most objective metrics fail at predicting perceived quality of synthesized views therefore it is legitimate to question the reliability of commonly used objective metrics to assess the quality of freeviewpoint video fvv sequences in this paper we analyze the performance of several commonly used objective quality metrics on fvv sequences which were synthesized from decompressed depth data using subjective scores as ground truth statistical analyses showed that commonly used metrics were not reliable predictors of perceived image_quality when different contents and distortions were considered however the correlation improved when considering individual conditions which indicates that the artifacts produced by some view_synthesis algorithms might not be correctly handled by current metrics
dealing with hardware in embedded_software a general framework based on the devil language,writing code that talks to hardware is a crucial part of any embedded project both productivity and quality are needed but some flaws in the traditional development process make these requirements difficult to meet  we have recently introduced a new approach of dealing with hardware based on the devil language devil allows to write a highlevel formal definition of the programming interface of a peripheral circuit a compiler automatically checks the consistency of a devil specification from which it generates the lowlevel hardwareoperating code  in our original framework the generated code is dependent of the host architecture cpu buses and bridges consequently any variation in the hardware environment requires a specific tuning of the compiler considering the variability of embedded architectures this is a serious drawback in addition this prevents from mixing different buses in the same circuit interface  in this paper we remove those limitations by improving our framework in two ways i we propose a better isolation between the devil compiler and the host architecture ii we introduce trident a language extension aimed at mapping one or several buses to each peripheral circuit
uniprocessor scheduling under precedence constraints,in this paper we present a novel approach to the constrained scheduling problem while addressing a more general class of constraints that arise from the timing requirements on realtime embedded controllers and from the implementation of mixed datafloweventdriven realtime systems we provide general necessary and sufficient_conditions for scheduling under precedence constraints and derive sufficient_conditions for two wellknown scheduling policies we define mathematical problems that provide optimum priority and deadline assignments while ensuring both precedence constraints and systems schedulabilitywe show how these problems can be relaxed to corresponding ilp formulations leveraging on available solvers
adaptive multilayer traffic_engineering with shared risk group protection,in this paper we propose a new traffic_engineering scheme to be used jointly with protection in multilayer groomingcapable opticalbeared networks to make the working and protection paths of demands better adapt to changing traffic and network conditions we propose the adaptive multilayer traffic_engineering amlte scheme that tailors ie fragments and defragments wavelength paths in a fully automatic distributed way
endowing spoken language dialogue_systems with emotional intelligence,while most dialogue_systems restrict themselves to the adjustment of the propositional contents our work concentrates on the generation of stylistic va riations in order to improve the users perception of the interaction to accomplish this goal our approach integrates a social theory of politeness with a cognitive theory of emotions we propose a hierarchical selection process for politeness behaviors in order to enable the refinement of decisions in case additional context_information becomes available
floorplan driven architecture and highlevel synthesis algorithm for dynamic multiple supply voltages,
a multiple subset sum formulation for feedback implosion suppression over satellite_networks,in this paper we present a feedback implosion suppression fis algorithm that reduces the volume of feedback information transmitted through the network without relying on any collaboration between users or on any infrastructure other than the satellite_network next generation satellite_systems that utilize the ka frequency band are likely to rely on various fade mitigation techniques in order to guarantee a service_quality that is comparable to other broadband technologies user feedback would be a valuable input for a number of such components however collecting periodic feedback from a large number of users would result in the wellknown feedback implosion problem feedback implosion is identified as a major problem when a large number of users try to transmit their feedback messages through the network holding up a significant portion of the uplink resources and clogging the shared uplink medium in this paper we look at a system where uplink channel_access is organized in timeslots the goal of the fis algorithm is to reduce the number of uplink timeslots hold up for the purpose of feedback transmission our analysis show that the fis algorithm effectively suppresses the feedback messages of 95 of all active users but still achieves acceptable performance results when the ratio of available timeslots to number of users is equal to or higher than 5
computing subgraph probability of random geometric graphs with applications in quantitative analysis of ad_hoc_networks,random geometric graphs rgg contain vertices whose points are uniformly distributed in a given plane and an edge between two distinct nodes exists when their distance is less than a given positive value rggs are appropriate for modeling ad_hoc_networks consisting of n mobile_devices that are independently and uniformly distributed randomly in an area to the best of our knowledge this work presents the first paradigm to compute the subgraph probability of rggs in a systematical way in contrast to previous asymptotic bounds or approximation which always assume that the number of nodes in the network tends to infinity the closedform formulas we derived herein are fairly accurate and of practical value moreover computing exact subgraph probability in rggs is shown to be a useful tool for counting the number of induced_subgraphs which explores fairly accurate quantitative property on topology of ad_hoc_networks
some further results on synchronizable block_codes corresp,
unequal_error_protection for video_streaming over wireless_lans using contentaware packet retry limit,in this paper we propose a contentaware retry limit adaptation scheme for video_streaming over ieee 80211 wireless_lans wlans video_packets of different importance are unequally protected with different retry limits at the mac_layer the loss impact of each packet is estimated to guide the selection of its retry limit more retry numbers are allocated to packets of higher loss impact to achieve unequal_error_protection experimental results show that the proposed adaptation scheme can effectively mitigate the error propagation due to packet_loss and assure the ontime arrival of packets for presentation thereby improving video_quality significantly
linearization of ancestral multichromosomal genomes,backgroundrnrecovering the structure of ancestral genomes can be formalized in terms of properties of binary matrices such as the consecutiveones property c1p the linearization problem asks to extract from a given binary matrix a maximum weight subset of rows that satisfies such a property this problem is in general intractable and in particular if the ancestral genome is expected to contain only linear chromosomes or a unique circular chromosome in the present work we consider a relaxation of this problem which allows ancestral genomes that can contain several chromosomes each either linear or circular
odintoolsmodeldriven development of intelligent mobile_services,todays computationally able mobile_devices are capable of acting as service providers as opposed to their traditional role as consumers to address the challenges associated with the development of these mobile_services we have developed odin a middleware which masks complexity allowing rapid development of mobile_services odin however does not allow crossplatform development which is an important concern with todays wide variety of mobile_devices to solve this problem we have designed odin tools  a modeldriven toolkit for crossplatform development of mobile_services leveraging appropriate metamodels a prototype has been implemented in eclipse and marama that allows developers to model mobile_services in a platformindependent manner we are currently working on transformations between levels of the model hierarchy which will allow full odinbased service implementations to be generated automatically
a socio political model of the relationship between it_investments and business performance,in recent years many studies have been published on the assessment of payoffs from investments in it the research has produced mainly mixed results different explanations can be given for these mixed results one possible explanation might be the dominance of the rational perspective in previous research the consequence of this overemphasis on rationality is that the social political nature of the it_investment process has largely been neglected in previous research on it business value this omission produces an incomplete picture and might contribute to the conflicting empirical results in this research we studied whether and how the socio political perspective can be used to explain the mixed results case study research was used to test whether the attitude towards the value of it destructive conflict and a low level of trust influence the relationship between it_investments and business performance
a new metrics set for evaluating testing efforts for objectoriented programs,software_metrics proposed and used for procedural paradigm have been found inadequate for object_oriented_software products mainly because of the distinguishing features of the object_oriented paradigm such as inheritance and polymorphism several object_oriented_software metrics have been described in the literature these metrics are goal driven in the sense that they are targeted towards specific software qualities we propose a new set of metrics for object_oriented programs this set is targeted towards estimating the testing efforts for these programs the definitions of these metrics are based on the concepts of object orientation and hence are independent of the object_oriented_programming languages the new metrics set has been critically compared with three other metrics sets published in the literature
towards a universal client for grid monitoring systems_design and implementation of the ovid browser,in this paper we present the design and implementation of ovid a browser for gridrelated information the key goal of ovid is to support the seamless navigation of users in the grid information space key aspects of ovid are i a set of navigational primitives which are designed to cope with problems such as network disorientation and information overloading ii a small set of ovid views which present the enduser with highlevel visual abstractions of grid information these abstractions correspond to simple models that capture essential aspects of a grid infrastructure iii support for embedding and implementing hyperlinks that connect related entities represented within different information views iv a plugin mechanism which enables the seamless integration with ovid of thirdparty software that retrieves and displays data from various grid information sources and v a modular software_design which allows the easy integration of different visualization algorithms that support the graphical_representation of large amounts of gridrelated information in the context of ovids views
a trainable singlepass algorithm for column segmentation,column segmentation logically precedes ocr in the document_analysis process the trainable algorithm xycut relies on horizontal and vertical binary profiles to produce an xytree representing the column structure of a page of a technical document in a single pass through the bit image training against ground truth adjusts a single resolution independent parameter using only local information and guided by an edit distance function the algorithm correctly segments the page image for a fairly wide range of parameter values although small local and repairable errors may be made an effect measured by a repair cost function
hybrid wordlength optimization_methods of pipelined fft_processors,quickly and accurately predicting the performance based on the requirements for ipbased system implementations optimizes the design and reduces the design time and overall cost this study describes a novel hybrid method for the wordlength optimization of pipelined fft_processors that is the arithmetic kernel of ofdmbased systems this methodology utilizes the rapid computing of statistical_analysis and the accurate evaluation of simulationbased analysis to investigate a speedy optimization flow a statistical error model for varying wordlengths of pe stages of an fft processor was developed to support this optimization flow experimental results designate that the wordlength optimization employing the speedy flow reduces the percentage of the total area of the fft processor that increases with an increasing fft length finally the proposed hybrid method requires a shorter prediction time than the absolute simulationbased method does and achieves more accurate outcomes than a statistical calculation does
generating organic textures with controlled anisotropy and directionality,this article presents a method for generating organic textures by tessellating a region into a set of pseudovoronoi polygons using a particle model and then generating the detailed geometry each of the polygons with fractal noise
guidelines for reporting an fmri study,in this editorial we outline a set of guidelines for the reporting of methods and results in functional magnetic resonance imaging studies and provide a checklist to assist authors in preparing manuscripts that meet these guidelines
on load regulated csma,in this paper we derive throughput of a thresholdbased transmission policy namely loadregulated csma taking into account the propagation delay of the medium and the offered load at different probability of the fading_channel in case of the saturated load regulated csma a trivial relationship between deterministic offered load to the channel at a particular fading_channel condition and the maximum possible offered load has been shown we further extend the load regulation concept into multichannel domain both single and multichannel load regulated csma improves the throughput of the system compared to the existing csma system which does not consider channel fading to control the packet_transmissions
component_based_design using constraint_programming for module placement on fpgas,constraint satisfaction modeling is both an efficient and an elegant approach to model and solve many real world problems in this paper we present a constraint solver targeting module placement in static and partial runtime reconfigurable_systems we use the constraint solver to compute feasible placement positions our placement model incorporates communication implementation variants and device configuration granularity in addition we model heterogeneous resources such as embedded_memory multipliers and logic furthermore we take into account that logic resources consist of different types including logic only luts arithmetic luts with carry chains and luts with distributed_memory our work targets state of the art fieldprogrammable gate arrays fpgas in both designtime and runtime applications in order to evaluate our placement model and module placer implementation we have implemented a repository containing 200 fully functional placed and routed relocatable modules the modules are used to implement complete systems this validates the feasibility of both the model and the module placer furthermore we present simulated results for runtime applications and compare this to other state of the art research in runtime applications the results point to improved resource utilization this is a result of using a finer tile grid and complex module shapes
ebrainstorming optimization of collaborative_learning thanks to online questionnaires,the purpose of this article is to present a methodology and tools allowing the use of online multiplechoice questionnaires to enhance collaborative_work the first goal is to allow the questionnaires generation and setting with a simple and ergonomic manner but also to let questioned people making comments and proposing new questions to other contributors the developed system provides a visualization of a synthesis of the questionnaire results that is also accessible by the mean of external applications through standard web services these principles were developed and tested on a sample of users
bioinformatics integration framework for metabolic pathway datamining,a vast amount of bioinformatics information is continuously being introduced to different databases around the world handling the various applications used to study this information present a major data_management and analysis challenge to researchers the present work investigates the problem of integrating heterogeneous applications and databases towards providing a more efficient datamining environment for bioinformatics research a framework is proposed and gexpert an application using the framework towards metabolic pathway determination is introduced some sample implementation results are also presented
schedulability_analysis of fixed priority realtime systems with offsets,for a number of years work has been performed in collaboration with industry to establish improved techniques for achieving and proving the system timing_constraints the specific requirements encountered during the course of this work for both uniprocessor and distributed_systems indicate a need for an efficient mechanism for handling the timing_analysis of task sets which feature offsets little research has been performed on this subject the paper describes a new technique tailored to a set of real world problems so that the results are effective and the complexity is manageable
change_impact_analysis for generic libraries,since the standard template library stl generic libraries in c rely on concepts to precisely specify the requirements of generic algorithms function templates on their parameters template arguments modifying the definition of a concept even slightly can have a potentially large impact on the interfaces of the entire library in particular the nonlocal effects of a change however make its impact difficult to determine by hand in this paper we propose a conceptual change_impact_analysis ccia which determines the impact of changes of the conceptual specification of a generic library the analysis is organized in a pipeandfilter manner where the first stage finds any kind of impact the second stage various specific kinds of impact both stages describe reachability algorithms which operate on a conceptual dependence graph in a case study we apply ccia to a new proposal for stl iterator concepts which is under review by the c standardization committee the analysis shows a number of unexpected incompatibilities and for certain stl algorithms a loss of genericity
a model and case study for efficient shelf usage and assortment analysis,in the rapidly changing environment of fast moving consumer goods sector where new product launches are frequent retail channels need to reallocate their shelf spaces intelligently while keeping up their total profit margins and to simultaneously avoid product pollution in this paper we propose an optimization model which yields the optimal product mix on the shelf in terms of profitability and thus helps the retailers to use their shelves more effectively the model is applied to the shampoo product class at two regional supermarket chains the results reveal not only a computationally viable model but also substantial potential increases in the profitability after the reorganization of the product list
multiprocessors may reduce system dependability under filebased race condition attacks,attacks exploiting race conditions have been considered rare and low risk however the increasing popularity of multiprocessors has changed this situation instead of waiting for the victim process to be suspended to carry out an attack the attacker can now run on a dedicated processor and actively seek attack opportunities this change from fortuitous encountering to active exploiting may greatly increase the success probability of race condition attacks this point is exemplified by studying the tocttou timeof checktotimeofuse race condition attacks in this paper we first propose a probabilistic_model for predicting tocttou attack success rate on both uniprocessors and multiprocessors then we confirm the applicability of this model by carrying out tocttou attacks against two widely used utility programs vi and gedit the success probability of attacking vi increases from low single digit percentage on a uniprocessor to almost 100 on a multiprocessor similarly the success rate of attacking gedit jumps from almost zero to 83 these case studies suggest that our model captures the sharply increased risks and hence the decreased dependability of our systems represented by race condition attacks such as tocttou on the next generation multiprocessors
members of random closed sets,the members of martinlof random closed sets under a distribution studied by barmpalias et al are exactly the infinite paths through martinlof random galtonwatson trees with survival parameter frac23 to be such a member a sufficient condition is to have effective hausdorff dimension strictly greater than gammalog_2 frac32 and a necessary condition is to have effective hausdorff dimension greater than or equal to    
multiple exposure images based traffic light recognition,
information_extraction from nanotoxicity related publications,highquality experimental data are important when developing predictive models for studying nanomaterial environmental impact nei given that raw data from experimental laboratories and manufacturing workplaces are usually proprietary and smallscaled extracting_information from publications is an attractive alternative for collecting data we developed an information_extraction system that can extract useful information from fulltext nanotoxicity related publications this information_extraction system consists of five components raw data transformation into machine readable format data preprocessing ontologybased named_entity_recognition rulebased numerical attribute extraction from both tables and unstructured text and relation_extraction among entities and attributes the information_extraction system is applied on a dataset made of 94 publications and results in an acceptable accuracy by storing extracted data into a table according to relations among the data a dataset that can be used to predict nanomaterial environmental impact is obtained such a system is unique in current nanomaterial community and can help nanomaterial scientists and practitioners quickly locate useful information they need without spending lots of time reading articles
cmos bodyenhanced cascode current mirror,a cascode current mirror with auxiliary bodydriven feedback loop is proposed main performance parameters are analytically evaluated and compared to those of a conventional highswing cascode and of a recentlyproposed bodydriven topology simulations are also provided confirming improvements in the achievable output resistance important for short channel technologies dc accuracy and input dynamic range linearity bandwidth noise and voltage requirements are substantially the same of the conventional highswing cascode solution
performance of the beaconless routing_protocol in realistic scenarios,the beaconless routing_protocol blr is a positionbased routing_protocol for mobile adhoc networks that makes use of location_information to reduce routing overhead unlike other positionbased routing_protocols blr does not require nodes to periodically broadcast hello messages this avoids drawbacks such as extensive use of scarce batterypower interferences with regular data transmission and outdated position information in case of high_mobility this paper discusses the behavior and performance of blr in realistic scenarios in particular with irregular transmission_ranges blr has been implemented using appropriate simulation models and in an outdoor testbed consisting of gnulinux laptops with wireless_lan network interfaces and gps_receivers
twodimensional orthogonal tiling from theory to practice,in pipelined parallel_computations the inner loops are often implemented in a block fashion in such programs an important compiler_optimization involves the need to statically determine the grain size this paper presents extensions and experimental validation of the previous results of andonov and rajopadhye 1994 on optimal grain size determination
a twolevel ecn marking for fair bandwidth_allocation between hstcp and tcp_reno,many versions of tcp have been proposed for transmitting data and among them tcp_reno is most widely used today however the problem that it is difficult to use the wide_bandwidth efficiently with tcp_reno has been pointed out hstcp is one of the several new versions of tcp that are proposed to address this problem but when its flows compete with tcp_reno flows at the same link hstcp gains most of the bandwidth and it is impossible to conduct fair transfer in order to address this problem we propose a twolevel ecn marking to increase the frequency of congestion controls of hstcp flows holding its throughput we evaluate our proposal through computer simulations and the results show that our proposal mitigates bandwidth_allocation to hstcp promoting fair transfer with tcp_reno
a distributed simulation based monitoring,mss a computerbased monitoring system with integrated cooperative objects is proposed mss uses an objectbased framework to interface with the user to guide a specific system evolution mss espouses a blackboard architecture and runs according a cooperating objects model to achieve monitoring tasks mss selects the appropriate techniques within a set of a high performance algorithms from the user viewpoint mss has been developed as a control assistant featuring different levels of interactivity a hierarchical design style and fully embedded algorithmic tools virtually mss is able to design a monitoring board for any dynamic system
leakage power reduction in dualvdd and dualvth designs through probabilistic analysis of vth variation,the noise sensitivity of low_power circuits is rapidly increasing with the increasing levels of process_variability and uncertainty in this work we study the problem of leakage power minimization in dualvdd and dualvth designs in the presence of significant vth variation the impact of the uncertainty in vth on leakage power and timing are studied through probabilistic analytical models we develop probabilistic_models for timing slack and leakage power considering threshold variations with the objective of achieving an optimal selection of vth an analysis of the models indicate that in the presence of variability the value of the second vth must be about 30mv higher than the vth value obtained without considering variability we show that our proposed method for the selection of vth yields the lowest leakage power ratio of the dualvdd and dualvth versus the singlevdd and singlevth designs in addition the proposed models can be used to determine the ideal values for the second vdd and vth values in the context of variability for a variety of process conditions
probabilistic cluster signature for modeling motion classes,in this paper a novel 3d motion trajectory signature is introduced to serve as an effective description to the raw trajectory more importantly based on the trajectory signature a probabilistic modelbased cluster signature is further developed for modeling a motion class the cluster signature is a mixture modelbased motion description that is useful for motion class perception recognition and to benefit a generalized robot task representation the signature modeling process is supported by integrating the em and ipra algorithms the conducted experiments verified the cluster signatures effectiveness
secure crossdomain positioning architecture for autonomic_systems,positioning as one of the prime components of context has been a driving factor in the development of ubiquitous_computing applications throughout the past two decades based on the redundant positioning architecture this paper discusses the issues of exchanging positioning data between applications and between different administrative domains with a focus on implementing security_and_privacy concepts in a selflearning and selfadapting autonomic_systems environment
refinement of medical knowledge_bases a neural_network approach,one important issue in designing medical knowledgebased systems is the management of uncertainty among the schemes that have been developed for this purpose probability and cf certainty factor are the most widely used if rules are organized according to a connectionist model then neural_network learning suggests a promising solution to this problem when most rules are correct semantically incorrect rules can be recognized if their associated certainty factors are weakened or change signs after training with correct samples the techniques for rule_base refinement are examined under this approach the concept has been implemented and tested in an actual medical expert_system 
an it appliance for remote collaborative review of mechanisms of injury to children in motor vehicle crashes,this paper describes the architecture and implementation of a javabased appliance for collaborative review of crashes involving injured children in order to determine mechanisms of injury the multidisciplinary expertise needed for such reviews is not available at any one institution resulting in the need for remote collaboration while the sensitive nature of the information requires secure transmission and controlled access of data the intended users of the appliance are researchers engineers medical doctors government regulators automobile and restraint manufacturers insurance company representatives and others who are interested in understanding the types and causes of injuries to children involved in motor vehicle crashes the ultimate goal is to devise engineering solutions that prevent similar injuries from occurring in the future the collaboration appliance called telecenter enables the following activities 1 the distributed asynchronous collection of digital_content needed for each crash case review under a scheme that consistently organizes content across multiple cases 2 the secure webbased remote participation of users in casereview meetings that involve viewing of casespecific content live communication written or verbal multimedia access and sharing slide presentations images and use of web_resources and 3 archival and postreview access of case reviews for followup activities and other functions eg statistics search and networking the telecenter design supports audio conferencing remote delivery and viewing of slide presentations and other collaboration features also available in commercial and publicdomain collaboration middleware products however it goes beyond existing solutions by also embedding a specific workflow and content organization suited for traffic injury reviews supporting spatiotemporal rolebased access_control distributed management of content and seamless integration of existing services the current status and experience from using an early prototype of the telecenter in actual case reviews are discussed along with planned extensions to its functionality
strategic design of the purchase system toward rd supply chain based on sna,in order to make the strategy for research and development rd purchase system better serve the personalization of material requirements in rd process the authors propose to develop a strategy set which will satisfy the internal as well as external constraints simultaneously social_network_analysis is used to analyze the vertical and horizontal relationships among the project department and enterprise layers through a case study the authors display the regulatory relationship of participants under given organization pattern and supply chain configuration to disclose the restricted equilibrium mechanism of participants involved changes under different strategies are compared which can assist enterprises to enforce the decision making and to improve the rd purchase system ability the authors outline some of the managerial implications arising from the research findings at the end of this paper
exploring early usage patterns of mobile data services,in this paper we study the nature of factors that facilitate mobile data services use as well as the characteristics of early adopters to shed light into diffusion patterns and inform predictions for future growth we advocate that the use of mobile data services can be associated with ones level of satisfaction with hisher life based on the findings of a questionnairebased survey n388 we have found that users satisfied with their personal life use information mobile email and stock broking services more frequently than dissatisfied ones while users satisfied with their professional life tend to use financial information and mobile email services more heavily furthermore we identify early adopters profiles in terms of their demographic characteristics gender age education and income to inform the design of effective target marketing strategies
a fast fixed point iteration algorithm for sparse channel_estimation,channels with a long but sparse impulse_response arise in a variety of wireless_communication applications such as high_definition_television hdtv terrestrial transmission and underwater_acoustic_communications by adopting the ell_1norm as the sparsity metric of the channel response the channel_estimation is formulated as a complexvalued convex optimization problem a fast fixed point iteration algorithm is developed to solve the resultant complexvalued ell_1minimization problem the proposed fast channel_estimation algorithm is easy to implement and has a low computational complexity of onlog n per iteration with n the signal length simulation results are provided to demonstrate the performance of the proposed fixed point algorithm
modelindependent recovery of object orientations,a novel algorithm is presented for determining the orientation of road vehicles in traffic scenes using video images the algorithm requires no specific 3d vehicle models and only uses local image gradient values it may easily be implemented in realtime experimental results with a variety of vehicles in routine traffic scenes are included to demonstrate the effectiveness of the algorithm
linear scale and rotation invariant matching,matching visual patterns that appear scaled rotated and deformed with respect to each other is a challenging problem we propose a linear formulation that simultaneously matches feature points and estimates global geometrical transformation in a constrained linear space the linear scheme enables search space reduction based on the lower convex hull property so that the problem size is largely decoupled from the original hard combinatorial_problem our method therefore can be used to solve large scale problems that involve a very large number of candidate feature points without using prepruning in the search this method is more robust in dealing with weak features and clutter we apply the proposed method to action detection and image_matching our results on a variety of images and videos demonstrate that our method is accurate efficient and robust
a builtin selftesting approach for minimizing hardware overhead,a builtin selftest bist hardware insertion technique is addressed applying to register_transfer_level designs this technique utilizes not only the circuit structure but also the module functionality in reducing test hardware overhead experimental results have shown up to 38 reduction in area_overhead over other system level bist techniques 
multistage tr scheme for papr_reduction in ofdm_signals,in the tone_reservation tr scheme of the orthogonal_frequency_division_multiplexing ofdm_systems there exists a tradeoff between the peak to average power ratio papr_reduction performance and the peak reduction tone prt set size in this paper we propose a multistage tr scheme for papr_reduction which adaptively selects one of several prt sets according to the papr of ofdm_signal while the prt set is fixed for the conventional tr scheme it is shown that the papr_reduction performance of the proposed scheme is better than that of the conventional tr scheme when the tone_reservation rate trr is the same
random key_predistribution schemes for sensor_networks,key_establishment in sensor_networks is a challenging problem because asymmetric key cryptosystems are unsuitable for use in resource constrained sensor_nodes and also because the nodes could be physically compromised by an adversary we present three new mechanisms for key_establishment using the framework of predistributing a random set of keys to each node first in the qcomposite keys scheme we trade off the unlikeliness of a largescale network_attack in order to significantly strengthen random key predistributions strength against smallerscale attacks second in the multipathreinforcement scheme we show how to strengthen the security between any two nodes by leveraging the security of other links finally we present the randompairwise keys scheme which perfectly preserves the secrecy of the rest of the network when any node is captured and also enables nodetonode authentication and quorumbased revocation
quantitative assessment of image_noise and streak artifact on ct_image comparison of zaxis automatic tube current modulation technique with fixed tube current technique,abstract  the purpose of our study is to quantitatively assess the effects of  z axis automatic tube current modulation technique on image_noise and streak artifact by comparing with fixed tube current technique standard deviation of ctvalues was employed as a physical index for evaluating image_noise and streak artifact was quantitatively evaluated using our devised gumbel evaluation method  z axis automatic tube current modulation technique will improve image_noise and streak artifact compared with fixed tube current technique and will make it possible to significantly reduce radiation doses at lung levels while maintaining the same image_quality as fixed tube current technique
coordination policy for a twostage supply chain considering quantity discounts and overlapped delivery with imperfect quality,unlike the traditional integrated supplierbuyer coordination model this research incorporates overlapped delivery and imperfect items into the productiondistribution model this model improves the observable fact that the system might experience shortage during the screening duration and also takes quantity discount into account this approach has not been discussed in previous integrated supplierbuyer coordination models the expected annual integrated total cost function is derived and properties and theorems are explored to help develop an algorithm a solution procedure free from the convexity associated with an algorithm is established to find the optimal solution a numerical_example is given to illustrate the proposed procedure and algorithm a sensitivity analysis is made to investigate the effects of five important parameters the inspect rate the annual demand the defective rate the holding cost and the receiving cost on the optimal solution managerial insights are also discussed
monte carlo modeling for implantable fluorescent analyte sensors,a monte carlo simulation of photon propagation through human skin and interaction with a subcutaneous fluorescent sensing layer is presented the algorithm will facilitate design of an optical probe for an implantable fluorescent sensor which holds potential for monitoring many parameters of biomedical interest results are analyzed with respect to output light intensity as a function of radial distance from source angle of exit for escaping photons and sensor fluorescence sf relative to tissue autofluorescence af a sensitivity study was performed to elucidate the effects on the output due to changes in optical properties thickness of tissue layers thickness of the sensor layer and both tissue and sensor quantum yields the optical properties as well as the thickness of the stratum corneum epidermis tissue layers through which photons must pass to reach the sensor and the papillary dermis tissue distal to sensor are highly influential the spatial emission profile of the sf is broad compared that of the tissue fluorescence and the ratio of sensor to tissue fluorescence increases with distance from the source the angular distribution of escaping photons is more concentrated around the normal for sf than for tissue af the information gained from these simulations will he helpful in designing appropriate optics for collection of the signal of interest
towards a data_publishing framework for primary biodiversity data challenges and potentials for the biodiversity informatics community,background currently primary scientific_data especially that dealing with biodiversity is neither easily discoverable nor accessible amongst several impediments one is a lack of professional recognition of scientific_data publishing efforts a possible solution is establishment of a data_publishing framework which would encourage and recognise investments and efforts by institutions and individuals towards management and publishing of primary scientific_data potentially on a par with recognitions received for scholarly publications discussion this paper reviews the stateoftheart of primary biodiversity data_publishing and conceptualises a data_publishing framework that would help incentivise efforts and investments by institutions and individuals in facilitating free and open_access to biodiversity data it further postulates the institutionalisation of a data usage index dui that would attribute due recognition to multiple players in the data collectioncreation management and publishing cycle conclusion we believe that institutionalisation of such a data_publishing framework that offers sociocultural legal technical economic and policy environment conducive for data_publishing will facilitate expedited discovery and mobilisation of an exponential increase in quantity of fitforuse primary biodiversity data much of which is currently invisible
on transistor level gate_sizing for increased robustness to transient_faults,in this paper we present a detailed analysis on how the critical charge qsub crit of a circuit node usually employed to evaluate the probability of transient fault tf occurrence as a consequence of a particle hit depends on transistors sizing we derive an analytical model allowing us to calculate a nodes qsub crit given the size of the nodes driving gate and fanout gates thus avoiding time costly electrical level simulations we verified that such a model features an accuracy of the 97 with respect to electrical level simulations performed by hspice our proposed model shows that qsub crit depends much more on the strength conductance of the gate driving the node than on the node total capacitance we also evaluated the impact of increasing the conductance of the driving gate on tfs propagation hence on soft_error susceptibility ses we found that such a conductance increase not only improves the tf robustness of the hardened node but also that of the whole circuit
designinclusive ux research design as a part of doing user_experience research,since the third wave in humancomputer interaction hci research on user_experience ux has gained momentum within the hci community the focus has shifted from systematic usability_requirements and measures towards guidance on designing for experiences this is a big change since design has traditionally not played a large role in hci research yet the literature addressing this shift in focus is very limited we believe that the field of ux research can learn from a field where design and experiential aspects have always been important design research in this article we discuss why design is needed in ux research and how research that includes design as a part of research can support and advance ux design practice we do this by investigating types of designinclusive ux research and by learning from reallife cases of uxrelated design research we report the results of an interview study with 41 researchers in three academic research units where design research meets ux research based on our interview findings and building on existing literature we describe the different roles design can play in research projects we also report how design research results can inform designing for experience methodologically or by providing new knowledge on ux the results are presented in a structured palette that can help ux researchers reflect and focus more on design in their research projects thereby tackling experience_design challenges in their own research
multifactory optimization enables kit reconfiguration in semiconductor manufacturing,to enable the huge saving of the kitbreakdown we developed maxit v12 to generate an optimal capacity plan at the kit component level for the midrange build plan in multifactory environment we describe the milp mixed integer linear programming_model and system_architecture of maxit v12 we also conduct detailed sensitivity analysis on parameter setting and objective prioritizing with the implementation in the intel shanghai and manila sites we have significantly improved data integrity and enabled a usspl ges cost_savings
remarks on algorithm 32 multint certification of algorithm 32,
eudaemonic computing underwearables,this paper presents a framework for wearable_computing based on the principle that it be unobtrusive and that it be integrated into ordinary clothing this design philosophy called eudaemonic computing named in honor of the group of physicists who designed the first truly unobtrusive wearable_computers with vibrotactile displays is reduced to practice through the underwearable computer underwearable for short the underwearable is a computer_system that is meant to be worn within or under ordinary clothing the first underwearables were built in the early 1980s and have evolved into a form that very much resembles a tanktop there were three reasons for the tank structure 1 weight is evenly and comfortably distributed over the body and bulk is distributed unobtrusively 2 it provides privacy by situating the apparatus within the corporeal boundary we consider our own personal space and others also soregard and 3 proximity to the body affords capability to both sense biological signal quantities such as respiration and heart signals which are both accessible to a vestbased device as well as produce output that we can sense unobtrusively the vibrotactile output modality vibravest was explored as a means of assisting the visually challenged to avoid bumping into objects through an ability to feel objects at a distance the success of vibravest suggests other possibilities for similar unobtrusive devices that can be worn over an extended period of time in all facets of daytoday life
information_systems for the age of consequences,this paper discusses what kinds of computer information_systems might be of broad social value in the context of the increasingly severe ecological and social consequences of economic_growth and how they might be built and maintained the paper has two parts the first offers a particular understanding of the ecological and social limits to economic_growth the second considers how this understanding can inform computer information_systems design and operation and characterizes good limitsaware computing research
active selection of training examples for metalearning,metalearning has been used to relate the performance of algorithms and the features of the problems being tackled the knowledge in metalearning is acquired from a set of metaexamples which are generated from the empirical evaluation of the algorithms on problems in the past in this work active_learning is used to reduce the number of metaexamples needed for metalearning the motivation is to select only the most relevant problems for metaexample generation and consequently to reduce the number of empirical evaluations of the candidate algorithms experiments were performed in two different case studies yielding promising results
engineering semantic_web information_systems,web information_systems wis use the web paradigm and technologies to retrieve information from sources connected to the web and present the information in a web or hypermedia presentation to the user hera is a design methodology that supports the design of wis it is a model driven method that distinguishes integration data_gathering and presentation generation in this paper we address the hera methodology and specifically explain the integration model that covers the different aspects of integration and the adaptation model that specifies how the generated presentations are adaptable eg device capabilities user_preferences the hera software_framework provides a set of transformations that allow a wis to go from integration to presentation generation these transformations are based on rdfs and we show how rdfs has proven its value in combining all relevant aspects of wis design in this way rdfs being the foundation of the semantic_web hera allows the engineering of semantic_web information_systems swis
optimal wiresizing for interconnects with multiple sources,the optimal wiresizing problem for nets with multiple sources is studied under the distributed elmore delay model we decompose such a net into a source subtree sst and a set of loading subtrees lsts and show the optimal wiresizing solution satisfies a number of interesting properties including the lst separability the lst monotone property the sst local monotone property and the general dominance property furthermore we study the optimal wiresizing problem using a variable grid and reveal the bundled refinement property these properties lead to efficient algorithms to compute the lower and upper bounds of the optimal_solutions experiment results on nets from an intel processor layout show an interconnect delay reduction of up to 359 when compared to the minimumwidth solution in addition the algorithm based on a variable grid yields a speedup of two orders of magnitude without loss of accuracy when compared with the fixed grid based methods
magneto and electroencephalographic manifestations of reward anticipation and delivery,article i nfo article history accepted 19 april 2012 available online 26 april 2012 the monetary incentive delay task was used to characterize reward anticipation and delivery with concur rently acquired evoked magnetic fields eeg potentials and eegmeg oscillatory responses obtaining a pre cise portrayal of their spatiotemporal evolution in the anticipation phase differential activity was most prominent over midline electrodes and parietooccipital sensors differences between nonreward and rewardpredicting cues were localized in the cuneus and later in the dorsal pcc suggesting a modulation by potential reward information during early visual processing followed by a coarse emotional evaluation of the cues oscillatory analysis revealed increased theta power after nonreward cues over frontocentral sites in the beta range power decreased with the magnitude of the potential reward and increased with re action time probably reflecting the influence of the striatal response to potential reward on the sensorimotor cortex at reward delivery negative prediction errors led to a larger mediofrontal negativity the spatiotem poral evolution of reward processing was modulated by prediction error whereas differences were located in pcc and putamen in the prediction error comparison in the case of expected outcomes they were located in pcc acc and parahippocampal gyrus in the oscillatory realm theta power was largest following rewards and in the case of nonrewards was largest when these were unexpected higher beta activity following re wards was also observed in both modalities but meg additionally showed a significant power decrease for this condition over parietooccipital sensors our results show how visual limbic and striatal structures are involved in the different stages of reward anticipation and delivery and how theta and beta oscillations have a prominent role in the processing of these stimuli
ranking weblogs by analyzing reading and commenting activities,in this paper we analyze peoples reading and commenting behaviors in blogspace and proposed an algorithm for blog ranking upon two selected communities ai and medical we show how comments reading records active browsing and multi time browsing can help to construct the weblog graph and reflect a blogs popularity based on these analysis we propose crank a graph based algorithm to rank blog among community members finally we divide our dataset temporally and present how the proposed algorithm can make prediction on blogs rankings the experiment shows that crank has a better performance upon several baseline systems
robust lip region segmentation for lip images with complex background,robust and accurate lip region segmentation is of vital importance for lip image_analysis however most of the current techniques break down in the presence of mustaches and beards with mustaches and beards the background region becomes complex and inhomogeneous we propose in this paper a novel multiclass shapeguided fcm msfcm clustering_algorithm to solve this problem for this new approach one cluster is set for the object ie the lip region and a combination of multiple clusters for the background which generally includes the skin region lip shadow or beards the proper number of background clusters is derived automatically which maximizes a cluster_validity_index a spatial penalty term considering the spatial location_information is introduced and incorporated into the objective function such that pixels having similar color but located in different regions can be differentiated this facilitates the separation of lip and background_pixels that otherwise are inseparable due to the similarity in color experimental results show that the proposed algorithm provides accurate lipbackground partition even for the images with complex background features like mustaches and beards
first formant difference for i and u a crosslinguistic study and an explanation,the value of the first formant of high back and high front vowels u and i has been determined for near minimal pairs in a 30language sample it is found that for 29 out of 30 languages the average of the first formant is higher for high back vowels than for high front vowels and that for 26 out of 28 languages the majority of minimal pairs has a high back vowel with a higher first formant than that of the high front vowel a trend towards smaller differences was found in women but this is not significant in the present data setrnrntwo factors may explain this observation firstly the human vocal tract can only vary the position of gradual and not abrupt transitions of crosssectional area secondly there is a narrow tube just above the glottis the epilarynx tube both factors cause the first formant of high back vowels to be raised but neither is sufficiently important to explain the observed differences on its own
energy efficient_scheduling with individual packet_delay constraints,
an elearning library on the web,the main topic addressed in this paper is how to help learners select some instructive hypermediabased learning resources according to their learning contexts from the web our approach is to provide a digital_library for webbased learning called elearning library which includes learning resource repository local indexing and adaptive navigation support this aims to promote their learning with diverse learning resources involving a certain topic
stability of a class of linear switching_systems with applications to two consensus_problems,in this paper we first establish a stability result for a class of linear switching_systems involving kronecker product the problem is intriguing in that the system matrix does not have to be hurwitz in any time instant we have established the main result by a combination of the lyapunov_stability analysis and a generalized barbalats lemma applicable to piecewise continuous linear_systems as applications of this stability result we study both the leaderless consensus problem and the leaderfollowing consensus problem for general marginally stable linear multiagent_systems under switching network_topology in contrast with many existing results our result only assume that the dynamic graph is uniformly connected
factoring nonnegative matrices with linear programs,this paper describes a new approach based on linear programming for computing nonnegative matrix_factorizations nmfs the key idea is a datadriven model for the factorization where the most salient features in the data are used to express the remaining features more precisely given a data matrix x the algorithm identifies a matrix c that satisfies x  cx and some linear constraints the constraints are chosen to ensure that the matrix c selects features these features can then be used to find a lowrank nmf of x a theoretical analysis demonstrates that this approach has guarantees similar to those of the recent nmf algorithm of arora et al 2012 in contrast with this earlier work the proposed method extends to more general noise models and leads to efficient scalable algorithms experiments with synthetic and real datasets provide evidence that the new approach is also superior in practice an optimized c implementation can factor a multigigabyte matrix in a matter of minutes
threedimensional subband coding of video,we describe and show the results of video_coding based on a threedimensional 3d spatiotemporal subband_decomposition the results include a 1mbps coder based on a new adaptive differential pulse code modulation scheme adpcm and adaptive bit_allocation this rate is useful for video storage on cdrom coding results are also shown for a 384kbps rate that are based on adpcm for the lowest frequency band and a new form of vector_quantization geometric vector_quantization gvq for the data in the higher frequency_bands gvq takes advantage of the inherent structure and sparseness of the data in the higher bands results are also shown for a 128kbps coder that is based on an unbalanced treestructured vector quantizer utsvq for the lowest frequency band and gvq for the higher frequency_bands the results are competitive with traditional video coding_techniques and provide the motivation for investigating the 3d subband framework for different coding_schemes and various applications 
translation of uml state machines to modelica handling semantic issues,modelicaml is a uml_profile that enables modeling and simulation of systems and their dynamic behavior modelicaml combines the power of the omg uml standardized graphical notation for systems and software modeling and the simulation power of modelica this addresses the increasing need for precise and integrated modeling of products containing both software and hardware this article discusses the usage of executable uml state machines for system modeling ie usage of the same formalism for describing the statebased dynamic behavior of physical system components and software moreover it points out that the usage of modelica as an action_language enables an integrated simulation of continuoustime and reactiveeventbased system dynamics the main purpose of this article is however to highlight issues that are identified regarding the uml specification which are experienced with typical executable implementations of uml state machines the issues identified are resolved and rationales for the taken design_decisions are provided
compressed sensing with linear correlation between signal and measurement_noise,existing convex relaxationbased approaches to reconstruction in compressed sensing assume that noise in the measurements is independent of the signal of interest we consider the case of noise being linearly correlated with the signal and introduce a simple technique for improving compressed sensing reconstruction from such measurements the technique is based on a linear_model of the correlation of additive_noise with the signal the modification of the reconstruction_algorithm based on this model is very simple and has negligible additional computational cost compared to standard reconstruction_algorithms but is not known in existing literature the proposed technique reduces reconstruction error considerably in the case of linearly correlated measurements and noise numerical_experiments confirm the efficacy of the technique the technique is demonstrated with application to lowrate quantization of compressed measurements which is known to introduce correlated_noise and improvements in reconstruction error compared to ordinary basis_pursuit denoising of up to approximately 7db are observed for 1bitsample quantization furthermore the proposed method is compared to binary iterative hard thresholding which it is demonstrated to outperform in terms of reconstruction error for sparse signals with a number of nonzero coefficients greater than approximately 110th of the number of compressed measurements
architecture optimization of a 3dof translational parallel_mechanism for machining applications the orthoglide,this paper addresses the architecture optimization of a threedegreeoffreedom translational parallel_mechanism designed for machining applications the design optimization is conducted on the basis of a prescribed cartesian workspace with prescribed kinetostatic performances the resulting machine the orthoglide features three fixed parallel linear joints which are mounted orthogonally and a mobile_platform which moves in the cartesian xyz space with fixed orientation the interesting features of the orthoglide are a regular cartesian workspace shape uniform performances in all directions and good compactness a smallscale prototype of the orthoglide under development is presented at the end of this paper
prototype learning with marginbased conditional loglikelihood loss,the classification_performance of nearest prototype classifiers largely relies on the prototype learning algorithms such as the learning_vector_quantization lvq and the minimum_classification_error mce this paper proposes a new prototype learning algorithm based on the minimization of a conditional loglikelihood loss cll called loglikelihood of margin logm a regularization term is added to avoid overfitting in training the cll loss in logm is a convex function of margin and so gives better convergence than the mce algorithm our empirical study on a large suite of benchmark datasets demonstrates that the proposed algorithm yields higher accuracies than the mce the generalized lvq glvq and the soft nearest prototype classifier snpc
smart_sensor architecture customized for image_processing applications,a system_level_design methodology is applied to the embedded_system_design for a typical sensor_network application face_detection for security purpose the tradeoff analysis is performed for hardware and software implementations of the tasks in this application the best system design is achieved with limited hardware_resources
a problemdriven collaborative approach to eliciting requirements of internetwares,in the software_development most stakeholders cannot clearly and objectively express their needs for the envisioned software_systems in this paper we propose a problemdriven collaborative requirements_elicitation approach with the purpose of helping identify and extract the requirements of the internetwares a complex and new software paradigm the basic idea of our approach is that the requirements of the software_systems should be stated by stakeholders in an objective way ie problemidentifyingsolving way that is first identify the problems existed in the asis problem domain and then find the solutions to the problems the solutions to the problems are the requirements of the envisioned software_systems to this end we propose the structure of problems and a collaborative process for achieving the solutions
a diversitybased method for infrequent purchase decision support in ecommerce,in this paper we propose a method for supporting consumer buying decisions in ecommerce we are advocating the diversitydriven approach to generating alternatives for infrequently purchased products ie computers vehicles etc our method is based upon the wellknown divergenceconvergence principle of problem solving the paper discusses the method based on fuzzy weightedsum model and cluster_analysis the architecture and the operation of the decision_support_system for generating product alternatives the preliminary experiments with the prototype for notebook selection provide some support in favor of our approach over the catalogbased systems
heuristic scheduling of jobs on parallel batch machines with incompatible job families and unequal ready times,this research is motivated by a scheduling problem found in the diffusion and oxidation areas of semiconductor wafer fabrication where the machines can be modeled as parallel batch processors we attempt to minimize total weighted tardiness on parallel batch machines with incompatible job families and unequal ready times of the jobs given that the problem is nphard we propose two different decomposition approaches the first approach forms fixed batches then assigns these batches to the machines using a genetic_algorithm ga and finally sequences the batches on individual machines the second approach first assigns jobs to machines using a ga then forms batches on each machine for the jobs assigned to it and finally sequences these batches dispatching and scheduling rules are used for the batching phase and the sequencing phase of the two approaches in addition as part of the second decomposition approach we develop variations of a time window heuristic based on a decision_theory approach for forming and sequencing the batches on a single machine
brainknowledge a human brain function mapping knowledgebase system,associating fmri image datasets with the available literature is crucial for the analysis and interpretation of fmri data here we present a human brain function mapping knowledgebase system brainknowledge that associates fmri data analysis and literature search functions brainknowledge not only contains indexed literature but also provides the ability to compare experimental data with those derived from the literature brainknowledge provides three major functions 1 to search for brain activation models by selecting a particular brain function 2 to query functions by brain structure 3 to compare the fmri data with data extracted from the literature all these functions are based on our literature extraction and mining module developed earlier hsiao chen chen journal of biomedical informatics 42 912922 2009 which automatically downloads and extracts information from a vast amount of fmri literature and generates cooccurrence models and brain association patterns to illustrate the relevance of brain structures and functions brainknowledge currently provides three cooccurrence models 1 a structuretofunction cooccurrence model 2 a functiontostructure cooccurrence model and 3 a brain structure cooccurrence model each model has been generated from over 15000 extracted medline abstracts in this study we illustrate the capabilities of brainknowledge and provide an application example with the studies of affect brainknowledge which combines fmri experimental results with medline abstracts may be of great assistance to scientists not only by freeing up resources and valuable time but also by providing a powerful tool that collects and organizes over ten thousand abstracts into readily usable and relevant sources of information for researchers
radio_resource_allocation for cellular_networks based on ofdma with qos guarantees,in this paper we address the problem of radio_resource_allocation for qos_support in the downlink of a cellular ofdma system the major impairments considered are cochannel_interference cci and frequency_selective_fading the allocation problem involves assignment of base_stations and subcarriers bit loading and power_control for multiple users we propose a threestage lowcomplexity heuristic algorithm to distribute radio_resources among multiple users according to their individual qos_requirements while at the same time maintaining the qos of already established links in all the cochannel cells the allocation objective is to minimize the total_transmit_power which adds to reducing cci simulation results show a superior performance of the proposed method when compared to classical radio_resource_management techniques our scheme allows us to achieve almost 6 times higher capacity sum data rate than the method based on fdma with power_control at a blocking_probability of 002
spectral texturing for realtime applications,in this sketch we present a new method for rendering largescale highresolution nonrepetitive textures in realtime using multi layer texturing the basic idea of spectral texturing is to construct the nal texture by multiple texture layers where each layer provides a certain range of the spectrum of the textures spatial frequencies alpha channels are used to introduce statistical dependencies between the frequency_bands this approach extends a method called detail texturing which does not use alpha channels to model higher statistical properties of the resulting texture other approaches to generate textures with specied statistical properties like debonet 1997 are not suitable for realtime use and would require storage of the generated texture spectral texturing is very easy to implement runs on all contemporary 3d_graphics cards and is especially suitable for naturalistic textures in realtime applications which are viewed from a large range of distances
streams on wires a query compiler for fpgas,taking advantage of manycore heterogeneous hardware for data processing tasks is a difficult problem in this paper we consider the use of fpgas for data_stream_processing as coprocessors in manycore architectures we present glacier a component library and compositional compiler that transforms continuous_queries into logic circuits by composing library components on an operatorlevel basis in the paper we consider selection aggregation grouping as well as windowing operators and discuss their design as modular elementsrnrnwe also show how significant performance improvements can be achieved by inserting the fpga into the systems data path eg between the network_interface and the host cpu our experiments show that queries on the fpga can process streams at more than one million tuples per second and that they can do this directly from the network removing much of the overhead of transferring the data to a conventional cpu
exploring componentbased approaches in forest landscape modeling,forest management issues are increasingly required to be addressed in a spatial context which has led to the development of spatially explicit forest landscape models the numerous processes complex spatial interactions and diverse applications in spatial modeling make the development of forest landscape models difficult for any single research group new developments in componentbased modeling approaches provide a viable solution componentbased modeling breaks a monolithic model into small interchangeable and binary components they have these advantages compared to the traditional modeling work 1 developing a component is a much smaller task than developing a whole model 2 a component can be developed using most programming_languages since the interface format is binary and 3 new components can replace the existing ones under the same model framework this reduces the duplication and allows the modeling community to focus resources on the common products and to compare results in this paper we explore the design of a spatially explicit forest landscape model in a componentbased modeling framework based on our work on objectoriented forest landscape modeling we examine the representation of the major components and the interactions between them our goal is to facilitate the use of the componentbased modeling approach at the early stage of spatially explicit landscape modeling  2002 elsevier science ltd all rights reserved
extensive feature detection of nterminal protein sorting signals,motivation the prediction of localization sites of various proteins is an important and challenging problem in the field of molecular biology targetp by emanuelsson et al j mol biol 300 10051016 2000 is a neural_network based system which is currently the best predictor in the literature for nterminal sorting signals one drawback of neural_networks however is that it is generally difficult to understand and interpret how and why they make such predictions in this paper we aim to generate simple and interpretable rules as predictors and still achieve a practical prediction accuracy we adopt an approach which consists of an extensive search for simple rules and various attributes which is partially guided by human intuition results we have succeeded in finding rules whose prediction accuracies come close to that of targetp while still retaining a very simple and interpretable form we also discuss and interpret the discovered rules availability an experimental web service using rules obtained by our method is provided at http
optimum color masking matrix determination for digital color platemaking using virtual color samples,abstractrnrnfor highfidelity color_reproduction with digital color platemaking systems it is the most important to determine the color masking matrix which converts the red green and blue intensities of the monitor to cyan magenta yellow and black inks halftone dot area rates with regard to this determination process the author previously developed a color_difference least square method by which the optimum matrix is determined using a simulation with the neugebauer equation and virtual color samples first this paper evaluates the method with actual images next the method is extended for adaptive masking matrix optimization by adapting the matrix to each color_image using black ink it was shown that the average color differences of reproduced images could be reduced to below six when black ink is used as much as possible ie in the case of achromatic printing this masking matrix adaption is a new concept which was impossible by conventional color scanner processing but became possible by using virtual color samples
downlink optimization with interference pricing and statistical csi,in this paper we propose a downlink transmission strategy based on intercell_interference pricing and a distributed_algorithm that enables each base_station bs to design locally its own beamforming vectors without relying on downlink channel_state_information of links from other bss to the users this algorithm is the solution to an optimization problem that minimizes a linear combination of data transmission_power and the resulting weighted intercell_interference with pricing factors at each bs and maintains the required signaltointerferenceplusnoise ratios sinr at user terminals we provide a convergence analysis for the proposed distributed_algorithm and derive conditions for its existence we characterize the impact of the pricing factors in expanding the operational range of sinr targets at user terminals in a powerefficient manner simulation results confirm that the proposed algorithm converges to a networkwide equilibrium_point by balancing and stabilizing the intercell_interference levels and assigning power optimal_beamforming vectors to the bss the results also show the effectiveness of the proposed algorithm in closely following the performance limits of its centralized coordinated_beamforming counterpart
secure authentication watermarking for localization against the hollimanmemon attack,authentication watermarking_schemes using blockwise watermarks for tamper localization are vulnerable to the hollimanmemon attack in this paper we propose a novel method based on the wongs localization scheme proceedings of the ist pic portland to resist this attack a unique image index scheme is used for computing the authentication signature that is embedded in the least significant bitplane of the block the informed detector estimates the correct image index by using the side information about the watermarked image the image index estimation from the fake image can definitely be an alternative to keeping a directory of image indices so it is not necessary to manage the database of image indices for the verification purpose the authenticity measure is defined to quantify the attack severity by taking the connectivity among possible authentic blocks into consideration there are more blocks verified as authentic when this measure is high for a fake image constructed using this attack as such the blocks for a fake image can be chosen from a reduced number of database_images the blocks from any such image are to be connected with each other to maximize the authenticity measure thus the attackers task to generate a fake image of reasonable perceptual quality becomes increasingly difficult with the proposed method there is no loss or ambiguity in localization after the hollimanmemon attack and content tampering in an image the localization_accuracy in the proposed method is demonstrated by the simulation results and is equal to the chosen block size similar to the wongs scheme
directions of external knowledge search investigating their different impact on firm performance in hightechnology industries,purpose  the aim of the paper is to identify the different directions of external knowledge search and to investigate their individual effect on performance at the firm level designmethodologyapproach  the empirical study is based on survey data gathered from two distinct informants of 248 large and mediumsized hightech manufacturing spanish firms in dealing with concerns on simultaneity and reverse causality perceived timelags among dependent and independent variables were introduced quantitative methods based on questionnaire answers were used findings  findings reveal six distinct external search_patterns and indicate that while market sources such as customers and competitors are positively associated with performance knowledge acquired from general information sources other firms beyond the core business and patents and databases have no significant effect moreover knowledge obtained from science and technology organizations and from suppliers displays an inversed ushaped effect o
an empirical evaluation of the studentnet delay_tolerant_network,radio equipped mobile_devices have enjoyed tremendous growth in the past few years we observe that in the near future it might be possible to build a network that routes delaytolerant packets by harnessing user mobility and the pervasive availability of wireless devices such a delaytolerant network could be used to supplement wireless infrastructure or provide service where none is available since mobile_devices in a delaytolerant network forward packets to nearby users the devices can use shortrange radio which potentially reduces device power consumption and radio contention the design of a user mobility based delaytolerant network raises two key challenges determining the connectivity of such a network and determining the latency characteristics and replication requirements of routing_algorithms in such a network to determine realistic contact patterns we collected user mobility data by conducting two user studies we outfitted groups of students with instrumented wirelessenabled pdas that logged pairwise contacts between study participants over a period of several weeks experiments conducted on these traces show that it is possible to form a delaytolerant network based on human_mobility the network has good connectivity so that routes exist between almost all study participants via some multihop path moreover it is possible to effectively route packets with modest replication
local strategy improvement for parity game solving,theproblemofsolvinga paritygameis atthecore ofmanyproblemsin modelcheckingsatisfiability checking and program synthesis some of the best algorithms for solving parity game are strategy improvement algorithms these are global in nature since they require the entire parity game to be present at the beginning this is a distinct disadvantagebecause in many applications one only needs to know which winning region a particular node belongs to an daw itnessing winning strategy may cover only a fractional part of the entire game graph we present a local strategy improvement algorithm which explores the game graph onthefly whilst performing the improvement steps we also compare it empirically with existing global strategy improvementalgorithms and the currently only other local algorithm for solving parity games it turnsout that local strategy improvementcan outperformthese othersby severalordersof magnitude
white matter integrity fiber count and other fallacies the dos and donts of diffusion_mri,diffusionweighted mri dwmri has been increasingly used in imaging neuroscience over the last decade an early form of this technique diffusion tensor imaging dti was rapidly implemented by major mri scanner companies as a scanner selling point due to the ease of use of such implementations and the plausibility of some of their results dti was leapt on by imaging neuroscientists who saw it as a powerful and unique new tool for exploring the structural connectivity of human brain however dti is a rather approximate technique and its results have frequently been given implausible interpretations that have escaped proper critique and have appeared misleadingly in journals of high reputation in order to encourage the use of improved dwmri methods which have a better chance of characterizing the actual fiber structure of white matter and to warn against the misuse and misinterpretation of dti we review the physics of dwmri indicate currently preferred methodology and explain the limits of interpretation of its results we conclude with a list of dos and donts which define good practice in this expanding area of imaging neuroscience
computational investigation of dna packing in confinement,
the international technology alliance in network and information sciences,in may 2006 the us army research laboratory and uk ministry of defense created the international technology alliance the consortium of 26 partners including the arl and mod offers an open research environment in which leading us and uk companies and universities can collaborate see table 1 it will also fuse the best aspects of the us armys collaborative technology alliances and uk mods defense technology centers on an international scale the ita aims to develop flexible distributed and secure decisionmaking procedures to improve networked coalition operations network science is a young discipline we have limited information models and network theories to describe the behavior and scaling of large complex mobile_ad_hoc networks1 moreover you cant understand a coalition networks performance without understanding its cognitive and sociocultural aspects and physical characteristics a key ita goal is to perform basic research in networkcentric coalition decision making across four technical areas network theory security across a system of systems sensor information processing and delivery and distributed coalition planning and decision making 2 we focus on the last area because this is where intelligent_systems will play the biggest role
standoff detection using millimeter and submillimeter wave spectroscopy,the millimeter mm wave and submm wave 30600 ghz frequency band contains fundamental rotational and vibrational resonances of many molecular gases composed of carbon nitrogen oxygen and sulphur the high specificity of rotational spectra to organic molecules affords mm wave spectroscopy having potential use in remotely sensing atmospheric pollutants and the detection of airborne chemicals is important for arms control treaty verification intelligence collection and environmental monitoring this paper considers the sensitivity requirements of radiofrequency receiver systems for measuring mm wave absorptionemission signatures the significance of receiver sensitivity and material optical depth to sensing is highlighted a background to the technology needed for sensing at mm and submm wavelengths then provides the basis for a review of mm wave spectroscopy and its role on profiling the concentrations of trace polar molecules and ionized radicals in the high altitude atmosphere the application of the mm wave spectroscopic technique in ambient conditions is then reviewed and the issues associated with developing the technique for standoff remote sensing is discussed
lets talk about rings,defining the ring topology of a molecule belongs to the central and elementary problems of cheminformatics questions like how many rings does a molecule contain or in how many rings is a specific atom involved are based on such a definition obviously the ring topology must be unique ie it does not depend on atom order chemical meaningful and of reasonable size at most polynomial in the number of atoms for a long period the smallest set of smallest rings sssr was used in cheminformatics applications ignoring a very critical flaw namely that it is not unique even for simple structures other definitions like the set of relevant cycles heal this flaw however they are sometimes chemically not meaningful and can become exponential in size among all attempts made none fulfils all three criteria at the same time 1rnrnrecently we developed a ring definition named unique ring family urf and a corresponding algorithm for calculating it in polynomial time 2 urfs match the common chemical sense of a molecules ring topology the definition results in a unique ring description consisting of a number of ring prototypes which is at most quadratic in the number of atoms in this talk we will present the algorithm benchmarks as well as several examples demonstrating the usefulness of urfs
instability of submicron anisotropic liquid cylinders and jets in magnetic field,the capillary instability of a magnetically anisotropic liquid cylinder and jets such as nematic liquid crystals lc in magnetic fields is considered using an energy approach the boundary problem is solved in the linear approximation of the anisotropy spl chisub a of the magnetic susceptibility spl chi the effect of the anisotropy in the region 1 spl chi spl chisub a spl chisup 2 can be strong enough to counteract and even reverse the tendency of the field to enhance stabilization by increasing the cutoff wave number ksub s beyond the conventional one set by rayleigh it is shown that the elastic effect which is typical of lc is significant on the scale of nanojets where it prevails over the magnetic effect the jet instability is determined by surface tension elasticity and magnetic permeability and anisotropy the relative influence of the elasticity and permeability on the jet stability depends on its radius this is particularly true on the nanoscale
information_retrieval methods for automatic_speech_recognition,in this paper we use information_retrieval ir techniques to improve a speech_recognition asr system the potential benefits include improved speed accuracy and scalability where conventional hmmbased speech_recognition_systems decode words directly our irbased system first decodes subword units these are then mapped to a target word by the ir system in this decoupled system the ir serves as a lightweight datadriven pronunciation model our proposed method is evaluated in the windows live search for mobile wls4m task and our best system has 12 fewer errors than a comparable hmm classifier we show that even using an inexpensive ir weighting scheme tfidf yields a 3 relative error rate reduction while maintaining all of the advantages of the ir approach
a decision making model using soft set and rough set on fuzzy approximation spaces,in modern era of computing there is a need of development in data analysis and decision making most of our tools are crisp deterministic and precise in character but general real life situations contains uncertainties to handle such uncertainties many theories are developed such as fuzzy set rough set rough set on fuzzy approximation spaces etc but all these theories have their own limitations to overcome the limitations the concept of soft set is introduced but soft set also fails if the attributes in the information_system are almost identical rather exactly identical in this paper we propose a decision making model that consists of two processes such as preprocess and postprocess to mine decisions in preprocess we use rough set on fuzzy approximation spaces to get the almost equivalence_classes whereas in postprocess we use soft set techniques to obtain decisions the proposed model is tested over an institutional dataset and the results show practical viability of the proposed research
the influence of parental and peer attachment on internet usage motives and addiction,the impact of parental and peer attachment on four internet usage motives and internet addiction was compared using path modelling of survey data from 1577 adolescent malaysian school students the model accounted for 31 percent of internet addiction score variance lesser parental attachment was associated with greater internet addiction risk psychological escape motives were more strongly related to internet addiction than other motives and had the largest mediating effect upon the parental attachmentaddiction relationship peer attachment was unrelated to addiction risk its main influence on internet usage motives being encouragement of use for social interaction it is concluded that dysfunctional parental attachment has a greater influence than peer attachment upon the likelihood of adolescents becoming addicted to internetrelated activities it is also concluded that the need to relieve dysphoria resulting from poor adolescentparent relationships may be a major reason for internet addiction and that parents fostering of strong bonds with their children should reduce addiction risk
an integer programming based approach for verification and diagnosis of workflows,workflow analysis is indispensable to capture modeling errors in workflow designs while several workflow analysis approaches have been defined previously these approaches do not give precise feedback thus making it hard for a designer to pinpoint the exact cause of modeling errors in this paper we introduce a novel approach for analyzing and diagnosing workflows based on integer programming ip each workflow model is translated into a set of ip constraints faulty control flow connectors can be easily detected using the approach by relaxing the corresponding constraints we have implemented this diagnosis approach in a tool called diagflow which reads and diagnoses xpdl models using an existing open source ip solver as a backend we show that the diagnosis approach is correct and illustrate it with realistic examples moreover the approach is flexible and can be extended to handle a variety of new constraints as well as to support new workflow_patterns results of testing on large process_models show that diagflow outperforms a state of the art tool like woflan in terms of the solution time
a hybrid wireless_network enhanced with multihopping for emergency communications,this paper proposes a hybrid wireless_network scheme enhanced with ad hoc networking for disaster damage assessment and emergency communications the network aims to maintain the connection between a base_station bs and nodes by way of multihopping in the event that a direct link between bs and a node is disconnected the node switches modes from cellular to ad hoc in order to access bs via neighboring nodes a routing_protocol proposed in this paper is capable of building a route using unicastbased route_discovery_process without route request flooding a proposed mac_protocol satisfies the requirement of maintaining accessibility and a short delay even in emergency circumstances we discuss an analytical model based on a markov_process experimental results are shown regarding reachability throughput and delay
a waveletlike filter based on neuron action potentials for analysis of human scalp electroencephalographs,this paper describes the development and testing of a waveletlike filter named the snap created from a neural activity simulation and used in place of a wavelet in a wavelet_transform for improving eeg wavelet_analysis intended for braincomputer interfaces the hypothesis is that an optimal wavelet can be approximated by deriving it from underlying components of the eeg the snap was compared to standard wavelets by measuring support_vector machinebased eeg_classification accuracy when using different waveletsfilters for eeg_analysis when classifying p300 evoked potentials the error as a function of the waveletfilter used ranged from 692 to 1199 almost twofold classification using the snap was more accurate than that with any of the six standard wavelets tested similarly when differentiating between preparation for left or righthand movements classification using the snap was more accurate 1003 error than for four out of five of the standard wavelets 954 to 1200 error and internationally competitive 7 error on the 2001 nips competition test_set phenomena shown only in maps of discriminatory eeg activity may explain why the snap appears to have promise for improving eeg wavelet_analysis it represents the initial exploration of a potential family of eegspecific wavelets
distributed multi class svm for large data sets,data mining_algorithms are originally designed by assuming the data is available at one centralized site these algorithms also assume that the whole data is fit into main memory while running the algorithm but in todays scenario the data has to be handled is distributed even geographically bringing the data into a centralized site is a bottleneck in terms of the bandwidth when compared with the size of the data in this paper for multiclass svm we propose an algorithm which builds a global svm_model by merging the local svms using a distributed approachdsvm and the global svm will be communicated to each site and made it available for further classification the experimental analysis has shown promising results with better accuracy when compared with both the centralized and ensemble method the time complexity is also reduced drastically because of the parallel construction of local svms the experiments are conducted by considering the data sets of size 100s to hundred of 100s which also addresses the issue of scalability
across boundaries of influence and accountability the multiple scales of public sector information_systems,the use of icts in the public sector has long been touted for its potential to transform the institutions that govern and provide social services the focus however has largely been on systems that are used within particular scales of the public sector such as at the scale of state or national government the scale of regional or municipal entity or at the scale of local service providers the work presented here takes aim at examining ict use that crosses these scales of influence and accountability we report on a year long ethnographic investigation conducted at a variety of social service outlets to understand how a shared information_system crosses the boundaries of these very distinct organizations we put forward that such systems are central to the work done in the public sector and represent a class of collaborative_work that has gone understudied
symmetric exponential integrators with an application to the cubic schrdinger equation,in this article we derive and study symmetric exponential integrators numerical_experiments are performed for the cubic schrodinger equation and comparisons with classical exponential integrators and other geometric methods are also given some of the proposed methods preserve the l 2norm andor the energy of the system
limits of homology detection by pairwise sequence comparison,motivation noise in database_searches resulting from random_sequence similarities increases as the databases expand rapidly the noise problems are not a technical shortcoming of the database_search programs but a logical consequence of the idea of homology searches the effect can be observed in simulation experiments results we have investigated noise levels in pairwise alignment based database_searches the noise levels of 38 releases of the swissprot database display perfect logarithmic growth with the total length of the databases clustering of real biological sequences reduces noise levels but the effect is marginal
word boundary decision with crf for chinese_word_segmentation,
business_process development in semanticallyenriched environment,middleware support for business_process_management bpm has met some of the challenges with respect to encoding performance and maintenance of workflows a remaining challenge is complexity business_processes are becoming widely distributed interoperating across a range of inter and intraorganizational behaviours vocabularies and semantics it is important that this semantic complexity is checked and analyzed for optimality and trustworthiness prior to deployment petri_nets are a formal_method that successfully provides behavioural analysis a shortcoming of petri_nets is that the data exchanged between business_activities abstract too far away from the importance of data in actual business_processes this paper addresses this abstraction gap via additional semantic enrichment through a two stage modeldriven approach
distributed dynamic scheduling for endtoend rate guarantees in wireless_ad_hoc_networks,we present a framework for the provision of deterministic endtoend bandwidth guarantees in wireless_ad_hoc_networks guided by a set of local feasibility conditions multihop sessions are dynamically offered allocations further translated to link demands using a distributed time_division_multiple_access tdma protocol nodes adapt to the demand changes on their adjacent links by local conflictfree slot reassignments as soon as the demand changes stabilize the nodes must incrementally converge to a tdma schedule that realizes the global link and session demand allocationwe first derive sufficient local feasibility conditions for certain topology classes and show that trees can be maximally utilizedwe then introduce a converging distributed link_scheduling algorithm that exploits the logical tree structure that arises in several ad_hoc_network applicationsdecoupling bandwidth_allocation to multihop sessions from link_scheduling allows support of various endtoend quality_of_service qos objectives we focus on the maxmin fairness mmf objective and design an endtoend asynchronous distributed_algorithm for the computation of the session mmf rates once the endtoend algorithm converges the link_scheduling algorithm converges to a tdma schedule that realizes these rateswe demonstrate the applicability of this framework through an implementation over an existing wireless_technology this implementation is free of restrictive assumptions of previous tdma approaches it does not require any apriori knowledge on the number of nodes in the network nor even networkwide slot synchronization
evaluating various branchprediction schemes for biomedicalimplant processors,this paper evaluates various branchprediction schemes under different cache configurations in terms of performance power energy and area on suitably selected biomedical workloads the benchmark suite used consists of compression encryption and dataintegrity algorithms as well as real implant applications all executed on realistic biomedical input datasets results are used to drive the microarchitectural design of a novel microprocessor targeting microelectronic implants our profiling study has revealed that under strict or relaxed area constraints and regardless of cache_size the always taken and always nottaken static prediction schemes are in almost all cases the most suitable choices for the envisioned implant processor it is further shown that bimodal predictors with small branchtargetbuffer btb tables are suboptimal yet also attractive solutions when processor idcache sizes are up to 1024kb512kb respectively
a new user authentication_protocol for mobile_terminals in wireless_network,for constructing a ubiquitous network the highspeed wireless_lan wlan attracts attention as an infrastructure for global access however some issues are impeding further adoption of the technology in particular security_problems including user_authentication message compromising password theft connection hijacking etc in this paper we discuss a fast authentication method of mobile ubiquitous terminals in wlan to achieve an efficient access_control between access_points aps and mobile_terminals and sharing of a session_key between terminals we propose a new user secure authentication method and a session key_distribution protocol based on service ticket issuing system
on the basis of the generated foundation of blended elearning  transcendence and integration,learning theory develops in the constant process of transcendence and integration not only from knowledge to people but also its theory approaches are all beyond each other and integration therefore the emergence of blended e_learning is inevitable which is based on the theoretical studys transcendence and integration and as a learning concepts and theories blended e_learning is also bound to each other than with the integration such mutual transcendence and integration of learning theory is just an important basis for the starting point
an improved signcryption scheme and its variation,signcryption is a new cryptographic primitive which simultaneously provides both confidentiality and authenticity this paper proposes an improved signcryption scheme and a variant scheme providing message_recovery the first scheme is revised from an authenticated encryption scheme which has been found to have a securityflaw our scheme solves the securityflaw and provides an additional property called the public_verifiability of the signature the second scheme is a message_recovery type it surpasses most of the current signcryption_schemes on the size of the signcrypted ciphertext that is in our second scheme we require only two parameters r s with r epsi z p  and s epsi z  q  while most signcryption_schemes require three parameters c r s with the additional parameter c epsi z p  this second scheme is modified from an authenticated encryption scheme with message_recovery and surpasses the based authenticated encryption scheme on the property of nonrepudiation of the origin
a new network_architecture with intelligent node in to enhance ieee 80214 hfc networks,in the hybrid fibercoax hfc architecture over several hundreds subscribers in catv community antenna tv network may cause serious collisions in this paper we propose a new network_architecture which using an intelligent node in to stand for a group of subscribers to request the demand resources the in has the ability to reduce the collision probability as well as the collision resolving period the simulation results show that the proposed architecture in terms of throughput buffer delay and fairness outperforms the standard architecture
dynamic_load_balancing schemes for computing accessible surface area of protein molecules,this paper presents an experimental study of dynamic_load_balancing methods for a parallelized solution to a wellknown problem in computational molecular biology computing the accessible surface areas asa of proteins the main contribution is a better understanding of how certain techniques for load estimation and redistribution must be combined carefully for effectiveness and how these combinations need to change during the course of a computation in particular the shrakerupley asa algorithm is implemented and three aspects of dynamic_load_balancing are studied how to estimate load_imbalance the estimation problem when to invoke load redistribution the invocation problem and how to load balance the mapping problem the results in this paper show that a dynamicallyselected mix of algorithms in each category that adapts to changing structure within the protein works better than a static periodic application of a static mix of algorithms
level crossing rate and average fade duration of mrc and egc diversity in ricean fading,the average level crossing rate and average fade duration of the output signal of a maximal ratio combiner mrc and equal gain combiner egc operating on independent ricean fading input branch signals are derived exact closedform results are obtained for mrc diversity while precise expressions for egc diversity are presented with an infinite series method the results are valid for an arbitrary number of independent identically distributed diversity branches isotropic scattering and a specular component perpendicular to the line of motion of the mobile
design and implementation of wire diameter,this paper presents the design and implementation of wire diameter the wire diameter is an open source implementation of diameter based protocol and diameter eap application developed by the wireless internet research  engineering wire laboratory research has shown that traditional radius protocol may suffer performance degradation and data loss in a large system diameter thus was proposed to address the deficiencies in radius both 3gpp and 3gpp2 have adopted diameter as their aaa protocol the wire diameter could be used to authenticate and authorize 8021x supplicant it provides various authentication schemes including eapmd5 eaptls eapttls and peap the wire diameter is developed to be independent of os as much as possible currently it supports linux freebsd and various versions of ms windows it is believed that the wire diameter is the first open source implementation of diameter eap application in the world the source_code can be downloaded freely the wire diameter should be useful for the research community this paper demonstrates the design and implementation of the wire diameter
stability and performance of intersecting aircraft flows under decentralized conflict avoidance rules,this paper considers the problem of two intersecting aircraft flows under decentralized conflict resolution rules considering aircraft flowing through a fixed control volume new air traffic control models and scenarios are defined that enable the study of longterm aircraft flow stability for a class of two intersecting aircraft flows this paper considers conflict scenarios involving arbitrary encounter angles it is shown that aircraft flow stability defined both in terms of safety and performance is preserved under the decentralized conflict resolution algorithm considered it is shown that the lateral deviations experienced by aircraft in each flow are bounded
informational acquisition and cognitive_models,abstractrnrnlife forms must organize information into cognitive_models reflecting the outside environment and in a complex and changing environment a life form must constantly select and organize this mass of information to avoid slipping into a chaotic cognitive state the task of developing and maintaining adaptive cognitive_models can be understood through two processes crucial to regulating the interconnections between environmental elements the inclusion and exclusion of information follows a process designated by p and the process by which cognitive_models change is designated by k higher order concepts are created by reducing the interconnections between elements to a minimal number to avoid cognitive chaos  2004 wiley periodicals inc complexity 93137 2004
contextual motion fieldbased distance for video analysis,in this work we propose a general method for computing distance between video frames or sequences unlike conventional appearancebased methods we first extract motion_fields from original videos to avoid the huge memory requirement demanded by the previous approaches we utilize the bag of motion_vectors model and select gaussian_mixture_model as compact representation thus estimating distance between two frames is equivalent to calculating the distance between their corresponding gaussian_mixture_models which is solved via earth mover distance emd in this paper on the basis of the interframe distance we further develop the distance measures for both full video sequencesrnrnour main contribution is fourfold firstly we operate on a tangent vector field of spatiotemporal 2d surface manifold generated by video motions rather than the intensity gradient space here we argue that the former space is more fundamental secondly the correlations between frames are explicitly exploited using a generative_model named dynamic conditional_random_fields dcrf under this framework motion_fields are estimated by markov volumetric regression which is more robust and may avoid the rank deficiency problem thirdly our definition for video distance is in accord with human intuition and makes a better tradeoff between frame dissimilarity and chronological ordering lastly our definition for frame distance allows for partial distance
minimum effort inverse_kinematics for redundant_manipulators,this paper investigates the use of an infinity norm in formulating the optimization measures for computing the inverse_kinematics of redundant arms the infinity norm of a vector is its maximum absolute value component and hence its minimization implies the determination of a minimum effort solution as opposed to the minimumenergy criterion associated with the euclidean norm in applications where individual magnitudes of the vector components are of concern this norm represents the physical requirements more closely than does the euclidean norm we first study the minimization of the infinitynorm of the joint velocity vector itself and discuss its physical interpretation next a new method of optimizing a subtask criterion defined using the infinitynorm to perform additional tasks such as obstacle_avoidance or joint limit avoidance is introduced simulations illustrating these methods and comparing the results with the euclidean norm solutions are presented
a traceability technique for specifications,traceability in software involves discovering links between different artifacts and is useful for a myriad of tasks in the software life cycle we compare several different information_retrieval techniques for this task across two datasets involving realworld software with the accompanying specifications and documentation the techniques compared include dimensionality_reduction methods probabilistic and information theoretic approaches and the standard vector_space_model
a generic tensionclosure analysis method for fullyconstrained cabledriven parallel_manipulators,cabledriven parallel_manipulators cdpms are a special class of parallel_manipulators that are driven by cables instead of rigid links due to the unilateral property of the cables all the driving cables in a fullyconstrained cdpm must always maintain positive tension as a result tension analysis is the most essential issue for these cdpms by drawing upon the mathematical theory from convex analysis a sufficient and necessary tensionclosure condition is proposed in this paper the key point of this tensionclosure condition is to construct a critical vector that must be positively expressed by the tension vectors associated with the driving cables it has been verified that such a tensionclosure condition is general enough to cater for cdpms with different numbers of cables and dofs using the tensionclosure condition a computationally efficient_algorithm is developed for the tensionclosure pose analysis of cdpms in which only a limited set of deterministic linear equation systems need to be resolved this algorithm has been employed for the tensionclosure workspace_analysis of cdpms and verified by a number of computational examples the computational_time required by the proposed algorithm is always shorter as compared to other existing algorithms
parallel support_vector_machines the cascade svm,we describe an algorithm for support_vector_machines svm that can be parallelized efficiently and scales to very large problems with hundreds of thousands of training vectors instead of analyzing the whole training_set in one optimization step the data are split into subsets and optimized separately with multiple svms the partial results are combined and filtered again in a cascade of svms until the global optimum is reached the cascade svm can be spread over multiple processors with minimal communication_overhead and requires far less memory since the kernel_matrices are much smaller than for a regular svm convergence to the global optimum is guaranteed with multiple passes through the cascade but already a single pass provides good generalization a single pass is 5x  10x faster than a regular svm for problems of 100000 vectors when implemented on a single processor parallel_implementations on a cluster of 16 processors were tested with over 1 million vectors 2class problems converging in a day or two while a regular svm never converged in over a week
estimation of heartsurface potentials using regularized multipole sources,direct inference of heartsurface potentials from bodysurface potentials has been the goal of most recent work on electrocardiographic inverse solutions we developed and tested indirect methods for inferring heartsurface potentials based on estimation of regularized multipole sources regularization was done using tikhonov constrainedleastsquares and multipoletruncation techniques these multipoleequivalent methods mems were compared to the conventional mixed boundaryvalue method bvm in a realistic torso model with up to 20 noise added to bodysurface potentials and spl plusmn1 cm error in heart position and size optimal regularization was used for all inverse solutions the relative error of inferred heartsurface potentials of the mem was significantly less p005 than that of the bvm using zerothorder tikhonov regularization in 10 of the 12 cases tested these improvements occurred with a fourthdegree 24 coefficients or smaller multipole moment from these multipole coefficients heartsurface potentials can be found at an unlimited number of heartsurface locations our indirect methods for estimating heartsurface potentials based on multipole inference appear to offer significant improvement over the conventional direct approach
towards runtime testing in automotive embedded_systems,runtime testing is a common way to detect faults during normal system operation to achieve a specific diagnostic coverage runtime testing is also used in safety critical automotive embedded_systems in this paper we propose a test architecture to consolidate the hardware resource consumption and timing needs of runtime tests and of application and system tasks in a hard realtime embedded_system as applied to the automotive domain special emphasis is put to timing requirements of embedded_systems with respect to hard realtime and concurrent hardware resource accesses of runtime tests and tasks running on the target system
an environment for reconfiguration and execution managenment of flexible radio platforms,this paper presents the flexible radio kernel frk a configuration and execution management environment for hybrid hardwaresoftware flexible radio platform the aim of frk is to manage platform reconfiguration for multimode multistandard operation with different levels of abstraction a high level framework is described to manage multiple mac layers and to enable mac cooperation algorithms for cognitive_radio a lowlevel environment is also available to manage platform reconfiguration for radio operations radio can be implemented using hardware or software elements configuration state is hidden to the highlevel layers offering pseudo concurrency time sharing properties this study presents a global view of frk with details on some specific parts of the environment a practical study with algorithmic description is presented
a simple approach to evaluate the ergodic_capacity and outage_probability of correlated rayleigh diversity channels with unequal signaltonoise ratios,in this article we propose a novel method to derive exact closedform ergodic_capacity and outage_probability expressions for correlated rayleigh_fading_channels with receive_diversity unlike the existing works the proposed method employ a simple approach for the capacity and outage_analysis for receiver diversity channels operating at different signaltonoise ratios depicted in the diagonal elements of matrix  with x being the channel gain vector random_variable of the form yaa  x x is considered novelty of the work resides in the fact that the distribution of ya is accurately determined by employing fourier representation of unit step function followed by complex integration in a straight forward way the ergodic channel_capacity is thus calculated by using the firstorder moment n                  n                    n                  n                  n                    n                      en                      n                      n                        n                          logn                        n                        n                          2n                        n                      n                      n                      yn                      n                      1n                      n                      n                      n                    n                  n                 while the outage_probability for a certain threshold 0is evaluated using n                  n                    n                  n                  n                    n                      n                        n                          n                        n                        n                          0n                        n                        n                          n                            n                              n                            n                            n                              0n                            n                          n                        n                      n                      n                        n                          fn                        n                        n                          yn                          n                          0n                          n                        n                      n                      n                      yn                      n                      dyn                    n                  n                 extensive experiments have been conducted demonstrating the accuracy of the proposed approach
on the feasibility of synthesizing cad software from specifications generating maze router tools in elf,the application of program synthesis techniques to the generation of technologysensitive vlsi physical design tools is described the architecture and implementation of a particular software generator called elf targeted at the generation of maze routing software is described elf strives to meet the demands of the target technology by automatically generating maze router implementations to match the application requirements elf has three key features first a very high level language lacking data structure implementation specifications is used to describe algorithm design styles second applicationspecific expertise about routing and application independent code synthesis techniques are used to guide search among alternative design styles for algorithms and data structures third code_generation is used to transform the resulting abstract descriptions of selected algorithms and data structures into final executable code code_generation is an incremental stepwise refinement process experimental results are presented covering several correct fully functional routers synthesized by elf from varying highlevel specifications results from synthetic and industrial benchmarks are examined to illustrate elfs capabilities 
canonical sequence directed tactics analyzer for computer go games,we present an approach used in csdta canonical sequence directed tactics analyzer that uses canonical sequences joseki in hoping to improve computer go programs we collect 1278 canonical sequences and their deviations in our system instead of trivially matching the current game to the collected sequences we define a notion of similarity to extract the most suitable move from the candidate sequences for the next move the simplicity of our method and its positive outcome make our approach a promising tool to be integrated into a complete computer go program for a foreseeable improvement
an improved approximation to the estimation of the critical f values in best subset regression,variable selection methods are routinely applied in regression_modeling to identify a small number of descriptors which best explain the variation in the response variable most statistical packages that perform regression have some form of stepping algorithm that can be used in this identification process unfortunately when a subset of p variables measured on a sample of n objects are selected from a set of kp to maximize the squared sample multiple_regression coefficient the significance of the resulting regression is upwardly biased the extent of this bias is investigated by using monte carlo simulation and is presented as an inflation factor which when multiplied by the usual tabulated f ratio gives an estimate of the true 5 critical value the results show that selection bias can be very high even for moderatesize data sets selecting three variables from 50 generated at random with 20 observations will almost certainly provide a significant result if the usual tabulated f values are used an interpolation formula is provided for the calculation of the inflation factor for different combinations of n p k four real_data_sets are examined to illustrate the effect of correlated descriptor variables on the degree of inflation
multikmhks a novel multiple kernel_learning algorithm,in this paper we develop a new effective multiple kernel_learning algorithm first we map the input data into m different feature spaces by m empirical kernels where each generated feature_space is taken as one view of the input space then through borrowing the motivating argument from canonical correlation_analysis cca that can maximally correlate the m views in the transformed coordinates we introduce a special term called interfunction similarity loss r ifsi  into the existing regularization framework so as to guarantee the agreement of multiview outputs in implementation we select the modification of hokashyap algorithm with squared approximation of the misclassification errors mhks as the incorporated paradigm and the experimental results on benchmark data sets demonstrate the feasibility and effectiveness of the proposed algorithm named multikmhks
on the rank of random sparse_matrices,we investigate the rank of random symmetric sparse_matrices our main finding is that with high probability any dependency that occurs in such a matrix is formed by a set of few rows that contains an overwhelming number of zeros this allows us to obtain an exact estimate for the corank
maximum utility peer_selection for p2p_streaming in wireless_ad_hoc_networks,in the recent years the peertopeer p2p overlay_network has been a promising architecture for multimedia_streaming services besides its common use for efficient file_sharing by simply increasing the number of peers the p2p overlay_network can meet the high bit rate requirements of multimedia applications optimal peer_selection for newly joining peers is one of the important problems especially in wireless_networks which have limited resources and capacity since the peer_selection process has a direct impact on the throughput of the underlay network and the coexisting unicast_traffic in this paper we tackle the problem of peer_selection for streaming_applications over wireless_ad_hoc_networks we devise a novel peer_selection algorithm which maximizes the throughput of the underlay network and at the same time makes p2p_streaming friendly towards the coexisting data traffic the proposed receiver based rate allocation and peer_selection rps algorithm is derived using the network_utility_maximization num framework the algorithm solves the peer_selection and rate allocation problem distributedly while optimally adapting the medium_access_control mac_layer parameters and is easily extensible to large p2p_networks simulation results show that by using the proper price exchange mechanism the peer receivers can effectively maximize the throughput of the underlay network by intelligently selecting its source peers
timedependent groundwater modeling using spreadsheet,abstractrnrntimedependent groundwater modeling using spreadsheet simulation tgmss model is developed as solution technique it is a practical method that uses spreadsheets instead of the conventional solution_methods all of the aquifer parameters can easily be described in tgmss model the results of tgmss are validated with modflow results showed that tgmss and modflow results were in good agreement in terms of resulting values of hydraulic heads  2005 wiley periodicals inc comput appl eng educ 13 192199 2005 published online in wiley interscience wwwintersciencewileycom doi 101002cae20048
a simulation tool to study highfrequency chest compression energy transfer mechanisms and waveforms for pulmonary disease applications,highfrequency chest compression hfcc can be used as a therapeutic intervention to assist in the transport and clearance of mucus and enhance water secretion for cystic fibrosis patients an hfcc pumpvest and half chestlung simulation with 23 lung generations has been developed using inertance compliance viscous friction relationships and newtons second law the simulation has proven to be useful in studying the effects of parameter variations and nonlinear_effects on hfcc system performance and pulmonary system response the simulation also reveals hfcc waveform structure and intensity changes in various segments of the pulmonary system the hfcc system simulation results agree with measurements indicating that the hfcc energy transport mechanism involves a mechanically induced pulsation or vibration waveform with average velocities in the lung that are dependent upon small air displacements over large areas associated with the vestchest interface in combination with information from lung physiology autopsies and a variety of other lung modeling efforts the results of the simulation can reveal a number of therapeutic implications
distributedinformation neural control the case of dynamic routing in traffic_networks,largescale traffic_networks can be modeled as graphs in which a set of nodes are connected through a set of links that cannot be loaded above their traffic capacities traffic flows may vary over time then the nodes may be requested to modify the traffic flows to be sent to their neighboring nodes in this case a dynamic routing problem arises the decision makers are realistically assumed 1 to generate their routing decisions on the basis of local information and possibly of some data received from other nodes typically the neighboring ones and 2 to cooperate on the accomplishment of a common goal that is the minimization of the total traffic cost therefore they can be regarded as the cooperating members of informationally distributed organizations which in control engineering and economics are called team organizations team optimal control_problems cannot be solved analytically unless special assumptions on the team model are verified in general this is not the case with traffic_networks an approximate resolutive method is then proposed in which each decision maker is assigned a fixedstructure routing function where some parameters have to be optimized among the various possible fixedstructure functions feedforward_neural_networks have been chosen for their powerful approximation capabilities the routing functions can also be computed or adapted locally at each node concerning traffic_networks we focus attention on storeandforward packet switching_networks which exhibit the essential peculiarities and difficulties of other traffic_networks simulations performed on complex communication_networks point out the effectiveness of the proposed method
matching of pdb chain sequences to information in public databases as a prerequisite for 3d functional site visualization,the 3d structures of biomacromolecules stored in the protein_data_bank 1 were correlated with different external biological information from public databases we have matched the feature table of swissprot 2 entries as well interpro 3 domains and function sites with the corresponding 3dstructures omim 4 online mendelian inheritance in man records containing information of genetic disorders were extracted and linked to the structures the exhaustive allagainstall 3d structure comparison of protein structures stored in dali 5 was condensed into single files for each pdb entry results are stored in xml_format facilitating its incorporation into related software the resulting annotation of the protein structures allows functional sites to be identified upon visualization availability httplegergbfdepdbxml
l2 optimization in discrete fir estimation exploiting statespace structure,this paper studies the l2 meansquare optimal_design of discretetime fir estimators a solution procedure which reduces the problem to a static matrix optimization problem admitting a closedform solution is proposed in the latter solution a special statespace structure of the associated matrices is exploited to obtain efficient formulae with the computational complexity proportional to the length of the impulse_response of the estimator unlike previously available leastsquare fir results our treatment does not impose unnecessarily restrictive assumptions on the process dynamics and can handle interpolation constraints on the unit circle which facilitates the inclusion of steadystate performance requirements
application of plane waves for accurate measurement of microwave scattering from geophysical surfaces,the authors utilized the concept of a compact antenna range to obtain planewave illumination to accurately measure scattering properties of simulated sea ice they also made simultaneous measurements using conventional antennas measured scattering coefficients obtained with the planewave system at 10 ghz decreased by about 35 db when the incidence angle increased from 0spl deg to 10spl deg scattering coefficients derived from data collected with the radar system at 135 ghz using conventional farfield antennas decreased by about 20 db over the same angular region this demonstrates that the farfield properties of a widebeam antenna are inadequate for measuring the angular scattering response of smooth surfaces they believe that application of the compact antenna range concept for scattering measurements has a wide range of applications and is the solution to the longstanding problem of how to directly measure scattering consisting of coherent and incoherent components 
an experimental clp platform for integrity constraints and abduction,
a parallelized surface extraction algorithm for large binary image data sets based on an adaptive 3d delaunay subdivision strategy,in this paper we describe a novel 3d subdivision strategy to extract the surface of binary image data this iterative approach generates a series of surface meshes that capture different levels of detail of the underlying structure at the highest level of detail the resulting surface mesh generated by our approach uses only about 10 percent of the triangles in comparison to the marching cube mc algorithm even in settings where almost no image_noise is present our approach also eliminates the socalled staircase effect which voxelbased algorithms like the mc are likely to show particularly if nonuniformly sampled images are processed finally we show how the presented algorithm can be parallelized by subdividing 3d image space into rectilinear blocks of subimages as the algorithm scales very well with an increasing number of processors in a multithreaded setting this approach is suited to process large image data sets of several gigabytes although the presented work is still computationally more expensive than simple voxelbased algorithms it produces fewer surface triangles while capturing the same level of detail is more robust toward image_noise and eliminates the abovementioned staircase effect in anisotropic settings these properties make it particularly useful for biomedical applications where these conditions are often encountered
autovision  flexible processor architecture for videoassisted driving,summary form only given future automotive security_systems will benefit from visual scene analysis based on a fusion of video infrared and radar images today we have already functions like lane departure warning and automatic cruise control acc for pretty well defined driving environments such as highways and primary roads recent research activities concentrate on more complex environments such as city traffic with a wide variety of traffic participants moving in an unpredictable manner eg bikes pedestrians children and even animals and under changing weather and lighting_conditions the itrs semiconductor roadmap for microelectronics forecasts a continued doubling of transistor capacity per chip every 2 to 25 years enabling billion transistor asic designs in the near future multi processor system_on_chip mpsoc solutions with 8 16 or even more standard risc cpu cores megabytes of fast ns access latencies onchip sram memories gigabyte per second interconnect buses or noc network_on_chip meshes highspeed serial ios and last but not least million gate equivalent dedicated hardware_accelerator functions in efpga embedded field_programmable_gate_array logic are becoming reality on a single silicon substrate examples of current research projects shall illustrate our perception on how this tremendous increase in functionality and computational performance per chip area may impact automotive control unit acu architectures for driver assistance applications the autovision processor is a dynamically reconfigurable mpsoc prototype where videospecific pixel processing engines are onthefly loaded or exchanged without interrupting regular system operations for the time being pixel processing engines cover functions such as object edge_detection or luminance segmentation and are implemented as dedicated hardware_accelerators to ensure realtime frame processing capabilities of the autovision processor dynamic replacement of processing engines ensures an automatic and area efficient adaptation to various driving conditions segmented objects are in a subsequent step characterized by means of standard mpeg7 descriptors and entered as search criteria into traffic scene analysis databases goal is to obtain a clean distinction between passenger cars trucks and big rectangular traffic signs and to identify pedestrians or bikers in complex traffic situations the autovision processor project is supported by the german research foundation dfg in the special emphasis research programme reconfigurable_computing
talp at gikiclef 2009,this paper describes our experiments in geographical information_retrieval with the wikipedia collection in the context of our participation in the gikiclef 2009 multilingual task in english and spanish our system called gikitalp follows a very simple approach that uses standard information_retrieval with the sphinx fulltext search_engine and some natural_language_processing techniques without geographical knowdledge
transmit_beamforming for frequencyselective channels,in this paper we propose beamforming schemes for frequencyselective channels with decisionfeedback equalization dfe at the receiver we consider both finite impulse_response fir and infinite_impulse_response iir beamforming filters bffs in case of iir beamforming we are able to derive closedform expressions for the optimum bffs in addition we provide an efficient numerical_method for recursive calculation of the optimum fir bffs simulation and numerical_results for typical gsmedge channels confirm the significant performance gains achievable with beamforming compared to singleantenna transmission and optimized delay diversity
all possible secondorder fourimpedance twostage colpitts oscillators,the authors report all the possible fourimpedance settings that yield a valid secondorder twostage colpitts oscillator these settings are obtained following an exhaustive search conducted on two possible structures of the oscillator modelled through twoport network transmission parameters only valid secondorder cases with a maximum of three reactive elements are reported experimental and spice verification of a selected example using both mos and bjt transistors is given
mining frequent sequential patterns under a similarity constraint,many practical applications are related to frequent sequential pattern mining ranging from web_usage_mining to bioinformatics to ensure an appropriate extraction cost for useful mining tasks a key issue is to push the userdefined constraints deep inside the mining_algorithms in this paper we study the search for frequent sequential patterns that are also similar to an userdefined reference pattern while the effective processing of the frequency constraints is wellunderstood our contribution concerns the identification of a relaxation of the similarity constraint into a convertible antimonotone constraint both constraints are then used to prune the search space during a levelwise search preliminary experimental validations have confirmed the algorithm efficiency
dispatching petroleum products,petroleum products are distributed worldwide from refineries and lube plants to retail outlets and industrial customers proper dispatching of shipments of such products packaged and in bulk may result in significant transportation and inventory cost_savings this work examines the variety of operational environments which exist in dispatching petroleum products and the operations research tools used by oil companies to dispatch such products in addition it identifies gaps where additional research is needed
classification averaging and reconstruction of macromolecules in electron_tomography,electron_tomography provides opportunities to determine threedimensional cellular architecture at resolutions high enough to identify individual macromolecules such as proteins image_analysis of such data poses a challenging problem due to the extremely low signaltonoise ratios that makes individual volumes simply too noisy to allow reliable structural interpretation this requires using averaging techniques to boost the signaltonoise ratios a common practice in electron microscopy single particle analysis where they have proven to be very powerful in elucidating high resolution structure although there are significant similarities in the way data is processed several new problems arise in the tomography case that have to be properly dealt with such problems involve dealing with the missing wedge characteristic of limited angle tomography the need for robust and efficient 3d alignment routines and design of methods that account for diverse conformations through the use of classification we present a framework for reconstruction via alignment classification and averaging of volumes obtained from limited angle electron_tomography providing a powerful tool for high resolution structure determination and description of conformational variability in a biological context
performance_analysis of the conventional complex lms and augmented complex lms_algorithms,recently the augmented complex lms aclms algorithm has been proposed for modeling complexvalued signal relationships in which a widelylinear model can be more appropriate 1 it is not clear however how the behavior of aclms differs from that of the conventional complex lms cclms algorithm in this paper we leverage a recentlydeveloped analysis for the complex lms_algorithm 2 to illuminate the performance relationships between the aclms and cclms algorithms our analysis shows that the aclms algorithm can potentially achieve a lower steadystate meansquared error as compared to that of cclms but the convergence speed of aclms is slowed in the presence of highly noncircular complexvalued input signals an adaptive_beamforming example indicates the utility of the results
extending wireless_sensor_network lifetime through orderbased genetic_algorithm,extending the lifetime is a key issue in wireless_sensor_networks an effective way to extend the lifetime is to partition the sensors into several covers and activate the covers one by one thus the more the covers the longer the lifetime to find the maximum number of covers has been modeled as the set kcover problem in this paper we propose using orderbased genetic_algorithm to solve the set kcover problem for extending the lifetime of wireless_sensor_networks the proposed algorithm needs neither an upper bound nor any assumption about the maximum number of covers experimental results show that the orderbased genetic_algorithm can achieve nearoptimal solutions efficiently
robust neural predictor for noisy chaotic time series prediction,a robust neural predictor is designed for noisy chaotic time series prediction in this paper the main idea is based on the consideration of the bounded uncertainty in predictor input and it is a typical errorsinvariables problem the robust design is based on the linearinparameters esn echo_state_network model by minimizing the worstcase residual induced by the bounded perturbations in the echo state variables the robust predictor is obtained in coping with the uncertainty in the noisy time series in the experiment the classical mackeyglass 84step benchmark prediction task is investigated the prediction performance is studied for the nominal and robust design of esn predictors
minimization strategies for maximally parallel multiset rewriting systems,maximally parallel multiset rewriting systems mpmrs give a convenient way to express relations between unstructured objects the functioning of various computational devices may be expressed in terms of mpmrs eg register machines and many variants of p systems in particular this means that mpmrs are turing universal however a direct translation leads to quite a large number of rules like for other classes of computationally complete devices there is a challenge to find a universal system having the smallest number of rules in this article we present different rule minimization strategies for mpmrs based on encodings and structural transformations we apply these strategies to the translation of a small universal register machine korec 1996 9 and we show that there exists a universal mpmrs with 23 rules since mpmrs are identical to a restricted variant of p systems with antiport rules the results we obtained improve previously known results on the number of rules for those systems
the introduction of the oscar database api oda,the oscar 14 cluster installation toolkit was created by the open cluster group ocg for one particular type of high performance computing hpc cluster oscar is currently one of the widely used cluster installation toolkits it boasts hundreds of thousands of downloads and active mailing lists oscar has expanded its area with several subprojects targeting other types of hpc_clusters each of these projects share a core set of oscar code including the oscar database and its access api oda oscar database api the oda abstraction layer consisting of a database_schema and corresponding api hides a commodity backend database eg mysql 15 because oscar and its subprojects are targeted at new innovative environments including nonhpc environments there are significant issues with managing various configurations of each project for example as we previously showed 8 previous versions of oda were unable to represent the complex evergrowing set of data required to accurately describe the clusters that it manages further its api was extremely complex requiring a steep learning curve for oscar developers therefore we have designed and implemented a new database_schema to deal with these issues this new version of oda has not only resolved the above problems but also as proposed in our previous paper enabled storage and retrieval of various configuration information and encouraged data reuse between the main oscar project and its derivative projects in addition the new version of oda has sped up the oscar installation process this document presents a simpler highly flexible design and implementation of oda slated to be included in oscar v50 it also suggests a blueprint for maintaining the database modules of oda in a systematic organized way
learning for sustainability transition through bounded sociotechnical experiments in personal mobility,abstract a bounded sociotechnical experiment bste attempts to introduce a new technology service or a social arrangement on a small scale many such experiments in personal mobility are ongoing worldwide they are carried out by coalitions of diverse actors and are driven by long term and large scale visions of advancing societys sustainability agenda this paper focuses on the processes of higherorder learning that occur through bstes based on the conceptual frameworks from theories of organizational_learning policyoriented learning and diffusion of innovation we identify two types of learning the first type occurs among the participants in the experiment and their immediate professional networks the second type occurs in the society at large both types play a key role in the societal transition towards sustainable mobility systems two case studies in which the design for sustainability group at technical university of delft has participated provide empirical data for the analysis one
attitude_control of a quadruped trot while turning,during a complete running stride which involves significant periods of flight during which no legs are contacting the ground a quadruped cannot employ static stability techniques instead the corrective forces necessary to maintain dynamic stability must be applied during the short stance intervals inherent to highspeed running because of this complexity and the large coupled forces required to run much of the research on the control of quadruped running has focused on planar systems which are not required to simultaneously control attitude in all three dimensions the 3d trot controller presented here overcomes these and other complexities to control a trot up to 375 ms approximately 3 body lengths per second and turning rates up to 20 degs the biomimetic method of banking into a highspeed turn is also investigated here along with the details of the attitude_control algorithm a set of control principles for highspeed legged motion is presented these principles such as the need to counteract the disturbance of swing leg return and the usefulness of force redistribution during stance are not dependent on a particular scale or actuation scheme and can be applied to a wider range of legged systems
knowledge_representation an approach to artificial_intelligence by tjm benchcapon academic press london 1990 pp 220 1950,
robust fuzzy and recurrent_neural_network motion_control among dynamic obstacles for robot_manipulators,an integration of a fuzzy controller and modified elman neural_networks nn approximationbased computedtorque controller is proposed for motion_control of autonomous manipulators in dynamic and partially known_environments containing moving_obstacles the navigation technique of robot_control using artificial_potential_fields is based on the fuzzy controller the nn controller can deal with unmodeled bounded disturbances and or unstructured unmodeled dynamics of the robot_arm the nn weights are tuned online with no offline learning phase required the stability of the closedloop system is guaranteed by the lyapunov_theory the purpose of the controller which is designed as a neurofuzzy controller is to generate the commands for the servosystems of the robot so it may choose its way to its goal autonomously while reacting in realtime to unexpected events the proposed scheme has been successfully tested the controller also demonstrates remarkable performance in adaptation to changes in manipulator dynamics sensorbased motion_control is an essential feature for dealing with model uncertainties and unexpected obstacles in realtime world systems
nonbinary protograph lowdensity paritycheck codes for space_communications,summaryrnrnprotographbased nonbinary lowdensity paritycheck ldpc_codes with ultrasparse paritycheck matrices are compared with binary ldpc and turbo_codes tcs from space communication standards it is shown that larger coding_gains are achieved outperforming the binary competitors by more than 03 db on the additive_white_gaussian_noise_channel awgn in the short block length regime the designed codes gain more than 1 db with respect to the binary protograph ldpc_codes recently proposed for the next generation uplink standard of the consultative committee for space data systems copyright  2012 john wiley  sons ltd
coding theoretic approach to image_segmentation,this paper introduces multiscale treebased approaches to image_segmentation using rissanens coding theoretic minimum description length mdl principle to penalize overly complex segmentations images are modelled as gaussian random fields of independent pixels with piecewise constant mean and variance this model captures variations in both intensity mean value and texture variance segmentation thus amounts to detecting changes in the mean andor variance one algorithm is based on an adaptive greedy rectangular recursive partitioning scheme the second algorithm is an optimally pruned wedgelet decorated dyadic partitioning we compare the two schemes with an alternative constant variance dyadic cart classification and regression tree scheme which accounts only for variations in mean and demonstrate their performance on sar_images
an empirical bayes approach to inferring largescale gene association networks,motivation genetic networks are often described statistically using graphical_models eg bayesian_networks however inferring the network structure offers a serious challenge in microarray analysis where the sample size is small compared to the number of considered genes this renders many standard algorithms for graphical_models inapplicable and inferring genetic networks an illposed inverse problemrnrnmethods we introduce a novel framework for smallsample inference of graphical_models from gene expression data specifically we focus on the socalled graphical gaussian models ggms that are now frequently used to describe gene association networks and to detect conditionally dependent genes our new approach is based on 1 improved regularized smallsample point estimates of partial correlation 2 an exact test of edge inclusion with adaptive estimation of the degree of freedom and 3 a heuristic network search based on false discovery rate multiple testing steps 2 and 3 correspond to an empirical bayes estimate of the network topologyrnrnresults using computer simulations we investigate the sensitivity power and specificity true negative rate of the proposed framework to estimate ggms from microarray data this shows that it is possible to recover the true network_topology with high accuracy even for smallsample datasets subsequently we analyze gene expression data from a breast cancer tumor study and illustrate our approach by inferring a corresponding largescale gene association network for 3883 genesrnrnavailability the authors have implemented the approach in the r package genets that is freely available from httpwwwstatunimuenchendestrimmergenets from the r archive cran and from the bioconductor websiternrncontact korbinianstrimmerlmude
analytical models for crosstalk excitation and propagation in vlsi_circuits,the authors develop a general methodology to analyze crosstalk effects that are likely to cause errors in deep submicron highspeed circuits they focus on crosstalk due to capacitive coupling between a pair of lines closed_form equations are derived that quantify the severity of these effects and describe qualitatively the dependence of these effects on the values of circuit parameters the risefall times of the input transitions and the skew between the transitions for noise propagation they present a new way for predicting the output waveform produced by an inverter due to a nonsquare wave pulse at its input to expedite the computation of the response of a logic_gate to an input pulse the authors have developed a novel way of modeling such gates by an equivalent inverter the results of their analysis provide conditions that must be satisfied by a sequence of vectors used for validation of designs as well as postmanufacturing testing of devices in the presence of significant crosstalk they present data to demonstrate accuracy of their results including example runs of a test generator that uses these results
land use dynamic monitoring using multitemporal spot data in beijing city from 1986 to 2004,remote sensing dynamic monitoring of land use can detect the change information of land use and update the current land use map which is important for rational utilization and scientific management to land resources this paper discussed the technological procedure of land use dynamic monitoring including the process of remote sensed images the information classification and extraction of remote sensed imagery and analysis of land use changes based on spot imagery data in three periods the paper took beijing city as an example extracted the land use information during 19862004 and the land use changes were required in the period the objectoriented method was used to extract information and contrastive method after classification was used to confirm change zones
voicexml and the w3c speech interface framework,voicexml is a markup language for creating voiceuser interfaces it uses speech and telephone touchtone recognition for input and prerecorded audio and texttospeech synthesis tts for output its based on the world_wide_web consortiums w3cs extensible_markup_language xml and leverages the web paradigm for application development and deployment by having a common language application developers platform vendors and tool providers all can benefit from code portability and reuse the paper discusses voicexml and the w3c speech interface framework
flexible asic shared masking for multiple media processors,asic provides more than an order of magnitude advantage in terms of density speed and power requirement per gate however economic cost of masks and technological deep micron manufacturability trends favor fpga as an implementation platform in order to combine the advantages of both platforms and alleviate their disadvantages recently a number of approaches such as structured asicregular fabrics have been proposed our goal is to introduce an approach that has the same objective but is orthogonal to those already proposed the idea is to implement several asic designs in such a way that they share the datapath memory structure and several bottom layers of interconnect while each design has only a few unique metal layers we identified and addressed two main problems in our quest to develop a cad flow for realization of such designs they are i the creation of the datapath and ii the identification of common and unique interconnects for each design both problems are solved optimally using ilp formulations we assembled a design_flow platform using two new programs and the trimaran and shade tools we quantitatively analyzed the advantages and disadvantages of the approach using the mediabench benchmark suite
buyatbulk network design with protection,we consider approximation_algorithms for buyatbulk network design with the additional constraint that demand pairs be protected against edge or node failures in the network in practice the most popular model used in high speed telecommunication_networks for protection against failures is the socalled 11 model in this model two edge or nodedisjoint paths are provisioned for each demand pair we obtain the first nontrivial approximation_algorithms for buyatbulk network design in the 11 model for both edge and nodedisjoint protection requirements our results are for the singlecable cost model which is prevalent in optical_networks more specifically we present a constantfactor approximation for the singlesink case and an olog 3  n approximation for the multicommodity case these results are of interest for practical applications and also suggest several new challenging theoretical problems
softwaredefined infrastructure and the future central office,this paper discusses the role of virtualization and softwaredefined infrastructure sdi in the design of future application platforms and in particular the future central office co a multitier computing cloud is presented in which resources in the smart edge of the network play a crucial role in the delivery of lowlatency and dataintensive applications resources in the smart edge are virtualized and managed using cloud_computing principles but these resources are more diverse than in conventional data centers including programmable hardware gpus etc we propose an architecture for future application platforms and we describe the savi testbed tb design for the smart edge the design features a novel softwaredefined infrastructure manager that operates on top of openstack and openflow we conclude with a discussion of the implications of the smart edge design on the future co
fisher kernels for handwritten wordspotting,the fisher kernel is a generic framework which combines the benefits of generative and discriminative approaches to pattern_classification in this contribution we propose to apply this framework to handwritten wordspotting given a word image and a keyword generative_model the idea is to generate a vector which describes how the parameters of the keyword model should be modified to best fit the word imagethis vector can then be used as the input of a discriminative classifier we compare the performance of the proposed approach with that of a generative baseline on a challenging realworld dataset of customer letters when the kernel used by the classifier is linear the performance improvement is marginal but the proposed system is approximately 15 times faster than the baseline if we use a nonlinear kernel devised for this task we obtain a 15 relative_reduction of the error but the detector is approximately 15 times slower
nonintrusive load monitoring and diagnostics in power systems,this paper describes a transient event classification scheme system identification techniques and implementation for use in nonintrusive load monitoring together these techniques form a system that can determine the operating schedule and find parameters of physical models of loads that are connected to an ac or dc power distribution_system the monitoring system requires only offtheshelf hardware and recognizes individual transients by disaggregating the signal from a minimal number of sensors that are installed at a central location in the distribution_system implementation details and field tests for ac and dc systems are presented
false alarm reduction by improved filler model and postprocessing in speech keyword spotting,this paper proposes four methods for improving the performance of keyword spotting kws systems keyword models are usually created by concatenating the phoneme hmms and garbage models consist of all phonemes hmms we present the results of investigations involving the use of skips in states of keyword hmms and we focus on improving the hit ratio then for false alarm reduction in kws we model the words that are similar to keywords and we create hmms for highly frequent words these models help to improve the performance of the filler model two postprocessing steps based on phoneme and word probabilities are used on the results of kws to reduce the false alarms we evaluate the performance of the improved keyword spotting in farsdat corpus and compare the approaches the presented techniques depict better performances than the popular kws systems
automated pathfinding tool chain for 3dstacked integrated_circuits practical case study,new technologies for manufacturing 3d stacked ics offer numerous opportunities for the design of complex and effcient embedded_systems but these technologies also introduce many design options at systemchip design level hard to grasp during the complete design cycle because of the sequential nature of current design practices designers are often forced to introduce design margins to meet required specications resulting in suboptimal designs in this paper we introduce new design methodology and practical tool chain called pathfinding flow that can help designers to easily tradeoff between different system_level_design choices physical design andor technology options and understand their impact on typical design parameters such as cost performance and power proposed methodology and the tool chain will be demonstrated on a practical case study involving fairly complex multiprocessor systemonchip using networkonchip for communication medium with this example we will show how highlevel synthesis can be used to quickly move from highlevel to rtl models necessary for accurate physical prototyping for both computation and communication we will also show how the possibility of design iteration through the mechanism of feedback based on physical information from physical prototyping can improve design performance finally we will show how we can move in no time from traditional 2d to 3d design and how we can measure benets of such design choice
managing traceability information in manufacture,in this paper an approach to design information_systems for traceability is proposed the paper applies gozinto graph modelling for traceability of the goods flow a gozinto graph represents a graphical listing of raw materials parts intermediates and subassemblies which a process transforms into an end product through a sequence of operations next the graphical listing has been translated into a reference data model that is the basis for designing an information_system for tracking and tracing materials that are modelled this way represent production andor purchase lots or batches the composition of a certain end product is then represented through modelling all its constituent materials along with their intermediate relations by registering all relations between subordinate and superordinate material lots a method of tracking the composition of the end product is obtained when the entire sequence of operations required for manufacturing an end product adheres to this registering of relations a multilevel bill of lots can be compiled that bill of lots then provides the necessary information to determine the composition of a material item out of component items these composition data can be used to recall any items having consumed a certain component of specific interest eg deficient but also to certify product quality or to proactively adjust production processes to optimise the product quality in relation to its production characteristics eg scarcity costs or time
phd forum calibrating and using the global network of outdoor webcams,the vast imaging resources available via the internet are underutilized we propose to lay the foundation for the use of cameras attached to the internet also known as webcams as free and flexible sensors developing an understanding of the relationship between signals in the world and the image variations they cause is critical to this effort we use this understanding to develop methods to calibrate webcams to estimate scene properties and to report the weather
acquiring a holistic picture the 4screens webbased simulator helping students to unify behaviours of electronic_systems,engineering knowledge is complex therefore discipline concepts cannot usually be introduced to students all at once normally individual models and behaviours are taught in several courses by different teaching staff and in separate years of study this often results in inadequate comprehension of the disciplinepsilas ldquobig picturerdquo  that is the interrelationship of various concepts models and behaviours students studying electronic circuit design for example often overlook important aspects of linear circuit performance their knowledge of the effects of steadystate and transient responses requires improved unification simple computerbased simulators can significantly help students to establish welldeveloped holistic views of systems in many engineering disciplines the 4screens webbased simulator has helped hundreds of students in becoming more proficient with electronics systems it has been developed by a group of undergraduates to make the four screens model easy to utilise a student using the simulator can simultaneously display four important characteristics of an electronic system on a computer screen its algebraic transfer function hs systempsilas polezero plot its bode plots of magnitude and phase as well as a graph of its time response to a unit step questionnaire responses and test performances over the last seven years confirm the effectiveness of the four screens and the 4screens webbased simulator in improving student_learning
benefits derived from restructuring the practical component of an introductory course in electronic communication_systems,in an effort to address students complaints regarding the tedious and frustrating nature of the practical component of the departments introductory communication course we embarked on a restructuring exercise the restructuring of this component of the course shows promise based on preliminary results the students are obtaining higher grades and gave the practical component a more favorable rating additionally the restructuring exercise allows for the addition of several more experiments enabling us to cover a broader portion of the course in the practical component this paper looks at the changes that were made in the restructuring exercise and present preliminary results on the improvement in students grade which can be attributed to this exercise over 16 of students are now obtaining grades over 80 compare to 0 before the restructuring exercise results from students evaluations of the practical component of the course before and after the restructuring exercise is also presented which shows an effective 24 improvement in the student rating
minimizing data and synchronization costs in oneway communication,minimizing communication and synchronization costs is crucial to the realization of the performance potential of parallel computers this paper presents a general technique which uses a global dataflow framework to optimize communication and synchronization in the context of the oneway communication_model in contrast to the conventional sendreceive messagepassing communication_model oneway communication is a new paradigm that decouples message transmission and synchronization in parallel_machines with appropriate lowlevel support this may open up new opportunities not only to further optimize communication but also to reduce the synchronization overhead we present optimization_techniques using our framework for eliminating redundant data communication and synchronization operations our approach works with the most general data alignments and distributions in languages like high performance fortran hpf and uses a combination of the traditional dataflow analysis and polyhedral algebra empirical results for several scientific benchmarks on a cray t3e multiprocessor machine demonstrate that our approach is successful in reducing the number of data communication and synchronization messages thereby reducing the overall execution times
pattern mining in visual concept streams,pattern mining_algorithms are often much easier applied than quantitatively assessed in this paper we address the pattern evaluation problem by looking at both the capability of models and the difficulty of target concepts we use four different data_mining models frequent_itemset_mining kmeans clustering hidden_markov_model and hierarchical hidden_markov_model to mine 39 concept streams from the a 137video broadcast_news collection from trecvid2005 we hypothesize that the discovered patterns can reveal semantics beyond the input space and thus evaluate the patterns against a much larger concept space containing 192 concepts defined by lscom results show that hhmm has the best average prediction among all models however different models seem to excel in different concepts depending on the concept prior and the ontological relationship results also show that the majority of the target concepts are better predicted with temporal or combination hypotheses and there are novel concepts found that are not part of the original lexicon this paper presents the first effort on temporal pattern mining in the large concept space there are many promising directions to use concept mining to help construct better concept detectors or to guide the design of multimedia ontology
adaptive goals for selfadaptive service_compositions,service_compositions need to continuously self adapt to cope with unexpected failures in this context adaptation becomes a fundamental requirement that must be elicited along with the other functional and non functional_requirements beside modelling effective adaptation also demands means to trigger it at runtime as soon as the actual behavior of the composition deviates from stated requirements this paper extends traditional goal_models with adaptive goals to support continuous adaptation goals become live runtime entities whose satisfaction level is dynamically updated furthermore boundary infringement triggers adaptation capabilities the paper also provides a methodology to trace goals onto the underlying composition assess goals satisfaction at runtime and activate adaptation consequently all the key elements are demonstrated on the definition of the process to control an advanced washing machine
integrating replenishment decisions with advance demand information,there is a growing consensus that a portfolio of customers with different demand lead times can lead to higher more regular revenues and better capacity utilization customers with positive demand lead times place orders in advance of their needs resulting inadvance demand information this gives rise to the problem of finding effective inventory control policies under advance demand information we show that statedependent  s s and basestock policies are optimal for stochastic inventory systems with and without fixed costs the state of the system reflects our knowledge of advance demand information we also determine conditions under which advance demand information has no operational value a numerical study allows us to obtain additional insights and to evaluate strategies to induce advance demand information
an adaptive fair sampling algorithm based on the reconfigurable counter arrays,
a new construction for ntrack d k codes with redundancy,digital magnetic and optical storage_systems employing nrzi recording use d k codes the dparameter specifies the minimum number of 0s occurring between 1s while the kparameter specifies the maximum number of 0s between ls the ntrack dk codes denoted as dkn codes are extensions of d k codes for use in multipletrack systems instead of imposing each track to individually satisfy both constraints dkn codes satisfy the dconstraint in each track individually while relaxing the kconstraint by allowing it to be satisfied jointly by the multiple tracks although dkn codes can provide significant capacity increases over d k codes they suffer from the fact that a single faulty track can cause loss of synchronization and hence loss of the data on all tracks orcutt and marcellin see ieee trans on inform theory sept 1993 introduced ntrack dk codes with a redundancy of r denoted as dknr codes which allow for r faulty tracks by mandating that all subsets of nr tracks satisfy the joint kconstraint we propose a new method to construct dk n r codes these codes have simple encoding and decoding schemes gain a large part of the capacity increase possible when using dk nr codes and are considerably more robust to faulty tracks 
structure and reaction based evaluation of synthetic accessibility,de novo design systems provide powerful methods to suggest a set of novel structures with high estimated binding affinity one deficiency of these methods is that some of the suggested structures could be synthesized only with great difficulty we devised a scoring method that rapidly evaluates synthetic accessibility of structures based on structural complexity similarity to available starting materials and assessment of strategic bonds where a structure can be decomposed to obtain simpler fragments these individual components were combined to an overall score of synthetic accessibility by an additive scheme the weights of the scoring function components were calculated by linear regression_analysis based on accessibility scores derived from medicinal chemists the calculated values for synthetic accessibility agree with the values proposed by chemists to an extent that compares well with how chemists agree with each other
evolution of protection technologies in metro core optical_networks,the market of metro optical_networking has increased rapidly over the last few years traditional telecommunication infrastructure has an emphasis on longhaul optical_transmission with ultra broadband capacity relying mostly on large pure dense_wavelength_division_multiplexing dwdm systems today however metro core optical_networks take the major role in provisioning local access services and interconnecting service points of presences pops with longhaul transmission this represents a pivotal point in business operations of data communication services for service providers and large enterprises in addition the upper layer data services completely leans upon the substrate wavelength communication and hence the survivability and reliability issues in the optical domain are now becoming crucial topics this paper provides a detailed discussion around the development process of protection technologies in metro core optical transport infrastructure
packet radio and the factory of the future,progress is reported in the design and analysis of spreadspectrum packet radio_networks for the factory of the future this progress includes the accurate analysis of the probability of packet success in a directsequence spreadspectrum communication channel and analytical_results on the transient behavior of packet radio_networks the first use of these networks will be to provide communications between a transport system controller and a fleet of autonomous guided vehicles 
manufacturing decision making with factor,the factor system is designed to support the effective management of the capacity of manufacturing organization this philosophy is best described as total capacity management tcm the tcm fundamental principle suggests that through a thorough understanding of a systems capacity and the ability to control that capacity a manufacturing system can profitably and predictably deliver quality products to its customers this tutorial covers the basic concepts of factor factor has been applied to engineering_design scheduling and planning problems within many manufacturing organizations topics covered include the factor modeling constructs integration with existing production data the use of factor for schedule creation and adjustment factoraim and new enhancements to the products
learning with technology using discussion forums to augment a traditionalstyle class,there is considerable evidence that using technology as an instructional tool improves student_learning and educational outcomes hanna  de nooy 2003 in developing countries preuniversity education focuses on memorization although meting the mission of aust requires students to manage technology and to think more independently this study examines the impact of incorporating a discussion forum on the achievement of university students enrolled in a distance_education course educational_technology department at ajman university of science and technology aust united arab emirates the study was conducted with 34 students divided into two sections one a treatment group and one a control group both sections were exposed to the same teaching techniques covering the same course material on distance_education four weeks after the course had commenced they were given the same teacher constructed test however after the first test the treated group was exposed to the use of a world_wide_web www interactive discussion forum at the end of the semesterlong treatment period a final test was given to both groups and student scores were analyzed for any statistically significant difference questionnaires and interviews were also conducted to see if students had enjoyed the experience the results of the study indicated that students in both groups showed learning improvement over the course of one semester but discussion forums had an obvious impact on student achievement and attitude in distance_learning educational_technology course
cchmm_prof a hmmbased coiledcoil predictor with evolutionary information,motivation the widespread coiledcoil structural motif in proteins is known to mediate a variety of biological interactions recognizing a coiledcoil containing sequence and locating its coiledcoil domains are key steps towards the determination of the protein structure and function different tools are available for predicting coiledcoil domains in protein sequences including those based on positionspecific score matrices and machine_learning_methods results in this article we introduce a hidden_markov_model cchmm_prof that exploits the information contained in multiple sequence alignments profiles to predict coiledcoil regions the new method discriminates coiledcoil sequences with an accuracy of 97 and achieves a true positive rate of 79 with only 1 of false positives furthermore when predicting the location of coiledcoil segments in protein sequences the method reaches an accuracy of 80 at the residue level and a best persegment and perprotein efficiency of 81 and 80 respectively the results indicate that cchmm_prof outperforms all the existing tools and can be adopted for largescale genome annotation availability the dataset is available at httpwwwbiocompunibo itlisacoiledcoils the predictor is freely available at httpgpcr biocompuniboitcgipredictorscchmmprofpred_cchmmprofcgi contact pierobiocompuniboit
performance of local area network_protocols for hard realtime applications,simulation experiments show that the token ring protocol gave a lower average message delay at low transfer rates but the token bus protocol gave a better overall performance for applications where only average delay is of interest on the other hand in hard realtime systems the criterion of importance is not the average message delay but the maximum message delay and the ability to meet deadlines slotted ring in this case is a much better protocol than the others because of its low maximum message delay and more predictable message delay because of this and because the average performance of the slotted ring remains good as the size or the transfer rate of the network increases the slotted ring protocol is preferred over the token ring and token bus protocols for hard realtime systems 
a simple algorithm for decomposing convex structuring elements,a finite subset of zsup 2 is called a structuring_element the paper presents a new and simple algorithm for decomposing a convex structuring_element as a sequence of minkowski additions of a minimum number of subsets of the elementary square ie the 3spl times3 square centered at the origin besides its simplicity the advantage of this algorithm over some known algorithms is that it generates a sequence of non necessarily convex subsets which means subsets with smaller cardinality and consequently faster implementation of the corresponding dilations and erosions the algorithm is based on algebraic and geometrical properties of minkowski additions theoretical analysis of correctness and computational_time complexity are also presented
a bist pattern generator design for nearperfect fault_coverage,a new design methodology for a pattern generator is proposed formulated in the context of onchip bist the design methodology is circuitspecific and uses synthesis techniques to design bist generators the pattern generator consists of two components a pseudorandom pattern generator like an lfsr or preferably a glfsr and a combinational logic to map the outputs of the pseudorandom pattern generator this combinational logic is synthesized to produce a given set of target patterns by mapping the outputs of the pseudorandom pattern generator it is shown that for a particular cut an areaefficient combinational logic block can be designedsynthesized to achieve 100 or almost 100 percent single stuckat fault_coverage using a small number of test the this method is significantly different from weighted pattern generation and can guarantee testing of all hardtodetect faults without expensive test point insertion experimental results on common benchmark netlists demonstrate that the fault_coverage of the proposed pattern generator is significantly higher compared to conventional pattern generation techniques the design technique for the logic mapper is unique and can be used effectively to improve existing pattern generators for combinational logic and scanbased bist structures
model of cardiac tissue as a conductive system with interacting pacemakers and refractory time,the model of the cardiac tissue as a conductive system with two interacting pacemakers and a refractory time is proposed in the parametric space of the model the phase locking areas are investigated in detail the obtained results make possible to predict the behavior of excitable systems with two pacemakers depending on the type and intensity of their interaction and the initial phase comparison of the described phenomena with intrinsic pathologies of cardiac rhythms is given
the systolic pixel a visible surface algorithm for vlsi,abstractrnrnthe systolic pixel or spixel is a novel_architecture for an intelligent pixelbased graphics database for geometricsolid models an algorithm is described which performs visible surface calculations for any complexity of coloured 3dimensional 3d surface and which structures geometricsolid model data in a natural way the algorithmarchitecture of the spixel features a simple set of priority rules acting upon data in nearest neighbour locations and a simple set of movement rules of data to nearest neighbour locations the spixel is constructed out of identical functional units these features are attractive for an implementation of the algorithm in very large scale integration vlsi
generating case markers in machine_translation,we study the use of rich syntaxbased statistical models for generating grammatical case for the purpose of machine_translation from a language which does not indicate case explicitly english to a language with a rich system of surface case markers japanese we propose an extension of nbest reranking as a method of integrating such models into a statistical mt system and show that this method substantially outperforms standard nbest reranking our best performing model achieves a statistically significant improvement over the baseline mt system according to the bleu metric human_evaluation also confirms the results
softlimiter receivers for coded dsdpsk systems,the performance of a softlimiter metric and a quantized softlimiter metric is evaluated for coded dsdpsk direct sequencedifferential phase_shift_keying in the presence of worst case pulse jamming and background_noise the metrics are easy to implement and do not require jammer state information instead they rely on the use of receiver thresholds which must be adjusted according to the code rate and the received bitenergytobackgroundnoise ratio the performance of the metrics is evaluated by using the cutoff rate criterion and a number of specific convolutional and block_codes it is shown that the metrics can offer a significant softdecision decoding gain and can perform to within 0515 db of the maximumlikelihood softdecision metric with perfect jammer state information 
technology dependence in function_point_analysis a case study and critical review,because function_point_analysis fpa has now been in use for a decade and in spite of its increasing popularity has met with some recent criticisms it is time to review how appropriate it still is for todays technologies a critical review of the fpa approach examines in particular the pioneering and continuing work of albrecht and more recent work by symons technological dependencies in fpatype metrics are identified and a general model for deriving a new fpatype metric for a new software technology is given a model for the calibration of fpatype metrics for new technologies in terms of a reference technology is also presented such calibration is essential for comparative productivity studies the role of module estimation in exposing parts of the anatomy of the fpa approach is investigated the derivation and calibration models are applied to a significant case study in which a new fpatype metric suited to a particular software_development technology is derived calibrated and compared with other published versions of fpa metrics
pruning recurrent_neural_networks for improved generalization performance,the experimental results in this paper demonstrate that a simple pruningretraining method effectively improves the generalization performance of recurrent_neural_networks trained to recognize regular_languages the technique also permits the extraction of symbolic knowledge in the form of deterministic finitestate automata dfa which are more consistent with the rules to be learned weight decay has also been shown to improve a networks generalization performance simulations with two small dfa spl les10 states and a large finitememory machine 64 states demonstrate that the performance improvement due to pruningretraining is generally superior to the improvement due to training with weight decay in addition there is no need to guess a good decay rate 
adaptive design of digital filters,in this paper we present a novel technique for the design of fir and iir_digital_filters the design approach begins with the specification of a discrete set of arbitrary magnitude and phase characteristics which describe a desired filter response these frequency_domain characteristics are used to create an ideal pseudofilter whose impulse_response is unknown and possibly noncausal but whose inputoutput characteristics can be determined for a finite sum of sinusoids timedomain techniques common to adaptive_system identification are then used to identify a realizable fir or iir digital filter which best matches the pseudofilter the advantages of this method include the ability to specify response at arbitrarilyspaced frequencies to use arbitrary cost weighting and to apply possibly nonlinear constraints to the range of the filter_coefficients
experimental examination in simulated interactive situation between people and mobile_robot with preliminaryannouncement and indication function of upcoming operation,this paper presents the result of the experimental examination by passing each other and positional prediction in simulated interactive situation between people and mobile_robot we have developed four prototype robots based on four proposed methods for preliminarily announcing and indicating to people the speed and direction of upcoming movement of mobile_robot moving on twodimensional plane we observed significant difference between when there was a preliminaryannouncement and indication pai function and when there was not even in each experiment therefore the effect of preliminaryannouncement and indication of upcoming operation was declared in addition the feature and effective usage of each type of preliminaryannouncement and indication method were clarified that is the method of announcing state of operation just after the present is effective when a person has to judge to which direction he should get on immediately due to the feature that simple information can be quickly transmitted the method of indicating operations from the present to some future time continuously is effective when a person wants to avoid contact or collision surely and correctly owing to the feature that complicated information can be accurately transmitted we would like to verify the result in various conditions such as the case that traffic lines are obliquely crossed
opportunistic_beamforming over rayleigh_channels with partial side information,in recent years diversity techniques have evolved into highly attractive technology for wireless_communications in different forms for instance the channel fluctuations of the users in a network are exploited as multiuser_diversity by scheduling the user with the best signaltonoise ratio snr when fading is slow beamforming at a multiple_antenna transmitter is used to induce artificial channel fluctuations to ensure multiuser_diversity in the network such a beamforming scheme is called opportunistic_beamforming since the transmitter uses random beamforming to artificially induce opportunism in the network 1 opportunism requires a large number of users in the system in order to reach the performance of the true beamforming that uses perfect_channel_state_information csi in this paper we investigate the benefit of having partial_csi at an opportunistic transmitter in the investigation we focus on the maximum normalized snr scheduling where users feedback consists of snr relative to its channel gain we show that opportunism can be beneficially used to increase the average throughput of the system simulations support the analytical average throughput results obtained as the amount of csi and the number of users vary
an ilp_formulation for system level throughput and power optimization in multiprocessor soc architectures,systemlevel low_power scheduling techniques are required for optimizing the performance and power of embedded applications that are mapped to multiprocessor systemonchip soc architectures in this paper we present an integer linear programming ilp_formulation that combines loop transformations pipelining and unrolling and systemlevel low_power optimization_techniques dynamic voltage scaling dvs and power_management dpm to minimize the power consumption while satisfying the period and deadline constraints of the application we also present three modifications that relax one or more constraints in the optimal formulation in order to obtain smaller run times we present experimental analysis by applying the formulations on an mpeg decoder algorithm all results are compared against two existing techniques our formulations result in large systemlevel power reductions max 482 min 1592 avg 319 the modified ilp formulations result in exponential decrease in runtimes and a corresponding linear degradation in the result quality
enhancement of semantics in cbir,although much research has been done in the area of content_based_image_retrieval cbir little progress has been made to fully implement an engine solely based on the search of image content this paper examines one of the basic problems in pattern_recognition which highlights the difficulty in the area of content understanding in cbir ie the inability of current systems to fully incorporate low level features of image such as intensity colour texture shape and spatial constraints characteristics with the high level features such as semantic content to further the development of content based image_processing semantic algorithms should be combined with low level features and be used to process the image objects
masish a database for gene expression in maize seeds,grass seeds are complex organs composed by multiple tissues and cell types which develop coordinately to produce a viable embryo the identification of genes involved in seed development is of great interest but systematic spatial analyses of gene expression on maize seeds at the cell level have not yet been performed masish is an online database holding information for gene expression spatial patterns in maize seeds based on in situ hybridization experiments the webbased query interface allows the execution of gene queries and provides hybridization images published references and information of the analyzed genes availability httpmasishuabcat contact cvsgmpcidcsices the maize kernel is classified botanically as a caryopsis in consequence it is a fruit composed by one seed and the remnants of the seed coats and nucellus and is permanently enclosed in the pericarp the endosperm occupies most of the seed and is basically a storage organ that accumulates starch and proteins the aleurone layer is part of the endosperm and consists in a continuous layer of large cubical cells which accumulate protein and lipid granules and surrounds most of the endosperm in the area of the pedicel which connects the seed to the mother plant the cells adopt a special morphology typical of transfer cells and form the basal transfer cell layer the embryo consists of an embryonic axis and a single cotyledon which is called the scutellum the embryo axis is formed by the plumule covered by the coleoptile and the radicle covered by coleorhiza all these organs are almost completely surrounded by the scutellum an organ whose major function is to accumulate nutrient reserves mainly lipids and proteins a single layer of cells directly in contact with the endosperm which is called the scutellar epithelium is important in the digestion and transport of the nutrients from the endosperm to the embryo axis during germination both endosperm and embryo derive from the fusion of gametes but while the embryo is derived from the fertilized egg triploid endosperm is derived from fertilized polar nuclei surrounding the endosperm and embryo lays the pericarp a protective organ derived from maternal tissues more information at httpmasishuabcatmasishimagesmaizeseedanatomypdf full genome sequencing allows the identification of the complete catalog of genes in a species however the roles of a high proportion of these genes remain unknown the description of
unified blind method for multiimage superresolution and singlemultiimage blur deconvolution,this paper presents for the first time a unified blind method for multiimage superresolution misr or sr singleimage blur deconvolution sibd and multiimage blur deconvolution mibd of lowresolution lr images degraded by linear spaceinvariant lsi blur aliasing and additive_white_gaussian_noise awgn the proposed approach is based on alternating minimization am of a new cost function with respect to the unknown highresolution hr image and blurs the regularization term for the hr image is based upon the hubermarkov random field hmrf model which is a type of variational integral that exploits the piecewise smooth nature of the hr image the blur estimation process is supported by an edgeemphasizing smoothing operation which improves the quality of blur estimates by enhancing strong soft edges toward step edges while filtering out weak structures the parameters are updated gradually so that the number of salient edges used for blur estimation increases at each iteration for better performance the blur estimation is done in the filter domain rather than the pixel_domain ie using the gradients of the lr and hr images the regularization term for the blur is gaussian l2 norm which allows for fast noniterative optimization in the frequency_domain we accelerate the processing time of sr reconstruction by separating the upsampling and registration processes from the optimization procedure simulation results on both synthetic and reallife images from a novel computational imager confirm the robustness and effectiveness of the proposed method
synthesis of multiqudit hybrid and dvalued quantum logic circuits by decomposition,recent research in generalizing quantum computation from 2valued qudits to dvalued qudits has shown practical advantages for scaling up a quantum computer a further generalization leads to quantum computing with hybrid qudits where two or more qudits have different finite dimensions advantages of hybrid and dvalued gates circuits and their physical realizations have been studied in detail by muthukrishnan and stroud multivalued_logic gates for quantum computation phys rev a 62 2000 052309 10 daboul et al quantum_gates on hybrid qudits j phys a math gen 36 2003 25252536 5 and bartlett et al quantum encodings in spin systems and harmonic oscillators phys rev a 65 2002 052316 17 in both cases a quantum computation is performed when a unitary evolution operator acting as a quantum logic_gate transforms the state of qudits in a quantum system unitary operators can be represented by square unitary matrices if the system consists of a single qudit then tilma et al generalized euler angle parameterization for sun j phys a math gen 35 2002 1046710501 15 have shown that the unitary evolution matrix gate can be synthesized in terms of its euler angle parametrization however if the quantum system consists of multiple qudits then a gate may be synthesized by matrix decomposition techniques such as qr_factorization and the cosinesine decomposition csd in this article we present a csd based synthesis method for n qudit hybrid quantum_gates and as a consequence derive a csd based synthesis method for n qudit gates where all the qudits have the same dimension
point location in zones of k flats in arrangements,abstract   let  a  h  be the arrangement of a set h of n hyperplanes in  d space a  k flat is a  k dimensional affine subspace of  d space the  zone  of a  k flat f with respect to h is the set of all faces in  a  h  that intersect f this paper we study some problems on zones of  k flats our most important result is a data structure for point location in the zone of a  k flat this structure uses   o  n        d  2  e    n     ke      preprocessing time and space and has a query time of olog 2   n  we also show how to test efficiently whether two flats are visible from each other with respect to a set of hyperplanes then point location in m faces in arrangements is studied our data structure for this problem has size   o  n        d  2  e    m        d  2    d       and the query time is olog 2   n 
dynamic optimal battery array management in high energy density fuel cellbattery hybrid power source,the goal of this paper is to address the problem of dynamic optimal battery array management to extend the life of battery array used in the hybrid power source unlike previous efforts which mainly solve the problem using the offline optimization our design addresses the problem using the idea of feedback the proposed approach can handle the possible dynamic uncertainty of the hybrid power source introduced by battery failure or insert of a new battery the detailed battery model and the converter model are derived to facilitate development of dynamic optimal management algorithm the other goal of this paper is to call possible attention of the control community in potential contribution for newest smart grid_technology
expectationmaximization approach to boolean factor analysis,methods for hidden structure of highdimensional binary data discovery are one of the most important challenges facing machine_learning community researchers there are many approaches in literature that try to solve this hitherto rather illdefined task in the present study we propose a most general generative_model of binary data for boolean factor analysis and introduce new expectationmaximization boolean factor analysis algorithm which maximizes likelihood of boolean factor analysis solution using the socalled bars problem benchmark we compare efficiencies of expectationmaximization boolean factor analysis algorithm with dendritic inhibition neural_network then we discuss advantages and disadvantages of both approaches as regards results quality and methods efficiency
bridgeless sepic pfc rectifier with reduced components and conduction losses,in this paper a new bridgeless singleended primary inductance converter powerfactorcorrection rectifier is introduced the proposed circuit provides lower conduction losses with reduced components simultaneously in conventional pfc converters continuousconductionmode boost converter a voltage loop and a current loop are required for pfc in the proposed converter the control circuit is simplified and no current loop is required while the converter operates in discontinuous conduction mode theoretical analysis and simulation results are provided to explain circuit operation a prototype of the proposed converter is realized and the results are presented the measured efficiency shows 1 improvement in comparison to conventional sepic rectifier
spuriousfree dynamic range of a uniform quantizer,quantization plays an important role in many systems where analogtodigital conversion andor digitaltoanalog conversion take place if the quantization error is correlated with the input signal then the spectrum of the quantization error will contain spurious peaks although analytical formulas describing this effect exist numerical evaluation can take much effort this brief provides approximations for the spuriousfree dynamic range sfdr of a uniform quantizer with a single sinusoidal input with and without additive_gaussian_noise it is shown that the sfdr increases by approximately 8 dbbit in case there is no noise generalizing this result to multitone inputs results in an additional 2 dbbit per additional tone additive_gaussian_noise decorrelates the sinusoids and the quantization error which results in a dramatic increase in sfdr
binding time in distributed shared_memory architectures,the paper revisits three distributed shared_memory dsm architectures to clarify them with their binding times for new addresses at the local memory page fault time node miss time and cache_miss time the dsm architectures which have different binding times arrange data in different ways with different overheads at an event of reference since a large number of cache misses can occur in a large relative to the cache_size working set binding at the page fault time alone cannot efficiently utilize locality of reference at the local memory in a small working set most of the addresses bound to the local memory at a node miss time are not effective due to the low cache_miss rate the paper shows that binding at the cache_miss time can improve system performance
reasoning about human intention change for individualized runtime software service evolution,while software_evolution has been studied extensively in software_engineering few of these efforts have involved a systematic exploration of human epistemological attitudes such as human desire and intention as the driving force of software service evolution our work proposes a theoretical framework to monitor and reason about human intention and its changes which in turn can be used to determine how software and services should evolve to be individualized and better serve each user extending the situ framework we explore the service satisfiability_problem through subworld coverage following kripke_semantics which enjoys wide application in ai and other fields related to human epistemic reasoning
a comparative assessment of web_accessibility and technical standards conformance in four eu states,the internet is playing a progressively more important part in our daytoday life through its power of making information universally available people_with_disabilities have particular opportunities to benefit using the internet in conjunction with dedicated assistive_technologies tasks that were very difficult if not impossible to achieve for people with various types of disability can now be made fully accessible  at least in principle however in practice many online resources and services are still poorly accessible to those with disability due to unsatisfactory web_content designnndesign of accessible web_content is codified in standards and guidelines of the world_wide_web consortium w3c conformance with w3cs web_content accessibility guidelines 10 wcag andor similar derivative guidelines is now the subject of considerable activity both legal and technical in many different jurisdictionsnnthis paper presents results of a comparative survey of web_accessibility guidelines and html standards conformance for samples of web sites drawn from ireland the united kingdom france and germany it also gives some recommendations on how to improve the accessibility level of web contentnna particular conclusion of the study is that the general level of web_accessibility guidelines and html standards conformance in all of the samples studied is very poor and that the pattern of failure is strikingly consistent in the four samples although considerable efforts are being made to promote web_accessibility for users_with_disabilities this is certainly not yet manifesting itself in improving web_accessibility and html validity
evolution of a four wheeled active_suspension rover with minimal actuation for rough terrain mobility,in this paper we deduce the evolution of a four wheeled active_suspension rover from a five wheeled passive_suspension rover the aim of this paper is to design a suspension mechanism which utilizes the advantages of both passive_suspension and active_suspension rover both the design considered here are simpler than the existing suspension mechanisms in the sense that the number of links as wells as the number of joints have been significantly reduced without compromising the climbing capability of the rover we first analyze the kinematics of the five wheeled rover and its motion pattern while climbing an obstacle and try to deduce the same motion pattern and capability in the four wheeled rover both the suspension mechanism consists of two planar closed kinematic chains on each side of the rover we also deduce the control strategy for the active_suspension rover wherein only two actuators are used to control the internal configuration of the rover to the best of authors knowledge this is the minimum number of actuators required to control the internal configuration of a active_suspension while operating on a fully 3d rough terrain extensive uneven terrain simulations are performed for both 5wheeled and 4wheeled rover and a comparative analysis has been done on maximum coefficient of friction and torque requirements
data fusion_algorithms for network_anomaly_detection classification and evaluation,in this paper the problem of discovering anomalies in a largescale network based on the data_fusion of heterogeneous monitors is considered we present a classification of anomaly detection_algorithms based on data_fusion and motivated by this classification the operational principles and characteristics of two different representative approaches one based on the demstershafer theory of evidence and one based on principal_component_analysis are described the detection effectiveness of these strategies are evaluated and compared under different attack scenarios based on both real data and simulations our study and corresponding numerical_results revealed that in principle the conditions under which they operate efficiently are complementary and therefore could be used effectively in an integrated way to detect a wider range of attacks
a method for realtime identification of malformed bgp messages,the bgp routing system is one of the key component of todays internet infrastructure responsible for carrying data traffic across different autonomous systems ases recently malformed bgp messages have become a threat to the operational community as they repeatedly cause bgp session resets until identified however the identification of the message itself is often difficult in large isp networks in this paper we propose a novel method for realtime identification of these messages by using passively collects bgp messages our method focuses on the frequency of observed attributes and values of prefixes advertised by each as based on our heuristics that common attributes are observed at similar time scale we periodically measure the usage frequency of attributes from bgp messages observed in realtime and mark attributes and values used by minority of the as as suspicious we verify the efficiency of our method using bgp data obtained from operational networks
link characteristics measuring in 24 ghz body area sensor_networks,with the increasing demands on the remote healthcare and the rich humanmachine interacting body area sensor_network basn has been attracting more and more attention in practice understanding the link performance and its dynamics in the emerging basn applications is very important to design reliable realtime and energyefficient protocols in this paper we study the link characteristics of body area sensor_network basn through extensive experiments with very realistic configurations we evaluate the packet reception ratio rssi lqi and movement intensity of body under indoor and outdoor environments all of which can provide direct insights to practical account
implicit spatial inference with sparse local_features,this paper introduces a novel way to leverage the implicit geometry of sparse local_features eg sift operator for the purposes of object_detection and segmentation a twoclass bayesian scheme is used as a framework and the likelihood is derived from the realvalued classification of machine_learning algorithm gentle adaboost whose output is transformed to a probabilistic_distribution using either of two models investigated logsigmoid or bigaussian the main contribution is a novel scheme for the injection of prior contextual spatial information this occurs on a uniquely designed markov_random_field defined by delaunay tri angulation of the feature points our experiments show that this framework is useful for object_detection and segmentation and we achieve good mostly invariant results in these tasks
a geometrical framework for the determination of ambiguous directions in subspace methods,in signal_subspace parameter estimation techniques like music degradations may occur due to parasite peaks in the spectrum which may be connected to high sidelobes in the beam_pattern or to ambiguities themselves this paper studies the presence of ambiguities in an array of given planar geometry we propose a general framework for the analysis and thus we obtain a generalisation of results published by lo and marple 1992 and by proukakis and manikas see proc icassp94 vol4 p54952 1994 for rank one and two ambiguities for rank kspl ges3 ambiguities the study is restricted to linear_arrays for which we derive original and synthetic results we present a geometrical construction that is able to determine all the ambiguous directions which can appear for a given linear_array the method allows determination of any rank ambiguities and for each ambiguous direction set the rank of ambiguity is obtained the search is exhaustive application of the method requires no assumption for the linear_array and is easy to implement an example is detailed for a nonuniform linear_array
nonrigid intraoperative cortical surface tracking using game_theory,during neurosurgery nonrigid brain deformation prevents preoperatively acquired images from accurately depicting the intraoperative brain stereo vision_systems can be used to track cortical surface deformation and update preoperative brain images in conjunction with a biomechanical model however these stereo systems are often plagued with calibration error which can corrupt the deformation estimation in order to decouple the effects of camera calibration and surface deformation a framework is needed which can solve for disparate and often competing variables game_theory which was developed specifically to handle decision making in this type of competitive environment has been applied to various fields from economics to biology in this paper we apply game_theory to cortical surface tracking and use it to infer information about the physical processes of brain deformation and image_acquisition
a quality control mechanism for networked virtual_reality system with video capability,introduction of motion video including live video into networked virtual_reality systems makes virtual_spaces more attractive to handle live video in networked virtual_reality systems based on vrml the scalability of networked virtual_reality systems becomes very important on the internet where the performance of the network and the end systems varies dynamically we propose a new quality control mechanism suitable for networked virtual_reality systems with live video capability our approach is to introduce the notion of the importance of presence iop which represents the importance of objects in virtual_spaces according to iop the degree of the deterioration of object presentation will be determined in case of the starvation of system resources
50th anniversary article the evolution of research on information_systems a fiftiethyear survey of the literature in management_science,the development of the information_systems is literature inmanagement science during the past 50 years reflects the inception growth and maturation of several different research streams the five research streams we identify incorporate different definitions of the managerial problems that relate to is the alternate theoretical perspectives and different methodological paradigms to study them and the levels of the organization at which their primary results impact managerial practice thedecision support and design science research stream studies the application of computers in decision support control and managerial decision making thevalue of information research stream reflects relationships established based on economic analysis of information as a commodity in the management of the firm thehumancomputer systems_design research stream emphasizes the cognitive basis for effective systems_design theis organization and strategy research stream focuses the level of analysis on the locus of value of the is investment instead of on the perceptions of a system or its user theeconomics of information_systems and technology research stream emphasizes the application of theoretical perspectives and methods from analytical and empirical economics to managerial problems involving is and information technologies it based on a discussion of these streams we evaluate the is literatures core contributions to theoretical and managerial knowledge and make some predictions about the road that lies ahead for is researchers
transient signal_detection with neural_networks the search for the desired_signal,matched filtering has been one of the most powerful techniques employed for transient detection here we will show that a dynamic neural_network outperforms the conventional approach when the artificial_neural_network ann is trained with supervised_learning schemes there is a need to supply the desired_signal for all time although we are only interested in detecting the transient in this paper we also show the effects on the detection agreement of different strategies to construct the desired_signal the extension of the bayes decision_rule 01 desired_signal optimal in static classification performs worse than desired signals constructed by random noise or prediction during the background
recursive construction and evolution of collaborative business_processes,virtual enterprises ves bring together expertise and processes of different companies to react to a market opportunitynhere we propose a novel approach to support the collaborative construction and evolution of such ves and their business processesncomprising a model of the ve and a set of model construction rules and operators our approach is based on the principlesnof iterative elaboration devolved decisionmaking and situatedness and achieves flexibility by treating the processes ofnwork coordination and selection in a uniform manner we argue that certain assumptions behind existing approaches make themnunsuitable to the business practices we observed in the target business ecosystem we then show how the proposed approachncan underpin software support for informal business practices of collaborative process construction by manufacturing smes
workspace of a sixrevolute decoupled robot_manipulator,in this paper we study the working space of a sixrevolute decoupled robot_manipulator a simple and direct method is presented to obtain the boundaries of the total and primary workspace the technique is based on finding the limit configurations of the general geometry positioning mechanism of the decoupled manipulator in order to do this we derive a fourthorder displacement equation in the first joint variable it is shown that the method only requires the simultaneous solution of two secondorder nonlinear_equations
the rise and fall of an executive information_system a case study,the progress of an executive information_system project within a manufacturing organization over a period of 9 years is described the case study illustrates the importance of the interaction between the business environment the organizational environment and the perceptions and interpretations of events and facts by stakeholders on the success or failure of an information_system it shows the importance of context in the development and implementation of an executive information_system and the dynamic nature of the influence of social economic and technical factors the reasons for the initial success and the subsequent failure of the eis within the company are explored from a contingency perspective
adapting the eblock platform for middle school stem projects initial platform usability_testing,the benefits of projectbased learning_environments are well documented however setting up and maintaining these environments can be challenging due to the high cost and expertise associated with these platforms to alleviate some of these roadblocks the existing eblock platform which is composed of fixed function building blocks targeted to enable nonexperts users to easily build a variety of interactive electronic_systems is expanded to incorporate newly defined integerbased building blocks to enable a wider range of project possibilities for middle school stem projects we discuss various interface possibilities including initial usability experiments and summarize our overall experiences and observations in working with local middles school students utilizing the eblock platform
cooperative multiantenna relaying in heterogeneous_networks,in this paper we investigate the performance of heterogeneous_networks with multiantenna cooperative relays specifically thresholdbased maximum ratio combining mrc and selection_combining sc schemes are adopted for decoding at the relays and the endtoend e2e error rate performance is analyzed by assuming a nakagami channel_model numerical_results show that the deployment of multiantennas can reduce the number of required relay nodes and thus significantly reduce the system cost on the other hand the selection of optimal decoding threshold depends on the number of relay nodes number of antennas as well as the average snr value at the receiver it is also demonstrated that when the ber requirement is not high the sc relaying scheme is sufficient to provide satisfactory performance such that the complexity of relays can be effectively reduced
a visual_representation for knowledge structures,knowledgebased systems often represent their knowledge as a network of interrelated units such networks are commonly presented to the user as a diagram of nodes connected by lines these diagrams have provided a powerful visual metaphor for knowledge_representation however their complexity can easily become unmanageable as the knowledge_base kb grows  this paper describes an alternate visual_representation for navigating knowledge structures based on a virtual museum metaphor this representation uses nested boxes rather than linked nodes to represent relations the intricate structure of the knowledge_base is conveyed by a combination of position size color and font cues mue museum unit editor was implemented using this representation to provide a graphic front end for the cyc knowledge_base
variety is the spice of virtual life,before an environment can be populated with characters a set of models must first be acquired and prepared sometimes it may be possible for artists to create each virtual character individually  for example if only a small number of individuals are needed or there are many artists available to create a larger population of characters however for most applications that need large and heterogeneous groups or crowds more automatic methods of generating large numbers of humans animals or other characters are needed fortunately depending on the context it is not the case that all types of variety are equally important sometimes quite simple methods for creating variations which do not overburden the computing resources available can be as effective as and perceptually equivalent to far more resourceintensive approaches in this paper we present some recent research and development efforts that aim to create and evaluate variety for characters in their bodies faces movements behaviours and sounds
finite horizon linear quadratic regulation for linear discrete timevarying systems with singlemultiple input delays,this paper studies the linear quadratic regulation problem for linear discrete timevarying systems with one or multiple delays in control input this type of inputdelay system can be used to model delayed actuation where the system depends on the input after various could be more than one time_delays we provide explicit forms of the finite horizon closedloop optimal_control laws numerical_examples are also provided to show the performance of our derived control laws
virusmeter preventing your cellphone from spies,due to the rapid advancement of mobile_communication technology mobile_devices nowadays can support a variety of data services that are not traditionally available with the growing popularity of mobile_devices in the last few years attacks targeting them are also surging existing mobile malware_detection techniques which are often borrowed from solutions to internet malware_detection do not perform as effectively due to the limited computing resources on mobile devicesrnrnin this paper we propose virusmeter a novel and general malware_detection method to detect anomalous behaviors on mobile_devices the rationale underlying virusmeter is the fact that mobile_devices are usually battery powered and any malicious activity would inevitably consume some battery power by monitoring power consumption on a mobile_device virusmeter catches misbehaviors that lead to abnormal power consumption for this purpose virusmeter relies on a concise usercentric power model that characterizes power consumption of common user behaviors in a realtime mode virusmeter can perform fast malware_detection with trivial runtime overhead when the battery is charging referred to as a batterycharging mode virusmeter applies more sophisticated machine_learning_techniques to further improve the detection_accuracy to demonstrate its feasibility and effectiveness we have implemented a virusmeter prototype on nokia 5500 sport and used it to evaluate some real cellphone malware including flexispy and cabir our experimental results show that virusmeter can effectively detect these malware activities with less than 15 additional power consumption in real time
author profiling for vietnamese blogs,this paper presents the first work in the task of author profiling for vietnamese blogs this task is important in threat identification and marketing intelligence we have developed a vietnamese blog profiling framework to automatically predict age gender geographic origin and occupation of weblogs authors purely based on language use the experiments on the blogs corpus we collected show very promising results with accuracy of around 80 across all traits
integrating heterogeneous personal devices with public displaybased information services,based on a requirements_analysis for public location_information displays in oncampus settings we describe the implementation of a system called synchroboard especially we elaborate on mechanisms to integrate different personal devices in this framework
performance_analysis of wireless fair queuing algorithms with compensation mechanism,scheduling packet transmission over wireless links requires quantification of the qos performance such as delay and packet_loss in terms of known system parameters one of the key issues is how to account for effects of the compensation mechanism on the systems qos performance in this paper we develop a model namely the twostage tandem queuing tstq to characterize the behaviors of packet flows in the system which applies the wireless fair scheduling with the compensation mechanism using queueing_analysis we derive performance parameters average delay and packet_loss_rate in closedform expressions these expressions are functions of the source wireless_channel and compensation mechanism parameters moreover the tradeoff relationship between delay and packet_loss_rate is revealed which is controlled by the parameter of lagging bound numerical and simulation results are used to verify the validity of the modeling_and_analysis work
channel_assignment for wireless_networks modelled as ddimensional square grids,in this paper we study the problem of channel_assignment for wireless_networks modelled as ddimensional grids in particular for ddimensional square grids we present optimal assignments that achieve a channel separation of 2 for adjacent stations where the reuse distance is 3 or 4 we also introduce the notion of a colouring schema for d dimensional square grids and present an algorithm that assigns colours to the vertices of the grid satisfying the schema constraints
efficient implementation of the overlap operator on multigpus,lattice qcd calculations were one of the first applications to show the potential of gpus in the area of high performance computing our interest is to find ways to effectively use gpus for lattice calculations using the overlap operator the large memory footprint of these codes requires the use of multiple gpus in parallel in this paper we show the methods we used to implement this operator efficiently we run our codes both on a gpu cluster and a cpu cluster with similar interconnects we find that to match performance the cpu cluster requires 2030 times more cpu cores than gpus
flat vs symbiotic evolutionary subspace clusterings,subspace clustering coevolves the attribute space supporting clusters at the same time as parameterizing the cluster location and combination typically a flat representation is pursued in which individuals describe both the property of individual clusters as well as the combination of clusters used to define the overall solution hereafter fesc conversely a symbiotic approach was recently proposed in which candidate clusters and the combination of clusters are coevolved from independent populations hereafter sesc in this work a common framework is pursued in order for flat and symbiotic evolutionary subspace clustering to be compared directly we show that fesc might match sesc results for data sets with high proportions of cluster support however the gap between the two algorithm increases as cluster support decreases
contextdependent forcefeedback steering wheel to enhance drivers onroad performances,in this paper the topic of the augmented cognition applied to the driving task and specifically to the steering maneuver is discussed we analyze how the presence of haptic_feedback on the steering wheel could help drivers to perform a visuallyguided task by providing relevant information like vehicle speed and trajectory starting from these considerations a contextdependant steering wheel force_feedback cdsw had been developed able to provide to the driver the most suitable feeling of the vehicle dynamics according to the driven context with a driving simulator the cswd software had been tested twice and then compared with a traditional steering wheel
direct segmentation of smooth multiple point regions,the purpose of reverse_engineering is to convert a large point_cloud into an accurate fair and consistent cad model for a class of conventional engineering objects we have the a priori assumption that the object is bounded exclusively by simple analytic surfaces in this case it is possible to generate the model with a minimal amount of user interaction the key issue is segmentation ie to separate the point_cloud into smaller regions where each can be approximated by a single surface while this is relatively simple where the regions are bounded by sharp edges problems arise when smoothly connected regions need to be separated the direct segmentation method described in this paper is based on a special sequence of tests by means of which a large point_cloud can be robustly splitted into smaller subregions until no further subdivision is possible surfaces of linear extrusion and revolution are also detected the structure of the smooth multiple regions is the basis of constrained surface_fitting in the final model building phase
research on dynamic reputation management model based on pagerank,for the purpose of developing a usable trust_relationship between the resource providers hosts and the resource consumers users in an open computing environment and providing a unified management of the reputation degree of the resource provides and users a dynamic reputation management model based on google pagerank drmpr is proposed the drmpr system can achieve selfstudy from a large amount of data and feedback and with the system obtaining a plenty of resources the judgment is more accurate at the end of the paper an experimental project has been built to demonstrate that the drmpr can provide a unified management of the reputation degree of the resource provides and users accurately
parameterization of the miso ifc rate region the case of partial_channel_state_information,we study the achievable_rate_region of the multipleinput singleoutput miso interference_channel ifc under the assumption that all receivers treat the interference as additive_gaussian_noise we assume the case of two users and that the channel_state_information csi is only partially known at the transmitters our main result is a characterization of paretooptimal transmit strategies for channel_matrices that satisfy a certain technical condition numerical_examples are provided to illustrate the theoretical results
introducing a raschtype anthropomorphism scale,in humanrobot interaction research much attention is given to the extent to which people perceive humanlike attributes in robots generally the concept anthropomorphism is used to describe this process anthropomorphism is defined in different ways with much focus on either typical human attributes or uniquely human attributes this difference has caused different measurement tools to be developed we argue that anthropomorphism can best be described as a continuum ranging from low to high human likeness and should be measured accordingly we found that anthropomorphic characteristics can be invariantly ordered according to the ease with which these can be ascribed to robots
knowledge formation and dialogue using the kraken toolset,the kraken toolset is a comprehensive interface for knowledge_acquisition that operates in conjunction with the cyc knowledge_base the kraken system is designed to allow subjectmatter experts to make meaningful additions to an existing knowledge_base without the benefit of training in the areas of artificial_intelligence ontology_development or logical representation users interact with kraken via a naturallanguage interface which translates back and forth between english and the kbs logical representation language a variety of specialized tools are available to guide users through the process of creating new concepts stating facts about those concepts and querying the knowledge_base kraken has undergone two independent performance evaluations in this paper we describe the general structure and several of the features of kraken focussing on key aspects of its functionality in light of the specific knowledgeformation and acquisition challenges they are intended to address
apteen a hybrid protocol for efficient_routing and comprehensive information_retrieval in wireless_sensor_networks,wireless_sensor_networks with thousands of tiny sensor_nodes are expected to find wide applicability and increasing deployment in coming years as they enable reliable monitoring and analysis of the environment in this paper we propose a hybrid routing_protocol apteen which allows for comprehensive information_retrieval the nodes in such a network not only react to timecritical situations but also give an overall picture of the network at periodic intervals in a very energy efficient manner such a network enables the user to request past present and future data from the network in the form of historical onetime and persistent queries respectively we evaluated the performance of these protocols and observe that these protocols are observed to outperform existing protocols in terms of energy consumption and longevity of the network
motion parameter estimation of multiple ground_moving_targets in multistatic passive_radar systems,multistatic passive_radar mpr systems typically use narrowband signals and operate under weak signal conditions making them difficult to reliably estimate motion parameters of ground_moving_targets on the other hand the availability of multiple spatially separated illuminators of opportunity provides a means to achieve multistatic diversity and overall signal enhancement in this paper we consider the problem of estimating motion parameters including velocity and acceleration of multiple closely located ground_moving_targets in a typical mpr platform with focus on weak signal conditions where traditional timefrequency analysisbased methods become unreliable or infeasible the underlying problem is reformulated as a sparse signal_reconstruction problem in a discretized parameter search space while the different bistatic links have distinct doppler signatures they share the same set of motion parameters of the ground_moving_targets therefore such motion parameters act as a common sparse support to enable the exploitation of group sparsitybased methods for robust motion parameter estimation this provides a means of combining signal energy from all available illuminators of opportunity and thereby obtaining a reliable estimation even when each individual signal is weak because the maximum_likelihood ml estimation of motion parameters involves a multidimensional search and its performance is sensitive to target_position errors we also propose a technique that decouples the target motion parameters yielding a twostep process that sequentially estimates the acceleration and velocity vectors with a reduced dimensionality of the parameter search space we compare the performance of the sequential method against the ml estimation with the consideration of imperfect knowledge of the initial target positions the cramerrao bound crb of the underlying parameter estimation problem is derived for a general multipletarget scenario in an mpr system simulation results are provided to compare the performance of the sparse signal reconstructionbased methods against the traditional timefrequencybased methods as well as the crb
topology preserving marching cubeslike algorithms on the facecentered cubic grid,the wellknown marching cubes algorithm is modified to apply to the facecentered cubic fee grid thus the local configurations that are considered when extracting the local surface patches are not cubic anymore this paper presents three different partitionings of the fee grid to be used for the local configurations the three candidates are evaluated theoretically and experimentally and compared with the original marching cubes algorithm it is proved that the reconstructed surface is topologically equivalent to the surface of the original object when the surface of the original object that is digitized is smooth and a sufficiently dense fee grid is used
an online signature verification system based on fusion of local and global information,an online signature verification system exploiting both local and global information through decisionlevel fusion is presented global information is extracted with a featurebased representation and recognized by using parzen windows classifiers local information is extracted as time functions of various dynamic properties and recognized by using hidden_markov_models experimental results are given on the large mcyt signature database 330 signers 16500 signatures for random and skilled forgeries feature_selection experiments based on feature_ranking are carried out it is shown experimentally that the machine expert based on local information outperforms the system based on global analysis when enough training data is available conversely it is found that global analysis is more appropriate in the case of small training_set size the two proposed systems are also shown to give complementary recognition information which is successfully exploited using decisionlevel score fusion
tumorimmune interaction surgical treatment and cancer recurrence in a mathematical model of melanoma,malignant melanoma is a cancer of the skin arising in the melanocytes we present a mathematical model of melanoma invasion into healthy tissue with an immune response we use this model as a framework with which to investigate primary tumor invasion and treatment by surgical excision we observe that the presence of immune cells can destroy tumors hold them to minimal expansion or through the production of angiogenic factors induce tumorigenic expansion we also find that the tumorimmune system dynamic is critically important in determining the likelihood and extent of tumor regrowth following resection we find that small metastatic lesions distal to the primary tumor mass can be held to a minimal size via the immune interaction with the larger primary tumor numerical_experiments further suggest that metastatic disease is optimally suppressed by immune activation when the primary tumor is moderately rather than minimally metastatic furthermore satellite lesions can become aggressively tumorigenic upon removal of the primary tumor and its associated immune tissue this can lead to recurrence where total cancer mass increases more quickly than in primary tumor invasion representing a clinically more dangerous disease state these results are in line with clinical case studies involving resection of a primary melanoma followed by recurrence in local metastases
metric rectification for perspective images of planes,we describe the geometry constraints and algorithmic implementation for metric rectification of planes the rectification allows metric properties such as angles and length ratios to be measured on the world plane from a perspective image the novel contributions are first that in a stratified context the various forms of providing metric information which include a known angle two equal though unknown angles and a known length ratio can all be represented as circular constraints on the parameters of an affine transformation of the planethis provides a simple and uniform framework for integrating constraints second direct rectification from right angles in the plane third it is shown that metric rectification enables calibration of the internal camera parameters fourth vanishing points are estimated using a maximum_likelihood_estimator fifth an algorithm for automatic rectification examples are given for a number of images and applications demonstrated for texture map acquisition and metric measurements
improved pairwise key_establishment for wireless_sensor_networks,due to the resource constraints predistributing secret keys into sensor_nodes before they are deployed is an applicable approach to achieve information_security in wireless_sensor_networks several key_predistribution schemes have been proposed in literature to establish pairwise keys between sensor_nodes they are either too complicated or insecure for some common attacks to address these weaknesses we propose an improved pairwise key_establishment mechanism for wireless_sensor_networks in this paper compared with existing approaches our scheme has better network resilience against node_capture_attack analysis and simulation results show that our scheme performs better than earlier proposed schemes in terms of network connectivity key storage_overhead maximum supported network size computational and communication_overheads
applications of plotkinterms partitions and morphisms for closed terms,this theoretical pearl is about the closed term model of pure untyped lambdaterms modulo convertibility a consequence of one of the results is that for arbitrary distinct combinators closed lambda terms m m n n there is a combinator h such thatdisplay formula herethe general result which comes from statman 1998 is that uniformly re partitions of the combinators such that each block is closed under conversion are of the form h1mm this is proved by making use of the idea behind the socalled plotkinterms originally devised to exhibit some global but nonuniform applicative behaviour for expository reasons we present the proof below the following consequences are derived a characterization of morphisms and a counterexample to the perpendicular lines lemma for conversion
costrevenue optimisation of multiservice cellular planning for city centre eumts,an overview of allip enhanced universal mobile telecommunication system eumts service needs in the business city centre bcc scenario is first presented then eumts traffic generation and activity_models are described and characterised system level simulations are carried out and the enhanced performance is demonstrated based in a single quality parameter which simultaneously accounts for call blocking and handover failure probabilities endtoend delays do not present a limitation by considering a grade of service of 1 for the quality parameter and different hypothesis for costs and prices an optimum coverage distance is obtained around 200425 m which maximises the supported throughput per km 2  however results for the profit in percentage indicates that coverage distances in the range 395425 m should be used in bcc
implementing faulttolerance in realtime systems by automatic program transformations,we present a formal_approach to implement and certify faulttolerance in realtime embedded_systems the faultintolerant initial system consists of a set of independent periodic_tasks scheduled onto a set of failsilent processors we transform the tasks such that assuming the availability of an additional spare processor the system tolerates one failure at a time transient or permanent failure detection is implemented using heartbeating and failure masking using checkpointing and rollback these techniques are described and implemented by automatic program transformations on the tasks programs the proposed formal_approach to faulttolerance by program transformation highlights the benefits of separation_of_concerns and allows us to establish correctness properties
using literaturebased discovery to identify disease candidate genes,summary  we present bitola an interactive literaturebased biomedical discovery support system the goal of this system is to discover new potentially meaningful relations between a given starting concept of interest and other concepts by mining the bibliographic database medline   to make the system more suitable for disease candidate gene discovery and to decrease the number of candidate relations we integrate background_knowledge about the chromosomal location of the starting disease as well as the chromosomal location of the candidate genes from resources such as locuslink and human_genome organization hugo bitola can also be used as an alternative way of searching the medline database the system is available at httpwwwmfuniljsibitola
modeling nondisclosure in terms of the subjectinstruction stream,a formal definition is given of nondisclosure for a computing_system and the author describes a functional decomposition of the system into two kinds of activities namely the selection and execution of subject instructions security_requirements for each of the two resulting subsystems are given and it is proved that if each subsystem satisfies its security_requirements then the entire system satisfies the given nondisclosure property finally in order to show how security can be enforced by the system an accesscontrol model is given for subjectinstruction processing that guarantees satisfaction of the given security_requirements for subjectinstruction processing 
galois theory and a new homotopy double groupoid of a map of spaces,the authors have used generalised galois theory to construct a homotopy double groupoid of a surjective bration of kan simplicial sets here we apply this to construct a new homotopy double groupoid of a map of spaces which includes constructions by others of a 2groupoid cat 1 group or crossed module an advantage of our construction is that the double groupoid can give an algebraic model of a foliated bundle 1
lanechange decision aid system based on motiondriven vehicle_tracking,overtaking and lane changing are very dangerous driving maneuvers due to possible driver distraction and blind spots we propose an aid system based on image_processing to help the driver in these situations the main purpose of an overtaking monitoring system is to segment the rear view and track the overtaking vehicle we address this task with an opticflowdriven scheme focusing on the visual field in the side mirror by placing a camera on top of it when driving a car the egomotion opticflow pattern is very regular ie all the static_objects such as trees buildings on the roadside or landmarks move backwards an overtaking vehicle on the other hand generates an opticflow pattern in the opposite direction ie moving forward toward the vehicle this wellstructured motion scenario facilitates the segmentation of regular motion patterns that correspond to the overtaking vehicle our approach is based on two main processing stages first the computation of optical_flow in real time uses a customized digital_signal_processor dsp particularly designed for this task and second the tracking stage itself based on motion pattern analysis which we address using a standard processor we present a validation benchmark scheme to evaluate the viability and robustness of the system using a set of overtaking vehicle sequences to determine a reliable vehicledetection distance
inquire predicatebased use and reuse,there are four fundamental aspects of use and reuse in building systems from components conceptualization retrieval selection and correct use the most important barrier to use and reuse is that of conceptualization the inscape environment is a specificationbased software_development environment integrated by the constructive use of formal interface specifications the purpose of the formal interface specifications and the semantic interconnections is to make explicit the invisible semantic dependencies that result in conventionallybuilt systems the important ingredient provided by inquire in conceptualization retrieval selection and use is the set of predicates that describe the semantics of the elements in the interface these predicates define the abstractions that are germane to the module interface and describe the properties of data objects and the assumptions and results of operations in a module use and reuse of components is based on a components ability to provide needed semantics at a particular point in a system it is the purpose of inquire the browser and predicatebased search mechanism to aid both the environment and the user in the search for the components that will provide the desired predicates that are required to build and evolve an implementation correctly 
least squares detection of multiple changes in fractional arima processes,we address the problem of estimating changes in fractional integrated arma farima processes these changes may be in the long range dependence lrd parameter or the arma parameters the signal is divided into elementary segments the objective is then to estimate the segments in which the changes occur this estimation is achieved by minimizing a penalized leastsquares criterion based on the parameter estimates computed in each segment the optimization problem is then solved using a dynamic programming algorithm simulation results on synthetic_data computer_network traffic are reported
shape from point features,we present a nonparametric and efficient method for shape localization that improves on the traditional subwindow search in capturing the fine geometry of an object from a small number of feature points our method implies that the discrete set of features capture more appearance and shape_information than is commonly exploited we use the acomplex by edelsbrunner et al to build a filtration of simplicial complexes from a userprovided set of features the optimal value of a is determined automatically by a search for the densest complex connected component resulting in a parameterfree algorithm given k features localization occurs in ok logk time for vgaresolution images computation takes typically less than 10 milliseconds we use our method for interactive object cut with promising results
formation_control for cooperative containment of a diffusing substance,we present a decentralized controller to keep a group of agents at equal spacing while moving around the perimeter of a loop defined by a constant distance from a convex polygon motivated by a cooperative containment problem traveling at constant speed the agents achieve and maintain their formation by using small steering adjustments to equalize the distance between themselves and their respective leading and following neighbors since the formation moves around a common loop an agent can move forward or back in formation by respectively steering slighting inside or outside the reference loop these adjustments are controlled with the use of variable radius parameters for each agent that are shown to converge to the desired reference loop as equal spacing is achieved we show that the proposed controller renders the desired formation locally asymptotically_stable and provide simulations to demonstrate the performance of the controller for an example scenario in which the formation must recover from the loss of an agent
a consensus support system model for group decisionmaking problems with multigranular linguistic preference relations,the group decisionmaking framework with linguistic preference relations is studied in this context we assume that there exist several experts who may have different background and knowledge to solve a particular problem and therefore different linguistic term sets multigranular linguistic information could be used to express their opinions the aim of this paper is to present a model of consensus support system to assist the experts in all phases of the consensus reaching process of group decisionmaking problems with multigranular linguistic preference relations this consensus support system model is based on i a multigranular linguistic methodology ii two consensus criteria consensus degrees and proximity measures and iii a guidance advice system the multigranular linguistic methodology permits the unification of the different linguistic domains to facilitate the calculus of consensus degrees and proximity measures on the basis of experts opinions the consensus degrees assess the agreement amongst all the experts opinions while the proximity measures are used to find out how far the individual opinions are from the group opinion the guidance advice system integrated in the consensus support system model acts as a feedback mechanism and it is based on a set of advice rules to help the experts change their opinions and to find out which direction that change should follow in order to obtain the highest degree of consensus possible there are two main advantages provided by this model of consensus support system firstly its ability to cope with group decisionmaking problems with multigranular linguistic preference relations and secondly the figure of the moderator traditionally presents in the consensus reaching process is replaced by the guidance advice system and in such a way the whole group decisionmaking process is automated
an aspectoriented approach to resource composition in petri netbased software architectural_models,petri_net has been widely used for modeling software_systems due to its mathematical soundness and support of various tools in many cases performancerelated analyses of petri_net for a software system need to consider the resource limitations caused by a specific platform in terms of aspectoriented approach the interference of various resources scattered across a software system can be regarded as a specific concern that is only necessary during its performancerelated analysis process to capture the interactions of resources within the petri netbased software architectural_model we propose an aspectoriented resource composition model and the xmlbased representation language called the resource extension markup language reml for its description in our approach one or more resource composition models can be developed and described in reml separately from the development of base software architectural_models through the weaving process a resource composition model can be applied to extend several petri netbased software architectural_models the resource weaver generates an augmented petri_net used for analyzing performance characteristics of the extended software architectural_model with resource interactions our aspectoriented approach facilitates selecting an optimal or superior resource composition model for a software architectural_model and vice versa which is illustrated using two exemplary server models
building subknowledge bases using concept_lattices,a theory of concept galois lattices was first introduced by wille an extension of his work to simple structures called concept sublattices has also been published this paper shows that concept sublattices can be applied to i determining subsumption of specifications and ii decomposing specifications in terms of others i show that the latter application of the theory may provide us with new conceptualizations of a specification
coordination strategies for networked_control_systems a power system application,in this paper we present a distributed supervisory strategy for loadfrequency control_problems in networked multiarea power systems coordination between the control center and the areas is accomplished via data networks subject to communication latency which is modelled by timevarying timedelay the aim here is at finding strategies able of reconfiguring whenever necessary in response to unexpected load changes andor faults the nominal setpoints on frequency and generated power of each area so that viable evolutions arise for the overall networked system and a new suitable equilibrium is reached
crosstalk attack monitoring and localization in alloptical networks,the effects of an attack connection can propagate quickly to different parts of an alloptical transparent network such attacks affect the normal traffic and can either cause service degradation or outright service denial quick detection and localization of an attack source can avoid losing large amounts of data in an alloptical network_attack monitors can collect the information from connections and nodes for diagnostic purpose however to detect attack sources it is not necessary to put monitors at all nodes since those connections affected by the attack connection would provide valuable information for diagnosis we show that by placing a relatively small number of monitors on a selected set of nodes in a network is sufficient to achieve the required level of performance however the actual monitor placement routing and attack diagnosis are challenging problems that need research attention in this paper we first develop our models of crosstalk attack and monitor node with these models we prove the necessary and sufficient condition for onecrosstalkattack diagnosable networks next we develop a scalable diagnosis method which can localize the attack connection efficiently with sparse monitor nodes in the network
adaptive_algorithms for balanced multidimensional clustering,the gkd tree generalized kd tree method aims at reducing the average number of data page accesses per query but it ignores the cost of index search the authors propose two adaptive_algorithms that take into consideration both data page access cost and index page access cost it attempts to find a minimum total cost experimental results indicate that the proposed algorithms are superior to the gkd tree method 
an optimized stereo_vision implementation for embedded_systems application to rgb and infrared images,the aim of this paper is to demonstrate the applicability and the effectiveness of a computationally demanding stereomatching algorithm in different lowcost and lowcomplexity embedded_devices by focusing on the analysis of timing and image_quality performances various optimizations have been implemented to allow its deployment on specific hardware architectures while decreasing memory and processing time requirements 1 reduction of color channel information and resolution for input images 2 lowlevel software optimizations such as parallel_computation replacement of function calls or loop unrolling 3 reduction of redundant data structures and internal data representation the feasibility of a stereo_vision_system on a lowcost platform is evaluated by using standard datasets and images taken from infrared cameras analysis of the resulting disparity_map accuracy with respect to a fullsize dataset is performed as well as the testing of suboptimal solutions
parametric mixing for centralized voip conferencing using itut recommendation g7222,voip conferencing with a centralized speech mixing bridge introduces additional endtoend latency into packetized voice communication this paper investigates how full tandem speech decoding timedomain mixing speech encoding cycle can be circumvented by instead extracting the coded speech parameters and performing the speech packet mixing without timedomain reconstruction by mixing through coded speech parameters we show that nearly an 85 decrease in computational complexity can be achieved over full tandem mixing of two speakers for g7222 thus significantly reducing the packet latency at the centralized speech mixing bridge for the g7222 parametric mixer presented linear prediction coefficients lpcs pitch lags fixed codebooks and gains are extracted without full speech reconstruction from the encoded bit_stream mixed and then reencoded instead of the full tandem approach where each speech frame must be fully reconstructed we investigate the mixing in two scenarios i mix two 1265 kbps g7222 speech streams at a mixed rate of 1265 kbps and ii mix two 1265 kbps g7222 speech streams at a mixed rate of 1825 kbps pams is used to evaluate the speech_quality of the parametric mixer resulting in an average distortion of 037 mos compared to tandem mixing as shown by simulations using typical conversation models
selfadaptive online reclustering of complex object data,a likely trend in the development of future cad case and office information_systems will be the use of objectoriented database_systems to manage their internal data stores the entities that these applications will retrieve such as electronic parts and their connections or customer_service records are typically large complex objects composed of many interconnected heterogeneous objects not thousands of tuples these applications may exhibit widely shifting usage patterns due to their interactive mode of operation such a class of applications would demand clustering_methods that are appropriate for clustering large complex objects and that can adapt online to the shifting usage patterns while most objectoriented clustering_methods allow grouping of heterogeneous objects they  are usually static and can only be changed offline we present one possible architecture for performing complex object reclustering in an online manner that is adaptive to changing usage patterns our architecture involves the decomposition of a clustering_method into concurrently operating components that each handle one of the fundamental tasks involved in reclustering namely statistics collection cluster_analysis and reorganization we present the results of an experiment performed to evaluate its behavior these results show that the average miss rate for object accesses can be effectively reduced using a combination of rules that we have developed for deciding when cluster analyses and reorganizations should be performed
reim  reiminfer checking and inference of reference immutability and method purity,reference immutability ensures that a reference is not used to modify the referenced object and enables the safe sharing of object structures a pure method does not cause sideeffects on the objects that existed in the prestate of the method execution checking and inference of reference immutability and method purity enables a variety of program analyses and optimizations we present reim a type_system for reference immutability and reiminfer a corresponding type_inference analysis the type_system is concise and contextsensitive the type_inference analysis is precise and scalable and requires no manual annotations in addition we present a novel application of the reference immutability type_system method purity inference   to support our theoretical results we implemented the type_system and the type_inference analysis for java we include a type checker to verify the correctness of the inference result empirical results on java_applications and libraries of up to 348kloc show that our approach achieves both scalability and precision
modelfree control and intelligent pid_controllers towards a possible trivialization of nonlinear_control,we are introducing a modelfree control and a control with a restricted model for finitedimensional complex systems this control_design may be viewed as a contribution to intelligent pid_controllers the tuning of which becomes quite straightforward even with highly nonlinear andor timevarying systems our main tool is a newly developed numerical differentiation differential algebra provides the theoretical framework our approach is validated by several numerical_experiments
modified_differential_evolution for constrained_optimization,in this paper we present a differentialevolution based approach to solve constrained_optimization_problems the aim of the approach is to increase the probability of each parent to generate a better offspring this is done by allowing each solution to generate more than one offspring but using a different mutation_operator which combines information of the best solution in the population and also information of the current parent to find new search directions three selection criteria based on feasibility are used to deal with the constraints of the problem and also a diversity mechanism is added to maintain infeasible solutions located in promising areas of the search space the approach is tested in a set of test problems proposed for the special session on constrained real parameter optimization the results obtained are discussed and some conclusions are established
performance evaluation of piggyback requests in ieee 80216,wimax worldwide_interoperability_for_microwave_access is a wireless access_technology that aims to provide last mile wireless broadband_access for fixed and mobile_users as an alternative to the wired dsl and cable access it is specified in the ieee 80216 standard the standard defines several possible bandwidth_request methods that can be implemented in an actual deployment of a wimax network in this paper we will study the performance of two different bandwidth_request mechanisms namely piggyback and broadcast requests and will show in which situations piggybacking performs better than the contention based broadcast bandwidth requests
use of coded infrared light as artificial landmarks for mobile_robot_localization,this paper presents mobile_robot_localization using coded infrared light as artificial landmarks different from rfid identification using infrared light has highly deterministic characteristics iridinfrared identification is implemented with ir leds and photo transistors by putting several infrared leds on the ceiling the floor is divided into several sectors and each sector is set to have a unique identification the coded infrared light tells which sector the robot is in but the size of the uncertainty is still too large if the sector size is large which usually occur deadreckoning provides the estimated robot configuration but the error is getting accumulated as the robot travels this paper presents an algorithm which fuses both the encoder and the irid information so that the size of the uncertainty becomes smaller it also introduces a framework which can be used with other types of the artificial landmarks the characteristics of the developed irid and the proposed algorithm are verified from the experiments
the growth of international collaboration in east european scholarly communities a bibliometric analysis of journal articles published between 1989 and 2009,in the last two decades international collaboration in the eastern european academic communities has strongly intensified scientists from developed countries within the european union play a key role in stimulating the international collaboration of academics in this region in addition many of the research projects that engage easteuropean scholars are only possible in the framework of the large european programmes the present study focuses on the role of eu and other developed nations as a partner of these countries and the analysis of the performance of collaborative research as reflected by the citation impact of internationally coauthored publications
inexact kleinmannewton method for riccati_equations,in this paper we consider the numerical_solution of the algebraic riccati_equation using newtons_method we propose an inexact variant which allows one control the number of the inner iterates used in an iterative solver for each newton step conditions are given under which the monotonicity and global convergence result of kleinman also hold for the inexact newton iterates numerical_results illustrate the efficiency of this method
dimensional_synthesis of a 3dof parallel_manipulator,kinematics_analysis and dimensional_synthesis are two important problems of a parallel_manipulator dimensional_synthesis is optimization of the kinematic parameters according to desired workspace or other design requirements in this paper the dimensional_synthesis of a 3dof parallel_manipulator which mimics delta robots is studied considering maximum inscribed workspace and reciprocal of the condition_number on the workspace section based on the concept the kinematic model of the 3dof parallel_manipulator is given at first the golden section search is used to search the workspace of the manipulator and mesh the boundary then the algorithm for calculating the inscribed workspace and dimensional_synthesis considering the inscribed workspace are presented thirdly the distribution of dexterity index on the workspace section and dimensional_synthesis considering the reciprocal of condition_number is studied the two dimensional_synthesis results are compared at the end of the paper while the synthesis methodology of the manipulator is studied it is helpful to improve design efficiency of the 3dof parallel_manipulator the quickly design for the manipulator can be performed through the proposed methods of dimensional_synthesis
kinematics_analysis for obstacleclimbing performance of a rescue robot,a tracked robot is designed for destroyed mine search and rescue the mechanical_system is introduced from reconfigurable structure suspension_system and anti explosive and waterproof the sensors include ccd camera co ch4 temperature and air speed are equipped on the robot two pairs of swing arms are equipped on the robot their motions help robot climb up obstacle because the center of gravity cg plays an important role in the process of climbing up an obstacle the cg kinematics model is built using this model the cg change situation is obtained and the maximum height of the obstacle which can be climbed up is obtained and the stability angle margin is obtained too the relationship between the robot pitch angle and the height of the obstacle is obtained using this relationship the geometry parameter of the uncertain environment can be known these analysis help to design_and_control the robot
dynamic software updates a vmcentric approach,software evolves to fix bugs and add features stopping and restarting programs to apply changes is inconvenient and often costly dynamic software updating dsu addresses this problem by updating programs while they execute but existing dsu systems for managed languages do not support many updates that occur in practice and are inefficient this paper presents the design and implementation of j volve  a dsuenhanced java vm updated programs may add delete and replace fields and methods anywhere within the class hierarchy jvolve implements these updates by adding to and coordinating vm classloading justintime compilation scheduling return barriers onstack replacement and garbage collection j volve  is  safe  its use of bytecode verification and vm thread synchronization ensures that an update will always produce typecorrect executions jvolve is  flexible  it can support 20 of 22 updates to three opensource programsjetty web_server javaemailserver and crossftp serverbased on actual releases occurring over 1 to 2 years jvolve is  efficient  performance experiments show that incurs  no overhead  during steadystate execution these results demonstrate that this work is a significant step towards practical support for dynamic updates in virtual_machines for managed languages
creating rule ensembles from automaticallyevolved rule_induction algorithms,ensembles are a set of classification_models that when combined produce better predictions than when used by themselves this chapter proposes a new evolutionary algorithmbased method for creating an ensemble of rule sets consisting of two stages first an evolutionary_algorithm more precisely a genetic_programming algorithm is used to automatically create complete rule_induction algorithms secondly the automaticallyevolved rule_induction algo rithms are used to produce rule sets that are then combined into an ensemble concerning this second stage we investigate the effectiveness of two different approaches for combining the votes of all rule sets in the ensemble and two dif ferent approaches for selecting which subset of evolved rule_induction algorithms out of all evolved algorithms should be used to produce the rule sets that will be combined into an ensemble
an improved twoway training for discriminatory channel_estimation via semiblind approach,
xml and meta data based edi for small enterprises,today in many cases electronic_data_interchange edi is limited to large scale industry connected to their own value added networks smallscale enterprises are not yet integrated in the communication flow because actual edi solutions are to complex to inflexible or to expensivernrnthe approach presented in this paper separates knowledge about data structures and data formats from the process of generation of destination files this knowledge is transformed into a meta data structure represented by xml document type definitions dtd which itself are stored within a database system if any changes of the data interchange specification are necessary it is sufficient to update the corresponding meta data information within the xml dtds the implementation of the data interchange processor remains unchanged this type of adaptation does not require a software specialist and therefore it meets an important requirement of smallscale enterprises data transmission is done using the advantages of xml and internet technology
an fpgabased embedded_system for fingerprint_matching using phaseonly correlation_algorithm,biometric_identification systems are defined as systems exploiting automated methods of personal recognition based on physiological or behavioural characteristics among these fingerprints are very reliable biometric identifiers trying to fasten the image_processing step makes the recognition process more efficient especially concerning embedded_systems for realtime authentication in this paper we propose an fpgabased architecture that efficiently implements the high computationally demanding core of a matching_algorithm based on phaseonly spatial_correlation moreover we show how it is possible to use cots components to embed an entire afis on chip and so reducing cost space and energy used
classification of breast masses in mammograms using neural_networks with shape edge sharpness and texture features,we propose an approach using artificial_neural_networks to classify masses in mammograms as malignant or benign single layer and multilayer perceptron networks are used in a study on perceptron topologies and training procedures for pattern classifica tion of breast masses the contours of a set of 111 regions on mam mograms related to breast masses and tumors are manually delin eated and represented by polygonal models for shape analysis ribbons of pixels are extracted around the boundaries of a subset of 57 masses by dilating and eroding the contours three shape fac tors three measures of edge sharpness and 14 texture features based on graylevel cooccurrence matrices of the pixels in the rib bons are computed several combinations of the features are used with perceptrons of varying topology and training procedures for the classification of benign masses and malignant tumors the results are compared in terms of the area az under the receiver operating characteristics curve values of az up to 099 are obtained with the shape factors and texture features however only feature_sets that included at least one shape factor provide consistently high perfor mance with respect to variations in network_topology and training
the analysis of the performance of multibeamforming in memory nonlinear power amplifier,with the increasingly diverse and complex requirements of radar_systems and communication_systems the application of multifunctionphased array radar has become a trend and the digital multibeamforming technology plays a crucial role in it in practice power amplifier pa is an essential component in radar_systems and communication_systems unfortunately it is always nonlinear to provide a high output power with the purpose of a high output power and efficiency it is necessary to study the influence of pa nonlinear characteristics on the digital multibeamforming in this paper a form of the multibeamforming signal and a nonlinear model with memory for pa are given the output signal via the pa model has been analyzed subsequently as the result of analysis it can be found that the output signal is divided into the original signal and the interferential signal the power ratio of original signal to interference signal can reflect the influence of pa nonlinear characteristics on the digital multibeamforming finally according to the ratio the results of computer simulation show that the memory effect plays a key role for the small power signal while the nonlinearity plays an important role for the large power signal
relaxed multiple routing configurations for ip fast reroute,multitopology routing is an increasingly popular ip network_management concept that allows transport of different traffic types over disjoint network_paths the concept is of particular interest for implementation of ip fast reroute ip frr first it can support guaranteed instantaneous recovery from any link or node_failure second different failures result in routing over different network topologies which augments the parameter space for load distribution optimizations multiple routing configurations mrc is the stateoftheart ip frr scheme based on multitopology routing today in this paper we present a new enhanced ip frr scheme which we call ldquorelaxed mrcrdquo rmrc rmrc simplifies the topology construction and increases the routing flexibility in each topology according to our experimental evaluation rmrc has several benefits compared to mrc the number of backup topologies required to provide protection against the same set of failures is reduced hence reducing state in routers in addition the backup paths are shorter and the link_utilization is significantly better
similarity measures between intuitionistic fuzzy vague sets a comparative analysis,existing similarity measures between intuitionistic fuzzy setsvague sets are analyzed compared and summarized by their counterintuitive examples in pattern_recognition the positive aspects of each similarity measure are demonstrated along with counter cases and discussion of the conditions under which each may not work as desired the research presented here could benefit selection and applications of similarity measures for intuitionistic fuzzy sets and vague sets in practice
distributed approximation of capacitated dominating sets,we study local distributed_algorithms for the capacitated minimum dominating set capmds problem which arises in various distributed network applications given a network graph  g    ve  and a capacity  capv    n  for each node  v    v   the capmds problem asks for a subset  s    v  of minimal cardinality such that every network node not in  s  is covered by at least one neighbor in  s  and every node  v    s  covers at most  capv  of its neighbors we prove that in general graphs and even with uniform capacities the problem is inherently  nonlocal  ie every distributed_algorithm achieving a nontrivial approximation_ratio must have a time complexity that essentially grows linearly with the network diameter on the other hand if for some parameter e  0 capacities can be violated by a factor of 1  e capmds becomes much more local particularly based on a novel distributed randomized rounding technique we present a distributed bicriteria algorithm that achieves an olog approximation in time olog 3  n   log n e where  n  and  denote the number of nodes and the maximal degree in  g  respectively finally we prove that in geometric network graphs typically arising in wireless settings the uniform problem can be approximated within a constant factor in logarithmic time whereas the nonuniform problem remains entirely nonlocal
using wordnet hypernyms and dependency features for phrasallevel event recognition and type classification,the goal of this research is to devise a method for recognizing and classifying timeml events in a more effective way timeml is the most recent annotation scheme for processing the event and temporal expressions in natural_language_processing fields in this paper we argue and demonstrate that unit feature dependency information and deeplevel wordnet hypernyms are useful for event recognition and type classification the proposed method utilizes various features including lexical semantic and dependencybased combined features the experimental results show that our proposed method outperforms a stateoftheart approach mainly due to the new strategies especially the performance of noun and adjective events which have been largely ignored and yet significant is significantly improved
threedimensional focusing with multipass sar_data,deals with the use of multipass synthetic_aperture_radar sar_data in order to achieve threedimensional tomography reconstruction in presence of volumetric scattering starting from azimuth and rangefocused sar_data relative to the same area neglecting any mutual interaction between the targets and assuming the propagation in homogeneous media we investigate the possibility to focus the data also in the elevation direction the problem is formulated in the framework of linear inverse_problem and the solution makes use of the singular value decomposition of the relevant operator this allows us to properly take into account nonuniform orbit separation and to exploit a priori knowledge regarding the size of the volume interested by the scattering mechanism thus leading to superresolution in the elevation direction results obtained on simulated data demonstrate the feasibility of the proposed processing technique
software_component independence,independence is a fundamental requirement for calculating system reliability from component reliabilities whether in hardware or software_systems markov analysis is often used in such calculation however procedures as conventionally used do not qualify as nodes in a markov system we outline the requirements for several classes of component independence and use the cps continuation passing style transformation to convert conventional procedures into fragments that are appropriate to markov analysis
interpolated allpass fractionaldelay filters using root displacement,fractionaldelay filter is the general name given to filters modelling noninteger delays such filters have a flat phase delay for a wide frequency band with the value of the phase delay approximating the fractional delay a maximallyflat delay iir fractionaldelay filter can be obtained by the thiran approximation a simple and efficient method for obtaining filters modelling intermediate fractional delays from two thiran fractionaldelay filters is proposed the proposed method allows continuously modifying the fractional delay computational complexity of the proposed method is discussed a practical application of the method in modelbased sound_synthesis is given as an example
mmsinc a largescale chemoinformatics database,mmsinc httpmmsdsfarmunipditmmsincsearch is a database of nonredundant richly annotated and biomedically relevant chemical structures a primary goal of mmsinc is to guarantee the highest quality and the uniqueness of each entry mmsinc then adds value to these entries by including the analysis of crucial chemical properties such as ionization and tautomerization processes and the in silico prediction of 24 important molecular properties in the biochemical profile of each structure mmsinc is consequently a natural input for different chemoinformatics and virtual screening applications in addition mmsinc supports various types of queries including substructure queries and the novel molecular scissoring query mmsinc is interfaced with other primary data collectors such as pubchem protein_data_bank pdb the food and drug administration database of approved drugs and zinc
ibm solves a mixedinteger program to optimize its semiconductor supply chain,ibm systems and technology group uses operations research models and methods extensively for solving largescale supply chain optimization sco problems for planning its extended enterprise semiconductor supply chain the largescale nature of these problems necessitates the use of computationally efficient solution_methods however the complexity of the models makes developing robust solution_methods a challenge we developed a mixedinteger programming mip model and supporting heuristics for optimizing ibms semiconductor supply chain we designed three heuristics driven by practical applications for capturing the discrete aspects of the mip we leverage the model structure to overcome computational hurdles resulting from the largescale problem ibm uses the model and method daily for operational and strategic_planning decisions and has saved substantial costs
on passivity based control of stochastic porthamiltonian systems,this paper introduces stochastic porthamiltonian systems sphss whose dynamics are described by ito stochastic_differential_equations sphss are extension of the deterministic porthamiltonian systems which are used to express various passive systems first we show a necessary and sufficient condition to preserve the stochastic porthamiltonian structure of the system under a class of coordinate transformations second we derive a condition for the system to be stochastic passive third we equip stochastic generalized canonical transformations sgcts which are pairs of coordinate and feedback transformations preserving the stochastic porthamiltonian structure finally we propose a stochastic stabilization framework based on stochastic passivity and sgcts
a survey of single and multihop link schedulers for mmwave wireless systems,wireless_communication at 60ghz aka mmwave provides extremely high data rates ie several gbs moreover devices have a much shorter transmission_range as compared to those operating in the 24 and 5ghz bands indeed links can be treated as pseudowires with minimal interference leakage as a result future 60ghz systems will have very high spatial reuse this however is at the expense of high propagation loss which can be overcome using directional or smart_antennas another promising solution is to employ relays to boost the signal of weak links in particular if relays are properly selected they are able to offer higher data rates than direct_links and also help circumvent obstacles to this end we review stateoftheart schedulers that take advantage of the high spatial reuse afforded by 60ghz wireless systems to activate multiple links within a channel time allocation moreover we survey works that use passive and active relays to overcome obstacles and to facilitate novel applications we also survey those that maximize both spatial reuse and throughput of both direct and indirect relay_links simultaneously
a datadriven approach for building macroeconomic decision_support_system,more and more economic and financial data have been collected by the governmental departments in china since china started its socialist market economy in late 1980s these government departments in particular the departments in charge of economic_development pay a great attention to the economic information in their decisionmaking there is an urgent demand for efficient decision_support_systems in macroeconomic decisionmaking in this paper we present a datadriven approach for building macroeconomic decision_support_system we first give a comprehensive discussion about the basic elements and their dataprocessing methods for macroeconomic decision_support_systems according to chinas situation these elements include leading indicator system about business cycle state identification of economic movement forecasting of economic trend promotion of successful cases choice of regulation instruments evaluation method of macroeconomic policies based on the discussion we put forward to a general structure of macroeconomic decision_support_system premier implementation shows that the structure can not only satisfy the governmental departments demand fairly but also reflect the future trend
tcp_performance in ieee 80211based ad_hoc_networks with multiple wireless lossy links,we propose a packetlevel model to investigate the impact of channel error on the transmission_control_protocol tcp_performance over ieee80211based multihop_wireless_networks a markov renewal approach is used to analyze the behavior of tcp_reno and tcp impatient newreno compared to previous work our main contributions are listed as follows 1 modeling multiple lossy links 2 investigating the interactions among tcp internet_protocol ip and media_access_control mac_protocol layers specifically the impact of 80211 mac_protocol and dynamic_source_routing dsr_protocol on tcp_throughput performance 3 considering the spatial reuse property of the wireless_channel the model takes into account the different proportions between the interference_range and transmission_range and 4 adopting more accurate and realistic analysis to the fast recovery process and showing the dependency of throughput and the risk of experiencing successive fast retransmits and timeouts on the packet error probability the analytical_results are validated against simulation results by using glomosim the results show that the impact of the channel error is reduced significantly due to the packet retransmissions on a perhop basis and a small bandwidth delay product of ad_hoc_networks the tcp_throughput always deteriorates less than  10 percent with a packet error rate ranging from 0 to 01 our model also provides a theoretical basis for designing an optimum long retry limit for ieee 80211 in ad_hoc_networks
toward secure multikeyword topk retrieval over encrypted cloud_data,cloud_computing has emerging as a promising pattern for data outsourcing and highquality data services however concerns of sensitive information on cloud potentially causes privacy_problems data_encryption protects data_security to some extent but at the cost of compromised efficiency searchable symmetric_encryption sse allows retrieval of encrypted_data over cloud in this paper we focus on addressing data_privacy issues using sse for the first time we formulate the privacy_issue from the aspect of similarity relevance and scheme robustness we observe that serverside ranking based on orderpreserving encryption ope inevitably leaks data_privacy to eliminate the leakage we propose a tworound searchable_encryption trse scheme that supports topk multikeyword retrieval in trse we employ a vector_space_model and homomorphic_encryption the vector_space_model helps to provide sufficient search accuracy and the homomorphic_encryption enables users to involve in the ranking while the majority of computing work is done on the server side by operations only on ciphertext as a result information leakage can be eliminated and data_security is ensured thorough security and performance_analysis show that the proposed scheme guarantees high security and practical efficiency
improved 16qam constellation labeling for bistcmid with the alamouti scheme,we design constellation labeling maps for bitinterleaved spacetime coded_modulation with iterative_decoding bistcmid over rayleigh blockfading channels using the alamouti scheme and nsub r receive_antennas to achieve the largest asymptotic coding_gain from the constellation labeling we propose a new design criterion that maximizes the 2nsub rth power mean of the squared euclidean distances associated with all errorfree feedback events in the constellation based on this power mean criterion we show that the labeling optimization problem falls into the category of quadratic_assignment_problems we propose two novel 16qam labeling maps that are particularly designed for nsub r1 and nsub r2 respectively numerical_results show that both labeling maps achieve about 1 db coding_gain over the conventional 16qam modified set partitioning labeling
perfect output_feedback in the twouser decentralized interference_channel,in this paper the    eta    nash_equilibrium    eta    ne region of the twouser gaussian interference_channel ic with perfect output_feedback is approximated to within 1 bitshz and    eta     arbitrarily close to 1 bitshz the relevance of the    eta    ne region is that it provides the set of rate pairs that are achievable and stable in the ic when both transmitterreceiver pairs autonomously tune their own transmitreceive configurations seeking an    eta    optimal individual transmission rate therefore any rate tuple outside the    eta    ne region is not stable as there always exists one link able to increase by at least    eta     bitsshz its own transmission rate by updating its own transmitreceive configuration the main insights that arise from this paper are as follows first the    eta    ne region achieved with feedback is larger than or equal to the    eta    ne region without feedback more importantly for each rate pair achievable at an    eta    ne without feedback there exists at least one rate pair achievable at an    eta    ne with feedback that is weakly pareto superior second there always exists an    eta    ne transmitreceive configuration that achieves a rate pair that is at most 1 bitshz per user away from the outer bound of the capacity_region
a design_and_control environment for internetbased telerobotics,this paper describes an environment for the design simulation and control of internetbased forcereflecting telerobotic_systems we define these systems as using a segment of the computer_network to connect the master to the slave computer_networks introduce a time_delay that is best described by a timevarying random process thus known techniques for controlling timedelay telerobots are not directly applicable and an environment for iterative designing and testing is necessary the underlying software_architecture sup ports tools for modeling the delay of the computer_network design ing a stable controller simulating the performance of a telerobotic system and testing the control algorithms using a forcereflecting input device furthermore this setup provides data about including the internet into more general telerobotic control architectures to demonstrate the features of this environment the complete proce dure for the design of a telerobotic controller is discussed first the delay pa
noflow underfill flip chip assemblyan experimental and modeling analysis,in the flipchip assembly process noflow underfill materials have a particular advantage over traditional underfill the application and curing of the former can be undertaken before and during the reflow process this advantage can be exploited to increase the flipchip manufacturing throughput however adopting a noflow underfill process may introduce reliability issues such as underfill entrapment delamination at interfaces between underfill and other materials and lower solder joint fatigue life this paper presents an analysis on the assembly and the reliability of flipchips with noflow underfill the methodology adopted in the work is a combination of experimental and computermodeling methods two types of noflow underfill materials have been used for the flip chips the samples have been inspected with xray and scanning acoustic microscope inspection systems to find voids and other defects eleven samples for each type of underfill material have been subjected to thermal shock test and the number of cycles to failure for these flip chips have been found in the computer modeling part of the work a comprehensive parametric study has provided details on the relationship between the material properties and reliability and on how underfill entrapment may affect the thermalmechanical fatigue life of flip chips with noflow underfill
reflections on designing networked exertion games,research in humancomputer interaction has begun to acknowledge the benefits of physicality in the way people interact with computers however the role of physicality is often understood in terms of the characteristics of physical smart_objects and their digital augmentation we are stressing that the physicality lies within the interaction not the object and use a subset of bodily actions exertion interactions as an example to demonstrate our point emerging game designs have shown that supporting such exertion interactions can enable beneficial experiences between geographically distant participants based on several designs from our own work as well as others in this area we articulate reflections for the design of systems that support and facilitate bodily aspects of physicality in networked environments we believe our work can serve as guidance for designers who are interested in creating future systems that support networked exertion interactions
stackfree processoriented simulation,the process interaction world view is widely used in the general simulation community for its expressive power and is supported by most modern simulation languages in parallel discrete event simulation however its use remains comparatively rare due to the perceived inefficiency and difficulty of parallel implementationswe present a new implementation strategy for parallel processoriented simulation languages this innovative semanticsbased approach directly addresses two common concerns of such languages by concentrating on the intrinsic threads of control we avoid the proliferation of simulation objects and their associated costs that might result from a naive translation more fundamentally the primary costs associated with processoriented languages  those of context switching between stacks and in an optimistic setting of saving the state of these stacks  are entirely eliminated since our explicit use of continuations avoids the need for stacks in the first place we similarly obtain cheap and natural thread preemption
network_congestion control with markovian multipath_routing,in this paper we consider an integrated model for tcpip protocols with multipath_routing the model combines a network_utility_maximization for rate_control based on endtoend queuing delays with a markovian traffic equilibrium for routing based on total expected delays we prove the existence of a unique equilibrium state which is characterized as the solution of an unconstrained strictly convex program a distributed_algorithm for solving this optimization problem is proposed with a brief discussion of how it can be implemented by adapting the current internet_protocols
motion compensated lossytolossless compression of 4d medical_images using integer wavelet_transforms,this paper proposes a method for progressive lossytolossless compression of fourdimensional 4d medical_images sequences of volumetric images over time by using a combination of threedimensional 3d integer wavelet_transform iwt and 3d motion_compensation a 3d extension of the setpartitioning in hierarchical trees spiht algorithm is employed for coding the wavelet_coefficients to effectively exploit the redundancy between consecutive 3d images the concepts of key and residual frames from video_coding is used a fast 3d cube matching_algorithm is employed to do motion_estimation the key and the residual volumes are then coded using 3d iwt and the modified 3d spiht the experimental results presented in this paper show that our proposed compression_scheme achieves better lossy and lossless compression_performance on 4d medical_images when compared with jpeg2000 and volumetric compression based on 3d spiht
a multiuser receiver for code_division_multiple_access communications over multipath_channels,a multiuser communication system is considered where k users share a channel with multipath_propagation by using code division for multiple_access data modulation is carried out by binary_phase_shift_keying and direct_sequence_spread_spectrum signaling the microcellular communication media is modeled as a frequency_selective_fading channel with multipath_propagation the multipath diversity of the received_signals from the k users is exploited by a bank of k rake correlators algorithms based on the maximum_likelihood rule have been developed for estimating the complex channel coefficients as well as for detection of the desired data packets from the sufficient statistics provided by the rake correlators the performance of the resulting multiuser detector is evaluated analytically and via monte carlo simulations the results indicate that the estimator of the channel coefficients has a variance close to the cramerrao lower bound and that the proposed multiuser detector is capable of eliminating the nearfar effect as well as processing the signals propagated through multiple_paths 
spatial synchronization using watermark key structure,recently we proposed a method for constructing a template for efficient temporal synchronization in video_watermarking 1 our temporal synchronization method uses a state machine key generator for producing the watermark embedded in successive frames of video a feature extractor allows the watermark key schedule to be content dependent increasing the difficulty of copy and ownership attacks it was shown that efficient synchronization can be achieved by adding temporal redundancy into the key schedule in this paper we explore and extend the concepts of our temporal synchronization method to spatial synchronization the key generator is used to construct the embedded watermark of nonoverlapping blocks of the video creating a tiled structure 24 the autocorrelation of the tiled watermark contains local maxima or peaks with a gridlike structure where the distance between the peaks indicates the scale of the watermark and the orientation of the peaks indicate the watermark rotation experimental results are obtained using digital_image watermarks scaling and rotation attacks are investigated
sdg modelbased analysis of fault propagation in control_systems,in the area of fault analysis sdg signed directed_graph models can be used to describe the system states and the fault propagation paths which are the composition of qualitative deviation from the normal state in control_systems besides the natural relations caused by the physical properties the forced control actions determine the dynamic properties of the systems which cause the particularity of sdg modelbased analysis in this paper the sdg description and the analysis methods of fault propagation in control_systems are presented and the typical cases like pid_control feedforward_control splitrange control cascade control etc are illustrated a graphical analysis method is proposed to substitute the algebraic methods based on equations these results can be expanded to various control_systems and even be applied to largescale industrial systems by the combination and connection of several basic elements
the distribution of citations from nation to nation on a field by field basis  a computer calculation of the parameters,following the methodology established byprice this paper analyzes the empirical evidence of citation matrices using the data cleaned and tabulated by computer horizons inc from the science citation index data banks it is shown that the nondiagonal elements of the square citation matrices can be accounted for very satisfactorily by assigning each nation a characteristic output and input coefficient in each field measured the ratio of these coefficients provides a measure of quality deviations from this simple model give measures of particular linkage strengths between nations showing some evidence of preferences and avoidances that exist for reason of language social structure etc it is also shown that the diagonal data can be accounted for by the measurable phenomenon that each nation seems to publish partly for the international knowledge system and party for its own domestic purposes thus three parameters and a cluster map can parsimoniously describe the citation data within the limits of random error
an approach to objectoriented discreteevent simulation of manufacturing systems,it is shown how the objectoriented approach can be applied to discreteevent simulation and in particular to discreteevent simulation of manufacturing systems a hierarchical_structure of object classes is proposed consisting of three class libraries base classes simulation support object classes and manufacturing systems simulation object classes the definition of each class and how the class objects interact with one another are discussed an example of a discreteevent simulation model developed using the object classes is presented the example illustrates the basic nature merits and drawbacks of this approach 
free subcarrier optimization for peaktoaverage power ratio minimization in ofdm_systems,peaktoaverage power ratio par reduction techniques are often employed to increase the power efficiency of orthogonal_frequency_division_multiplexing ofdm_systems a recently proposed par optimization_method demonstrates how the par can be minimized when free subcarriers and a certain distortion allowance on the error_vector_magnitude evm are available in this paper we derive the lower bound on the capacity for such a system and investigate the capacity maximizing number of free subcarriers that should be used
a spatial mapping algorithm for heterogeneous coarsegrained reconfigurable_architectures,in this work we investigate the problem of automatically mapping applications onto a coarsegrained reconfigurable_architecture and propose an efficient_algorithm to solve the problem we formalize the mapping problem and show that it is npcomplete to solve the problem within a reasonable amount of time we divide it into three subproblems covering partitioning and layout our empirical results demonstrate that our technique produces nearly as good performance as handoptimized outputs for many kernels
market equilibrium via a primaldual algorithm for a convex program,we give the first polynomial_time_algorithm for exactly computing an equilibrium for the linear utilities case of the market model defined by fisher our algorithm uses the primaldual paradigm in the enhanced setting of kkt conditions and convex programs we pinpoint the added difficulty raised by this setting and the manner in which our algorithm circumvents it
enhancement of an optical_fiber_sensor source_separation based on brillouin spectrum,distributed optical_fiber_sensors have gained an increasingly prominent role in structuralhealth monitoring these are composed of an optical_fiber cable in which a light impulse is launched by an optoelectronic device the scattered light is of interest in the spectral domain the spontaneous brillouin spectrum is centered on the brillouin frequency which is related to the local strain and temperature changes in the optical_fiber when coupled with an industrial brillouin optical timedomain analyzer botda an optical_fiber cable can provide distributed measurements of strain andor temperature with a spatial resolution over kilometers of 40 cm this paper focuses on the functioning of a botda device where we address the problem of the improvement of spatial resolution we model a brillouin spectrum measured within an integration base of 1 m as the superposition of the elementary spectra contained in the base then the spectral distortion phenomenon can be mathematically explained if the strain is not constant within the integration base the brillouin spectrum is composed of several elementary spectra that are centered on different local brillouin frequencies we propose a source_separation methodology approach to decompose a measured brillouin spectrum into its spectral components the local brillouin frequencies and amplitudes are related to a portion of the integration base where the strain is constant a layout algorithm allows the estimation of a strain profile with new spatial resolution chosen by the user numerical tests enable the finding of the optimal parameters which provides a reduction to 1 cm of the 40cm spatial resolution of the botda device these parameters are highlighted during a comparison with a reference strain profile acquired by a 5cmresolution rayleigh scatter analyzer under controlled conditions in comparison with the botda strain profile our estimated strain profile has better accuracy with centimeter spatial resolution
intra dynamic scenario relations and a dynamic decision making algorithm,in this paper we present the composition of a general dynamic scenario the relations of its components and a dynamic making algorithm called eca evaluation and correction algorithm the paper proposes three relations namely stable relation controllable relation and uncontrollable relation numerical_results are given by some simple dynamic scenarios
pgmra a web_server for phenotype  genotype manytomany relation analysis in gwas,it has been proposed that single nucleotide polymorphisms snps discovered by genomewide association studies gwas account for only a small fraction of the genetic_variation of complex traits in human population the remaining unexplained variance or missing heritability is thought to be due to marginal effects of many loci with small effects and has eluded attempts to identify its sources combination of different studies appears to resolve in part this problem however neither individual gwas nor metaanalytic combinations thereof are helpful for disclosing which genetic variants contribute to explain a particular phenotype here we propose that most of the missing heritability is latent in the gwas data which conceals intermediate phenotypes to uncover such latent information we propose the pgmra server that introduces phenomicsthe full set of phenotype features of an individualto identify snpset structures in a broader sense ie causally cohesive genotype phenotype relations these relations are agnostically identified without considering disease status of the subjects and organized in an interpretable fashion then by incorporating a posteriori the subject status within each relation we can establish the risk surface of a disease in an unbiased mode this approach complementsinstead of replaces current analysis methods the server is publically available at httpphopugresfenogeno
parallel lanczos bidiagonalization for total least squares filter in robot_navigation,in the robot_navigation problem noisy sensor_data must be filtered to obtain the best estimate of the robot position the discrete kalman_filter which usually is used for prediction and detection of signals in communication and control_problems has become a commonly used method to reduce the effect of uncertainty from the sensor_data however due to the special domain of robot_navigation the kalman approach is very limited the use of total least squares filter has been proposed boley and sutherland 1993 which is capable of converging with many fewer readings and achieving greater accuracy than the classical kalman_filter the main disadvantage of those approaches is that they can not deal with the case where the noise_subspace is of dimension higher than one here a parallel krylov_subspace_method on parallel distributed_memory computers which uses the lanczos bidiagonalization process with updating techniques is proposed which is more computationally attractive to solve the total least squares problems the parallel_algorithm is derived such that all inner products of a single iteration step are independent therefore the cost of global communication which represents the bottleneck of the parallel_performance on parallel distributed_memory computers can be significantly reduced this filter is very promising for very large data information and from our very preliminary experiments we can obtain more precise accuracy and better speedup
bit and power loading for the multiband impulse_radio uwb architecture,in this paper we present two different bit and power loading algorithms for the noncoherent multiband impulse uwb architecture the first one is a very simple threshold based bit loading algorithm and an extension of the detect and avoid daa algorithm presented in 1 the second one is a more powerful algorithm enabling also power loading these algorithms allow a more efficient and flexible spectrum use higher data rates and use less transmission_power the bit_error_rate_performance is significantly improved both algorithms support inherent powerful daa
intelligent decision system and its application in business innovation self assessment,in this paper it is described how a multiple criteria decision_analysis software tool the intelligent decision system ids can be used to help business selfassessment following a brief outline of a model for assessing business innovation capability and the ids software the process of using ids to implement different types of assessment questions is discussed it is demonstrated that ids is a flexible tool capable of handling different types of data in selfassessment including uncertain and incomplete data and providing a wide range of information including scores performance diversity strength and weakness profile and graphics
cold delay defect screening,delay defects can escape detection during the normal production test flow particularly if they do not affect any of the long paths included in the test flow some delay defects can have their delay increased making them easier to detect by carrying out the test with a very low supply voltage vlv testing however vlv testing is not effective for delay defects caused by high resistance interconnects this paper presents a screening technique for such defects this technique cold testing relies on carrying out the test at low temperature one particular type of defect silicide open is analyzed and experimental data are presented to demonstrate the effectiveness of cold testing
knowledge_representation for conceptual simulation modeling,simulation is a powerful tool that helps decision makers in business and industry to solve difficult and complex problems reduce cost improve quality and productivity and shorten timetomarket however the technology is still underutilized in many applications due to several reasons in this study we address these issues using a knowledge_engineering approach ie develop efficient and robust models and formats to capture represent and organize the knowledge for developing conceptual simulation models that can be generalized and interfaced with different applications and implementation tools the research fits into a larger project effort that aims to create a sustained research program on knowledgebased simulation
an applicationlevel implementation of causal timestamps and causal ordering,maintenance of causality information in distributed_systems has previously been implemented in the communications infrastructure with the focus on providing reliability and availability for distributed services while this approach has a number of advantages moving causality information up into the view and control of the application programmer is useful and in some cases preferable in an experiment at the university of queensland libraries to support applicationlevel maintenance of causality information have been implemented the libraries allow the collection and use of causality information under programmer control supplying a basis for making causal dependency information available for application management and troubleshooting the libraries are also unique in supporting existing distributed_systems based on the remote procedure call paradigm this paper describes the underlying theory of causality and the design and implementation of the libraries an event reporting service example is used to motivate the approach and a number of previously unresolved practical problems are addressed in the design process
a highquality multirate realtime celp coder,the design and implementation of a realtime celp coder for mobile_communication applications are discussed to realize a singlechip implementation several tradeoffs were made without compromising speech_quality in addition techniques that make the coder more robust under a variety of channel_conditions are discussed the realtime coder can be operated at different bit_rates 8 68 46 kbs by simply changing the frame update rates the speech_quality was evaluated through a formal listening test and it was found that this coder compares favorably with other standardized coders operating at similar or higher rates 
weighted least squares training of support_vector classifiers leading to compact and adaptive schemes,an iterative block training method for support_vector classifiers svcs based on weighted least squares wls optimization is presented the algorithm which minimizes structural risk in the primal space is applicable to both linear and nonlinear machines in some nonlinear cases it is necessary to previously find a projection of data onto an intermediatedimensional space by means of either principal_component_analysis or clustering_techniques the proposed approach yields very compact machines the complexity reduction with respect to the svc solution is especially notable in problems with highly overlapped classes furthermore the formulation in terms of wls minimization makes the development of adaptive svcs straightforward opening up new fields of application for this type of model mainly online processing of large amounts of staticstationary data as well as online update in nonstationary scenarios adaptive solutions the performance of this new type of algorithm is analyzed by means of several simulations
a distributed parallel approach for bgp routing_table partitioning in next generation routers,the rapid growth of routing tables represents a major challenge facing the scalability of bgp and indeed the whole internet infrastructure in this paper we introduce a novel distributed algorithmic scheme for partitioning the bgp routing_table on multiple controller cards where we exploit parallelism to enhance both the lookup speed and the scalability of the rib routing_information base the proposed scheme increases the lookup performance by letting unrelated tasks such as the best match prefix bmp lookup and the bgp decision process to be executed in parallel at different controller cards simulations show that our proposal outperforms classical central lookup mechanisms with a reasonably acceptable cost while it increases considerably the space scalability of the bgp routing_table
fast committee machines for regression and classification,in many data_mining_applications we are given a set of training examples and asked to construct a regression machine or a classifier that has low prediction error or low error rate on new examples respectively an important issue is speed especially when there are large amounts of data we show how both classification and prediction error can be reduced by using boosting techniques to implement committee machines in our implementation of committees using either classification trees or regression trees we show how we can trade off speed against either error rate or prediction error
maximumlikelihood detection of nonlinearly distorted multicarrier symbols by iterative_decoding,this paper proposes a new method for decoding multicarrier symbols with severe nonlinear distortion the first part evaluates mutual_information expressions for practical nonlinear models and shows the performance bounds for commonly used receiver structures then we derive the maximumlikelihood ml sequence estimator which unfortunately has an exponential complexity due to the nonlinear distortion this extremely large complexity can be reduced with a simple algorithm that iteratively estimates the nonlinear distortion thereby reducing the exponential ml to the standard ml without nonlinear distortion the proposed method can be used to reduce the peaktoaverage power ratio of multicarrier signals by clipping the transmit sequence it can also be used to correct any nonlinear distortion present in transmitterreceiver amplifiers that are operating close to saturation
pricing longevity bonds based on stochastic mortality forecasting by panel data procedures,in order to hedge the longevity risk longevity bonds are designed whose payoff structure depends on the changes in mortality to forecast the mortality more precisely we use a timedynamic stochastic_model by utilizing a panel data approach to forecast the mortality rates and get a survival index empirical study is conducted with the data in china then we apply these forecasting mortality rates to evaluate one kind of longevity bond it turns out that it is reliable for the social security_systems and the life insurance industry
partitions and edge colourings of multigraphs,erds and lovasz conjectured in 1968 that for every graph_g with chigomegag and any two integers stgeq 2 with stchig1 there is a partition st of the vertex_set vg such that chigsgeq s and chigtgeq t except for a few cases this conjecture is still unsolved in this note we prove the conjecture for line graphs of multigraphs
robust adaptive fuzzy_sliding_mode_control for a class of uncertain_nonlinear_systems with unknown deadzone,in this paper a robust adaptive fuzzy_sliding_mode_control scheme is presented for a class of uncertain_nonlinear_systems preceded by an unknown deadzone deadzone characteristics are quite commonly encountered in actuators such as hydraulic and pneumatic valves electric servomotors and electronic circuits etc therefore by using a description of a deadzone and exploring the properties of this deadzone model intuitively and mathematically a robust adaptive_fuzzy sliding control method is presented without constructing the deadzone inverse the unknown nonlinear_functions of the plant are approximated by the fuzzy_logic_system according to some adaptive_laws based on lyapunov_stability_theorem and the theory of variable_structure_control the proposed robust adaptive fuzzy_sliding_mode_control scheme can guarantee the robust_stability of the whole closedloop system with an unknown deadzone in the actuator and obtain good tracking_performance as well finally an example and simulation results are provided to illustrate the effectiveness of the proposed method
tracer kinetic modeling by moralessmith hypothesis in hepatic perfusion ct,most of the existing tracer kinetic models for dynamic contrastenhanced ct or mri do not fully describe the principles of intra and transcapillary transport of tracers one point is to disregard the concentration profiles between the inlets and outlets of capillaries which may cause a biased estimation of tissue parameters by a systematic error the moralessmith hypothesis enables one to resolve this ambiguity by assuming that the difference between arterial and venous concentrations is proportional to the difference between the arterial and capillary concentrations if the backflow of administered tracer into the plasma compartment is negligible compared to its outflow into the interstitial compartment during the initial enhancement phase after tracer administration the capillary concentration can be considered to fall exponentially along the capillary from the arterial concentration to the venous concentration by the renkincrone model ie unidirectional extraction fraction which can be incorporated in the concept of the moralessmith hypothesis in this study we reformed the massbalance equations and mathematical solutions of several representative and wellknown tracer kinetic models so that the moralessmith hypothesis could be incorporated into their compartment tracer kinetics considering a tissuespecific factor independent of time as proposed by brix et al 5 the tissuespecific factor was applied to a liver tumor case study in perfusion ct to illustrate the potential effectiveness of the moralessmith hypothesis the proposed scheme was shown to be potentially useful for more consistent and reliable estimation of physiologic tissue parameters
introduction to infrastructure security minitrack,
algorithm for decomposing an analytic_signal into am and positive fm components,an analytic_signal permits unambiguous characterization of the phase and envelope of a real signal but the analytic_signals phasederivative ie the instantaneous_frequency if is typically a wild function and can take on values ranging from negative infinity to positive infinity fortunately any analytic_signal can be decomposed into a minimum phase minp signal component and an allphase allp signal component while the minp signals logenvelope and its phase form a hilbert_transform pair the allp signal has a positive definite instantaneous_frequency pif unlike that of the original analytic_signal we propose an elegant computational algorithm that separates the minp and allp components of the analytic_signal the envelope of the minp component corresponds to the am and the pif of the allp component corresponds to the positive fm
processing nested complex sequence pattern queries over event streams,complex event processing cep has become increasingly important for tracking and monitoring applications ranging from health care supply chain management to surveillance these monitoring applications submit complex event queries to track sequences of events that match a given pattern as these systems mature the need for increasingly complex nested sequence queries arises while the stateoftheart cep systems mostly focus on the execution of flat sequence queries only in this paper we now introduce an iterative execution strategy for nested cep queries composed of sequence negation and and or operators lastly we have introduced the promising direction of applying selective caching of intermediate results to optimize the execution our experimental study using realworld stock trades evaluates the performance of our proposed iterative execution strategy for different query types
learning topdown grouping of compositional hierarchies for recognition,the complexity of real world image categorization and scene analysis requires compositional strategies for object representation this contribution establishes a compositional hierarchy by first performing a perceptual bottomup grouping of edge pixels to generate salient contour curves a subsequent recursive topdown grouping yields a hierarchy of compositions all entities in the compositional hierarchy are incorporated in a bayesian_network that couples them together by means of a shape model the probabilistic_model underlying topdown grouping as well as the shape model is learned automatically from a set of training images for the given categories as a consequence compositionality simplifies the learning of complex category models by building them from simple frequently used compositions the architecture is evaluated on the highly challenging caltech 101 database1 which exhibits large intracategory variations the proposed compositional approach shows competitive retrieval rates in the range of 53 0  0 49
on the energyefficiency of speculative hardware,microprocessor trends are moving towards wider architectures and more aggressive speculation with the increasing transistor budgets energy consumption has become a critical design constraint to address this problem several researchers have proposed and evaluated energyefficient variants of speculation mechanisms however such hardware is typically evaluated in isolation and its impact on the energy consumption of the rest of the processor for example due to wrongpath executions is ignored moreover the available metrics that would provide a thorough evaluation of an architectural optimization employ somewhat complicated formulas with hardtomeasure parametersin this paper we introduce a simple method to accurately compare the energyefficiency of speculative architectures our metric is based on runtime analysis of the entire processor chip and thus captures the energy consumption due to the positive as well as the negative activities that arise from the speculation activities we demonstrate the usefulness of our metric on the example of value speculation where we found some proposed value predictors including lowpower designs not to be energyefficient
a new clutter rejection algorithm for doppler ultrasound,several strategies known as clutter or wall doppler filtering were proposed to remove the strong echoes produced by stationary or slow moving tissue structures from the doppler blood flow signal in this study the matching pursuit mp method is proposed to remove clutter components the mp method decomposes the doppler signal into wavelet atoms that are selected in a decreasing energy order thus the highenergy clutter components are extracted first in the present study the pulsatile doppler signal sn was simulated by a sum of randomphase sinusoids two types of highamplitude clutter signals were then superimposed on sn timevarying lowfrequency components covering systole and early diastole and short transient clutter signals distributed within the whole cardiac cycle the doppler signals were modeled with the mp method and the most dominant atoms were subtracted from the timedomain signal sn until the signaltoclutter sc ratio reached a maximum for the lowfrequency clutter signal the improvement in sc ratio was 190 spl plusmn 06 db and 720 spl plusmn 45 atoms were required to reach this performance for the transient clutter signal ten atoms were required and the maximum improvement in sc ratio was 55 spl plusmn 05 db the performance of the mp method was also tested on real data recorded over the common carotid artery of a normal subject removing 15 atoms significantly improved the appearance of the doppler sonogram contaminated with lowfrequency clutter many more atoms over 200 were required to remove transient clutter components these results suggest the possibility of using this signal_processing approach to implement clutter rejection filters on ultrasound commercial instruments
causal time series analysis of functional magnetic resonance imaging data,this review focuses on dynamic causal analysis of functional magnetic resonance fmri data to infer brain connectivity from a time series analysis and dynamical systems perspective causal influence is expressed in the wienerakaikegrangerschweder wags tradition and dynamical systems are treated in a state_space modeling framework the nature of the fmri signal is reviewed with emphasis on the involved neuronal physiological and physical processes and their modeling as dynamical systems in this context two streams of development in modeling causal brain connectivity using fmri are discussed time series approaches to causality in a discrete_time tradition and dynamic systems and control_theory approaches in a continuous_time tradition this review closes with discussion of ongoing work and future perspectives on the integration of the two approaches
medium_access_control in ad_hoc_networks with mimo links optimization considerations and algorithms,we present a medium_access_control mac_protocol for ad_hoc_networks with multiple_input_multiple_output mimo links mimo links provide extremely high spectral_efficiencies in multipath_channels by simultaneously transmitting multiple independent data_streams in the same channel mac_protocols have been proposed in related work for ad_hoc_networks with other classes of smart_antennas such as switched beam antennas however as we substantiate in the paper the unique characteristics of mimo links coupled with several key optimization considerations necessitate an entirely new mac_protocol we identify several advantages of mimo links and discuss key optimization considerations that can help in realizing an effective mac_protocol for such an environment we present a centralized algorithm called streamcontrolled medium_access scma that has the key optimization considerations incorporated in its design finally we present a distributed scma protocol that approximates the centralized algorithm and compare its performance against that of baseline protocols that are csmaca variants
design and implementation of physical_layer private_key setting for wireless_networks,due to the enormous spreading of applied wireless_networks security is actually one of the most important issues for telecommunications one of the main issue in the field of securing wireless information exchanging is the initial common knowledge between source and destination a shared secret is normally mandatory in order to decide the encryption_algorithm or code or key of the information stream it is usual to exchange this common a priori knowledge by using a secure channel now a days a secure wireless_channel is not possible in fact normally the common a priori knowledge is already established but this is not secure or by using a nonradio channel that implies a waste of time and resource this contribution deals with the proposal of a new modulation technique ensuring secure communication in a full wireless environment the information is modulated at physical_layer by the thermal noise experienced by the link between two terminals a loop scheme is designed for unique recovering of mutual_information the probability of errordetection is analytically derived for the legal users and for the third unwanted listener the proposed scheme has also been implemented in a xilinx virtex ii fpgarnrnall the results show that the performance of the proposed scheme yields the advantage of intrinsic security ie the mutual_information cannot be physically demodulated passive attack or denied active attack by a third terminal leading us to conclude that the proposed technique is really useful for private key_distribution in every wireless_network
circular acoustic vectorsensor array for mode beamforming,undersea warfare relies heavily on acoustic means to detect a submerged vessel the frequency of the acoustic_signal radiated by the vessel is typically very low thus requires a large array aperture to achieve acceptable angular resolution in this paper we present a novel approach for lowfrequency directionofarrival doa estimation using miniature circular vectorsensor array mounted on the perimeter of a cylinder under this approach we conduct beamforming using decomposition in the acoustic mode domain rather than frequency_domain to avoid the long wavelength constraints we first introduce a multilayer acoustic gradient scattering model to provide a guideline and performance predication tool for the mode beamformer design and algorithm we optimize the array_gain and frequency response with this model we further develop the adaptive doa estimation algorithm based on this model we formulate the capon spectra of the mode beamformer which is independent of the frequency band after the mode decomposition numerical simulations are conducted to quantify the performance and evaluate the theoretical results developed in this study
on the expressive power of klaimbased calculi,we study the expressive power of variants of klaim an experimental language with programming primitives for networkaware programming that combines the process algebra approach with the coordinationoriented one klaim has proved to be suitable for programming a wide range of distributed_applications with agents and code mobility and has been implemented on the top of a runtime_system written in java in this paper the expressivity of its constructs is tested by distilling from it a few more and more foundational languages and by studying the encoding of each of them into a simpler one the expressive power of the considered calculi is finally tested by comparing one of them with asynchronous calculus
fast principal_component_analysis using eigenspace merging,in this paper we propose a fast algorithm for principal_component_analysis pca dealing with large highdimensional data sets a large data set is firstly divided into several small data sets then the traditional pca_method is applied on each small data set and several eigenspace models are obtained where each eigenspace model is computed from a small data set at last these eigenspace models are merged into one eigenspace model which contains the pca result of the original data set experiments on the feret data set show that this algorithm is much faster than the traditional pca_method while the principal_components and the reconstruction errors are almost the same as that given by the traditional method
towards the automated generation of hard disk models through physical geometry discovery,as the high performance computing industry moves towards the exascale era of computing parallel scientific and engineering applications are becoming increasingly complex the use of simulation allows us to predict how an applications performance will change with the adoption of new hardware or software helping to inform procurement decisions in this paper we present a disk simulator designed to predict the performance of read and write_operations to a single hard disk drive hdd our simulator uses a geometry discovery benchmark diskovery in order to estimate the data layout of the hdd as well as the time spent moving the readwrite head we validate our simulator against two different hdds using a benchmark designed to simulate common disk read and write patterns demonstrating accuracy to within 5 of the observed io time for sequential operations and to within 10 of the observed time for seekheavy workloads
a novel approach for binarization of overlay text,in this paper we presents a new binarization approach to extract text pixels from complex background in video frames the binarization computation is a crucial step for video text_recognition which can greatly increase the recognition accuracy of an ocr software the proposed approach consists of four phases first the text polarity is determined ie light text with dark background or dark text with light background then the pixels in the given image are clustered into k clusters using the kmeans algorithm in the rgb_color_space and the text cluster is selected based on the text polarity further the mrf model is exploited to get the binarization result finally the result is further refined by the loggabor filter the experimental results on a large dataset show that the significant gains have been obtained according to the segmentation_performance on the pixel level as well as the ocr accuracy
an efficient_architecture for jpeg2000 coprocessor,jpeg2000 is a new international standard for still image_compression it provides various functions in one single coding stream and the better compression quality than the traditional jpeg especially in the high_compression_ratio however the heavy computation and large internal memory requirement still restrict the consumer electronics applications in this paper we propose a qcb quad code blockbased dwt method to achieve the higher parallelism than the traditional dwt approach of jpeg2000 coding process based on the qcbbased dwt engine three code blocks can be completely generated after every fixed time slice recursively thus the dwt and ebcot processors can process simultaneously and the high computational ebcot then has the higher parallelism of the jpeg2000 encoding system by changing the output timing of the dwt process and parallelizing with ebcot the internal tile memory size can be reduced by a factor of 4 the memory_access cycles between the internal tile memory and the code block memory also decrease with the smooth encoding flow
predicting billboard success using datamining in p2p_networks,peer_to_peer_networks are the leading cause for music piracy but also used for music sampling prior to purchase in this paper we investigate the relations between music file_sharing and sales both physical and digitalusing large peertopeer query database information we compare file sharing_information on songs to their popularity on the billboard hot 100 and the billboard digital songs charts and show that popularity trends of songs on the billboard have very strong correlation 088089 to their popularity on a peertopeer network we then show how this correlation can be utilized by common data mining_algorithms to predict a songs success in the billboard in advance using peertopeer information
effects of popular exemplars in television news,common people that are apparently randomly selected by journalists to illustrate a news story popular exemplars have a substantial effect on what the audience think about the issue this effect may be partly due to the mere fact that popular exemplars attract attention and act as attention commanders just like many other speaking sources in the news yet popular exemplars effects extend well beyond that of other talking sources due to their similarity trustworthiness and the vividness of their account popular exemplars have significantly more impact than experts that are being interviewed or in particular than politicians that are quoted in the news we show this drawing on an internetbased experiment that uses fake television news items as stimuli and that systematically compares the effect of these talking sources in the news we also find that taking into account preexisting attitudes changes the findings substantially the effects are more robust and yield a more nuanced picture of what typ
on secondorder statistics of logperiodogram with correlated components,we derive an explicit expression for the covariance of the logperiodogram power spectral density estimator for a zero mean gaussian_process we do not make the assumption that the spectral components of the process are uncorrelated applications to spectral estimation and to cepstral modeling in automatic_speech_recognition are discussed
an additive exponential noise channel with a transmission deadline,we derive the maximum_mutual_information for an additive exponential noise aen channel with a peak input constraint we find that the optimizing input density is mixed with singularities similar to previous results for aen channels with a mean input constraint likewise the maximum_mutual_information takes a similar form though obviously the maximum for the peak constraint is smaller than for the corresponding meanconstrained channel this model is inspired by multiple biological phenomena and processes which can be abstracted as follows inscribed matter is sent by an emitter moves through a medium and arrives eventually at its destination receptor the inscribed matter can convey information in a variety of ways such as the number of signaling quanta  molecules macromolecular complexes organelles cells and tissues  that are emitted as well as the detailed pattern of their release however rather than focus on a general class of emitterreceptor systems or a particular exemplar of biomedical importance our ultimate goal is to provide bounds on the potential efficacy of timedrelease signaling for any system which emits identical signaling quanta that is we seek to apply one of the most potent aspects of information_theory to biological signaling  mechanism blindness  in the hopes of gaining insights applicable to diverse systems that span a wide range of spatiotemporal scales
exhaustive search for small fully absorbing sets and the corresponding low errorfloor decoder,this work provides an exhaustive search algorithm for finding small fully absorbing sets fass of arbitrary lowdensity paritycheck ldpc_codes in particular given any ldpc_code the problem of finding all fass of size less than t is formulated as an integer programming problem for which a new branchbound algorithm is devised new node selection and the treetrimming mechanisms are designed to further enhance the efficiency of the algorithm the proposed algorithm is capable of finding all fass of size  11 with no larger than 2 induced odddegree check_nodes for ldpc_codes of length  1000 the resulting exhaustive list of small fass is then used to devise a new postprocessing decoder numerical_results show that by taking advantage of the exhaustive list of small fass the proposed decoder significantly lowers the error_floor for codes of practical lengths and outperforms the stateoftheart lowerrorfloor decoders
a multiobjective artificial immune algorithm for parameter optimization in support_vector_machine,support_vector_machine svm is a classification method based on the structured risk minimization principle penalize c and kernel s parameters of svm must be carefully selected in establishing an efficient svm_model these parameters are selected by trial and error or mans experience artificial immune system ais can be defined as a soft_computing method inspired by theoretical immune system in order to solve science and engineering problems a multiobjective artificial immune algorithm has been used to optimize the kernel and penalize parameters of svm in this paper in training stage of svm multiple solutions are found by using multiobjective artificial immune algorithm and then these parameters are evaluated in test stage the proposed algorithm is applied to fault diagnosis of induction motors and anomaly detection_problems and successful results are obtained
a novel hidden station detection mechanism in ieee 80211 wlan,the popular ieee 80211 wireless local area network wlan is based on a carrier_sense_multiple_access with collision_avoidance csmaca where a station listens to the medium before transmission in order to avoid collision if there exist stations which can not hear each other ie hidden stations the potential collision probability increases thus dramatically degrading the network throughput the rtscts requesttosendcleartosend frame exchange is a solution for the hidden station problem but the rtscts exchange itself consumes the network resources by transmitting the control frames in order to maximize the network throughput we need to use the rtscts exchange adaptively only when hidden stations exist in the network in this letter a simple but very effective hidden station detection mechanism is proposed once a station detects the hidden stations via the proposed detection mechanism it can trigger the usage of the rtscts exchange the simulation results demonstrate that the proposed mechanism can provide the maximum system throughput performance
low complexity algorithm for robust video frame rate upconversion fruc technique,two challenging situations for video frame rate upconversion fruc are first identified and analyzed namely when the input video has abrupt illumination change andor a low frame rate then a lowcomplexity processing technique and robust fruc algorithm are proposed to address these two issues the proposed algorithm utilizes a translational motion_vector model of the first and the secondorder and detects the continuity of these motion_vectors additionally in order to improve perceptual quality of interpolated frame spatial smoothness criterion is employed the superior performance of the proposed algorithm has been tested extensively and representative examples are given in this work
timestamping schemes for mpeg2 systems layer and their effect on receiver clock_recovery,we propose and analyze several strategies for performing timestamping of an mpeg2 transport stream transmitted over a packetswitched network using the pcrunaware encapsulation scheme and analyze their effect on the quality of the recovered clock at the mpeg2 systems decoder when the timestamping scheme is based on a timer with a fixed period the pcr values in the packet stream may switch polarity deterministically at a frequency determined by the timer period and the transport rate of the mpeg signal this in turn can degrade the duality of the recovered clock at the receiver beyond acceptable limits we consider three timestamping schemes for solving this problem 1 selecting a deterministic timer period to avoid the phase difference in pcr values altogether 2 finetuning the deterministic timer period to maximize the frequency of pcr polarity changes and 3 selecting the timer period randomly to eliminate the deterministic pcr polarity changes for the case of deterministic timer period we derive the frequency of the pcr polarity changes as a function of the timer period and the transport rate and use it to find ranges of the timer period for acceptable quality of the recovered clock we also analyze a random timestamping procedure based on a random telegraph process and obtain lower bounds on the rate of pcr polarity changes such that the recovered clock does not violate the palntsc clock specifications the analytical_results are verified by simulations with both synthetic and actual mpeg2 transport streams sent to a simulation model of an mpeg2 systems decoder
pyramid transform and scalespace analysis in image_analysis,the pyramid transform compresses images while preserving global_features such as edges and segments the pyramid transform is efficiently used in optical_flow_computation starting from planar images captured by pinhole camera systems since the propagation of features from coarse sampling to fine sampling allows the computation of both large displacements in lowresolution images sampled by a coarse grid and small displacements in highresolution images sampled by a fine gridrnrnthe image pyramid transform involves the resizing of an image by downsampling after convolution with the gaussian_kernel since the convolution with the gaussian_kernel for smoothing is derived as the solution of a linear diffusion_equation the pyramid transform is performed by applying a downsampling operation to the solution of the linear diffusion_equation
adaptive modelling of digital hearing aids using a subband affine projection algorithm,adaptive modeling of digital hearing aids is useful in characterizing the hearing aid behavior in response to real world stimuli such as speech and music most modern hearing aids employ amplitude compression in different_frequency bands for effective mapping of the wide_dynamic_range audio_signals into the reduced dynamic range of the hearing impaired listeners due to the presence of independent compression channels the conventional fullband adaptive model might not adequately characterize the performance of a multichannel compression hearing aid mcha in this paper we propose a subband adaptive modeling approach to characterize the electroacoustic performance of a mcha the proposed structure employs uniform oversampled dft filterbanks for analysis and synthesis and the affine projection algorithm for adaptive modeling in each subband experiments with simulated mchas showed that the subband structure outperforms the fullband structure under a variety of operating conditions
a cost effective centralized adaptive_routing for networksonchip,as the number of applications and programmable units in cmps and mpsocs increases the networkonchip noc encounters unpredictable heterogeneous and time dependent traffic loads this motivates the introduction of adaptive_routing mechanisms that balance the nocs loads and achieve higher throughput compared with traditional oblivious routing_schemes an effective adaptive_routing scheme should be based on a global view of the network state however most current adaptive_routing schemes following offchip networks are based on distributed reactions to local congestion in this paper we leverage the unique onchip capabilities and introduce a novel paradigm of noc centralized adaptive_routing our scheme continuously monitors the global traffic load in the network and modifies the routing of packets to improve load balancing accordingly we present a specific design for the case of mesh_topology where xy or yx routes are adaptively selected for each sourcedestination pair we show that while our implementation is lightweight and scalable in hardware costs it outperforms oblivious and distributed adaptive_routing schemes in terms of load balancing and average packet_delay
robust directional features for wordspotting in degraded syriac manuscripts,this paper presents a contribution to word_spotting applied for digitized syriac manuscripts the syriac language was wrongfully accused of being a dead language and has been set aside by the domain of handwriting_recognition yet it is a very fascinating handwriting that combines the word structure and calligraphy of the arabic handwriting with the particularity of being intentionally written tilted by an angle of approximately 45deg for the spotting process we developed a method that should find all occurrences of a certain query word image based on a selective sliding window technique from which we extract directional features and afterwards perform a matching using euclidean distance correspondence between features the proposed method does not require any prior_information and does not depend of a word to character_segmentation algorithm which would be extremely complex to realize due to the tilted nature of the handwriting
statistical semantics for enhancing document_clustering,document clustering_algorithms usually use vector_space_model vsm as their underlying model for document_representation vsm assumes that terms are independent and accordingly ignores any semantic_relations between them this results in mapping documents to a space where the proximity between document vectors does not reflect their true semantic_similarity this paper proposes new models for document_representation that capture semantic_similarity between documents based on measures of correlations between their terms the paper uses the proposed models to enhance the effectiveness of different algorithms for document_clustering the proposed representation models define a corpusspecific semantic_similarity by estimating measures of termterm correlations from the documents to be clustered the corpus of documents accordingly defines a context in which semantic_similarity is calculated experiments have been conducted on thirteen benchmark data sets to empirically evaluate the effectiveness of the proposed models and compare them to vsm and other wellknown models for capturing semantic_similarity
transcript mapping for historic handwritten_document images,there is a large number of scanned historical_documents that need to be indexed for archival and retrieval purposes a visual word_spotting scheme that would serve these purposes is a challenging task even when the transcription of the document_image is available we propose a framework for mapping each word in the transcript to the associated word image in the document coarse word mapping based on document constraints is used for lexicon reduction then word mappings are refined using word recognition results by a dynamic programming algorithm that finds the best match while satisfying the constraints
less is more mixedinitiative modelpredictive control with human inputs,this paper presents a new method for injecting human inputs into mixedinitiative interactions between humans and robots the method is based on a modelpredictive control mpc formulation which inevitably involves predicting the system robot_dynamics as well as human input into the future these predictions are complicated by the fact that the human is interacting with the robot causing the prediction method itself to have an effect on future human inputs we investigate and develop different prediction schemes including fixed and variable horizon mpcs and human input estimators of different orders through a searchandrescueinspired human_operator study we arrive at the conclusion that the simplest prediction methods outperform the more complex ones ie in this particular case less is indeed more
enhanced colortheorybased dynamic localization in mobile_wireless_sensor_networks,there are few localization_schemes targeted at mobile_wireless_sensor_networks this paper proposed an enhanced colortheorybased dynamic localization ecdl which is based on the cdl algorithm shee 2005 however the location accuracy of this algorithm depends on the accuracy of the average hop distance derivation therefore the authors present two novel schemes to estimate the average hop distance the authors analyzed the behavior of sensor_nodes communication and computed the expected value of the average hop distance which is 7r9 where r is the radio range in addition since cdl is based on the dvhop scheme the derived shortest_path length is usually larger than the corresponding euclidean distance with this observation the derived shortest_path length can be adjusted by the ratio of the euclidean distance and the shortest_path distance to further enhance the location accuracy finally in mobile_wireless_sensor_networks sensor_nodes may become isolated by employing mobile anchor_nodes the isolation problem can be relieved and hence the location accuracy can be improved simulation results have shown that the location accuracy of ecdl is 5055 better than that of cdl and 7580 better than that of mcl monte_carlo_localization lingxuan and david 2004 in addition the authors have implemented and verified our algorithm on the micaz mote developers kit
biometric binary string generation with detection_rate optimized bit_allocation,extracting binary strings from realvalued templates has been a fundamental issue in many biometric template_protection systems in this paper we present an optimal_bit_allocation method oba by means of it a binary string at a predefined length with maximized overall detection_rate is generated experiments with the binary strings and a hamming_distance classifier on frgc and feret databases show promising performance in terms of far and frr
dynamic identification of a 6 dof robot without joint position data,offline robot dynamic identification methods are mostly based on the use of the inverse dynamic model which is linear with respect to the dynamic parameters this model is calculated with torque and position sampled data while the robot is tracking reference trajectories that excite the system dynamics this allows using linear leastsquares techniques to estimate the parameters this method requires the joint forcetorque and position measurements and the estimate of the joint velocity and acceleration through the bandpass filtering of the joint position at high sampling rates a new method called didim direct and inverse dynamic identification models has been proposed and validated on a 2 degreeoffreedom robot 1 didim method requires only the joint forcetorque measurement it is based on a closedloop simulation of the robot using the direct dynamic model the same structure of the control law and the same reference trajectory for both the actual and the simulated robot the optimal parameters minimize the 2norm of the error between the actual forcetorque and the simulated forcetorque a validation experiment on a 6 dof staubli tx40 robot shows that didim method is very efficient on industrial_robots
bitstream switching in multiple bitrate video_streaming using wynerziv coding,it has been commonly recognized that multiple bitrate mbr encoding provides a concise method for video_streaming over bandwidthfluctuant networks the key problem of the mbr technique lies in how to seamlessly switch one bitstream to another one to tackle this problem we propose a bitstream switching framework based on the wynerziv coding within the propose framework the multiple bitstreams can be individually encoded without data_exchange which also supports the random switching at any desired frame without affecting the original coding_efficiency of the regular bitstream in particular two different implementation schemes under the same framework are presented different from the traditional switching schemes the proposed method can use the same switching frame for the switching from any other bitstream to the current one which means less storage and less encoding efforts simulation results and comparison between the proposed method and the traditional switching method in h264 are also presented
an architecture for linking medical decisionsupport applications to clinical databases and its evaluation,we describe and evaluate a framework the medical_database adaptor meida for linking knowledgebased medical decisionsupport systems mdsss to multiple clinical databases using standard medical schemata and vocabularies our solution involves a set of tools for embedding standard terms and units within knowledge_bases kbs of mdsss a set of methods and tools for mapping the local database db schema and the terms and units relevant to the kb of the mdss into standardized schema terms and units using three heuristics choice of a vocabulary choice of a key term and choice of a measurement unit and a set of tools which at runtime automatically map standard term queries originating from the kb to queries formulated using the local dbs schema terms and units the methodology was successfully evaluated by mapping three kbs to three dbs using a unitdomain matching heuristic reduced the number of termmapping candidates by a mean of 71 even after other heuristics were used runtime access of 10000 records required one second we conclude that mapping mdsss to different local clinical dbs using the threephase methodology and several termmapping heuristics is both feasible and efficient
animation metaphors for objectoriented concepts,program visualization and animation has traditionally been done at the level of the programming_language and its implementation in a computer however novices do not know these concepts and visualizations that build upon programming_language implementation may easily fail in helping novices to learn programming concepts metaphor on the contrary involves the presentation of a new idea in terms of a more familiar one and can facilitate active_learning this paper applies a metaphor approach to objectoriented programming by presenting new metaphors for such concepts as class object object instantiation method invocation parameter passing object reference and garbage collection the use of these metaphors in introductory_programming education is also discussed
amino acid preference and stacking stability of selfassembled cyclic d lalphapeptide nanotube,
performance of recovery_time improvement algorithms for software raids,a software raid is a raid implemented purely in software running on a host computer one problem with software raids is that they do not have access to special hardware such as nvram thus software raids may need to check every parity group of an array for consistency following a host crash or power failure this process of checking parity groups is called recovery and results in long delays when the software raid is restarted the authors review two algorithms to reduce this recovery_time for software raids the pgs bitmap algorithm and the list algorithm they compare the performance of these two algorithms using tracedriven simulations their results show that the pgs bitmap algorithm can reduce recovery_time by a factor of 12 with a response time penalty of less than 1 or by a factor of 50 with a response time penalty of less than 2 and a memory requirement of around 9 kbytes the list algorithm can reduce recovery_time by a factor of 50 but cannot achieve a response time penalty of less than 16
independent gate sram based on asymmetric gate to sourcedrain overlapunderlap device finfet,the readwrite ability of sram cells is one of the major concern in nanometer regime this paper analyzes the stability and performance of asymmetric finfet based different schematic of 6t sram cells the proposed structure exploits asymmetrical behavior of current to improve readwrite stability of sram by exploiting the asymmetricity in proposed structure contradiction between read and write noise margin rnm and wnm is relaxed the overall improvements in static read and write noise margins for proposed asymmetric finfet based independent gate sram igsram are 28 71 and 31 respectively
distributed scalable and static parallel arc consistency algorithms on private memory machines,several arc consistency algorithms for sequential and parallel_processing computers are reviewed three distributed parallel arc consistency algorithmsdspac1 dspac2 and dspac3are introduced and compared with existing algorithms through actual machine experimentation the time required for the dspac algorithms was measured and compared with that for existing sequential algorithms results indicate that the parallel arc consistency algorithms are very effective and that scalability can be efficiently maintained 
provisioning onchip networks under buffered rc interconnect delay variations,a networkonchip noc replaces onchip communication implemented by pointtopoint interconnects in a multicore environment by a set of shared interconnects connected through programmable crosspoints since an noc may provide a number of paths between a given source and destination manufacturing or runtime faults on one interconnect does not necessarily render the chip useless it is partly because of this fault_tolerance that nocs have emerged as a viable alternative for implementing communication between functional units of a chip in the nanometer regime where high defect rates are prevalent in this paper the authors quantify the fault_tolerance offered by an noc against process_variations specifically the authors develop an analytical model for the probability of failure in buffered global noc links due to interconnect dishing and effective channel length variation using the developed probability model the authors study the impact of link failure on the number of cycles required to establish communications in noc applications
accurate distributed rangebased positioning algorithm for wireless_sensor_networks,localization of sensor_nodes is a fundamental and important problem in wireless_sensor_networks in this correspondence a recursive distributed positioning algorithm is devised with the use of range_measurements computer simulations are included to contrast the performance of the proposed approach with the conventional semidefinite_relaxation positioning method as well as crameacuterrao lower bound
prospective multicentre voxel_based_morphometry study employing scanner specific segmentations  procedure development using calibrain structural mri data,backgroundrnstructural magnetic resonance imaging smri of the brain is employed in the assessment of a wide range of neuropsychiatric disorders in order to improve statistical power in such studies it is desirable to pool scanning resources from multiple centres the calibrain project was designed to provide for an assessment of scanner differences at three centres in scotland and to assess the practicality of pooling scans from multiplecentres
improving the scalability of cloudbased resilient database servers,many rely now on public cloud infrastructureasaservice for database servers mainly by pushing the limits of existing pooling and replication software to operate large sharednothing virtual server clusters yet it is unclear whether this is still the best architectural choice namely when cloud infrastructure provides seamless virtual shared storage and bills clients on actual disk usage   this paper addresses this challenge with resilient asynchronous commit rasc an improvement to awellknown sharednothing design based on the assumption that a much larger number of servers is required for scale than for resilience then we compare this proposal to other database server architectures using an analytical model focused on peak throughput and conclude that it provides the best performancecost tradeoff while at the same time addressing a wide range of fault scenarios
a neural networkbased image_processing system for detection of vandal acts in unmanned railway environments,lately the interest in advanced videobased surveillance_applications has been increasing this is especially true in the field of urban railway transport where videobased surveillance can be exploited to face many relevant security aspects eg vandalism overcrowding abandoned object_detection etc this paper aims at investigating an open problem in the implementation of videobased surveillance_systems for transport applications ie the implementation of reliable image_understanding modules in order to recognize dangerous situations with reduced false alarm and misdetection rates we considered the use of a neural networkbased classifier for detecting vandal behavior in metro stations the achieved results show that the classifier achieves very good performance even in the presence of high scene complexity
recursive binary dilation and erosion using digital line structuring elements in arbitrary orientations,performing morphological_operations such as dilation and erosion of binary images using very long line structuring elements is computationally expensive when performed bruteforce following definitions we present twopass algorithms that run at constant time for obtaining binary dilations and erosions with all possible length line structuring elements simultaneously the algorithms run at constant time for any orientation of the line structuring_element another contribution of this paper is the use of the concept of orientation error between a continuous line and its discrete counterpart the orientation error is used in determining the minimum length of the basic digital line structuring_element used in obtaining what we call dilation and erosion transforms the transforms are then thresholded by the length of the desired structuring_element to obtain the dilation and erosion results the algorithms require only one maximum operation for erosion transform and only one minimum operation for dilation transform and one thresholding step and one translation step per result pixel we tested the algorithms on sun sparc station 10 on a set of 240spl times250 salt and pepper_noise images with probability of a pixel being a 1pixel set to 025 for orientations of the normals of the structuring elements in the range spl pi23spl pi2 and lengths in pixels in the range 5145 we achieved a speed up of about 50 and for special orientations spl theta spl isin spl pi2 3spl pi4 spl pi 5spl pi4 3spl pi2 a speed up of about 100 when the structuring elements had lengths of 145 pixels over the bruteforce methods in these experiments we compared the results of our dilation algorithm with those of the algorithm discussed by soille et al see ieee trans pattern anal machine intell vol18 p56267 1996 and showed that for binary dilation and erosion since it is just the dilation of the background with the reflected structuring_element our algorithm performed better and achieved a speed up of about four when dilation or erosion transform alone is obtained
a floating memristor emulator based relaxation oscillator,
using assignment examples to infer weights for electre tri method some experimental results,given a finite set of alternatives a the sorting or assignment_problem consists in the assignment of each alternative to one of the predefined categories in this paper we are interested in multiple criteria sorting problems and more precisely in the existing method electre tri this method requires the elicitation of preferential parameters weights thresholds category limits in order to construct a preference model which the decision maker dm accepts as a working hypothesis in the decision aid study a direct elicitation of these parameters requiring a high cognitive effort from the dm v mosseau r slowinski journal of global optimization 12 2 1998 174 proposed an interactive aggregationdisaggregation approach that infers electre tri parameters indirectly from holistic information ie assignment examples in this approach the determination of electre tri parameters that best restore the assignment examples is formulated through a nonlinear optimization programrnrnin this paper we consider the subproblem of the determination of the weights only the thresholds and category limits being fixed this subproblem leads to solve a linear program rather than nonlinear in the global inference model numerical_experiments were conducted so as to check the behaviour of this disaggregation tool results showed that this tool is able to infer weights that restores in a stable way the assignment examples and that it is able to identify inconsistencies in the assignment examples
a 250 mv 8 kb 40 nm ultralow power 9t supply feedback sram sfsram,low voltage operation of digital circuits continues to be an attractive option for aggressive power reduction as standard sram bitcells are limited to operation in the stronginversion regimes due to process_variations and local mismatch the development of specially designed srams for low voltage operation has become popular in recent years in this paper we present a novel 9t bitcell implementing a supply feedback concept to internally weaken the pullup current during write cycles and thus enable lowvoltage write_operations as opposed to the majority of existing solutions this is achieved without the need for additional peripheral circuits and techniques the proposed bitcell is fully functional under global and local variations at voltages from 250 mv to 11 v in addition the proposed cell presents a lowleakage state reducing power up to 60 as compared to an identically supplied 8t bitcell an 8 kbit sfsram array was implemented and fabricated in a lowpower 40 nm process showing full functionality and ultralow power
mapaided evidential grids for driving scene_understanding,evidential grids have recently been shown to have interesting properties for mobile object perception possessing only partial information is a frequent situation when driving in complex urban areas and by making use of the dempstershafer framework evidential grids are able to handle partial information efficiently this article deals with a lidar perception scheme that is enhanced by georeferenced maps used as an additional source of information in a multigrid fusion framework the paper looks at the key stages of such a data fusion_process and presents an adaptation of the conjunctive combination rule for refining the analysis of conflicting information this method relies on temporal accumulation to distinguish between stationary and moving_objects and applies contextual discounting for modeling information obsolescence as a result the method is able to better characterize the state of the occupied cells by differentiating moving_objects parked cars urban infrastructure and buildings another advantage of this approach is its ability to separate the drivable from the nondrivable free space experiments carried out in real_traffic conditions with a specially equipped car illustrate the performance of this approach
interferential packet detection scheme for a solution to overlapping bss issues in ieee 80211 wlans,in this manuscript an interferential packet detection scheme in ieee 80211 wlans is proposed if another basic service set bss is overlapping domestic bss some stations stas may suffer from interference from a hidden_terminal in overlapping bss obss one of the best ways to avoid this undesirable situation is channel_switching after detecting obss however there are some difficulties to recognize existence of obss one difficulty is that stas in domestic bss cant receive frames from obss correctly and cant check bssid of frames if traffic in domestic bss is heavy and if transmission rates of frames from obss are higher another difficulty is that if the cell radius of domestic bss is smaller than that of obss some stas in obss may transmit frames asynchronously and interfere with transmissions in domestic bss if interference causes frame error domestic sta cant distinguish interference from degradation of channel condition this paper proposes a method whereby stas can detect interferential packet even while stas receive frames from domestic bss if stas detect interference_channel switching is performed dynamically the probability of detecting interferential packets is evaluated by computer simulation and the results confirm the effectiveness of the proposed method
optimal error_estimates for finite element discretization of elliptic optimal control_problems with finitely many pointwise state constraints,in this paper we consider a model elliptic optimal_control_problem with finitely many state constraints in two and three dimensions such problems are challenging due to low regularity of the adjoint variable for the discretization of the problem we consider continuous linear elements on quasiuniform and graded meshes separately our main result establishes optimal a priori error_estimates for the state adjoint and the lagrange multiplier on the two types of meshes in particular in three dimensions the optimal second order convergence rate for all three variables is possible only on properly refined meshes numerical_examples at the end of the paper support our theoretical results
taxonomy of trust categorizing p2p reputation systems,the field of peertopeer reputation systems has exploded in the last few years our goal is to organize existing ideas and work to facilitate system design we present a taxonomy of reputation system components their properties and discuss how user behavior and technical constraints can conflict in our discussion we describe research that exemplifies compromises made to deliver a useable implementable system
performance of a 60ghz dcmofdm and bpskimpulse ultrawideband system with radiooverfiber and wireless transmission employing a directlymodulated vcsel,the performance of radiooverfiber optical_transmission employing verticalcavity surfaceemitting lasers vcsels and further wireless transmission of the two major ultrawideband uwb implementations is reported when operating in the 60ghz radio band performance is evaluated at 144 gbits bitrate the two uwb implementations considered employ dualcarrier modulation orthogonal frequencydivision multiplexing dcmofdm and binary phaseshift keying impulse_radio bpskir modulation respectively optical_transmission distances up to 40 km in standard singlemode fiber and up to 500 m in bendinsensitive singlemode fiber with wireless transmission up to 5 m in both cases is demonstrated with no penalty a simulation analysis has also been performed in order to investigate the operational limits the analysis results are in excellent agreement with the experimental work and indicate good tolerance to chromatic_dispersion due to the chirp characteristics of electrooptical conversion when a directlymodulated vcsel is employed the performance comparison indicates that bpskir uwb exhibits better tolerance to optical_transmission impairments requiring lower received optical power than its dcmofdm uwb counterpart when operating in the 60ghz band
dpll bit synchronizer with rapid acquisition using adaptive_kalman_filtering techniques,a secondorder dpll with timevarying loop gains is applied to the symbol synchronization of burst mode data signals an algorithm to control the dpll loop gains is derived from adaptive_kalman_filtering theory simulation results for the variable gain dpll compared to a fixed gain dpll demonstrate the improved acquisition performance 
a 65 nm cmos quadband sawless receiver soc for gsmgprsedge,a quadband 25g receiver is designed to replace the frontend saw filters with onchip bandpass filters and to integrate the lna matching components as well as the rf baluns the receiver achieves a typical sensitivity of 110 dbm or better while saving a considerable amount of bom utilizing an arrangement of four baseband capacitors and mos switches driven by 4phase 25 dutycycle clocks highq bpfs are realized to attenuate the 0 dbm outofband blocker the 65 nm cmos sawless receiver integrated as a part of a 25g soc draws 55 ma from the battery and measures an outofband 1 dbcompression of greater than 2 dbm measured as a standalone as well as the baseband running in call mode in the platform level the receiver passes the 3gpp specifications with margin
automated evaluation of her2neu immunohistochemical expression in breast cancer using digital microscopy,her2neu her2 has been shown to be a valuable biomarker for breast cancer however interobserver variability has been reported in the evaluation of her2 with immunohistochemistry it has been suggested that automated computerbased evaluation can provide a consistent and objective measure of her2 expression in this manuscript we present an automated method for the quantitative assessment of her2 using digital microscopy the method employs imaging algorithms on whole slide images of tissue specimens for the extraction of two features describing her2 membrane staining namely membrane staining completeness and membrane staining intensity a classifier was trained to merge the extracted features into an overall slide assessment score preliminary results showed good agreement with the provided truth the developed automated method has the potential to be used as a computer aid for the immunohistochemical evaluation of her2 expression with the objective of increasing observer reproducibility
adding expressiveness to musical messages,a system to add expressiveness to musical messages has been developed starting from the results of acoustic and perceptual analyses the system allows to obtain different performances by modifying the acoustic parameters of a given neutral performance the modification of the input performance is performed by a model that uses the hierarchical segmentation of the musical organization for every hierarchical level opportune curves are applied to the principal acoustic parameters levels selfsimilarity is the main criteria to construct the curves the modular structure of the system defines an open architecture where the rendering steps can be realized both with synthesis and postprocessing techniques different synthesis techniques like fm physical models or wavetable have been explored
observations on using empirical studies on developing a knowledgebased software_engineering tool,there exist a wide variety of techniques for performing empirical studies which researchers in humancomputer interaction have adapted from fields of cognitive psychology sociology and anthropology an analysis of several of these techniques is presented through an approach that balances empirical study with tool development the analysis is based on and illustrated with a severalyear experience of consulting in a scientific software environment and in building an evaluating a prototype knowledgebased tool to capture aspects of that experience guidelines for applying specific techniques and cautions about potential pitfalls are discussed many additional examples of using the techniques are cited from the literature 
fire detection by microwave radiometric sensors modeling a scenario in the presence of obstacles,this paper deals with the problem of fire detection in the presence of obstacles that are nontransparent to visible or infrared wavelengths exploiting the obstacle penetration capability of microwaves a solution based on passive microwave radiometry has been proposed to investigate such a solution a theoretical model of the scene sensed by a microwave radiometer is developed accounting for the presence of both fire spot and walllike obstacles by reversing the models equations it is possible to directly relate the obstacle emissivity reflectivity and transmissivity to the antenna noise temperatures measured in several conditions these temperatures have been sensed with a portable lowcost instrument the selected 1265ghz operation frequency features good wall penetration capability to be balanced with a reasonable antenna size in order to verify the aforementioned model several fire experiments have been carried out resulting in an overall good agreement between measurements and developed theory in particular a 2cmthick plasterboard wall typically used for indoor building construction shows a transmissivity equal to 086 and can easily be penetrated by a microwave radiometer in the xband
efficient evaluation of queries with mining predicates,modern relational_database_systems are beginning to support adhoc queries on data_mining models in this paper we explore novel techniques for optimizing queries that apply mining models to relational_data for such queries we use the internal structure of the mining model to automatically derive traditional database predicates we present algorithms for deriving such predicates for some popular discrete mining models decision_trees naive_bayes and clustering our experiments on a microsoft_sql_server 2000 demonstrate that these derived predicates can significantly reduce the cost of evaluating such queries
faster maliciously secure twoparty computation using the gpu,
stochastic_processes via the pathway model,after collecting data from observations or experiments the next step is to analyze the data to build an appropriate mathematical or stochastic_model to describe the data so that further studies can be done with the help of the model in this article the inputoutput type mechanism is considered first where reaction diffusion reactiondiffusion and productiondestruction type physical situations can fit in then techniques are described to produce thicker or thinner tails power law behavior in stochastic_models then the pathway idea is described where one can switch to different functional forms of the probability_density_function through a parameter called the pathway parameter the paper is a continuation of related solar neutrino research published previously in this journal
predictive metamorphic control,model predictive control mpc has become widely accepted in industry the reason for its success are manifold including easy implementation ability to handle constraints capacity to deal with nonlinearities etc however the method does have drawbacks including tuning difficulties in this paper we propose an embellishment to the basic mpc strategy by incorporating a tuning parameter such that one can move continuously from an existing controller to a new mpc strategy the continuous change of this tuning parameter leads to a continuously varying stabilizing control law since the proposed strategy allows one to slowly move from an existing control law to a new and better one we term the strategy predictive metamorphic control for the case of an infinite horizon problem without constraints and for the general case with state and input constraints stability results are established the merits of the proposed method are illustrated by examples
efficient network flow based mincut balanced partitioning,we consider the problem of bipartitioning a circuit into two balanced components that minimizes the number of crossing nets previously the kernighan and lin type kl heuristics the simulated_annealing approach and the spectral method were given to solve the problem however network flow techniques were overlooked as a viable approach to mincut balanced bipartition to due its high complexity in this paper we propose a balanced bipartition heuristic based on repeated maxflow mincut techniques and give an efficient implementation that has the same asymptotic time complexity as that of one maxflow computation we implemented our heuristic algorithm in a package called fbb the experimental results demonstrate that fbb outperforms the kl heuristics and the spectral method in terms of the number of crossing nets and the efficient implementation makes it possible to partition large circuit instances with reasonable runtime for example the average elapsed time for bipartitioning a circuit s35932 of almost 20k gates is less than 20 minutes
reconstruction of polynomial systems from noisy timeseries measurements using genetic_programming,the problem of functional reconstruction of a polynomial system from its noisy timeseries measurement is addressed in this paper the reconstruction requires the determination of the embedding dimension and the unknown polynomial structure the authors propose the use of genetic_programming gp to find the exact functional form and embedding dimension of an unknown polynomial system from its timeseries measurement using functional operators of addition multiplication and time_delay they use gp to reconstruct the exact polynomial system and its embedding dimension the proposed gp approach uses an improved leastsquares ils method to determine the parameters of a polynomial system the ils method is based on the orthogonal euclidean distance to obtain an accurate parameter estimate when the series is corrupted by measurement_noise simulations show that the proposed ilsgp method can successfully reconstruct a polynomial system from its noisy timeseries measurements
grid_service for environmental data retrieval and disasters detection based on satellite image_analysis,considering that one of todays biggest global concerns is related to the climate change and its imminent undesired effects we present the approach of creating and offering a public web service to provide realtime access to environmental data and information one of services direct usages is natural disasters detection but it could be further used for developing complex statistics and prediction generators or for other environment related applications the data is extracted from a satellite imagery repository implemented on a grid infrastructure for testing the capabilities of the service for different type of users a visualization and interaction web_application has been developed the service is integrated in the mediogrid system
an evaluation framework for energy_aware buildings using statistical_model checking,cyberphysical systems are to be found in numerous applications throughout society the principal barrier to develop trustworthy cyberphysical systems is the lack of expressive modelling and specification formalisms supported by efficient tools and methodologies to overcome this barrier we extend in this paper the modelling formalism of the tool uppaalsmc to stochastic hybrid_automata thus providing the expressive power required for modelling complex cyberphysical systems the application of statistical_model checking provides a highly scalable technique for analyzing performance properties of this formalisms
mlflex a flexible toolbox for performing classification analyses in parallel,motivated by a need to classify highdimensional heterogeneous data from the bioinformatics domain we developed mlflex a machinelearning toolbox that enables users to perform twoclass and multiclass_classification analyses in a systematic yet flexible manner mlflex was written in java but is capable of interfacing with thirdparty packages written in other programming_languages it can handle multiple inputdata formats and supports a variety of customizations mlflex provides implementations of various validation strategies which can be executed in parallel across multiple computing cores processors and nodes additionally mlflex supports aggregating evidence across multiple algorithms and data sets via ensemble_learning this opensource software package is freely available from httpmlflexsourceforgenet
pacemaker interference and lowfrequency electric induction in humans by external fields and electrodes,the possibility of interference by lowfrequency external electric fields with cardiac pacemakers is a matter of practical concern for pragmatic reasons experimental investigations into such interference have used contact electrode current sources however the applicability to the external electric field problem remains unclear the recent development of anatomically based electromagnetic models of the human body together with progress in computational_electromagnetics enable the use of numerical_modeling to quantify the relationship between external field and contact electrode excitation this paper presents a comparison between the computed fields induced in a 36mmresolution conductivity model of the human body by an external electric field and by several electrode source configurations involving the feet and either the head or shoulders the application to cardiac pacemaker interference is also indicated
a structurally stable globally adaptive internal model regulator for mimo linear_systems,the problem of compensating an uncertain disturbance andor tracking some reference signals for a general linear mimo system is studied in this work using the robust regulation theory frame the disturbances are assumed to be composed by a known number of distinct sinusoidal_signals with unknown phases amplitude and frequencies under suitable assumptions an exponentially convergent estimator of the unknown disturbance parameters is proposed and introduced into the classical robust regulator design to obtain an adaptive_controller this controller guarantees that the closedloop robust regulation is attained in some neighborhood of the nominal values of the parameters of system a simulated example shows the validity of the proposed approach
developing discriminate model and comparative analysis of differentially expressed genes and pathways for bloodstream samples of diabetes mellitus type 2,backgroundrndiabetes mellitus of type 2 t2d also known as noninsulindependent diabetes mellitus niddm or adultonset diabetes is a common disease it is estimated that more than 300 million people worldwide suffer from t2d in this study we investigated the t2d prediabetic and healthy human no diabetes bloodstream samples using genomic genealogical and phonemic information we identified differentially expressed genes and pathways the study has provided deeper insights into the development of t2d and provided useful information for further effective prevention and treatment of the disease
an autonomous sewer robots navigation based on stereo_camera information,in this paper we propose a method for autonomous sewer robots to navigate through a sewer pipe system based on stereo_camera information in this method local_features such as manholes and pipe joints are extracting as a feature pixels in the region of interest roi of left image then an accurate and fast stereo_matching measure named linear computation is implemented in this roi image to compute the distance between the robots and local_features finally the distance data can be used for navigation map in sewer pipe system the experimental results show that our method can provide sufficient information for autonomous sewer robots navigation
cheshire ii at geoclef fusion and query_expansion for gir,
introducing a geographic_information_system as computer tool to apply the problembased learning process in public buildings indoor routing,abstractrnrna geographic_information_system gis is presented in this work with the aim of helping the application of problembased learning process to show the students how to adopt the appropriate decisions for the adaptation of architectural barriers to ensure the universal accessibility in public buildings the gis developed here consists of three layers based on vector maps corresponding to buildings potential routes and architectural barriers hyperlinks in the last layer allow access to some relevant information about each barrier such as type description and adaptation cost several tests have been carried out to show the capability of the implemented gis to locate indoor barriers determine suitable indoor routes by considering criteria such as paths lengths or the total cost of barrier elimination and update the information corresponding to each architectural barrier in addition the application of the proposed gis has also been explored for indoor route guidance with promising results this investigation has been carried out with the aim of being combined with the problembased learning process  2010 wiley periodicals inc comput appl eng educ 21 573580 2013
a practical locationaided energyaware routing method for uwbbased sensor_networks,in this paper a practical locationaided routing method for sensor_networks based on ultrawideband uwb technique is proposed and evaluated this method makes use of the positioning function of uwb and takes into account the energy consumption in the network by modeling the property of energy consumption we find that energy and qualityofservice qos issues are greatly influenced by the route selected accordingly a new routing_algorithm is derived to search for energyefficient routes that can support adequate qos_requirements the simulation results have proved the advantages of this routing_scheme
waveband switching_networks with limited wavelength_conversion,we study reconfigurable multigranular optical crossconnects mgoxcs in waveband switching_networks with limited wavelength_conversion and propose a heuristic algorithm to minimize the number of used wavelength_converters while reducing the blocking_probability
automatic generation of quadrature formulae for oscillatory integrals,
an identitybased identification scheme based on discrete_logarithms modulo a composite number,we first describe a modification of schnorrs identification scheme in which the modulus is composite instead of prime this modification has some similarity with brickellmccurkys one presented at the same conference then by establishing a new setup we derive the first identitybased identification scheme based on discrete_logarithms more precisely it is based on discrete_logarithm modulo a composite number a problem known to be harder than factorization problem this scheme has interesting and somewhat paradoxical features in particular any user can choose his own secret and provided the parameters have convenient sizes even the trusted center is unable to retrieve it from the public_key contrary to any identitybased scheme known until now
control of artificial pneumatic_muscle for robot application,pneumatic_muscle has many advantages such as elasticity high power and structural similarity to a living things muscle there has been many researches to control robot actuated by pneumatic muscles but conventional theories are hard to apply on real_robot plants because of their assumptions and disregards of pneumatic muscles physical aspects like size of pneumatic_muscle and its controller here the new method for saving space which is occupied by many controllers to operate robot actuated by pneumatic muscles is proposed actually there is easy way to control pneumatic_muscle using the commercial proportional pressure regulator but its size is not suitable to be embedded on stand alone robot so new method using the pressure switches of compact size and encoders is suggested this new method is tested on a robot link with ball joints actuated by four pneumatic muscles
the role of edm in information_management within smes,electronic_document management edm is a new form of information_management edm is described to have certain business values in organizations but no research has been found about edm and small and medium sized enterprises sme in this paper we present an ongoing investigation in two smes guided by the following research questions how are electronic_documents used in the smes and what are the business needs of the smes and how do they correspond with stated edm business values the study was carried out as two qualitative case studies in two smes in the north of sweden the results show that the business need for an sme corresponds with the business values of edm yet is management of electronic_document too dependent on individuals competence very complex when many systems are involved and the context where the document is created is not preserved there is also an emergent need for an organizationspecific classification scheme to enable information_sharing between systems
forensic acquisition and analysis of magnetic tapes,recovering evidential data from magnetic tapes in a forensically sound manner is a difficult task there are many different tape technologies in existence today and an even greater number of archive formats used this paper discusses the issues and challenges involved in the forensic acquisition and analysis of magnetic tapes it identifies areas of slack space on tapes and discusses the challenges of low level acquisition of an entire length of tape it suggests a basic methodology for determining the contents of a tape acquiring tape files and preparing them for forensic_analysis
bifurcation chaos and their control in a timedelay digital tanlock loop,this paper reports the detailed parameter space study of the nonlinear dynamical behaviors and their control in a timedelay digital tanlock loop tdtl at first we explore the nonlinear dynamics of the tdtl in parameter space and show that beyond a certain value of loop gain parameter the system manifests bifurcation and chaos next we consider two variants of the delayed feedback_control dfc technique namely the timedelayed feedback_control tdfc technique and its modified version the extended timedelayed feedback_control etdfc technique stability analyses are carried out to find out the stable phaselocked zone of the system for both the controlled cases we employ twoparameter bifurcation diagrams and the lyapunov_exponent spectrum to explore the dynamics of the system in the global parameter space we establish that the control techniques can extend the stable phaselocked region of operation by controlling the occurrence of bifurcation and chaos we also derive an estimate of the optimum parameter values for which the controlled system has the fastest convergence time even for a larger acquisition range the present study provides a necessary detailed parameter space study that will enable one to design an improved tdtl system
contribution to the determination of in vivo mechanical characteristics of human skin by indentation test,this paper proposes a triphasic model of intact skin in vivo based on a general phenomenological thermohydromechanical and physicochemical thmpc approach of heterogeneous media the skin is seen here as a deforming stratified medium composed of four layers and made out of different fluidsaturated materials which contain also an ionic component all the layers are treated as linear isotropic_materials described by their own behaviour law the numerical simulations of in vivo indentation test performed on human skin are given the numerical_results correlate reasonably well with the typical observations of indented human skin the discussion shows the versatility of this approach to obtain a better understanding on the mechanical behaviour of human skin layers separately
transmit_power adaptation for multiuser ofdm_systems,in this paper we develop a transmit_power adaptation method that maximizes the total data rate of multiuser orthogonal_frequency_division_multiplexing ofdm_systems in a downlink transmission we generally formulate the data rate maximization problem by allowing that a subcarrier could be shared by multiple users the transmit_power adaptation scheme is derived by solving the maximization problem via two steps subcarrier assignment for users and power_allocation for subcarriers we have found that the data rate of a multiuser ofdm system is maximized when each subcarrier is assigned to only one user with the best channel gain for that subcarrier and the transmit_power is distributed over the subcarriers by the waterfilling policy in order to reduce the computational complexity in calculating waterfilling level in the proposed transmit_power adaptation method we also propose a simple method where users with the best channel gain for each subcarrier are selected and then the transmit_power is equally distributed among the subcarriers results show that the total data rate for the proposed transmit_power adaptation methods significantly increases with the number of users owing to the multiuser_diversity effects and is greater than that for the conventional frequencydivision multiple_access fdmalike transmit_power adaptation schemes furthermore we have found that the total data rate of the multiuser ofdm system with the proposed transmit_power adaptation methods becomes even higher than the capacity of the awgn_channel when the number of users is large enough
fine grain associative feature reasoning in collaborative engineering,this paper explores the vast domain of systematic collaborative engineering with reference to product lifecycle management approach from the angle of featurelevel collaboration among partners a new method of fine grain feature association modelling and reasoning is proposed the original contribution is on the explicit modelling and reasoning of collaborative feature relations within a dynamic context a case study has been carried out to illustrate the interweaving feature relations in collaborative oil rig space management and the effective application of such relations modelled in design solution optimisation
multiple ant tracking with global foreground maximization and variable target proposal_distribution,motion and behavior analysis of social insects such as ants requires tracking many ants over time this process is highly laborintensive and tedious automatic_tracking is challenging as ants often interact with one another resulting in frequent occlusions that cause drifts in tracking in addition tracking many objects is computationally expensive in this paper we present a robust and efficient method for tracking multiple ants we first prevent drifts by maximizing the coverage of foreground pixels at at global scale secondly we improve speed by reducing markov_chain length through dynamically changing the target proposal_distribution for perturbed ant selection using a real dataset with ground truth we demonstrate that our algorithm was able to improve the accuracy by 15 resulting in 98 tracking_accuracy and the speed by 76
vqbd exploring semistructured data,
a framework for discrete modeling of juxtacrine signaling systems,juxtacrine signaling is intercellular communication in which the receptor of the signal typically a protein as well as the ligand also typically a protein responsible for the activation of the receptor are anchored in the plasma membranes so that in this type of signaling the activation of the receptor depends on direct contact between the membranes of the cells involved juxtacrine signaling is present in many important cellular events of several organisms especially in the development process we propose a generic formal_model a modeling framework for juxtacrine signaling systems that is a class of dynamic discrete_systems it possesses desirable characteristics in a good modeling framework such as a structural similarity with biological models b capacity of operating in different scales of time and c capacity of explicitly treating both the events and molecular elements that occur in the membrane and those that occur in the intracellular environment and are involved in the juxtacrine signaling process we implemented this framework and used to develop a new discrete model for the neurogenic network and its participation in neuroblast segregation
diy interface for enhanced service customization of remote iot devices a coap based prototype,diy vision for the design of a smart and customizable world in the form of iot demands the involvement of general public in its development process general public lacks the technical depths for programming stateoftheart prototyping and development kits latest iot kits for example intel edison are revolutionizing the diy paradigm for iot and more than ever a diy intuitive programming interface is required to enable masses to interact with and customize the behavior of remote iot devices on the internet this paper presents the novel implementation of such a system enabling general public to customize the behavior of remote iot devices through a visual interface the interface enables the visualization of the resources exposed by a remote coap device in the form of graphical virtual_objects the vos are used to create service design through simple operations like draganddrop and properties settings the design is maintained as an xml document thus being easily distributable and recognizable coap proxy acts as an operation client for the remote device and also provides communication link between the designer and the device the paper presents the architecture detailed design and prototype implementation of the system using stateoftheart technologies
the application and research of ontology_construction technology,in the field of search the application of ontology is an important research topic introduction of ontology_technology in the retrieval system with massive data can make the searching results more comprehensive however now days the ontology is constructed by domain_experts and there are a lot of shortcomings such as complex process long time for the project and difficulty to update thereby in this paper a method of semiautomaticly building ontology is proposed after synthetically analyzing a variety of methods and techniques about it the building process which is based on user_interests mines not only the concepts but also the potential relationships between concepts from the texts by the method of concepts clustering on the basis of such research an unique patent information_retrieval system based on ontology has been completed
intelligent_systems in accounting finance and management isi journal and proceeding citations and research issues from mostcited papers,this paper analyses the citations from intelligent_systems in accounting finance and management that have occurred in isis web of knowledge in february 2010 i found roughly 1000 citations to the journal under 10 different journal name abbreviations with roughly 25p of the citations occurring during 20082009 associated with 27 of the more frequently cited papers using that citation data the hindex and the 40 42 with ties mostcited papers are presented i found that isis new proceedings data appear to have a different citation pattern than isis journal citation data resulting in citations to more sources but fewer citations per source i also examine the research methodologies and applications of the mostcited papers in an attempt to determine what areas have been cited most and where there are potential gaps in the research copyright  2010 john wiley  sons ltd
anomalous network packet detection using data_stream_mining,in recent years significant research has been devoted to the development of intrusion_detection_systems ids able to detect anomalous computer_network traffic indicative of malicious activity while signaturebased ids have proven effective in discovering known attacks anomalybased ids hold the even greater promise of being able to automatically detect previously undocumented threats traditional ids are generally trained in batch mode and therefore cannot adapt to evolving network data_streams in real time to resolve this limitation data_stream_mining techniques can be utilized to create a new type of ids able to dynamically model a stream of network_traffic in this paper we present two methods for anomalous network packet detection based on the data_stream_mining paradigm the first of these is an adapted version of the denstream algorithm for stream clustering specifically tailored to evaluate network_traffic in this algorithm individual packets are treated as points and are flagged as normal or abnormal based on their belonging to either normal or outlier clusters the second algorithm utilizes a histogram to create a model of the evolving network_traffic to which incoming traffic can be compared using pearson_correlation both of these algorithms were tested using the first week of data from the darpa 99 dataset with generic http shellcode and polymorphic attacks inserted we were able to achieve reasonably high detection_rates with moderately low false positive percentages for different types of attacks though detection_rates varied between the two algorithms overall the histogrambased detection_algorithm achieved slightly superior results but required more parameters than the clusteringbased algorithm as a result of its fewer parameter requirements the clustering_approach can be more easily generalized to different types of network_traffic streams
editorial stable funding for open_source_software,
machine_learning in automated text_categorization,the automated categorization or classification of texts into predefined categories has witnessed a booming interest in the last 10 years due to the increased availability of documents in digital form and the ensuing need to organize them in the research community the dominant approach to this problem is based on machine_learning_techniques a general inductive process automatically builds a classifier by learning from a set of preclassified documents the characteristics of the categories the advantages of this approach over the knowledge_engineering approach consisting in the manual definition of a classifier by domain_experts are a very good effectiveness considerable savings in terms of expert labor power and straightforward portability to different domains this survey discusses the main approaches to text_categorization that fall within the machine_learning paradigm we will discuss in detail issues pertaining to three different problems namely document_representation classifier construction and classifier evaluation
intelligent energy management agent for a parallel hybrid vehiclepart ii torque distribution charge sustenance strategies and performance results,this paper represents the second part of a twopart paper on development of an intelligent energy management agent iema for parallel hybrid vehicles in this part energy management strategies for the torque distribution and charge sustenance tasks are established and implemented driving situation awarenessbased fuzzy rule bases are developed to make intelligent decisions on the power split function a charge sustenance strategy is developed in parallel to maintain adequate reserves of energy in the storage device for supporting an extended range of driving simulation study is conducted for the proposed iema and performance results are analyzed to evaluate its viability as a possible solution to and an extendable framework for energy management for parallel hybrid electric vehicles
evolution of adaptive synapses robots with fast adaptive behavior in new environments,this paper is concerned with adaptation capabilities of evolved neural controllers we propose to evolve mechanisms for parameter selforganization instead of evolving the parameters themselves the method consists of encoding a set of local adaptation rules that synapses follow while the robot freely moves in the environment in the experiments presented here the performance of the robot is measured in environments that are different in significant ways from those used during evolution the results show that evolutionary adaptive_controllers solve the task much faster and better than evolutionary standard fixedweight controllers that the method scales up well to large architectures and that evolutionary adaptive_controllers can adapt to environmental changes that involve new sensory characteristics including transfer from simulation to reality and across different robotic_platforms and new spatial relationships
setvalued cooperative_games with fuzzy payoffs the fuzzy assignment game,in this paper we study cooperative_games with fuzzy payoffs the main advantage of the approach presented is the incorporation into the analysis of the problem of ambiguity inherent in many realworld collective decision situations we propose extensions of core concepts which maintain the fuzzy nature of allocations and lead to a more satisfactory study of the problem within the fuzzy context finally we illustrate the extended core concepts and the approach to obtain the corresponding allocations through the analysis of assignment games with uncertain profits
symmetric feedback capacity of the gaussian interference_channel to within one bit,we characterize the symmetric capacity of the twouser gaussian interference_channel with feedback to within 1 bitshz the result makes use of a deterministic_model to provide insights into the gaussian_channel we derive a new outer bound to show that a proposed scheme can achieve the symmetric capacity to within one bit for all channel parameters one consequence of the result is that feedback provides unbounded gain ie the gain becomes arbitrarily large for certain channel parameters it is a surprising result because feedback has been so far known to provide no gain in memoryless pointtopoint channels and only power gain bounded gain in the multiple_access channels the gain comes from using feedback to fully exploit the side information provided by the broadcast nature of the wireless medium
joint prosodic and segmental unit_selection speech_synthesis,
parallel_processing of spatial joins using rtrees,we show that spatial joins are very suitable to be processed on a parallel hardware_platform the parallel system is equipped with a so called shared virtual memory which is well suited for the design and implementation of parallel spatial_join algorithms we start with an algorithm that consists of three phases task creation task assignment and parallel task execution in order to reduce cpu and io cost the three phases are processed in a fashion that preserves spatial locality dynamic_load_balancing is achieved by splitting tasks into smaller ones and reassigning some of the smaller tasks to idle processors in an experimental performance comparison we identify the advantages and disadvantages of several variants of our algorithm the most efficient one shows an almost optimal speed up under the assumption that the number of disks is sufficiently large
a faulttolerant strategy for improving the reliability of service_composition,service_composition is an important means for integrating the individual web services for creating new value added systems that satisfy complex demands since web services exist in the heterogeneous environments on the internet study on how to guarantee the reliability of service_composition in a distributed dynamic and complex environment becomes more and more important this paper proposes a service_composition netscn and faulttolerant strategy to improve the reliability of service_composition the strategy consists of static strategy dynamic strategy and exception handling mechanism which can be used to dynamically adjust component service for achieving good reliability as well as good overall performance scn is adopted to model different components of service_composition the fault_detection and fault recovery_mechanisms are also considered based on the constructed model theories of petri_nets help prove the consistency of processing states and the effectiveness of the strategy a case study of export service illustrates the feasibility of proposed method
cybersim geographic temporal and organizational dynamics of malware propagation,cyberinfractions into a nations strategic security envelope pose a constant and daunting challenge we present the modular cybersim tool which has been developed in response to the need to realistically simulate at a national level software_vulnerabilities and resulting malware propagation in online_social_networks cybersim suite a can generate realistic scalefree networks from a database of geocoordinated computers to closely model social_networks arising from personal and business email contacts and online_communities b maintains for each host a list of installed software along with the latest published vulnerabilities c allows to designate initial nodes where malware gets introduced d simulates using distributed discrete eventdriven technology the spread of malware exploiting a specific vulnerability with packet_delay and user online behavior models e provides a graphical visualization of spread of infection its severity businesses affected etc to the analyst we present sample simulations on a national level network with millions of computers
preface to the special issue commutativity of algebraic diagrams,the problem of the commutativity of algebraic categorical diagrams has attracted the attention of researchers for a long time for example the related notion of coherence was discussed in mac lanes homology book mac lane 1963 see also his ams presidential address mac lane 1976 researchers in category theory view this problem from a specific angle and for them it is not just a question of convenient notation though it is worth mentioning the important role that notation plays in the development of science take for example the progress made after the introduction of symbolic notation in logics or matrix notation in algebra in 1976 peter freyd published the paper properties invariant within equivalence types of categories freyd 1976 where the central role is played by the notion of a diagrammatic property we may also recall the process of diagram chasing and its applications in topology and algebra but before we can use diagrams and the principal property of a diagram is its commutativity it is vital for us to be able to check whether a diagram is commutative
adaptive estimation for spectraltemporal characterization of energetic transient events,we describe a new approach for performing pseudoimaging of point energy sources from spectraltemporal sensor_data pseudoimaging which involves the automatic localization spectrum estimation and identification of energetic sources can be difficult for dim sources andor noisy images or in data containing multiple sources which are closely spaced such that their signatures overlap the new approach is specifically designed for these difficult cases it is developed within the framework of modeling field theory mft a biologicallyinspired neural_network system that has demonstrated practical value in many diverse areas mft performs an efficient optimization over the space of all model parameters and mappings between image pixels and sources or clutter the optimized set of parameters is then used for detection localization and identification of the multiple sources in the data the paper includes results computed from experimental spectrometer data
syntaxbased alignment of multiple translations extracting paraphrases and generating new sentences,we describe a syntaxbased algorithm that automatically builds finite_state_automata word lattices from semantically equivalent translation sets these fsas are good representations of paraphrases they can be used to extract lexical and syntactic paraphrase pairs and to generate new unseen sentences that express the same meaning as the sentences in the input sets our fsas can also predict the correctness of alternative semantic renderings which may be used to evaluate the quality of translations
exception diagnosis in agentbased grid_computing,diagnosing exceptions in multiagent_systems mas is a complex task due to the distributed nature of the data and control in such systems this complexity is exacerbated in open environments where independently developed autonomous_agents interact with each other in order to achieve their goals inevitably exceptions would occur in such mas and these exceptions can arise at one of three levels namely environmental knowledge and social levels in this paper we propose a novel exception diagnosis system that is able to analyse and detect exceptions effectively the proposed architecture consists of specialised exception diagnosis agents called sentinel agents the sentinel agents are equipped with knowledge of observable abnormal situations their underlying causes and resolution strategies associated with these causes the sentinel agent applies a heuristic classification_approach to collect related data from affected agents in order to uncover the underlying causes of the observed symptoms we illustrate and evaluate our proposed architecture using an agentbased grid_computing case study
a hybrid global optimisation algorithm based on locally filled functions and cluster_analysis,in this paper we will extend the definition of a filled function and propose a new definition of a locally filled function the difference between the locally filled function and the classical filled function is illustrated by an example the existence of a locally filled function is also studied in theory based on the locally filled function and cluster_analysis technique we will present a hybrid global optimisation algorithm the algorithm integrates the deterministic and stochastic searching techniques and has a very powerful globally searching ability numerical performance of the new hybrid_algorithm is demonstrated by two examples about the shubert i and sinesquare i functions
multipath aware tcp matcp,on the internet many different paths exist between each source and destination when single_path routing is used these paths can be under utilized not used fairly or not used at all one way to overcome this is to allow multipath_routing but when multiple_paths are used tcp_congestion_control can be negatively affected and cause poor goodput performance due to the reordering of packets we proposematcp multipath aware tcp which makes modifications to tcp that allows it to monitor and select which path it takes through the network for each flow matcp is compared to single_path routing and is validated using extensive simulation matcp is found to greatly improve fairness between flows while providing equal or better utilization of links than single best path networks
a model for mapping between printed and digital document instances,the first steps towards bridging the paperdigital divide have been achieved with the development of a range of technologies that allow printed_documents to be linked to digital_content and services however the static nature of paper and limited structural information encoded in classical paginated formats make it difficult to map between parts of a printed instance of a document and logical elements of a digital instance of the same document especially taking document revisions into account we present a solution to this problem based on a model that combines metadata of the digital and printed instances to enable a seamless mapping between digital documents and their physical counterparts on paper we also describe how the model was used to develop idoc a framework that supports the authoring and publishing of interactive paper documents
bias analysis of source localization using the maximum_likelihood_estimator,the nonlinear nature of the source localization problem creates bias to a location estimate the bias could play a significant role in limiting the performance of localization and tracking when multiple measurements at different instants are available this paper performs bias analysis of the source location estimate obtained by the maximum_likelihood_estimator where the positioning measurements can be toa tdoa or aoa the effect of bias to the meansquare localization error is examined and the amounts of bias introduced by the three types of measurements are contrasted
schedulability_analysis for automated implementations of realtime objectoriented models,the increasing complexity of real time software has led to a recent trend in the use of high level modeling_languages for development of real time software one representative example is the modeling_language room real time object_oriented modeling which provides features such as object orientation state machine description of behaviors formal_semantics for executability of models and possibility of automated code_generation however these modeling_languages largely ignore the timeliness aspect of real_time_systems and fail to provide any guidance for a designer to a priori predict and analyze temporal behavior we consider schedulability_analysis for automated implementations of room models based on the objectime toolset this work builds on results presented by m saksena 1997 where we developed some guidelines for the design and implementation of real time object_oriented models using the guidelines we have modified the run time system library provided by the objectime toolset to make it amenable to schedulability_analysis based on the modified toolset we show how a room model can be analyzed for schedulability taking into account the implementation overheads and structure the analysis is validated experimentally first using simple periodic models and then using a large case study of a train tilting system
multimodal people detection_and_tracking in crowded scenes,this paper presents a novel people detection_and_tracking method based on a multimodal sensor_fusion approach that utilizes 2d laser range and camera data the data_points in the laser scans are clustered using a novel graphbased method and an svm based version of the cascaded adaboost classifier is trained with a set of geometrical features of these clusters in the detection phase the classified laser data is projected into the camera image to define a region of interest for the visionbased people detector this detector is a fast version of the implicit shape model ism that learns an appearance codebook of local sift_descriptors from a set of handlabeled images of pedestrians and uses them in a voting scheme to vote for centers of detected people the extension consists in a fast and detailed analysis of the spatial distribution of voters per detected person each detected person is tracked using a greedy data_association method and multiple extended kalman_filters that use different motion_models this way the filter can cope with a variety of different motion patterns the tracker is asynchronously updated by the detections from the laser and the camera data experiments conducted in realworld outdoor scenarios with crowds of pedestrians demonstrate the usefulness of our approach
interpretable classifiers using rules and bayesian_analysis building a better stroke prediction model,we aim to produce predictive models that are not only accurate but are also interpretable to human experts our models are decision lists which consist of a series of if thenstatements eg if high blood pressure then stroke that discretize a highdimensional multivariate feature_space into a series of simple readily interpretable decision statements we introduce a generative_model called bayesian rule lists that yields a posterior distribution over possible decision lists it employs a novel prior structure to encourage sparsity our experiments show that bayesian rule lists has predictive accuracy on par with the current top algorithms for prediction in machine_learning our method is motivated by recent developments in personalized medicine and can be used to produce highly accurate and interpretable medical scoring systems we demonstrate this by producing an alternative to the chads2 score actively used in clinical practice for estimating the risk of stroke in patients that have atrial fibrillation our model is as interpretable as chads2 but more accurate
stability vs effectiveness improved sentencelevel combination of machine_translation based on weighted mbr,we describe an improved strategy to combine the outputs of machine_translation on sentencelevel balancing the stability and the effectiveness of the combination the new method alternates the classical mbrbased sentencelevel combination with weighted minimum bayes risk wmbr during the calculation of the risk we weight the hypotheses with the performance of the mt system which is measured by the automatic_evaluation metrics on the development data in experiments the wmbrbased method stably achieve better results than other sentencelevel methods and get the best_position in cwmt08 evaluation track outperforming the other wordlevel and sentencelevel combination systems
joint source_and_channel_coding using trellis coded cpm soft decoding,joint source and channel jsc coding using combined trellis coded quantization tcq and continuous_phase_modulation cpm is studied the channel is assumed to be the additive_white_gaussian_noise awgn_channel optimal soft decoding for jsc coding using jointly designed tcqcpm is studied in this paper the soft decoder is based on the a posteriori probability app algorithm for trellis coded cpm it is shown that the systems with soft decoding outperform the systems with hard decoding especially when the systems operate at low to medium signaltonoise ratio snr furthermore a tcq design algorithm for the noisy channel is developed it has been demonstrated that the combined tcqcpm systems are both power and bandwidth efficient compared with the combined tcqtcm8psk systems the novelty of this work is the use of a soft decoder and the app algorithm for combined tcqcpm systems
google phd forum at percom 2009,sixteen phd students presented their research at the phd forum at percom 2009 the phd forum offered the students an opportunity for interaction with and feedback from senior researchers in pervasive_computing
adaptive estimation of human posture using a componentbased model,to detect a human body and recognize its posture a componentbased approach is less susceptible to changes in posture and lighting_conditions this paper proposes a componentbased humanbody model that comprises ten components and their flexible_links each component contains geometrical information appearance information and information on the links with other components the proposed method in this paper uses hierarchical links between components of human body so that it allows to make coarsetofine searches and makes humanbody matching more timeefficient to adaptively estimate the posture in change of posture and illumination we update the component online every time a new human body is incoming
impact of fading wireless_channel on the performance of game theoretic power_control algorithms for cdma wireless data,our goal in this paper is to study the performance of the gametheoretic power_control algorithms for wireless data introduced by saraydar et al 1 in two realistic channels a1 fast flat fading_channel and a2 slow flat fading_channel the fading coefficients under both a1 and a2 are studied under an appropriate small scale channel_model that is used in the cdma cellular_systems namely nakagami channel_model to do so we derive a closed_form expression of the average utility_function which represents the number of bits received correctly at the receiver per one joule expended then using this expression we study the existence uniqueness of nash_equilibrium ne and the social desirability of ne in the pareto sense
guidelines for the creation of braincompatible cyber_security educational material in moodle 20,most current approaches towards information_security education do not have a sound theoretical basis this could lead to the failure of these educational programs furthermore the need for information_security knowledge is no longer only of concern to organizations but has also become a concern for individuals using online services for personal entertainment social networking banking and other activities thus there is a need for cyber_security education for both individuals and organizations such cyber_security educational programs should be based on sound pedagogical theories one such a pedagogically sound approach that could potentially play a role in cyber_security educational programs is brain compatible learning this paper will perform a critical evaluation of an existing information_security education course and evaluate the subject matter in terms of brain compatible learning approaches the aim of the paper is to propose a set of brain compatible learning guidelines for the creation of cyber_security educational material the paper will also argue in favour of the use elearning as a delivery mechanism for such content as such the guidelines will be proposed in the context of a moodle 20 elearning environment
ofdm code division multiplexing with unequal_error_protection and flexible data rate_adaptation,an ofdmcdm orthogonal_frequency_division_multiplexing code division multiplexing system with adaptive symbol mapping is presented this combination enables a robust transmission with flexible error protection and data rate_adaptation for parallel data_streams by exploiting additional diversity due to cdm performance results are presented for fading_channels where ofdmcdm with adaptive symbol mapping and soft interference_cancellation is compared to conventional ofdm_systems also taking into account channel_coding with variable code_rates
realtime communication in distributed environmentrealtime packet filter approach,recent modern operating_system technology enables protocol processing in user space using inkernel packet filter and userlevel protocol processing library for flexibility without sacrificing the performance of traditional kernelized protocol processing this technology can be adapted to build a highly preemptable protocol processing mechanism for distributed realtime environment in this paper we discuss the structural difference of various operating_systems from the protocol processing point of view and propose an extended mechanism of packet filter and userlevel protocol processing library for realtime communication using this mechanism priority of the client can be handed off to the server without priority inversion problem during protocol processing
lets meet at the mobile  learning dialogs with a video_conferencing software for mobile_devices,mobile_phones and related gadgets in networks are omnipresent at our students advertising itself as the platform for mobile pervasive learning currently these devices rapidly open and enhance being soon able to serve as a major platform for rich open multimedia applications and communication in this report we introduce a video_conferencing software which seamlessly integrates mobile with stationary users into fully distributed multiparty conversations following the paradigm of flexible userinitiated group_communication we present an integrated solution which scales well for mediumsize conferences and accounts for the heterogeneous nature of mobile and stationary participants this approach allows for a spontaneous location independent establishment of video dialogs which is of particular importance in interactive learning scenarios the work is based on a highly optimized realization of a h264 codec
security_issues in the development of a wireless bloodglucose monitoring system,this paper is a progress report on an ongoing research effort that makes use of wireless biometric data_management specifically a wireless bloodglucose monitoring system wbgm the goal of this research is to realize appropriate timely intervention by healthcare providers in response to records of unregulated bloodglucose level in order to maintain nearnormal levels the focus of this paper is securitytheir is the possibility that medical information may be used consciously by malicious eavesdropper to the detriment of a patient in search of new employment or a new insurer there is the possibility of malicious or accidental data corruption from database intrusion mobile data transmission is especially problematic in preventing these we present requirements and our approach taken to realize a secure system_architecture key techniques and their implementation details to maintain privacy authentication and data integrity are discussed here
state abstraction discovery from irrelevant state variables,abstraction is a powerful form of domain_knowledge that allows reinforcementlearning agents to cope with complex environments but in most cases a human must supply this knowledge in the absence of such prior knowledge or a given model we propose an algorithm for the automatic discovery of state abstraction from policies learned in one domain for use in other domains that have similar structure to this end we introduce a novel condition for state abstraction in terms of the relevance of state features to optimal behavior and we exhibit statistical_methods that detect this condition robustly finally we show how to apply temporal abstraction to benefit safely from even partial state abstraction in the presence of generalization error
radiation genes a database devoted to microarrays screenings revealing transcriptome alterations induced by ionizing radiation in mammalian cells,the analysis of the great extent of data generated by using dna microarrays technologies has shown that the transcriptional response to radiation can be considerably different depending on the quality the dose range and dose rate of radiation as well as the timing selected for the analysis at present it is very difficult to integrate data obtained under several experimental conditions in different biological systems to reach overall conclusions or build regulatory models which may be tested and validated in fact most available data is buried in different websites public or private in general or local repositories or in files included in published papers it is often in various formats which makes a wide comparison even more difficult the radiation genes database httpwwwcaspuritradiationgenes collects microarrays data from various local and public repositories or from published papers and supplementary materials the database classifies it in terms of significant variables such as radiation quality dose dose rate and sampling timing as to provide userfriendly tools to facilitate data_integration and comparison
divisible load scheduling inwireless sensor_networks with information utility,optimal data scheduling strategies in a hierarchical wireless_sensor_network wsn are considered data_aggregation in clusterheads is considered to find a closed_form solution for the optimal amount of data that is to be reported by each sensor_node using a newly introduced parameter information utility the optimal conditions for the feasible measurement instruction assignment time and for the minimum round time are derived and examined based on the optimal conditions a performance evaluation is demonstrated via simulation study
lowcomplexity detections for downlink mimo mccdma systems,in this paper we investigate and propose the detection techniques for spatially layered multipleinput multipleoutput mimo multicarrier_code_division_multiple_access mccdma systems first we propose a noisepredictive linear detector it has the same bit_error_rate ber_performance as symbollevel detector and reduces the complexity significantly when the system load is almost full we also propose a partial minimum_mean_square_error mmseordered successive_interference_cancellation osic based on multiuser_detection which first detects the most powerful interfering data_symbols transmitted through the determined transmit_antenna and then cancels their contribution from the received signal by the multiplexed data symbol vector the nulling and cancelling processes between the user data_symbols from the same transmit_antenna are not performed the proposed algorithms are verified by computer simulation
automatic synthesis and technology_mapping of combinational logic,skol a system for the synthesis of combinational logic using a library of cells that emphasizes technologymapping algorithms is described it combines current multilevel optimization_techniques with a novel approach to technology_mapping each factor or the factorized boolean equation can be implemented by itself or collapsed into the higher level expression containing it which is then implemented an expression can be implemented in several ways which differ in the degree of factorization a number of selected implementations is evaluated and the one with minimal cost area or delay is chosen the mapping_algorithms are independent of the library of cells which can be easily modified results from benchmark examples were better than or comparable to those for existing systems 
multichannel watermarking of color_images,in the field of image_watermarking research has been mainly focused on grayscale image_watermarking whereas the extension to the color case is usually accomplished by marking the image luminance or by processing each color channel separately a dct_domain watermarking technique expressly designed to exploit the peculiarities of color_images is presented the watermark is hidden within the data by modifying a subset of fullframe dct_coefficients of each color channel detection is based on a global correlation measure which is computed by taking into account the information conveyed by the three color channels as well as their interdependency to ultimately decide whether or not the image contains the watermark the correlation value is compared to a threshold with respect to existing grayscale algorithms a new approach to threshold_selection is proposed which permits reducing the probability of missed detection to a minimum while ensuring a given false detection_probability experimental results as well as theoretical analysis are presented to demonstrate the validity of the new approach with respect to algorithms operating on image luminance only
what to measure next to improve decision making on topdown task driven feature saliency,topdown attention is modeled as decision making based on incomplete information we consider decisions made in a sequential measurement situation where initially only an incomplete input feature_vector is available however where we are given the possibility to acquire additional input values among the missing features the procecure thus poses the question what to do next we take an information theoretical approach implemented for generality in a generative mixture model the framework allows us reduce the decision about what to measure next in a classification problem to the estimation of a few onedimensional integrals per missing feature we demonstrate the viability of the framework on four wellknown classification problems
offline handwriting_recognition using genetic_algorithm,handwriting_recognition enables a person to scribble something on a piece of paper and then convert it into text if we look into the practical reality there are enumerable styles in which a character may be written these styles can be self combined to generate more styles even if a small child knows the basic styles a character can be written he would be able to recognize characters written in styles intermediate between them or formed by their mixture this motivates the use of genetic_algorithms for the problem in order to prove this we made a pool of images of characters we converted them to graphs the graph of every character was intermixed to generate styles intermediate between the styles of parent character character_recognition involved the matching of the graph generated from the unknown character image with the graphs generated by mixing using this method we received an accuracy of 9844
spatiospectral sufficient statistic for mental imagery eeg_signals,classification of mental_tasks from electroencephalogram eeg_signals has important applications in braincomputer interfacing bci however classification of the highly redundant and highdimensional eeg signal with high spatial and spectral correlations is quite challenging therefore the discriminant information especially that of the first and second data moments need to be extracted in the form of uncorrelated features this work addresses this need by approximating a linear minimaldimension sufficient statistic of the eeg matrix data in both spatial and spectral domains as a result of the twodimensional spatiotemporal approach and the generalized sufficiency approximation a significant improvement on the classification accuracy is achieved
privacy_preserving olap,we present techniques for privacypreserving computation of multidimensional aggregates on data partitioned across multiple clients data from different clients is perturbed randomized in order to preserve privacy before it is integrated at the server we develop formal notions of privacy obtained from data_perturbation and show that our perturbation provides guarantees against privacy breaches we develop and analyze algorithms for reconstructing counts of subcubes over perturbed data we also evaluate the tradeoff between privacy guarantees and reconstruction_accuracy and show the practicality of our approach
a new conditioning rule its generalization and evidential_reasoning,in evidence theory several conditioning rules for updating belief have been proposed including dempsters rule of conditioning the paper views the conditioning rules proposed so far and proposes a new rule of conditioning based on three requirements then it generalizes the rule to be applied to the case where condition is given by an uncertain belief the paper also discusses a few interpretations of an equation used for evidential_reasoning one of which is interpreted as conditioning with an uncertain condition
channel_assignment on stronglysimplicial graphs,given a vector spl deltasub 1 spl delta2 spl deltasub t of non increasing positive integers and an undirected_graph g  v e an lspl deltasub 1 spl delta2 spl deltasub tcoloring of g is a function f from the vertex_set v to a set of nonnegative_integers such that fu  f v spl ges spl deltasub i if du v  i 1 spl les i spl les t where duv is the distance ie the minimum number of edges between the vertices u and v this paper presents efficient algorithms for finding optimal l1 1colorings of trees and interval graphs moreover efficient algorithms are also provided for finding approximate lspl deltasub 1 1 1colorings of trees and interval graphs as well as approximate lspl deltasub 1 spl deltasub 2 colorings of unit interval graphs
decomposition methods for multihour synthesis of private telecommunication_networks,the optimal synthesis of private telecommunication_networks subject to multiple nonsimultaneous demands is considered a model is given for the multiservice network synthesis model with an accurate representation of the conversion and access costs while maintaining a classical multicommodity flow representation of the network this is done by increasing the original network to produce an augmented network two decomposition methods for solving the network synthesis problem are presented the first one is a classical lagrangian relaxation method and the second is a resourcedecompositiontype technique some preliminary computational results in both cases are presented these show that the two methods converge within reasonable computation times the results also demonstrate that the multihour aspect of the synthesis of multiservice networks should be taken into account in order to realize fully the economies that are possible due to the noncoincidence of different types of demands 
does web 30 come after web 20 deconstructing theoretical assumptions through practice,current internet research has been influenced by application developers and computer engineers who see the development of the web as being divided into three different stages web 10 web 20 and web 30 this article will argue that this understanding  although important when analysing the political economy of the web  can have serious limitations when applied to everyday contexts and the lived experience of technologies drawing from the context of the italian student movement we show that the division between web 10 web 20 and web 30 is often deconstructed by activists media practices therefore we highlight the importance of developing an approach that  by focusing on practice  draws attention to the interplay between web platforms rather than their transition this approach we believe is essential to the understanding of the complex relationship between web developments human negotiations and everyday social contexts
integrated contextual representation for objects identities and their locations,visual context plays a prominent role in everyday perception contextual information can facilitate recognition of objects within scenes by providing predictions about objects that are most likely to appear in a specific setting along with the locations that are most likely to contain objects in the scene is such identityrelated semantic and locationrelated spatial contextual knowledge represented separately or jointly as a bound representation we conducted a functional magnetic resonance imaging fmri priming experiment whereby semantic and spatial contextual relations between prime and target object pictures were independently manipulated this method allowed us to determine whether the two contextual factors affect object_recognition with or without interacting supporting a unified versus independent representations respectively results revealed a semantic spatial interaction in reaction times for target object_recognition namely significant semantic_priming was obtained when targets were positioned in expected congruent but not in unexpected incongruent locations fmri results showed corresponding interactive effects in brain regions associated with semantic processing inferior prefrontal cortex visual contextual processing parahippocampal cortex and objectrelated processing lateral occipital complex in addition activation in frontoparietal areas suggests that attention and memoryrelated processes might also contribute to the contextual effects observed these findings indicate that object_recognition benefits from associative representations that integrate information about objects identities and their locations and directly modulate activation in objectprocessing cortical regions such context frames are useful in maintaining a coherent and meaningful representation of the visual world and in providing a platform from which predictions can be generated to facilitate perception and action
a retrieval model based on an extended modal_logic and its application to the rime experimental approach,this paper focuses on the query_processing module of rime an experimental prototype of an intelligent information_retrieval system designed to manage highprecision queries on a corpus of medical reports though highly specific this particular corpus is representative of an important class of applications information_retrieval among fulltext specialized documents which constitute critical sources of information in several organizations medicine law space industry this experience allowed us to design and implement an elaborate model for the semantic content of the documents which is an extension of the conceptual dependency approach the underlying retrieval model is inspired from the logic model proposed by cj van rijsbergen which has been considerably refined using an extended modal_logic after presenting the context of the rime project we briefly describe the models designed for the internal representation of medical reports and queries the main part of the paper is then devoted to the retrieval model and its application to the query_processing module of rime which has a natural_language_interface processing a query involves two main phases the interpretation which transforms the natural_language query into a search expression and the evaluation phases which retrieves the corresponding medical reports we focus here on the evaluation phases and show its relationship with the underlying retrieval model evaluations from practical experiments are also given along with indications about current developments of the project
nontermination sets of simple linear loops,a simple linear loop is a simple while loop with linear assignments and linear loop guards if a simple linear loop has only two program variables we give a complete algorithm for computing the set of all the inputs on which the loop does not terminate for the case of more program variables we show that the nontermination set cannot be described by tarski formulae in general
tracking people with twists and exponential maps,this paper demonstrates a new visual motion_estimation technique that is able to recover high degreeoffreedom articulated human body configurations in complex video sequences we introduce the use of a novel mathematical technique the product of exponential maps and twist motions and its integration into a differential motion_estimation this results in solving simple linear_systems and enables us to recover robustly the kinematic degreesoffreedom in noise and complex self occluded configurations we demonstrate this on several image_sequences of people doing articulated full_body movements and visualize the results in reanimating an artificial 3d human model we are also able to recover and reanimate the famous movements of eadweard muybridges motion studies from the last century to the best of our knowledge this is the first computer_vision based system that is able to process such challenging footage and recover complex motions with such high accuracy
developing multilayer information infrastructures advancing social innovation through publicprivate governance,information infrastructures of businesses and government are increasingly interwoven the development of these information infrastructures often has a technological focus and the concurrent social innovation is ill understood to address this gap we study publicprivate information infrastructure developments at three layers over a prolonged period of time stakeholders have to alter existing social practices to realize the potential of information infrastructures new social practices need to be developed and sustaining innovations requires new governance mechanisms
morphological segmentation of lidar digital elevation models to extract stream channels in forested terrain,our paper proposes an approach for the extraction of stream channels from airborne laser swath mapping alsm data recent advances in technology have led to highresolution topographic data acquisition by means of airborne_lidar ie alsm which can yield digital elevation model dem datasets with horizontal resolutions of 1 m and vertical rms errors in the range of 10  15 cm the extraction of a stream network from a dem plays a fundamental role in modeling spatially distributed hydrological processes and flow routing we apply morphological_filtering to an alsm dem to detect and characterize stream channels in forested terrain since the size and shape of morphological structuring elements ses is known to strongly affect filtered results we test for accuracy by developing a set of error measures over simulated terrain we subsequently apply the filter to actual alsm data for linking disconnected stream segments a measure of pixel connectedness known as the connectivity number is used the method presented is shown to enable systematic characterization and comparisons of streams even in heavily forested terrain
interconnections technologies for vlsi_circuits,abstractrnrnvlsi technologies led to the possibility of integration of more than a million of active devices on a single silicon chip a significant part of the effort in the geometrical circuitry shrinkage was set in development of suitable interconnection technologies the goals persued were the improvement of the electrical properties conductivity and contact resistance with the simultaneous improvement of reliability performances
why unbiased computational processes can lead to discriminative decision_procedures,
an event notification service based on xml messaging on different transport technologies,with the size and increasing complexity of telecom networks there is a need to interconnect management systems at different levels this requires information flow across many applications in a domain independent way xml is a widely deployed standard which is being used for integration of network_management systemnms with other applications we examine the performance of different transport mechanisms jms corba http and rmi for an xml message based event notification service to improve the performance of xml message based event notifications event grouping is examined and is found to perform well
ssimbased perceptual rate_control for video_coding,the quality of video is ultimately judged by human eye however mean_squared_error and the like that have been used as quality metrics are poorly correlated with human perception although the characteristics of human visual system have been incorporated into perceptualbased rate_control most existing schemes do not take ratedistortion optimization into consideration in this paper we use the structural similarity index as the quality metric for ratedistortion modeling and develop an optimum bit_allocation and rate_control_scheme for video_coding this scheme achieves up to 25 bitrate reduction over the jm reference_software of h264 under the ratedistortion optimization framework the proposed scheme can be easily integrated with the perceptualbased mode_decision scheme the overall bitrate reduction may reach as high as 32 over the jm reference_software
adaptive compensation techniques for communications_systems with tomlinsonharashima precoding,to improve compensation to channel or interference changes we propose adapting an auxiliary feedback filter fbf in the receiver of systems which use tomlinsonharashima 1971 1972 precoding we show how the auxiliary fbf can be adapted in conjunction with the receiver feedforward filter fff simulations demonstrate the performance advantage of our auxiliary fbf technique relative to fff updating alone and how the fff combines interference_suppression with despreading in wideband applications error propagation can be effectively avoided by using the auxiliary fbf values to decide when to update the precoder while transient increases in meansquared error are avoided by using the fbf values in the update equation
a robust audit mechanism to prevent malicious behaviors in multirobot_systems,marketbased mechanisms can be used to coordinate selfinterested multirobot_systems in fully distributed_environments where by selfinterested we mean that each robot agent attempts to maximize a payoff_function that accounts for both the resources consumed and the contribution made by the robot in previous work we have studied the effect of various market rules and bidding_strategies on the global performance of the multirobot system however rather than use a central monitoring and enforcement_mechanisms we rely on agents to selfreport their actions this assumes that the agents act honestly in this paper we drop the honesty assumption raising the possibility that agents may exaggerate their contribution in order to increase their payoff to address the problem of such malicious_behavior we propose an audit mechanism to maintain the integrity of reported payoffs our algorithm extends previous work on preventing freeriding in peertopeer networks specifically we consider locality and mobility in multirobot_systems we show that our approach efficiently detects malicious behaviors with a high probability
interactive emotional content communications system using portable wireless biofeedback device,in this paper we implemented an interactive emotional content communication system using a portable wireless biofeedback device to support convenient emotion_recognition and immersive emotional content representation for users the newly designed system consists of the portable wireless biofeedback device and a novel emotional content rendering system the former performs the acquisition and transmission of three different physiological signals photoplethysmography skin temperature and galvanic skin response to the remote emotional content rendering system via bluetooth links in real time the latter displays video content concurrently manipulated using the feedback of the users emotional state the results of effectiveness of the system indicated that the response time of the emotional content communication system was nearly instant the changes of between emotional contents and emotional states base on physiological signals was corresponded the users concentration was increased by watching the measuredemotion based rendered visual stimuli in the near future the users of this proposed system will be able to create further substantial useroriented content based on emotional changes
intelligent semantic question_answering_system,the volume of information available on the world_wide_web and the rate of its growth requires new techniques to handle and organize this data ontologies are becoming the pivotal methodology to represent domainspecific conceptual knowledge and hence help in providing solutions for question_answering qa systems this paper introduces an approach for enhancing the capabilities of qa systems using semantic_technologies we implemented an approach to convert the natural_language user queries to resource_description_framework rdf triples and find relevant answers the experiment results show that the proposed technique works very well for single word answers we believe that with some modifications this approach can be expanded to a wider scale
temporal edges the detection of motion and the computation of optical_flow,a new method for the detection of motion and the computation of optical_flow is presented in the first step of the calculation the intensity history at each pixel is convolved with the second derivative in time of a temporal gaussian smoothing function the zero crossings in a single frame of the resulting function indicate the positions of moving edges spatial and temporal derivatives of the function at the zerocrossing locations are then used to compute the component of the flow that is normal to the zerocrossing contours both the detection of motion and the computation of the normal velocity are insensitive to slow temporal and spatial changes in the image intensity that are caused by illumination effects rather than motion a framework in which to relate the present work to a number of gradient based flow measurement techniques is also presented
on averaging multiview relations for 3d scan registration,in this paper we present an extension of the iterative closest point icp algorithm that simultaneously registers multiple 3d scans while icp fails to utilize the multiview constraints available our method exploits the information redundancy in a set of 3d scans by using the averaging of relative motions this averaging method utilizes the lie group structure of motions resulting in a 3d registration method that is both efficient and accurate in addition we present two variants of our approach ie a method that solves for multiview 3d registration while obeying causality and a transitive correspondence variant that efficiently solves the correspondence problem across multiple scans we present experimental results to characterize our method and explain its behavior as well as those of some other multiview registration methods in the literature we establish the superior accuracy of our method in comparison to these multiview methods with registration results on a set of wellknown real datasets of 3d scans
effects of input shaping on manual control of flexible and timedelayed systems,objective the objective was to study the performance of a manual tracking task with system flexibility and time_delays in the input channel and to examine the effects of input shaping the human operators commands background it has long been known that lowfrequency lightly damped vibration hinders performance of a manually controlled system recently input shaping has been shown to improve the performance of such systems in a compensatorydisplay tracking task it is unknown if similar improvements are seen with pursuitdisplay tasks or how the improvement changes when time_delays are added to the system method a total of 18 novice participants performed a pursuitview tracking experiment with a springcentered joystick controlled elements included an integrator an integrator with a lightly damped flexible mode and an inputshaped integrator with a flexible mode the input to these controlled elements was delayed between 0 and 1 s tracking_performance was quantified by root mean_square tracking_error and subjective difficulty was quantified by ratings on a cooperharper scale results performance was best with the undelayed integrator both time_delay and flexibility degraded performance input shaping improved control of the flexible element with a diminishing benefit as the time_delay increased tracking_error and subjective rating were significantly related some operators used a pulsive control strategy conclusion input shaping can improve the performance of a manually controlled system with flexibility even when time_delays are present application this study is useful to designers of humancontrolled systems especially those with problematic flexibility andor time_delays language en
the transformer database biotransformation of xenobiotics,as the number of prescribed drugs is constantly rising drugdrug interactions are an important issue the simultaneous administration of several drugs can cause severe adverse effects based on interactions with the same metabolizing enzymes the transformer database httpbioinformaticscharitedetransformer contains integrated information on the three phases of biotransformation modification conjugation and excretion of 3000 drugs and 350 relevant food ingredients eg grapefruit juice and herbs which are catalyzed by 400 proteins a total of 100 000 interactions were found through text_mining and manual validation the 3d structures of 200 relevant proteins are included the database enables users to search for drugs with a visual display of known interactions with phase i cytochrome p450 and phase ii enzymes transporters food and herbs for each interaction pubmed references are given to detect mutual impairments of drugs the drugcocktail tool displays interactions between selected drugs by choosing the indication for a drug the tool offers suggestions for alternative medications to avoid metabolic conflicts drug interactions can also be visualized in an interactive network view additionally prodrugs including their mechanisms of activation and further information on enzymes of biotransformation including 3d_models can be viewed
combinedwavelet domain and motion compensated filtering compliant with video_codecs,in this paper we introduce the idea of using motion_estimation resources from a video_codec for video denoising this is not straightforward because the motion estimators aimed for video_compression and coding tolerate errors in the estimated motion field and hence are not directly applicable to video denoising to solve this problem we propose a novel motion field filtering step that refines the accuracy of the motion estimates to a degree that is required for denoising we illustrate the use of the proposed motion_estimation method within a waveletbased video denoising scheme the resulting video denoising method is of lowcomplexity and receives comparable results with respect to the latest video denoising_methods
greybox gui_testing efficient generation of event sequences,graphical_user_interfaces guis encode as event sequences potentially unbounded ways to interact with software during testing it becomes necessary to effectively sample the guis event space ideally for increasing the efficiency and effectiveness of gui_testing one would like to sample the guis event space by only generating sequences that 1 are allowed by the guis structure and 2 chain together only those events that have data dependencies between their event handlers we propose a new model called an eventdependency graph edg of the gui that captures data dependencies between the code of event handlers we develop a mapping between an edg and an existing blackbox model of the guis structure called an eventflow graph efg we automate the edg construction in a tool that analyzes the bytecode of each event handler we evaluate ourgreyboxapproach using four opensource applications and compare it with the efg approach our results show that using the edg reduces the number of event sequences with respect to the efg while still achieving at least the same coverage furthermore we are able to detect 2 new bugs in the subject applications
an automatic registration method for frameless stereotaxy image guided surgery and enhanced reality visualization,there is a need for frameless guidance systems to help surgeons plan the exact location for incisions to define the margins of tumors and to precisely identify locations of neighboring critical structures the authors have developed an automatic technique for registering clinical data such as segmented magnetic resonance imaging mri or computed tomography ct reconstructions with any view of the patient on the operating table the authors demonstrate on the specific example of neurosurgery the method enables a visual mix of live video of the patient and the segmented threedimensional 3d mri or ct model this supports enhanced reality techniques for planning and guiding neurosurgical procedures and allows us to interactively view extracranial or intracranial structures nonintrusively extensions of the method include image guided biopsies focused therapeutic procedures and clinical studies involving change_detection over time sequences of images
the fastest gradient waveforms for arbitrary and optimized kspace trajectories,a method for finding the fastest possible gradient waveforms for any given kspace trajectory is presented it is an extension of our previously introduced solution the original scheme provides an efficient and noniterative method for designing the fastest freely rotatable gradient waveforms here the hardware constraints are relaxed so that each axis is constrained independently this produces the fastest possible nonrotatable waveforms that can be up to 10 faster than their previous counterparts in addition for circular trajectories we relax the path constraints this results in new diamondshaped trajectories which are more optimized than circles for separable gradient sets reducing the total travel time by up to an additional 11 analysis of performance for a variety of parameters including the sensitivity to field inhomogeneity compared to freely rotatable circle trajectories is presented
the interaction database synergy of science and practive in pharmacy,in social pharmacy and pharmacoepidemiology the distribution use and performance of medication after registration is studied in both fields the pharmacists are the main source of data on drug use to increase the value of research we think it important to exchange ideas and suggestions between scientific researchers and pharmacists who work in community pharmacies hence the department of social pharmacy and pharmacoepidemiology of the university of groningen sought close collaboration with some community pharmacies in the region resulting in the interaction project the pharmacists deliver data to the interaction database and are explicitly invited to raise questions and issues from their practice and to participate in research consequently science and practice benefit from each others input and expertise this paper describes the architecture and contents of the interaction project additionally the first experiences with the database as a laboratory for social pharmacy and pharmacoepidemiology are discussed
friendship prediction and homophily in social_media,social_media have attracted considerable attention because their openended nature allows users to create lightweight semantic scaffolding to organize and share content to date the interplay of the social and topical components of social_media has been only partially explored here we study the presence of homophily in three systems that combine tagging social_media with online_social_networks we find a substantial level of topical similarity among users who are close to each other in the social_network we introduce a null model that preserves user_activity while removing local correlations allowing us to disentangle the actual local similarity between users from statistical effects due to the assortative mixing of user_activity and centrality in the social_network this analysis suggests that users with similar interests are more likely to be friends and therefore topical similarity measures among users based solely on their annotation metadata should be predictive of social links we test this hypothesis on several datasets confirming that social_networks constructed from topical similarity capture actual friendship accurately when combined with topological features topical similarity achieves a link_prediction accuracy of about 92p
examining the role of linguistic knowledge sources in the automatic_identification and classification of reviews,this paper examines two problems in documentlevel sentiment_analysis 1 determining whether a given document is a review or not and 2 classifying the polarity of a review as positive or negative we first demonstrate that review identification can be performed with high accuracy using only unigrams as features we then examine the role of four types of simple linguistic knowledge sources in a polarity_classification system
development and deployment of ipv6based sip voip networks,this paper presents an ipv6based sip voip network deployed in taiwan this deployment project is supported by nici ipv6 rd division the major contributions of this paper are exercising the enum deployment on ipv6 sip network developing the ipv6 sip user agent and the ipv6 sip analyzer
an integrated approach of variable_ordering and logic mapping into lutarraybased pld,this paper presents an approach of logic mapping into lutarraybased pld where boolean_functions in the form of the sum of generalized complex terms sgcts can be mapped directly while previous mapping approach requires predetermined variable_ordering our approach performs mapping and variable reordering simultaneously for the purpose we propose a directed_acyclic_graph based on the multiple valued decision_diagram mdd and an algorithm to construct the graph our algorithm generates candidates of sgct expressions for each node in a bottomup manner and selects the variables in the current level by evaluating the sizes of sgct expressions directly experimental results show that our approach reduces the number of terms maximum to 71 percent for the mcnc benchmark_circuits
an error model to study the behavior of transient errors in sequential_circuits,in sequential logic circuits the transient errors that occur in a particular time frame will propagate to consecutive time frames thereby making the device more vulnerable in this work we propose a probabilistic error model for sequential logic that can measure the expected output error probability given a probabilistic input space that account for both spatial dependencies and temporal_correlations across the logic using a time evolving causal network we demonstrate our error model using mcnc and iscas benchmark_circuits and validate it with hspice simulations our observations show that significantly low individual gate error probabilities produce at least 5 fold higher output error probabilities the average error percentage of our results with reference to hspice simulation results is only 443 our observations show that the order of temporal dependency of error varies for different sequential_circuits
validityguided fuzzy_clustering evaluation for neural networkbased timefrequency reassignment,this paper describes the validityguided fuzzy_clustering evaluation for optimal training of localized neural_networks lnns used for reassigning timefrequency representations tfrs our experiments show that the validityguided fuzzy approach ameliorates the difficulty of choosing correct number_of_clusters and in conjunction with neural networkbased processing technique utilizing a hybrid approach can effectively reduce the blur in the spectrograms in the course of every partitioning problem the number of subsets must be given before the calculation but it is rarely known apriori in this case it must be searched also with using validity measures experimental results demonstrate the effectiveness of the approach
diagnosability of discrete_event_systems with modular structure,the diagnosis of unobservable faults in large and complex discrete_event_systems modeled by parallel composition of automata is considered a modular approach is developed for diagnosing such systems the notion of modular diagnosability is introduced and the corresponding necessary and sufficient_conditions to ensure it are presented the verification of modular diagnosability is performed by a new algorithm that incrementally exploits the modular structure of the system to save on computational effort the correctness of the algorithm is proved online diagnosis of modularly diagnosable systems is achieved using only local diagnosers
field modifiable architecture with fpgas and its designverificationdebugging methodologies,in the age of highly_integrated system lsis design methodologies for shorter timetomarket and higher reprogrammability after the chip fabrications are now key research issues because of the difficulty of complete verification before tapeout of lsi designs in this paper we first introduce an ipbased vlsi_architecture that consists of a main processor and an additional hardware both custom hard macros and fpga on a single chip specialized to be in charge of the specific instructions we further replace the controller circuits of the specialized hardware with compact microcontrollers and memories by using ip libraries hard macros which results in the increase of the debuggability and the flexibility of design even for computations realized by hard macros we call the proposed architecture as field modifiable architecture fma experimental results confirm that our architecture can achieve significant performance improvement in terms of execution cycles and that ec engineering change can be successfully accommodated after chip fabrications
tracking articulated motion using a mixture of autoregressive models,we present a novel approach to modelling the nonlinear and time varying dynamics of human_motion using statistical_methods to capture the char acteristic motion patterns that exist in typical human activities our method is based on automatically clustering the body pose space into connected regions ex hibiting similar dynamical characteristics modelling the dynamics in each region as a gaussian autoregressive process activities that would require large numbers of exemplars in example based methods are covered by comparatively few motion_models different regions correspond roughly to different actionfragments and our class inference scheme allows for smooth transitions between these thus mak ing it useful for activity_recognition tasks the method is used to track activities including walking running etc using a planar 2d body model its effectiveness is demonstrated by its success in tracking complicated motions like turns without any key_frames or 3d information
knowledge analysis with tree patterns,treestructured knowledge representations are increasingly being used since the relationships between data objects can be represented in a more meaningful way a number of tree mining_algorithms were developed for mining different subtree types using different parameters at this point in research it would be useful to discuss what kind of subproblems can be solved within the current tree mining framework in this paper we provide a general overview of the development in the area of tree mining and discuss motivations and useful application areas for each development implications of using different tree mining parameters and constraints are discussed such an overview will be particularly useful for those not so familiar with the area of tree mining as it can reveal useful applications within their domain of interest it gives guidance as to which type of tree mining will be most useful for their particular application
local broadcasting in the physical interference model,in this work we analyze the complexity of local broadcasting in the physical interference model we present two distributed randomized_algorithms one that assumes that each node knows how many nodes there are in its geographical proximity and another which makes no assumptions about topology knowledge we show that if the transmission probability of each node meets certain characteristics the analysis can be decoupled from the global nature of the physical interference model and each node performs a successful local broadcast in time proportional to the number of neighbors in its physical proximity we also provide worstcase optimality guarantees for both algorithms and demonstrate their behavior in average scenarios through simulations
industrial implementation of a dynamic sampling algorithm in semiconductor manufacturing approach and challenges,in a worldwide environment sustaining high yield with a minimum number of quality controls is key for manufacturing plants to remain competitive in highmix semiconductor plants where more than 200 products are concurrently run the complexity of designing efficient control plans comes from the larger amount of data and number of production parameters to handle several sampling algorithms were proposed in the literature but most of them are seen impracticable when coming to an industrial implementation in this paper we present and discuss the industrial implementation of a dynamic sampling algorithm in a highmix semiconductor plant we describe how the sampling algorithm has been modified and point out the set of questions that have been raised by the industrial program results indicate that more than 30 of control operations on lots could be avoided without increasing the material at risk in production
quality assessment of pareto_set approximations,this chapter reviews methods for the assessment and comparison of pareto_set approximations existing set quality measures from the literature are critically evaluated based on a number of orthogonal criteria including invariance to scaling monotonicity and computational effort statistical aspects of quality assessment are also considered in the chapter three main methods for the statistical treatment of pareto_set approximations deriving from stochastic generating methods are reviewed the  dominance ranking method  is a generalization to partiallyordered sets of a standard nonparametric statistical test allowing collections of pareto_set approximations from two or more stochastic optimizers to be directly compared statistically the  quality indicator method   the dominant method in the literature  maps each pareto_set approximation to a number and performs statistics on the resulting distributions of numbers the  attainment function method  estimates the probability of attaining each goal in the objective_space and looks for significant differences between these probability density functions for different optimizers all three methods are valid approaches to quality assessment but give different information we explain the scope and drawbacks of each approach and also consider some more advanced topics including multiple testing issues and using combinations of indicators the chapter should be of interest to anyone concerned with generating and analysing pareto_set approximations
hybrid filter_banks with fractional delays minimax design and application to multichannel sampling,this paper is motivated by multichannel sampling applications we consider a hybrid filter_banks consisting of a set of fractional delays operators slow ad converters with different antialiasing filters digital expanders and digital synthesis filters to be designed the synthesis filters are designed to minimize the maximum gain of a hybrid induced error system we show that the induced error system is equivalent to a digital system this digital system enables the design of stable synthesis filters using existing control_theory tools such as modelmatching and linear_matrix_inequalities moreover the induced error is robust against delay estimate errors numerical_experiments show the proposed approach yields better performance compared to existing techniques
the drift table designing for ludic engagement,the drift table is an electronic coffee table that displays slowly moving aerial photography controlled by the distribution of weight on its surface it was designed to investigate our ideas about how technologies for the home could support ludic activitiesthat is activities motivated by curiosity exploration and reflection rather than externallydefined tasks the many design choices we made for example to block or disguise utilitarian functionality helped to articulate our emerging understanding of ludic design observations of the drift table being used in volunteers homes over several weeks gave greater insight into how playful exploration is practically achieved and the issues involved in designing for ludic engagement
facetoface and electronic communications in maintaining social_networks the influence of geographical and relational distance and of information_content,using data collected among 742 respondents this article aims at gaining greater insight into i the interaction between facetoface f2f and electronic contacts ii the influence of information_content and relational distance on the communication mode service choice and iii the influence of relational and geographical distance in addition to other factors on the frequency of f2f and electronic contacts with relatives and friends the results show that the frequency of f2f contacts is positively correlated with that for electronic communication pointing at a complementarity effectwith respect to information_content and relational distance we find on the basis of descriptive analyses that synchronous modesservices f2f and telephone conversations are used more for urgent matters and that asynchronous modes in particular email become more influential as the relational distance increases finally ordered probit analyses confirm that the frequency of both f2f and electronic communication d
design of spatial data_broadcast for mobile navigation applications,existing mobile navigators can readily display any static data related to the area surrounding a user however their ability to display any dynamic data is limited in this paper we enable the viewing of dynamic data on a navigator using wireless data_broadcast the dynamic data are periodically broadcast via base_stations of wireless systems and can be filtered by navigators fetching desired data we address several crucial issues related to the design of this application including data organizing indexing caching and querying we simulate the performance of the resulting system by measuring the time and energy costs associated with retrieving the dynamic data
tool integration at the metamodel level the fujaba approach,todays development_processes employ a variety of notations and tools eg the unified_modeling_language uml the standard description language sdl requirements databases design tools code_generators model_checkers etc for better process support the employed tools may be organized within a tool suite or integration platform eg rational rose or eclipse while these toolintegration platforms usually provide gui adaption mechanisms and functional adaption via application programming interfaces they frequently do not provide appropriate means for data_integration at the metamodel level thus overlapping and redundant data from different integrated tools may easily become inconsistent and unusable we propose two design_patterns that provide a flexible basis for the integration of different tool data at the metamodel level to achieve consistency between metamodels we describe rulebased mechanisms providing generic solutions for managing overlapping and redundant data the proposed mechanisms are widely used within the fujaba tool suite we report about our implementation and application experiences 
tls parameter estimation for filtering chaotic time series,we present a new algorithm for simultaneous filtering and parameter estimation of chaotic time series corrupted by additive measurement_noise this method is based on iteratively minimizing the total least squares tls error in the phasespace using steepest descent in contrast to prior work we assume that the dynamic equations modeling the nonlinear time series are known but the corresponding parameters are not specifically the data is assumed to be generated from time series satisfying a set of coupled logistics equations corrupted by additive_white_gaussian_noise this work is motivated in part by language_modeling where the dynamics of a conversation are argued to satisfy the coupled logistics equations and the time series data represents noisy measurements taken from protocols accurate estimation of the process parameters is critical eg in diagnosing and treating different types of language disorders and also may prove valuable for testing and improving language understanding systems
industry track portals at bnp paribas a brief testimony april 2005,this paper intends to bear witness on the past present and future of portals at bnp paribas with a focus on 1 the b2e intranet portal echonet 2 the french b2c home banking portal bnpparibasnet it introduce the challenges that confronted bnp paribas in each case reflect on lessons learnt and future trends and finally suggest what financial services firms expect from ebusiness infrastructure hardware and software vendors and service providers
improvement of response properties of mrfluid actuator by torque feedback_control,magnetorheological mr fluids are substances that respond to an applied magnetic field with a change in their theological behavior though they are functionally similar to electrorheological er fluids mr fluids exhibit much higher yield strengths for the applied magnetic fields than er fluids for the applied electric fields the devices using mr fluids have an ability to provide hightorque lowinertia a safe device and simple interface in this study we report on an actuator developed using the mr fluid which consists of an input part an output part and an mr fluid clutch between them first the basic experiments are examined to investigate the characteristics of the actuator next the torque_control system of the mrfluid actuator is proposed finally the closedloop control experiments were carried out and it is confirmed that the torquefeedback control is effective for improving the response properties of mr actuators
schedulabilitydriven performance_analysis of multiple mode embedded realtime systems,providing multiple modes to support dynamically changing environments standards and new services is prevalent in embedded_systems especially in mobile_radio_systems because such a system frequently contains timeconstrained tasks it is important to analyze the temporal requirements as well as the functional correctness this paper presents a method to analyze temporal requirements imposed on an embedded realtime system supporting multiple modes while most performance_analysis methods focus only on testing the feasibility of a task or a system our method goes further by addressing the problem of locating hot spots of a system thereby helping the designer to choose among alternative designs or architectures we formally define the analysis problem and show that it is very unlikely to be solved efficiently we present a heuristic algorithm which is accurate and fast enough to be used in iterative processes in systemlevel analysis and design the analysis problem is extended to accommodate probabilistic behavior exhibited by soft realtime tasks
performance_analysis of cognitive coexistence systems with sensing errors,in this paper we focus on the performance_analysis of the cognitive coexistence between bluetooth and wlan_systems with sensing errors the packet transmission rate and packet error probability are derived corresponding to the sensor operating points consisting of the false_alarm_probability and the miss detection_probability then the optimization problem is established as maximizing the throughput of cognitive user with the constraint of the colliding probability with primary user simulation results illustrate that the proposed error analysis approach can obtain the optimal performance when considering the sensing errors moreover the influence on the throughput of cognitive user caused by different sensor operating points is investigated by simulation
the chic interactive task chici at clef2013,the interactive task in cultural heritage in clef 2013 used a standardised interactive protocol information_retrieval system and interface to observe a set of participants remotely via the web as well as in the lab access an english language collection from the europeana digital_library both user response and log data were collected from the 208 participants
advanced fault_tolerant bus for multicore system implemented in fpga,in the paper a technique for design of highly dependable communication structure in srambased fpga is presented the architecture of the multicore system and the structure of fault_tolerant bus with cache_memories are demonstrated the fault_tolerant properties are achieved by the replication and utilization of the self checking techniques together with partial dynamic_reconfiguration the experimental results show that presented system has small overhead if the high number of function units are used all experiments were done on the virtex5 and virtex6 platform
performance_analysis of maximal_ratio_combining in the presence of multiple equalpower cochannel interferers in a nakagami_fading_channel,the effect of cochannel_interference on the performance of digital mobile_radio_systems in a nakagami 1960 fading_channel is studied the performance of maximal_ratio_combining mrc diversity is analyzed in the presence of multiple equalpower cochannel interferers and additive_white_gaussian_noise closedform expressions are derived for the average probability of error as well as outage_probability of both coherent and noncoherent differentially coherent binary frequencyshift keying and binary phaseshift keying schemes in an environment with cochannel_interference and noise the results are expressed in terms of the confluent hypergeometric function of the second kind a function that can be easily evaluated numerically the analysis assumes an arbitrary number of independent and identically distributed nakagami interferers
new scheduling algorithm for uplink lte system,
proof_search for propositional abstract separation logics via labelled sequents,abstract separation logics are a family of extensions of hoare_logic for reasoning about programs that mutate memory these logics are abstract because they are independent of any particular concrete memory model their assertion languages called propositional abstract separation logics extend the logic of boolean bunched implications bbi in various ways   we develop a modular proof theory for various propositional abstract separation logics using cutfree labelled sequent calculi we first extend the cutfee labelled sequent_calculus for bbi of hou et al to handle calcagno et als original logic of separation algebras by adding sound rules for partialdeterminism and cancellativity while preserving cutelimination we prove the completeness of our calculus via a sound intermediate calculus that enables us to construct countermodels from the failure to find a proof we then capture other propositional abstract separation logics by adding sound rules for indivisible unit and disjointness while maintaining completeness and cutelimination we present a theorem prover based on our labelled calculus for these logics
storyboarding an empirical determination of best practices and effective guidelines,storyboarding is a common technique in hci and design for demonstrating system interfaces and contexts of use despite its recognized benefits novice designers still encounter challenges in the creation of storyboards furthermore as computing becomes increasingly integrated into the environment blurring the distinction between the system and its surrounding context it is imperative to depict context explicitly in storyboards in this paper we present two formative studies designed to uncover the important elements of storyboards these elements include the use of text inclusion of people level of detail number of panels and representation of the passage of time we further present an empirical study to assess the effects of these elements on the understanding and enjoyment of storyboard consumers finally we demonstrate how these guidelines were successfully used in an undergraduate hci class
constraint based automated synthesis of nonmasking and stabilizing faulttolerance,we focus on constraintbased automated addition of nonmasking and stabilizing faulttolerance to hierarchical programs we specify legitimate states of the program in terms of constraints that should be satisfied in those states to deal with faults that may violate these constraints we add recovery actions while ensuring interference freedom among the recovery actions added for satisfying different constraints since the constraintbasedem manual design of faulttolerance is wellknown to be applicable in the manual design of nonmasking faulttolerance we expect our approach to have a significant benefit in automation of faulttolerant programs we illustrate our algorithms with three case studiesstabilizing mutual exclusion stabilizing diffusing computation and a data dissemination problem in sensor_networks with experimental resultswe show that the complexity of synthesis is reasonable and that it can be reduced using the em structure of the hierarchical systems to our knowledge this is the first instance where automated synthesis has been successfully used in synthesizing programs that are correct under fairness assumptions moreover in two of the case studies considered in this paper the structure of the recovery paths is too complex to permit existing heuristic based approaches for adding recovery
automatic histogram threshold using fuzzy_measures,in this paper an automatic histogram threshold approach based on a fuzziness measure is presented this work is an improvement of an existing method using fuzzy logic concepts the problems involved in finding the minimum of a criterion function are avoided similarity between gray levels is the key to find an optimal threshold two initial regions of gray levels located at the boundaries of the histogram are defined then using an index of fuzziness a similarity process is started to find the threshold point a significant contrast between objects and background is assumed previous histogram_equalization is used in small contrast images no prior knowledge of the image is required
ksensor multithreaded kernellevel probe for passive qos monitoring,traffic monitoring is an increasingly important discipline for nowadays networking as accounting security and traffic_engineering lay on it besides traffic bandwidth has increased exponentially in the last few years and highspeed network_monitoring has become a challenging task performance requirements are highly relevant for passive qos monitoring systems a lowlevel study of the capturing and processing stages on a traffic analysis system tas has shown room for improvement we provide an architecture able to cope with highspeed traffic monitoring using commodity hardware our system is intended to exploit the parallelism available in upto date workstations which also introduces constraints for multithreaded qos analysis this paper presents a kernellevel framework ksensor that keeping the previous requirements removes some issues from userlevel processing and effectively integrates qos algorithms improving the overall performance
chaosmodulated ramp ic for emi reduction in pwm buck converters design and analysis of critical issues,various nonconventional methods have been employed in the past to reduce the cost and weight of traditional conducted emi filters and radiation screens for emi suppression in switching power electronic converters this paper points out various shortcomings of these methods which are mainly frequency modulation based and describes the design of a rampgenerator ic based on a modified modulation scheme this ic can be used on any voltage mode controlled converter and has a feature that enables the user to tune the same converter to various emc norms test_results from a prototype showing significant reduction in harmonic power level have been presented moreover this paper discusses a theoretical formulation for calculating the output capacitor size to maintain ripple specifications when operating under chaotic modulation
fault_tolerant kcenter problems,
simultaneous placement and assignment for exploration in mobile backbone networks,this paper presents new algorithms for conducting cooperative_sensing using a mobile backbone network this hierarchical sensing approach combines backbone nodes which have superior mobility and communication capability with regular nodes which are constrained in mobility and communication capability but which can sense the environment in the framework of a cooperative exploration problem a technique is developed for simultaneous placement and assignment of regular and mobile backbone nodes this method a generalization of existing techniques that only consider stationary regular nodes optimally solves the simultaneous placement and assignment_problem in computationally tractable time for problems of moderate size for largescale instances of this problem a polynomialtime approximation_algorithm is developed this algorithm carries the benefit of a theoretical performance guarantee and also performs well in practice finally the simultaneous placement and assignment technique is incorporated into a cooperative exploration algorithm and its performance is shown to compare favorably with that of a benchmark based on existing assignment algorithms for mobile backbone networks
the mystique of numbers belief in quantitative approaches to segmentation and persona development,quantitative market research and qualitative usercentered design research have long had an uneasy and complex relationship a trend toward increasingly complex statistical segmentations and associated personas will once again increase the urgency of addressing paradigm differences to allow the two disciplines to collaborate effectively   we present an instructive case in which qualitative field research helped contribute to abandoning a state of the art quantitative user segmentation that was used in an attempt to unify both marketing and user_experience planning around a shared model of users this case exposes risks in quantitative segmentation research common fallacies in the evolving practice of segmentation and use of personas and the dangers of excessive deference to quantitative research generally
analyzing the effects of diskpointer corruption,the longterm availability of data stored in a file_system depends on how well it safeguards ondisk pointers used to access the data ideally a system would correct all pointer errors in this paper we examine how well corruptionhandling techniques work in reality we develop a new technique called typeaware pointer corruption to systematically explore how a file_system reacts to corrupt pointers this approach reduces the exploration space for corruption experiments and works without source_code we use typeaware pointer corruption to examine windows ntfs and linux ext3 we find that they rely on type and sanity checks to detect corruption and ntfs recovers using replication in some instances however ntfs and ext3 do not recover from most corruptions including many scenarios for which they possess sufficient redundant information leading to further corruption crashes and unmountable file_systems we use our study to identify important lessons for handling corrupt pointers
the nature of dialog structural and lexical markers of dialogic teacherlearner interactions,in this paper i argue that dialog facilitates learning as a consequence teaching_and_learning should be dialogic in principal against this background the question arises what dialog actually is and how it can be implemented for teaching_and_learning a pilot corpus study of a dialog and a monolog corpus of selected teacherlearner interactions sheds light on some of the structural and lexical characteristics of dialogic teaching_and_learning the analysis of the data discloses five communicative functions of selected keywords of the dialog corpus which indicate how speakers are affiliated in dialogic interaction
evaluating the enjoyability of the ghosts in ms pacman,the video_games industry is one of the fastestgrowing industries in the world bolstered by sophisticated technology in gaming consoles and modern trends such as mobile and social gaming the goal of most video_games is to entertain the gamer and in most games this stems from the interaction between the gamer and the nonplayer characters npcs it is no longer sufficient for a game to be visually appealing but instead the gamer must be challenged at the right level of difficulty to be engaged by the game it is thus necessary to develop suitable npcs that are fun to play against and the realm of computational intelligence offers a variety of techniques to do so however the perception of fun is clearly subjective and indeed difficult to quantify in this paper we make use of the ms pacman vs ghosts gaming competition to gather and analyse data from human gamers regarding their preference of opponent each gamer plays two games against different ghost teams indicating their preference at the end we subsequently use this data to establish which ghost teams are generally preferred and demonstrate that there are measurable differences between these ghost teams these differences are sufficient to group the ghosts into different categories with a good degree of accuracy this work is a first step in better understanding the attributes required by npcs for players to be engaged
learning human multimodal dialogue strategies,we investigate the use of different machine_learning_methods in combination with feature_selection techniques to explore human multimodal dialogue strategies and the use of those strategies for automated dialogue_systems we learn policies from data collected in a wizardofoz study where different human wizards decide whether to ask a clarification request in a multimodal manner or else to use speech alone we first describe the data collection the coding_scheme and annotated corpus and the validation of the multimodal annotations we then show that there is a uniform multimodal dialogue strategy across wizards which is based on multiple features in the dialogue context these are generic features available at runtime which can be implemented in dialogue_systems our prediction models for human wizard behaviour achieve a weighted fscore of 886 per cent which is a 256 per cent improvement over the majority baseline we interpret and discuss the learned strategy we conclude that human wizard behaviour is not optimal for automatic dialogue_systems and argue for the use of automatic optimization_methods such as reinforcement_learning throughout the investigation we also discuss the issues arising from using small initial wizardofoz data sets and we show that feature engineering is an essential step when learning dialogue strategies from such limited data
multiobjective hierarchical genetic_algorithm for interpretable fuzzy rulebased knowledge extraction,a new scheme based on multiobjective hierarchical genetic_algorithm mohga is proposed to extract interpretable rulebased knowledge from data the approach is derived from the use of multiple objective genetic_algorithm moga where the genes of the chromosome are arranged into control genes and parameter genes these genes are in a hierarchical form so that the control genes can manipulate the parameter genes in a more effective manner the effectiveness of this chromosome formulation enables the fuzzy sets and rules to be optimally reduced some important concepts about the interpretability are introduced and the fitness_function in the moga will consider both the accuracy and interpretability of the fuzzy model in order to remove the redundancy of the rule_base proactively we further apply an interpretabilitydriven simplification method to newborn individuals in our approach we first apply the fuzzy_clustering to generate an initial rulebased model then the multiobjective hierarchical genetic_algorithm and the recursive_least_square method are used to obtain the optimized fuzzy models the accuracy and the interpretability of fuzzy models derived by this approach are studied and presented in this paper we compare our work with other methods reported in the literature on four examples a synthetic nonlinear dynamic system a nonlinear static system the lorenz_system and the mackeyglass system simulation results show that the proposed approach is effective and practical in knowledge extraction
cognitive dust linking cscw theories to creative_design processes,results from empirical research in requirements_engineering lead us to characterise work processes not as evolutionary and systematic but as semi structured emergent and creative these same properties are found in situated action models where work is described as emergent and self defining at another level within these processes we use the term cognitive dust to describe the external and distributed cognitive representations suspended in a small group workspace these cognitive representations are components of communicative actions between humans or between humans and their technological infrastructures and are part of these creative processes we use a multi modal sensor infrastructure which is situated in a ubiquitous workspace to observe and capture this cognitive dust we hypothesise that if an infrastructure can capture enough of these representations and extract enough semantic meaning to understand the communicative intentions the infrastructure can dynamically support creative unstructured activities this paper sets out the conceptual theoretical framework for this research by linking known cscw theories with the emergent unstructured or semi structured nature of creative work processes within small groups such as designers
error control using retransmission schemes in multicast transport_protocols for realtime media,we analyze different retransmission arq schemes for error control in multicast_protocols geared toward realtime multimedia applications we discuss why retransmission schemes are not inappropriate for such applications but in fact can be quite effective we present a quantitative analysis of such schemes as well as simulation results taking into account four different parameters and not just the source throughput 1 the probability of dropping a packet due to limited time for retransmissions 2 the average time required to deliver a packet correctly to end receivers 3 the number of times a packet will be retransmitted and 4 the cost to the network in terms of packet duplications of retransmitting a packet we reach the counterintuitive conclusion that the optimum scheme in terms of all four of the above parameters in the most general scenarios where several hosts with widely varying propagation delays and quality of connections are participating in the session is to immediately retransmit packetspreferably multicastupon reception of a nack from any receiver we also demonstrate again through quantitative analysis the circumstances under which it would be beneficial as well as those under which it would be counterproductive to multicast control messages in the hope of suppressing duplicates and preventing the source from being overwhelmed by control messages
outerloop vectorization revisited for short simd architectures,vectorization has been an important method of using datalevel parallelism to accelerate scientific workloads on vector machines such as cray for the past three decades in the last decade it has also proven useful for accelerating multimedia and embedded applications on short simd architectures such as mmx sse and altivec most of the focus has been directed at innermost loops effectively executing their iterations concurrently as much as possible outer loop vectorization refers to vectorizing a level of a loop nest other than the innermost which can be beneficial if the outer loop exhibits greater datalevel parallelism and locality than the innermost loop outer loop vectorization has traditionally been performed by interchanging an outerloop with the innermost loop followed by vectorizing it at the innermost position a more direct unrollandjam approach can be used to vectorize an outerloop without involving loop interchange which can be especially suitable for short simd architectures   in this paper we revisit the method of outer loop vectorization paying special attention to properties of modern short simd architectures we show that even though current optimizing_compilers for such targets do not apply outerloop vectorization in general it can provide significant performance improvements over innermost loop vectorization our implementation of direct outerloop vectorization available in gcc 43 achieves speedup factors of 313 and 277 on average across a set of benchmarks compared to 153 and 139 achieved by innermost loop vectorization when running on a cell be spu and powerpc970 processors respectively moreover outerloop vectorization provides new reuse opportunities that can be vital for such short simd architectures including efficient handling of alignment we present an optimization tapping such opportunities capable of further boosting the performance obtained by outerloop vectorization to achieve average speedup factors of 526 and 364
designing gestures for hands and feet in daily life,in wearable computing_environments people handles various information anytime and anywhere with a wearing computer in such situation a gesture is one of powerful methods as input method because it needs no physical devices to touch and a user can input quickly however there are various restrictions for gesture input in daily life gestures must be socially acceptable because a user has to gesture with unusual movements in a crowd gestures must be flexible because a user cannot gesture when heshe has a bag with his hand that is used for a gesture in this paper we clarify the restrictions on gesture_interfaces in daily life then propose practical gestures for selecting simple menu items with hands and feet
modeling of heart systolic murmurs based on multivariate matching pursuit for diagnosis of valvular disorders,heart murmurs are pathological sounds produced by turbulent blood flow due to certain cardiac defects such as valves disorders detection of murmurs via auscultation is a task that depends on the proficiency of physician there are many cases in which the accuracy of detection is questionable the purpose of this study is development of a new mathematical model of systolic murmurs to extract their crucial features for identifying the heart diseases a high resolution algorithm multivariate matching pursuit was used to model the murmurs by decomposing them into a series of parametric timefrequency atoms then a novel modelbased feature_extraction method which uses the model parameters was performed to identify the cardiac sound signals the proposed framework was applied to a database of 70 heart sound signals containing 35 normal and 35 abnormal samples we achieved 925 accuracy in distinguishing subjects with valvular diseases using a mlp classifier as compared to the matching pursuitbased features with an accuracy of 775
parallel_implementation of the sparsematrixcanonical grid method for the analysis of twodimensional random rough surfaces threedimensional scattering problem on a beowulf system,wave scattering from twodimensional 2d random rough surfaces threedimensional 3d scattering problem has been previously analyzed using the sparsematrixcanonical grid smcg method the computational complexity and memory requirement of the smcg method are on log n per iteration and on respectively where n is the number of surface unknowns furthermore the smcg method is fft based which facilitates the implementation on parallel processors in this paper we present a costeffective solution by implementing the smcg method on a beowulf system consisting of pcs processors connected by a 100 base tx ethernet switch the workloads of computing the sparsematrixvector multiplication corresponding to the near interactions and the fast_fourier_transform fft operations corresponding to the far interactions in the smcg method can be easily distributed among all the processors both perfectly conducting and lossy dielectric surfaces of gaussian spectrum and ocean spectrum are analyzed thereafter when possible speedup factors against a single processor are given it is shown that the smcg method for a single realization of rough surface scattering can be efficiently adapted for parallel_implementation the largest number of surface unknowns solved in this paper is over 15 million on the other hand a problem of 131072 surface unknowns for a pec random rough surface of 1024 square wavelengths only requires a cpu time of less than 20 min we demonstrate that analysis of a largescale 2d random rough surface feasible for a single realization and for one incident angle is possible using the lowcost beowulf system
active noise_cancellation using aggressoraware clamping circuit for robust onchip communication,as the ic process technology scales the onchip wiring network becomes denser increasing aspect ratios of the onchip interconnects lead to higher coupling capacitances and ultimately higher crosstalk_noise which degrades signal_integrity in this paper we propose a clamping circuit for onchip busses which clamps a victim wire in an onchip bus based on the states of its immediate aggressors these clampers help the driver of the victim wire in draining the charge which is induced due to crosstalk between aggressors and victim wires this helps in decreasing the crosstalk peak noise and also the delay variability referred to as delay noise simulation results for a 10 mm long communication bus parallel wires laid at minimum pitch in 013 spl mum cmos technology show that a reduction of 30176 37272 and 26658 in crosstalk peak noise amplitude delay noise is observed for point to point parallel repeater inserted and staggered repeater inserted respectively when only immediate neighbours are considered 1sup st order furthermore the aggressoraware clamper is very effective in avoiding glitches which may occur when more aggressors in addition to the immediate ones are also switching simultaneously in the same
high resolution twodimensional arma spectral estimation,the authors present a practical algorithm for estimating the power spectrum of a 2d homogeneous random field based on 2d autoregressive moving average arma modeling this algorithm is a twostep approach first the ar parameters are estimated by solving a version of the 2d modified yulewalker equation for which some existing efficient algorithms are available then the ma spectrum parameters are obtained by simple computations the potential capability and the highresolution performance of the algorithm are demonstrated by using some numerical_examples 
compiler verification in lf,a methodology for the verification of compiler correctness based on the lf logical framework as realized within the elf programming_language is presented this technique is used to specify implement and verify a compiler from a simple functional_programming language to a variant of the categorical abstract machine cam 
blind amplifyandforward relaying in multipleantenna relay networks,in this paper we investigate the performance of a singlerelay cooperative scenario where the source relay and destination terminals are equipped with multiple transmitreceive antennas we particularly focus on the socalled blind amplifyandforward relaying in which the availability of channel_state_information at the relay terminal is not required through the derivation of pairwise error probability we quantify analytically the impact of multiple_antenna deployment assuming various scenarios which involve relay location and power_allocation assumptions imposed on the cooperating nodes
coupling of two 2link robots with a passive joint for reconfigurable planar parallel_robot,this paper proposes a reconfigurable planar parallel_robot by coupling two 2r open kinematic chains or 2link robots the first joints of which are passive we show that they can reconfigure to a 5r closed kinematic_chain which has the same number of actuators as its degrees_of_freedom they can also reconfigure to a 4r closed kinematic_chain plus one actuated link the parallel_robot has only two actuators but can have multiple functions by reconfigurations due to the passive joints whether or not the 2r open kinematic chains can couple with each other is a nontrivial problem we propose coupling sequences for forming the 4r and 5r closed kinematic chains and verify those experimentally
on the fingerprinting capacity under the marking assumption,we address the maximum attainable rate of fingerprinting codes under the marking assumption studying lower and upper bounds on the value of the rate for various sizes of the attacker coalition lower bounds are obtained by considering typical coalitions which represents a new idea in the area of fingerprinting and enables us to improve the previously known lower bounds for coalitions of size two and three for upper bounds the fingerprinting problem is modeled as a communications problem it is shown that the maximum code rate is bounded above by the capacity of a certain class of channels which are similar to the multipleaccess channel mac converse coding theorems proved in the paper provide new upper bounds on fingerprinting capacity it is proved that capacity for fingerprinting against coalitions of size two and three over the binary alphabet satisfies and  respectively for coalitions of an arbitrary fixed size  we derive an upper bound on fingerprinting capacity in the binary case finally for general alphabets we establish upper bounds on the fingerprinting capacity involving only singleletter mutual_information quantities
methods for reducing events in sequential circuit fault_simulation,methods are investigated for reducing events in sequential circuit fault_simulation by reducing the number of faults simulated for each test vector inactive faults which are guaranteed to have no effect on the output or the next state are identified using local information from the faultfree circuit in one technique in a second technique the staralgorithm is extended to handle sequential_circuits and provides global information about inactive faults based on the faultfree circuit state both techniques are integrated into the proofs synchronous sequential circuit fault simulator an average 28 reduction in faulty circuit gate evaluations is obtained for the 19 iscas89 benchmark_circuits studied using the first technique and 33 reduction for the two techniques combined execution times decrease by an average of 17 when the first technique is used for the largest circuits further improvements in execution time are made when the staralgorithm is included 
tracking_performance of the coherent and noncoherent discriminators in strong multipath,signal multipath leads to undesirable tracking_errors and inaccurate ranging information for gps_receivers the extent of the tracking_error in compromising the receiver discriminator performance depends on the multipath amplitude delay and phase relative to the direct path compared with the rural area the gps receiver in the semienclosed area such as city canyons and building shadows is subject to much weaker line of sight propagation environment which further compromise its performance in this paper we derive analytical_expressions of the multipath effect on the gps tracking_errors for both coherent discriminator and noncoherent earlyminuslate power discriminators in strong multipath environment
designing hardware with dynamic memory abstraction,recent progress in program_analysis has produced tools that are able to compute upper bounds on the use of dynamic memory this opens up a space for the use of dynamic memory abstraction in highlevel synthesis in this paper we explain how to design hardware using c_programs with  malloc  and  free  a compilation process is outlined for transforming c_programs with heap operations into a hardware description language as demonstrated by our experiments this approach is feasible further automatic_parallelization of the generated circuits improves by a factor up to 19 in terms of clock_frequency and a factor up to 27 in terms of clock_cycles over the previous work
a new dynamic ovsf code allocation method based on adaptive simulated_annealing genetic_algorithm asaga,orthogonal variable spreading factor ovsf codes are widely used to provide variable data rates for supporting different bandwidth requirements in wideband code_division_multiple_access wcdma systems many works in the literature have intensively investigated to find an optimal dynamic code assignment scheme for ovsf codes unlike earlier studies which assign ovsf codes using conventional cca or dynamic dca code allocation schemes in this paper adaptive simulated_annealing genetic_algorithm asaga was applied which population is adaptively constructed according to existing traffic density in the ovsf codetree also the influences of the asaga parameters selection crossover_and_mutation techniques and cooling schedules were examined on the dynamic ovsf code allocation problem the simulation results show that the asaga provides reduced code blocking_probability and improved spectral_efficiency in the system when compared to the cca and dca schemes asaga is also tested with its components sa and ga
an analysis of multicast forwarding state scalability,scalability of multicast forwarding state is likely to be a major issue facing interdomain multicast deployment we present a comprehensive analysis of the multicast forwarding state problem our goal is to understand the scaling trends of multicast forwarding state in the internet and to explore the intuitions that have motivated state reduction research we conducted simulation experiments on both real and generated network topologies with a range of parameters driven by multicast_application characteristics we found that the increase in peering among internet backbone networks has led to more multicast forwarding state at a handful of core domains but less state in the rest of the domains we observed that scalability of multicast forwarding state with respect to session size follows a power law our findings show that distribution and concentration of multicast forwarding state in the internet is significantly impacted by the application characteristics we investigated the proposals on nonbranching multicast forwarding state elimination and found substantial reduction is attainable even with very dense multicast_sessions
customized interface generation model based on knowledge and template for web service,with the development of serviceoriented architecture more and more researches have provided automatic and semiautomatic approaches to enduser users can construct their own applications with web services however it is hard for most endusers to customize the interfaces of applications with current service_composition methods to address this issue an interface generation model was proposed to provide customized user_interface and interaction workflow in this model knowledge was involved to instruct the workflow of interaction templates were adopted to describe the user_interface some significant points such as user definition data profile user interaction workflow interface description were discussed in detail a prototype system was implemented some demos have been shown to verify the customized interface generation model with this model endusers can define the interfaces and interaction workflows of web services with rules and templates it supplies the gap of user_interface in service_composition compared with the current interface generation in service_composition the proposed model is more flexible and more effective for endusers
petri_nets and ontologies tools for the learning player assessment in serious games,serious games are now an increasingly used tool in business training the question of the effectiveness of such devices on learning is a research issue the indicators provided at the end of a video_game are insufficient to understand and follow the path of a learning player it is therefore necessary to track not only the players actions but also to provide tools in order to analyze and diagnose the knowledge_acquisition of the learner we developed an approach based on petri_nets used to model the accurate behavior of the player we complete this tool with ontology to explain learners mistakes
epileptic eeg_classification based on kernel sparse representation,the automatic_identification of epileptic eeg_signals is significant in both relieving heavy workload of visual_inspection of eeg recordings and treatment of epilepsy this paper presents a novel method based on the theory of sparse representation to identify epileptic eegs at first the raw eeg epochs are preprocessed via gaussian low pass filtering and differential operation then in the scheme of sparse representation based classification src a test eeg sample is sparsely represented on the training_set by solving l1minimization problem and the represented residuals associated with ictal and interictal training_samples are computed the test eeg sample is categorized as the class that yields the minimum represented residual so unlike the conventional eeg classification_methods the choice and calculation of eeg features are avoided in the proposed framework moreover the kernel_trick is employed to generate a kernel version of the src method for improving the separability between ictal and interictal classes the satisfactory recognition accuracy of 9863 for ictal and interictal eeg_classification and for ictal and normal eeg_classification has been achieved by the kernel src in addition the fast speed makes the kernel src suit for the realtime seizure monitoring application in the near future
selfimmunity technique to improve register file integrity against soft_errors,continuous shrinking in feature size increasing power density etc increase the vulnerability of microprocessors against soft_errors even in terrestrial applications the register file is one of the essential architectural components where soft_errors can be very mischievous because errors may rapidly spread from there throughout the whole system thus register files are recognized as one of the major concerns when it comes to reliability this paper introduces selfimmunity a technique that improves the integrity of the register file with respect to soft_errors based on the observation that a certain number of register bits are not always used to represent a value stored in a register this paper deals with the difficulty to exploit this obvious observation to enhance the register file integrity against soft_errors we show that our technique can reduce the vulnerability of the register file considerably while exhibiting smaller overhead in terms of area and power consumption compared to stateoftheart in register file protection
measuring semantic associaiton in domain_ontology,in domain_ontology semantic association sa is used to depict the correlation between two concepts in this paper we define semantic association degree sad for measuring sa in the domain_ontology we first present a method to measure sad of two direct related concepts by evaluating the semantic relationship between them and then give another method to measure sad of two indirect related concepts though sad of two directed neighboring concepts a set of comparison experiments show the benefit of our approaches
the h index and the number of citations two fuzzy_integrals,in this paper we review two of the most wellknown citation indexes and establish their connections with the choquet and sugeno_integrals in particular we show that the recently established hindex is a particular case of the sugeno_integral and that the number of citations corresponds to the choquet_integral in both cases they use the same fuzzy_measure the results presented here permit one to envision new indexes defined in terms of fuzzy_integrals using other types of fuzzy_measures a few considerations in this respect are also included in this paper indexes for taking into account recent research and the publisher credibility are outlined
the soft_error problem an architectural perspective,radiationinduced soft_errors have emerged as a key challenge in computer_system design if the industry is to continue to provide customers with the level of reliability they expect microprocessor architects must address this challenge directly this effort has two parts first architects must understand the impact of soft_errors on their designs second they must select judiciously from among available techniques to reduce this impact in order to meet their reliability targets with minimum overhead to provide a foundation for these efforts this paper gives a broad overview of the soft_error problem from an architectural perspective we start with basic definitions followed by a description of techniques to compute the soft_error_rate then we summarize techniques used to reduce the soft_error_rate this paper also describes problems with doublebit errors finally this paper outlines future directions for architecture research in soft_errors
a 55ghz 1mw fullmodulusrange programmable frequency divider in 90nm cmos process,operating up to 55 ghz with 1mw power consumption a 90nm cmos programmable frequency divider with eight stages of new static dflipflopbased 21 divider cells is presented where the supply voltage of 10 v is employed the divider achieves a full modulus range from 1 to 256 and operates over a wide range maintaining up to 4 ghz with 30dbm input power the divider also accomplishes a power efficiency of 128 ghzmw with 05v supply voltage it is favorable for advanced processes
space curve recognition based on the wavelet_transform and stringmatching techniques,a technique for representing and recognising 3d or space curves is presented in the proposed algorithm the space curves are represented by a set of two zerocrossing representations which are constructed based on the dyadic wavelet_transform these representations are then described in the form of an ordered set of complex numbers which is referred to as the compact representation of the space curves a stringmatching technique is adapted for comparing two curves using their compact representations experimental results show that the proposed technique can be used for recognising space curves under similarity_transformation with and without additive_noise
five weeks in the robot house exploratory humanrobot interaction trials in a domestic setting,this paper presents five exploratory trials investigating scenarios likely to occur when a personal robot shares a home with a person the scenarios are a human and robot working on a collaborative task a human and robot sharing a physical space in a domestic setting a robot recording and revealing personal_information a robot interrupting a human in order to serve them and finally a robot seeking assistance from a human through various combinations of physical and verbal cues findings indicate that participants attribute more blame and less credit to a robot than compared to themselves when working together on a collaborative task safety is a main concern when determining participants comfort when sharing living space with their robot findings suggest that the robot should keep its interruption of the users activities to a minimum participants were happy for the robot to store information which is essential for the robot to improve its functionality however their main concerns were related to the storing of sensitive information and security measures to safeguard such information
workstation based parallel test_generation,generation of test_vectors for the vlsi devices used in contemporary digital system is becoming much more difficult as these devices increase in size automatic_test_pattern_generation atpg techniques are commonly used to generate these tests since atpg is an np complete problem with complexity exponential to circuit size the application of parallel_processing techniques to accelerate the process of finding test patterns is an active area of research this paper presents an approach to parallelization of the test_generation problem that is targeted to a networkofworkstations environment the system is based upon partitioning of the fault list across multiple processors and includes enhancements designed to address the main drawbacks of this technique namely unequal load balancing and generation of redundant vectors the technique is generalized enough that it can be applied to any test_generation system regardless of the atpg or fault_simulation algorithm employed results were gathered to determine the impact of workstation processing load and network communications load on the performance of the system 
classroom presenter enhancing interactive education with digital ink,classroom presenter is a tablet pcbased interaction system that supports the sharing of digital ink on slides between instructors and students initial deployments show that using the technology can achieve a wide range of educational goals and foster a more participatory classroom environment
two oddsradiobased text classification_algorithms,since 1990s the exponential growth of theseweb documents has led to a great deal of interestin developing efficient tools and software toassist users in finding relevant information textclassification has been proved to be useful inhelping organize and search text information onthe web although there have been existed anumber of text classification_algorithms most ofthem are either inefficient or too complex in thispaper we present two oddsradiobased textclassification algorithms which are called orand tfor respectively we have evaluated ouralgorithm on two text collections and compared itagainst knn and svm experimental resultsshow that or and tfor are competitive withknn and svm furthermore or and tfor ismuch simpler and faster than them the resultsalso indicate that it is not tf but relevancefactors derived from odds radio that play thedecisive role in document_categorization
congestion adaptive_routing in mobile_ad_hoc_networks,mobility channel error and congestion are the main causes for packet_loss in mobile_ad_hoc_networks reducing packet_loss typically involves congestion_control operating on top of a mobility and failure adaptive_routing protocol at the network layer in the current designs routing is not congestionadaptive routing may let a congestion happen which is detected by congestion_control but dealing with congestion in this reactive manner results in longer delay and unnecessary packet_loss and requires significant overhead if a new route is needed this problem becomes more visible especially in largescale transmission of heavy traffic such as multimedia_data where congestion is more probable and the negative impact of packet_loss on the service_quality is of more significance we argue that routing should not only be aware of but also be adaptive to network_congestion hence we propose a routing_protocol crp with such properties our ns2 simulation results confirm that crp improves the packet_loss_rate and endtoend delay while enjoying significantly smaller protocol overhead and higher energy efficiency as compared to aodv and dsr
implementation and evaluation for dependable bus control using cpld,bus systems are used in computers as essential architecture and dependability of bus systems should be accomplished reasonably for various applications in this paper we will present dependable bus operations with actual implementation and evaluation by cpld most of the bus systems control transition of some classified phases with synchronous clock or guard time to avoid incorrect phase transition however these phase control methods may degrade system performance or cause incorrect operations we design an asynchronous sequential circuit for bus phase control without clock or guard time this circuit prevents incorrect phase transition at the time when large input delay or erroneous input occurs we estimate probability of incorrect phase transition with single stuckat fault on input signals from the result of estimation we also design checking system verifying outputs of initiator and target devices incorrect phase transition with single stuckat fault occurred between both sequential_circuits is inhibited completely by implementation of the system
portscan detection with sampled netflow,sampling techniques are often used for traffic monitoring in highspeed links in order to avoid saturation of network resources although there is a wide existing research dealing with anomaly_detection few studies analyzed the impact of sampling on the performance of portscan detection_algorithms in this paper we performed several experiments on two already existing portscan detection mechanisms to test whether they are robust enough to different sampling techniques unlike previous works we found that flow sampling is not always better than packet sampling to continue detecting portscans reliably
morphemebased chinese nested named_entity_recognition,named_entity_recognition plays an important role in many natural_language_processing applications while considerable attention has been pain in the past to research issues related to named_entity_recognition few studies have been reported on the recognition of nested named_entities this paper presents a morphemebased method to chinese nested named_entity_recognition to approach this task we first employ the logistic_regression_model to extract multilevel entity morphemes from an entitytagged corpus and thus explore a variety of lexical features under the framework of conditional_random_fields to perform chinese nested named_entity_recognition our experimental results on different data set show that our system is effective for most nested named_entities under evaluation
voice priority_queue scheduling system models for voip over wlans,the voice_over_internet_protocol voip is a delay_sensitive traffic due to realtime applications on networks the assessment of voice flow quality in the voip is an essential requirement for technical and commercial motivation the packets of voip streaming may experience drops because of the competition among the different kinds of traffic flow over the network a voip application is also sensitive to delay and requires the voice packets to arrive on time from the sender to the receiver side without any delay over wlan the scheduling system model for voip traffic is an unresolved problem in this research paper the author proposes a new voice priority_queue vpq scheduling system models and algorithms for the voip over wlans to solve scheduling issues over ipbased networks they present new contributions through the three stages of the vpq the vpq scheduling algorithm is provided as an essential technique in the voip communication_networks to guarantee the qos_requirements the design of the vpq is managed by the limited bandwidth utilization and has been proven to have an efficient performance over wlans
harmonic retrieval using higher order statistics a deterministic formulation,given a single record the authors consider the problem of estimating the parameters of a harmonic signal buried in noise the observed data are modeled as a sinusoidal signal plus additive_gaussian_noise of unknown covariance the authors define novel higher order statisticsreferred to as mixed cumulantsthat can be consistently estimated using a single record and are insensitive to colored gaussian_noise employing fourthorder mixed cumulants they estimate the sinusoid parameters using a consistent nonlinear matching approach the algorithm requires an initial estimate that is obtained from a consistent linear estimator finally the authors examine the performance of the proposed method via simulations 
a comparison between strand spaces and multiset rewriting for security_protocol analysis,formal analysis of security_protocols is largely based on a set of assumptions commonly referred to as the dolevyao model two formalisms that state the basic assumptions of this model are related here strand spaces and multiset rewriting with existential quantification strand spaces provide a simple and economical approach to analysis of completed protocol runs by emphasizing causal interactions among protocol participants the multiset rewriting formalism provides a very precise way of specifying finitelength protocols with unboundedly many instances of each protocol role such as client_server initiator or responder a number of modifications to each system are required to produce a meaningful comparison in particular we extend the strand formalism with a way of incrementally growing bundles in order to emulate an execution of a protocol with parametric strands the correspondence between the modified formalisms directly relates the intruder theory from the multiset rewriting formalism to the penetrator strands the relationship we illustrate here between multiset rewriting specifications and strand spaces thus suggests refinements to both frameworks and deepens our understanding of the dolevyao model
offthepath flow handling mechanism forhighspeed and programmable traffic management,in this paper we propose a highspeed and programmable traffic management mechanism to enable easy and timely innovations a control framework introduced by 4d tesseract or openflow separates control functions from the switch nodes to a control server so that a variety of network control policies can be implemented outside of the switches within this framework we propose a mechanism to enable flexible flowbased traffic management so that a variety of innovative traffic management schemes can be realized perflow traffic management however requires packetbypacket state updates which can spoil this control framework the proposed mechanism consists of a control server that monitors traffic conditions using sampled packets sent from the switches and calculates perflow packet discarding rate and switches that discard incoming packets according to the discarding rate packet sampling and discarding do not require packetbypacket state handling at the switches and thus allows controls from a control server we also propose a mechanism to compress the discarding information using a time series of bloom filters so that frequent control updates are allowed we tested the mechanism with perflow wfq emulation and the simulation results showed very good perflow fairness furthermore we found that the flow table is compressed 600 times smaller and that the processing cost at the server and the switches is small enough for use with 10 gbps links
genetic_algorithms and instruction_scheduling,
plda parallel latent dirichlet allocation for largescale applications,this paper presents plda our parallel_implementation of latent dirichlet allocation on mpi and mapreduce plda smooths out storage and computation bottlenecks and provides fault recovery for lengthy distributed computations we show that plda can be applied to large realworld applications and achieves good scalability we have released  mpiplda  to open source at httpcodegooglecompplda under the apache license
a fast thresholded linear convolution representation of morphological_operations,in this correspondence we present a fast thresholded linear convolution representation of morphological_operations the thresholded linear convolution representation of dilation and erosion is first proposed a comparison of the efficiency of the direct implementation of morphological_operations and the thresholded linear convolution representation of morphological_operations is subsequently conducted mathematical_morphology has emerged as a powerful new tool for image_processing 
facebook users have become much more private a largescale study,we investigate whether facebook users have become more private in recent years specifically we examine if there have been any important trends in the information facebook users reveal about themselves on their public profile pages since early 2010 to this end we have crawled the public profile pages of 14 million new york city nyc facebook users in march 2010 and again in june 2011 we have found that nyc users in our sample have become dramatically more private during this period for example in march 2010 only 172 of users in our sample hid their friend lists whereas in june 2011 just 15 months later 526 of the users hid their friend lists we explore privacy trends for several personal attributes including friend list networks relationship high school name and graduation year gender and hometown we find that privacy trends have become more pronounced for certain demographics finally we attempt to determine the primary causes behind the dramatic decrease in the amount of information facebook users reveal about themselves to the general public
improved compression by coupling of coding_techniques and redundant transform,the techniques commonly used in image_coding jpeg mpeg  have as the main objective to compress as much as possible while retaining most of the information these methods are often based on the use of the discrete_cosine_transform dct and the wavelet_transform wt our purpose is to consider the necessary redundancy to achieve a good reception in the case of heavy interruptions of bits transmission there is however a contradiction between an optimal compression and the redundancy required it is thus necessary to master and compress the information to be transmitted as much as possible and to withstand the noise on the transmission channel this article makes a contribution to this difficult problem its originality resides in the coupling of orthogonal transforms and a redundant transform simulation results are provided using our method and the results are compared with that of dct and wt based methods for lena and mountain images
principal_component_analysis for large scale problems with lots of missing values,principal_component_analysis pca is a wellknown classical data analysis technique there are a number of algorithms for solving the problem some scaling better than others to problems with high_dimensionality they also differ in their ability to handle missing values in the data we study a case where the data are highdimensional and a majority of the values are missing in case of very sparse data overfitting becomes a severe problem even in simple linear_models such as pca we propose an algorithm based on speeding up a simple principal subspace rule and extend it to use regularization and variational_bayesian vb learning the experiments with netflix data confirm that the proposed algorithm is much faster than any of the compared methods and that vbpca method provides more accurate predictions for new data than traditional pca or regularized pca
on the guaranteed error correction capability of ldpc_codes,we investigate the relation between the girth and the guaranteed error correction capability of gammaleft regular ldpc_codes when decoded using the bit flipping serial and parallel_algorithms a lower bound on the number of variable nodes which expand by a factor of at least 3gamma4 is found based on the moore bound an upper bound on the guaranteed correction capability is established by studying the sizes of smallest possible trapping_sets
image recovery using a new nonlinear adaptive_filter based on neural_networks,this work defines a new nonlinear adaptive_filter based on a feedforward_neural_network with the capacity of significantly reducing the additive_noise of an image even though measurements have been carried out using xray images with additive_white_gaussian_noise it is possible to extend the results to other type of images comparisons have been carried out with the weiner filter because it is the most effective option for reducing gaussian_noise in most of the cases image_reconstruction using the proposed method has produced satisfactory results finally some conclusions and future work lines are presented
blob analysis of the head and hands a method for deception detection,behavioral indicators of deception and behavioral state are extremely difficult for humans to analyze blob analysis a method for analyzing the movement of the head and hands based on the identification of skin color is presented this method is validated with numerous skin tones a proofofconcept study is presented that uses blob analysis to explore behavioral state identification in the detection of deception
constrained angular motion_estimation in a gyrofree imu,in this paper we present an extended kalman_filter ekfbased solution for the estimation of the angular motion using a gyrofree inertial measurement unit gfimu built of twelve separate monoaxial accelerometers using such a gfimu produces a vector which we call the angular information vector aiv that consists of 3d angular acceleration terms and six quadratic terms of angular velocities we consider the multiple distributed orthogonal triads of accelerometers that consist of three nonplanar distributed triads equally spaced from a central triad as a specific case to solve during research for the possible filter schemes we derived equality constraints hence we incorporate the constraints in the filter to improve the accuracy of the angular motion_estimation which in turn improves the attitude accuracy direction cosine matrix dcm or quaternion vector
subject searching in online catalogs metaknowledge used by experienced searchers,this paper begins to identify and characterize the knowledge used by experienced librarians while searching for subject information in online catalogs ten experienced librarians performed the same set of six subject searches in an online catalog investigated was the knowledge used to solve retrieval problems this knowledge represents expertise in the use of the catalog data were collected through the use of thinkaloud protocols transaction logs and structured interviews knowledge was defined as knowledge of objects factual knowledge knowledge of events experiential knowledge knowledge of performance process knowledge and metaknowledge metaknowledge is the sense of whole derived from the integration of factual process and experiential knowledge about the search and the conditions under which it is performed the focus of this paper is on metaknowledge for evidence of metaknowledge the data were examined for explanations that participants gave for their actions and observations and for ways that participants evaluated their own progress during the process of searching reasons and explanations given by searchers were related to all phases of the library information_retrieval process from the users receipt of material to policies for collection development and not just events directly related to the performance of a particular search task
injectionlocked clocking a lowpower clock_distribution scheme for highperformance microprocessors,we propose injectionlocked clocking ilc to combat deteriorating clock_skew and jitter and reduce power consumption in highperformance microprocessors in the new clocking scheme injectionlocked oscillators are used as local clock receivers compared to conventional clocking with buffered trees or grids ilc can achieve better power efficiency lower jitter and much simpler skew compensation thanks to its builtin deskewing capability unlike other alternatives ilc is fully compatible with conventional clock_distribution_networks in this paper a quantitative study based on circuit and microarchitecturallevel simulations is performed alpha21264 is used as the baseline processor and is scaled to 013 m and 3 ghz simulations show 20 and 23ps jitter reduction 101 and 17 power savings in two ilc configurations a test chip distributing 5ghz clock is implemented in a standard 018 m cmos technology and achieved excellent jitter performance and a deskew range up to 80 ps
state of the art in modeling and deployment of electronic contracts,modeling and deployment of econtracts is a challenging task because of the involvement of both technological and business aspects there are several frameworks and systems available in the literature some works mainly deal with the automatic handling of paper contracts and others provide monitoring and enactment of contracts because contracts evolve it is useful to have a system that models and enacts the evolution of econtracts
codex exploration of semantic changes between ontology versions,summary life science ontologies substantially change over time to meet the requirements of their users and to include the newest domain_knowledge thus an important task is to know what has been modified between two versions of an ontology diff  this diff should contain all performed changes as compact and understandable as possible we present codex complex ontology diff explorer a tool that allows determining semantic changes between two versions of an ontology which users can interactively analyze in multiple ways availability and implementation codex is available under http wwwizbidecodex and is supported by all major browsers it is implemented in java based on google web toolkit technology additionally users can access a web service interface to use the diff functionality in their applications and analyses
reliability analysis in the early development of realtime reactive_systems,the increasing trend toward complex_software_systems has highlighted the need to incorporate quality_requirements earlier in the development process reliability is one of the important quality indicators of such systems this paper proposes a reliability analysis approach to measure reliability in the early development of realtime reactive_systems rtrs the goal is to provide decision support and detect the first signs of low or decreasing reliability as the system design evolves the analysis is conducted in a formal development environment for rtrs formalized mathematically and illustrated using a traingatecontroller case study
tools for creating documents in preferred format for visually_impaired_people,a suite of tools is described that allow a wordprocessor operator to produce documents in large print braille and on cassette tape so that the documents can subsequently be read by visually_impaired or blind_users the tools integrate with microsoft word are written in visual basic for applications and make use of the word object model
ncac network_congestion analyzer and controller,the stability of the current internet_architecture depends mostly on endtoend tcp_congestion_control mechanisms the network_congestion analyzer and controller ncac is an effort to build a complete user_interface application to ns2 that provides graphical access to most of ns2s functionalities besides the application provides powerful visual tools for monitoring and displaying network_performance metrics calculated by the simulation the application is developed in java to benefit from javas portability and platform independence there are three main parts in ncac 1 ns2 interface 2 network animation and alarm and 3 graph plotting application the functionality of each part is discussed
on the comparison of bilinear cubic_spline and fuzzy interpolation techniques for robotic position measurements,this paper describes a novel technique for position error compensations of robots based on a fuzzy error interpolation method a traditional robot calibration implements either model or modelless methods the compensation of position error in a modelless method is to move the robots endeffector to a target_position in the robot workspace and to find the target_position error online based on the measured neighboring fourpoint errors around the target_position for this purpose a stereo_camera or other measurement device can be used to measure offline the position errors of the robots endeffector at predefined grid points by using the proposed fuzzy error interpolation technique the accuracy of the position error compensation can be greatly improved which is confirmed by the simulation results given in this paper a comparison study among various interpolation methods such as bilinear cubic_spline and the fuzzy error interpolation technique is also made via simulation the simulation results show that more accurate compensation results can be achieved using the fuzzy error interpolation technique compared with its bilinear and cubic_spline counterparts
locally discriminative coclustering,different from traditional onesided clustering_techniques coclustering makes use of the duality between samples and features to partition them simultaneously most of the existing coclustering algorithms focus on modeling the relationship between samples and features whereas the intersample and interfeature relationships are ignored in this paper we propose a novel coclustering algorithm named locally discriminative coclustering ldcc to explore the relationship between samples and features as well as the intersample and interfeature relationships specifically the samplefeature relationship is modeled by a bipartite_graph between samples and features and we apply local linear regression to discovering the intrinsic discriminative structures of both sample space and feature_space for each local patch in the sample and feature spaces a local linear function is estimated to predict the labels of the points in this patch the intersample and interfeature relationships are thus captured by minimizing the fitting errors of all the local linear functions in this way ldcc groups strongly associated samples and features together while respecting the local structures of both sample and feature spaces our experimental results on several benchmark data sets have demonstrated the effectiveness of the proposed method
automatic annotation of drosophila developmental stages using association classification and information_integration,in current developmental research one of the challenging tasks is to understand the spatiotemporal gene expression patterns and the relationships among different genes in situ hybridization ish assay which shows mrna spatiotemporal expression patterns in cells and tissues directly is currently widely utilized in the bench work with the increasing of available ish images automatic annotation systems are highly demanded in this paper an automatic_classification system is proposed for annotating the in situ hybridization images with respect to the developmental stages the embryo is first segmented from the original image registered and normalized the segmented embryo image is then divided into 100 blocks from which the pixel intensity and texture features are extracted and discretized the multiple correspondence analysis mca based association classification_approach is proposed to generate classification_rules for different stages based on the training data set the testing instance is classified by applying the rules generated in the training process and a classification coordination module is incorporated to resolve the conflicts utilizing the weights derived from angle values in the mca procedure experimental results show that our proposed method achieves promising results and outperforms other stateoftheart algorithms
theoretical calculations of homoconjugation equilibrium constants in systems modeling acidbase interactions in side chains of biomolecules using the potential of mean force,the potentials of mean force pmfs were determined for systems forming cationic and anionic homocomplexes composed of acetic acid phenol isopropylamine nbutylamine imidazole and 45methylimidazole and their conjugated bases or acids respectively in three solvents with different polarity and hydrogenbonding propensity acetonitrile an dimethyl sulfoxide dmso and water h 2 o for each pair and each solvent a series of umbrellasampling molecular dynamics simulations with the amber force field explicit solvent and counterions added to maintain a zero net charge of a system were carried out and the pmf was calculated by using the weighted histogram analysis method wham subsequently homoconjugationequilibrium constants were calculated by numerical_integration of the respective pmf profiles in all cases but imidazole stable homocomplexes were found to form in solution which was manifested as the presence of contact minima corresponding to hydrogenbonded species in the pmf curves the calculated homoconjugation constants were found to be greater for complexes with the oho bridge acetic acid and phenol than with the nhn bridge and they were found to decrease with increasing polarity and hydrogenbonding propensity of the solvent ie in the series an  dmso  h 2 o both facts being in agreement with the available experimental data it was also found that interactions with counterions are manifested as the broadening of the contact minimum or appearance of additional minima in the pmf profiles of the acetic acidacetate phenol phenolate system in acetonitrile and the 45methylimidazole45methylimidzole cation conjugated base system in dimethyl sulfoxide
costeffective computeraided manufacturing of prototype parts,computeraided manufacturing cam can be made cost effective even for oneofakind jobs to do so requires reevaluation of computerized numerical_control cnc from the point of view of a computer_system rather than a glorified machine shop good systematization useful software and cooperative cnc shop staff can improve the productivity of a typical cnc shop by a factor of five
datadriven design of hmm topology for online handwriting_recognition,although hmm is widely used for online handwriting_recognition there is no simple and wellestablished method of designing the hmm topology we propose a datadriven systematic method to design hmm topology data samples in a single pattern class are structurally simplified into a sequence of straightline segments then the resulting multiple models of the class are combined to form an architecture of a multiple parallelpath hmm which behaves as single hmm to avoid excessive growing of the number of the states parameter trying is applied such that structural similarity among patterns is reflected experiments on online hangul recognition showed about 19 of error reductions compared to the intuitive deisgn method
fast block size prediction for mpeg2 to h264avc transcoding,one objective in mpeg2 to h264 transcoding is to improve the h264 compression_ratio by using more accurate h264 motion_vectors motion reestimation is by far the most time consuming process in video_transcoding and improving the searching speed is a challenging problem we introduce a new transcoding scheme that uses the mpeg2 dct_coefficients to predict the block size partitioning for h264 performance evaluations have shown that for the same ratedistortion performance our proposed scheme achieves an impressive reduction in the computational complexity of more than 82 compared to the full range motion_estimation used by h264
a curvaturebased approach to contour motion_estimation,we present a novel method of velocity field estimation for points on moving contours in an image_sequence the method determines the corresponding point in the next image frame by considering curvature changes at each point on a contour in previous methods there are errors in estimation for the points which have low curvature variations since those methods compute the solutions by approximating the normal component of optical_flow the proposed method computes optical_flow vectors of contour points by minimizing the curvature changes as a first step snakes are used to locate smooth curves in 2d imagery then the extracted curves are tracked continuously we excluded the rearranging process in snakes and allowed the snaxel distance to vary each point on a contour has a unique corresponding point in the nest frame experimental results showed that the proposed method computes accurate optical_flow vectors for various moving contours
the case for timingcentric distributed software invited paper,this paper makes the case that the time is right to introduce temporal semantics into programming_models for cyberphysical systems specifically we argue for a programming_model called ptides that provides a coordination language rooted in discreteevent semantics supported by a lightweight runtime framework and tools for verifying concurrent_software components ptides leverages recent innovations in network time_synchronization to deliver distributed realtime systems with determinate concurrent semantics decentralized and robust_control and the potential for rigorous schedulability_analysis
balancing resource utilization to mitigate power density in processor pipelines,power density is a growing problem in highperformance processors in which small highactivity resources overheat two categories of techniques temporal and spatial can address power density in a processor temporal solutions slow computation and heating either through frequency and voltage scaling or through stopping computation long enough to allow the processor to cool both degrade performance spatial solutions reduce heat by moving computation from a hot resource to an alternate resource eg a spare alu to allow cooling spatial solutions are appealing because they have negligible impact on performance but they require availability of spatial slack in the form of spare or underutilized resource copies previous work focusing on spatial slack within a pipeline has proposed adding extra resource copies to the pipeline which adds substantial complexity because the resources that overheat issue logic register files and alus are the resources in some of the tightest critical paths in the pipeline previous work has not considered exploiting the spatial slack already existing within pipeline resource copies utilization can be quite asymmetric across resource copies leaving some copies substantially cooler than others we observe that asymmetric utilization within copies of three key backend resources the issue queue register files and alus creates spatial slack opportunities by balancing asymmetry in their utilization we can reduce power density scheduling policies for these resources were designed for maximum simplicity before power density was a concern our challenge is to address asymmetric heating while keeping the pipeline simple balancing asymmetric utilization reduces the need for other performancedegrading temporal powerdensity techniques while our techniques do not obviate temporal techniques in highresourceutilization applications we greatly reduce their use improving overall performance
efficient verticalhorizontalspace 1ddct processing based on massiveparallel matrixprocessing engine,this paper reports an efficient discrete_cosine_transform dct processing for the jpeg algorithm using a massiveparallel memoryembedded simd matrix processor the matrixprocessing engine has 2048 2bit processing_elements which are connected by a flexible switching network and supports 2bit 2048way bitserial and wordparallel operations with a single command for compatibility with this matrixprocessing architecture the conventional dct algorithm has been improved in arithmetic order and the verticalhorizontalspace 1 dimensional 1ddct processing has been further developed evaluation results of the matrixenginebased dct processing show that the necessary clock_cycles per image blocks can be reduced by 87 in comparison to a conventional dsp architecture the determined performances in mops and mopsmm are factors 8 and 56 better than with a conventional dsp respectively moreover the matrixprocessing engine can reduce the number of total clock_cycles for jpeg application about 49 in comparison to a conventional dsp architecture
parallel online ranking of web_pages,modern search_engines use link_structure of the world_wide_web in order to gain better results for ranking the results of users queries one of the most popular ranking algorithms which is based on link_analysis is hits it generates very accurate outputs but because of huge amount of online computations this algorithm is relatively slow in this paper we introduce phits a parallelized version of the hits algorithm that is suitable for working with huge web_graphs in a reason able time for implementing this algorithm we use webgraph framework and we focus on parallelizing access to web_graph as the main bottleneck in the hits algorithm i introduction search technology is one of the most important reasons for success of the web the huge amount of information available on the web its high growth rate and its unstructured nature all increase the need for search_engines with high performance and accurate results one of the major components of each search_engine is its ranking algorithm traditional information_retrieval ir systems usually use some models like vms 4 and compute rank of results using content similarity measures between users query and retrieved documents but in the context of the web there are some problems with these approaches for example spamming may lead to inefficient ranking some methods have been proposed to encounter these problems most of which uses some implicit information which is embedded in the web_graph these methods are known as linkanalysis based algorithms pagerank 5 and hits hyperlink induced topic search 1 are the most well known algorithms in this category pagerank which is used by google for ranking its results is an offline and queryindependent ranking algorithm this means that the ranking is independent of the specific queries of users and therefore can be done once and used for all of the upcoming queries on the other hand hits is an online and querydependent algorithm being query dependent makes hits more precise but it has some disadvantages too in fact required online computations for this algorithm is too much and the response time of the search_engine after submitting queries by users is not acceptable to overcome this problem in this paper we will exploit the parallel_processing methods to improve the execution performance of the algorithm the rest of this paper is organized as follows in section ii linkanalysis based algorithms in general and hits as a special case are discussed at the end of this section some of the variations and improvements for the hits algorithm that are suggested in the literature are also described implementing the hits algorithm and its parallel_version phits are discussed in sections iii and iv respectively finally last section of this paper contains conclusion and some ideas for future work in this topic
authentication in distributed_systems theory and practice,we describe a theory of authentication and a system that implements it our theory is based on the notion of principal and a speaks for relation between principals a simple principal either has a name or is a communication channel a compound principal can express an adopted role or delegation of authority the theory explains how to reason about a principals authority by deducing the other principals that it can speak for authenticating a channel is one important application we use the theory to explain many existing and proposed mechanisms for security in particular we describe the system we have built it passes principals efficiently as arguments or results of remote procedure calls and it handles public and shared key encryption name lookup in a large name space groups of principals loading programs delegation access_control and revocation
human gait_recognition with matrix representation,human gait is an important biometric feature it can be perceived from a great distance and has recently attracted greater attention in videosurveillancerelated applications such as closedcircuit television we explore gait_recognition based on a matrix representation in this paper first binary silhouettes over one gait cycle are averaged as a result each gait video sequence containing a number of gait_cycles is represented by a series of graylevel averaged images then a matrixbased unsupervised algorithm namely coupled subspace analysis csa is employed as a preprocessing step to remove_noise and retain the most representative information finally a supervised algorithm namely discriminant_analysis with tensor representation is applied to further improve classification ability this matrixbased scheme demonstrates a much better gait_recognition performance than stateoftheart algorithms on the standard usf humanid gait database
design_for_testability based on singleportchange delay_testing for data paths,this paper introduces a new concept of hierarchical testability called singleportchange spc twopattern testability we propose a nonscan designfortestability dft method which makes each path that needs to be tested in a data path spc twopattern testable an spc twopattern test guarantees robust resp nonrobust test if the path is robust resp nonrobust testable since it is easy to find justification paths for spc twopattern tests at registertransfer level the proposed dft method can reduce hardware overhead compared to that of our previous dft method for arbitrary twopattern tests furthermore we propose a method to reduce test_generation effort by removing a subset of sequentially untestable paths from targets of test_generation experimental results show that the proposed method can reduce hardware overhead without losing the quality of test
daids an architecture for modular mobile ids,the popularity of mobile_devices and the enormous number of third party mobile_applications in the market have naturally lead to several vulnerabilities being identified and abused this is coupled with the immaturity of intrusion_detection_system ids technology targeting mobile_devices in this paper we propose a modular hostbased ids framework for mobile_devices that uses behavior analysis to profile applications on the android platform anomaly_detection can then be used to categorize malicious_behavior and alert users the proposed system accommodates different detection_algorithms and is being tested at a major telecom operator in north america this paper highlights the architecture findings and lessons learned
an integrated_data mining system to automate discovery of measures of association,many data analysts require tools which can integrate their database_management packages eg microsoft access with their data analysis ones eg sas spss and provide guidance for the selection of appropriate mining_algorithms in addition the analysts need to extract and validate statistical results to facilitate data_mining in this paper we describe an integrated_data mining system called the linear correlation discovery system lcds that meets the above requirement lcds consists of four major subcomponents two of which the selection assistant and the statistics coupler we discussed in this paper the former examines the scheme and instances to determine appropriate association measurement functions eg chisquare linear regression anova the latter involves the appropriate statistical function on a sample data set and extracts relevant statistical output such as spl etasup 2 and rsup 2 for effective mining of data we also describe a new validation algorithm based on measuring the consistency of mining results applied to multiple test sets
configuring sensors by user learning for a locomotion aid interface,this article presents a study about a mobility aid system which integrates adaptive_control interface for a robotic walker the objective is to give to the patient more flexibility in the choice of a technical aid the specificity of our system is based on an autoadaptive interface which improves the configuration choice of the patients driving commands the results obtained from experimentations show the capabilities of our method for a technical aid the learning process is applied on line to a neural_network controller the experiment is focused on sensors configuration and command of a powered walker interface
hermite normality tests,nous presentons dans cet article une etude complete du test de normalite dhermite ce test utilise les proprietes des polynomes dhermite et une statistique de sphericite modifiee pour decider si un echantillon monodimensionnel standardise et blanc est gaussien ou non lavantage majeur de cette approche est de definir une famille de statistiques qui va permettre dadapter le choix dun test particulier aux donnees nous avons etabli la distribution asymptotique du test dhermite sous lhypothese nulle et sous lhypothese alternative et etudie en details le cas particulier de tests a deux polynomes nous avons determine les tests asymptotiquement les plus puissants pour quelques distributions alternatives et effectue un grand nombre de simulations afin de comparer le test dhermite a trois autres tests les bons resultats obtenus nous encouragent a generaliser le test dhermite aux cas de donnees colorees et multivariees
modeling and simulation of a fuzzy supervisory controller for an industrial boiler,in this paper we compare and discuss the performance of a boiler evaporator system when the system is controlled by a traditional pidtype strategy and when the system is enhanced by using fuzzy logic blocks to provide setpoints for the system the strategy used in fuzzy_logic_controllers flcs is called fuzzy supervisory_control and it generates setpoints for the conventional_controllers the boiler under test is a vu60 industrial system that produces 180000 pounds of steam per hour the mathematical model of the plant is a scaled version model of that obtained for a thermoelectric unit the new model simplifies the largescale thermoelectric boiler model to an industrial smallscale type vu60 boiler model based upon first principle mass and energy balance equations the main change consists of representing only the behavior of the drumevaporator system having a partial model of the combustion process with a simplified combustion control_system and a threeelement boiler feedwater controller the control_system for combustion and boiler feedwater receives a supervisory signal or setpoint tracking signal that comes from the flc to improve the performance of the overall control_system the behavior of the supervisory controller brings some advantages to the system performance compared with the traditional control schemes the comparison reflects fuel improvements from 25 to 65 depending upon the steam load ramp regime the simulations are performed using the simulink shell running under the matlab  platform
partial functional manipulation based wirelength minimization,inplace flipping of rectangular blockscells can potentially reduce the wirelength of a floorplanplacement solution without changing the chip area in a recent work hao 05 the flipping optimization is solved through a binary_decision_diagram bdd based approach however the bddbased approach is not scalable for large soc_designs with many blocks due to memory and runtime blowup this paper presents a new approach using the partitioned ordered partial decision_diagrams popdd for wirelength minimization popdd is based on a novel compact partial functional representation between flip configurations and corresponding wirelengths by controlling the number of nodes allowed per popdd and the iterations easy tradeoff between runtimememory and accuracyoptimality can be achieved experimental results clearly demonstrate the efficiency of the proposed approach
assessment of an operational system for crop type map production using high temporal and spatial resolution satellite optical imagery,crop area extent estimates and crop type maps provide crucial information for agricultural monitoring and management remote_sensing_imagery in general and more specifically high temporal and high_spatial_resolution data as the ones which will be available with upcoming systems such as sentinel2 constitute a major asset for this kind of application the goal of this paper is to assess to what extent stateoftheart supervised_classification methods can be applied to high resolution multitemporal optical imagery to produce accurate crop type maps at the global scale five concurrent strategies for automatic crop type map production have been selected and benchmarked using spot4 take5 and landsat 8 data over 12 test sites spread all over the globe four in europe four in africa two in america and two in asia this variety of tests sites allows one to draw conclusions applicable to a wide variety of landscapes and crop systems the results show that a random_forest_classifier operating on linearly temporally gapfilled images can achieve overall accuracies above 80 for most sites only two sites showed low performances madagascar due to the presence of fields smaller than the pixel size and burkina faso due to a mix of trees and crops in the fields the approach is based on supervised machine_learning_techniques which need in situ data collection for the training step but the map production is fully automatic
data driven frequency mapping for computationally scalable object_detection,nonlinear kernel support_vector_machines achieve better generalizations yet their training and evaluation speeds are prohibitively slow for realtime object_detection tasks where the number of data_points in training and the number of hypotheses to be tested in evaluation are in the order of millions to accelerate the training and particularly testing of such nonlinear kernel machines we map the input data onto a lowdimensional spectral fourier feature_space using a cosine transform design a kernel that approximates the classification objective in a supervised setting and apply a fast linear classifier instead of the conventional radial_basis_functions we present a data driven hypotheses generation technique and a logistboost feature_selection our experimental results demonstrate the computational improvements 20100 while maintaining a high classification accuracy in comparison to svm linear and radial kernel basis function classifiers
towards computerassisted photoidentification of humpback whales,this paper describes current work on a photoid system for humpback whales individuals of this species can be uniquely identified by the light and dark pigmentation patches on their tails flukes we developed an interface that assists the user in segmenting the animals tail from the sea and fitting an affine invariant coordinate grid to it a numerical feature_vector capturing the patchdistribution with respect to the grid is then automatically extracted and used to match the individual against a database of similarly processed images
representing semantic_information in pulley problems,
a novel fast approach for estimating error propagation in decision_feedback detectors,the study of errorburst statistics is important for all detection systems and more so for the decision_feedback class in data storage applications many detection systems use decision_feedback in one form or another fixeddelay tree search with decision_feedback fdtsdf and decision_feedback equalization dfe are the direct forms whereas the partial response detectors such as the reduced state sequence estimator rsse and noise predictive maximum_likelihood npml detectors are the other forms although df reduces the system complexity it is inevitably linked with error propagation ep which can be quantified using errorburst statistics analytical evaluation of these statistics is difficult if not impossible because of the complexity of the problem hence the usual practice is to use computer simulations however the computational_time in traditional bitbybit simulations can be prohibitive at meaningful signaltonoise ratios in this paper we propose a novel approach for fast estimation of errorburst statistics in fdtsdf detectors which is also applicable to other detection systems in this approach error events are initiated more frequently than natural by artificially injecting noise samples these noise samples are generated using a transformation that results in significant reduction in computational complexity simulation studies show that the ep performance obtained by the proposed method matches closely with those obtained by bitbybit simulations while saving as much as 99 of simulation time
incrementally maintaining classification using an rdbms,the proliferation of imprecise data has motivated both researchers and the database industry to push statistical techniques into relational_database_management_systems rdbmses we study strategies to maintain modelbased views for a popular statistical technique classification inside an rdbms in the presence of updates to the set of training examples we make three technical contributions 1 a strategy that incrementally maintains classification inside an rdbms 2 an analysis of the above algorithm that shows that our algorithm is optimal among all deterministic algorithms and asymptotically within a factor of 2 of a nondeterministic optimal_strategy 3 a novel hybridarchitecture based on the technical ideas that underlie the above algorithm which allows us to store only a fraction of the entities in memory we apply our techniques to text_processing and we demonstrate that our algorithms provide an order of magnitude improvement over nonincremental approaches to classification on a variety of data sets such as the citeseer and dblife
optimal band_selection for future satellite sensor dedicated to soil science,hyperspectral_imaging_systems could be used for identifying the different soil types from the satellites however detecting the reflectance of the soils in all the wavelengths involves the use of a large number of sensors with high accuracy and also creates a problem in transmitting the data to earth_stations for processing the current sensors can reach a bandwidth of 20 nm and hence the reflectance obtained using the sensors are the integration of reflectance obtained in each of the wavelength present in the spectral_band moreover not all spectral bands contribute equally to classification and hence identifying the bands necessary to have a good classification is necessary to reduce sensor cost and problem in data transmission from the satellite the work presents the spectral bands selected using a pcabased forward sequential band_selection algorithm
positive macroscopic approximation for fast attribute_reduction,attribute_reduction is one of the challenging problems facing the effective application of computational intelligence technology for artificial_intelligence its task is to eliminate dispensable attributes and search for a feature subset that possesses the same classification capacity as that of the original attribute set to accomplish efficient attribute_reduction many heuristic_search_algorithms have been developed most of them are based on the model that the approximation of all the target concepts associated with a decision system is dividable into that of a single target concept represented by a pair of definable concepts known as lower_and_upper_approximations this paper proposes a novel model called macroscopic approximation considering all the target concepts as an indivisible whole to be approximated by rough set boundary region derived from inconsistent tolerance blocks as well as an efficient approximation framework called positive macroscopic approximation pma addressing macroscopic approximations with respect to a series of attribute subsets based on pma a fast heuristic search algorithm for attribute_reduction in incomplete decision_systems is designed and achieves obviously better computational_efficiency than other available algorithms which is also demonstrated by the experimental results
managing multiple engineering projects in a manufacturing support environment,business trends require frontline managers to integrate multiproject concepts with those of traditional singleproject management since very rarely can one find major organizations managing just one project a typical situation entails a limited pool of resources which is applied to the management of several projects with people moving back and forth among different assignments in different projects yet few studies on project management have started to explore the issue of how to manage an organization with multiple inter or intradepartmental projects using a case study method our exploratory research investigates the specific problems associated with the management of multiple engineering projects in a manufacturing support environment with the intent to identify common factors of success knowing the factors of success is but the first step toward improving multiproject management our findings provide insight into how the most important multipleproject success factors in this environment differ from factors of success in traditional singleproject management and are consistent with other emerging research in product development environments the differences center on resource allocation and flexibility some factors such as ownership staff experience and communication take on additional dimensions when considered in a multipleversus a singleproject environment division and assignment of resources prioritization and customized management style which have little relevance in relation to single projects are shown to play a major role in the success of multiproject management
coupled and ksided placements generalizing generalized assignment,
performance_analysis of a recursive fractional superexponential algorithm,the superexponential algorithm is a blockbased technique for blind_channel_equalization and system identification due to its fast convergence rate and no a priori parameterization other than the block length it is a useful tool for linear equalization of moderately distortive channels this paper presents a recursive implementation of the superexponential algorithm for fractionallysampled pam signals although the resulting algorithm is still blockbased recursive propagation of several key variables allows the block length to be significantly reduced without compromising the algorithms accuracy or speed thereby enhancing its ability to track channel variations the convergence rate is only mildly influenced by specific channel responses and oversampling provides smaller output variance and almost perfect tolerance to sampling errors simulation results demonstrate the effectiveness of the proposed technique
freeviewpoint image_generation using different focal length camera array,the availability of multiview images of a scene makes new and exciting applications possible including freeviewpoint tv ftv ftv allows us to change viewpoint freely in a 3d world where the virtual viewpoint images are synthesized by imagebased rendering ibr in this paper we introduce a ftv depth estimation_method for forward virtual viewpoints moreover we introduce a view generation method by using a zoom camera in our camera setup to improve virtual viewpointts image_quality simulation results confirm reduced error during depth_estimation using our proposed method in comparison with conventional stereo_matching scheme we have demonstrated the improvement in image_resolution of virtually moved forward camera using a zoom camera setup
automatic location and tracking of the facial region in color video sequences,a novel technique is introduced to locate and track the facial area in videophonetype sequences the proposed method essentially consists of two components i a color processing unit and ii a knowledgebased shape and color analysis module the color processing component utilizes the distribution of skintones in the hsv color space to obtain an initial set of candidate regions or objects the second component in the segmentation scheme that is the shape and color analysis module is used to correctly identify and select the facial region in the case where more than one object has been extracted a number of fuzzy membership functions are devised to provide information about each objects shape orientation location and average hue an aggregation operator finally combines these measures and correctly selects the facial area the suggested approach is robust with regard to dierent skin types and various types of object or background motion within the scene furthermore the algorithm can be implemented at a low computational complexity due to the binary nature of the operations involved experimental results are presented for a series of cif and qcif video sequences  1999 elsevier science bv all rights reserved
on deriving the secondstage training_set for trainable combiners,unlike fixed combining rules the trainable combiner is applicable to ensembles of diverse base classifier architectures with incomparable outputs the trainable combiner however requires the additional step of deriving a secondstage training dataset from the base classifier outputs although several strategies have been devised it is thus far unclear which is superior for a given situation in this paper we investigate three principal training techniques namely the reuse of the training dataset for both stages an independent validation set and the stacked generalization on experiments with several datasets we have observed that the stacked generalization outperforms the other techniques in most situations with the exception of very small sample sizes in which the reusing strategy behaves better we illustrate that the stacked generalization introduces additional noise to the secondstage training dataset and should therefore be bundled with simple combiners that are insensitive to the noise we propose an extension of the stacked generalization approach which significantly improves the combiner robustness
automated service_composition with adaptive planning,serviceoriented computing is a cornerstone for the realization of user needs through the automatic composition_of_services from service descriptions and user tasks ie highlevel descriptions of the user needs yet automatic_service_composition processes commonly assume that service descriptions and user tasks share the same abstraction level and that services have been predesigned to integrate to release these strong assumptions and to augment the possibilities of composition we add adaptation features into the service_composition process using semantic_descriptions and adaptive extensions to graph planning
base_station scheduling of requests with fixed deadlines,we consider a packet switched wireless_network in which a base_station serves the mobiles the packets for the mobiles arrive at the base_station and have to be scheduled for transmission to the mobiles the capacity of the channel from the base_station to the mobiles is varying with time due to fading we assume the mobiles can obtain different types of service based on the prices they are willing to pay the objective of the base_station is to schedule packets for transmission to mobiles to maximize the revenue earned our main result is that a simple greedy algorithm does at least 12 as good as the optimal offline algorithm that knows the complete future request pattern and channel_conditions we also show that no other online_algorithm can do better
the sri nist 2008 speaker_recognition evaluation_system,the sri speaker_recognition_system for the 2010 nist speaker_recognition evaluation sre incorporates multiple subsystems with a variety of features and modeling techniques we describe our strategy for this years evaluation from the use of speech_recognition and speech segmentation to the individual system descriptions as well as the final combination our results show that under most conditions the cepstral systems tend to perform the best but that other noncepstral systems have the most complementarity the combination of several subsystems with the use of adequate side information gives a 35 improvement on the standard telephone condition we also show that a constrained cepstral system based on nasal syllables tends to be more robust to vocal effort variabilities
rehabilitation of handwriting skills in stroke patients using interactive games a pilot study,this paper describes an interactive application that aims to support the rehabilitation of handwriting skills in people that suffer from paralysis after a stroke the purpose of the application is to make the rehabilitation of handwriting skills fun and engaging four platformindependent games with adjustable levels of difficulty were created in order to target varying levels of skills the application also features a performance history audiovisual feedback and posture reminders it was evaluated with medical staff and patients from the hoensbroeck rehabilitation centre in the netherlands the initial results indicated that the games are more motivating and fun than traditional pen and paper exercises the feedback received from therapists supports our claim that the games are a useful addition to the rehabilitation of handwriting
a digital switching demodulator for electrical_capacitance_tomography,in this paper a digital switching demodulator is presented for use in acbased electrical_capacitance_tomography systems implementing a switching phasesensitive demodulator psd digitally offers the following advantages 1 demodulation can be implemented using a programmable digital device and hence cmos switches which are used in a conventional switching psd are no longer needed 2 compared with the widely used digital quadrature psd this proposed demodulator is simple in configuration because neither a reference signal nor multiplication is required 3 according to the specific requirements the new demodulator can be implemented in two operation modes ie the amplitude mode and the phasesensitive mode and 4 because only subtractions and accumulations are needed the proposed demodulator can be easily implemented with lowcost logic_devices eg a complex programmable_logic_device cpld by simulation the feasibility and effectiveness of the proposed demodulator have been confirmed cpldbased and fieldprogrammablegatearraybased capacitance measurement circuits are constructed and the performances of different demodulation methods are compared both simulation and experiment show that the proposed demodulator can provide demodulation results with high signaltonoise ratio the system design can be simplified using the digital switching demodulator
psychophysical evaluation of insitu ultrasound visualization,we present a novel psychophysical method for evaluating ultrasonography based on realtime tomographic reflection rttr in comparison to conventional ultrasound cus the method measures the users perception of the location of an ultrasoundimaged target independently from assessing the action employed to reach it three experiments were conducted with the sonic flashlight sf an rttr device and cus the first two experiments determined subjects perception of target location with a triangulationbypointing task depth perception with the sf was comparable to direct vision while cus caused considerable underestimation of target depth binocular depth information in the sf was shown to significantly contribute to its superiority the third experiment tested subjects in an ultrasoundguided needle_insertion task because the sf provides visualization of the target at its actual location subjects performed insertions faster and more accurately by using the sf rather than cus furthermore the trajectory analysis showed that insertions with the sf generally went directly to the target along the desired path while cus often led to a large deviation from the correct path consistent with the observed underestimation of target depth these findings lend great promise to the use of rttrbased imaging in clinical practice and provide precise means of assessing efficacy
probabilistic_model_checking of complex biological pathways,probabilistic_model_checking is a formal_verification technique that has been successfully applied to the analysis of systems from a broad range of domains including security and communication_protocols distributed_algorithms and power_management in this paper we illustrate its applicability to a complex biological system the fgf fibroblast growth factor signalling pathway we give a detailed description of how this case study can be modelled in the probabilistic_model checker prism discussing some of the issues that arise in doing so and show how we can thus examine a rich selection of quantitative properties of this model we present experimental results for the case study under several different scenarios and provide a detailed analysis illustrating how this approach can be used to yield a better understanding of the dynamics of the pathway finally we outline a number of exact and approximate techniques to enable the verification of larger and more complex pathways and apply several of them to the fgf case study
a resistancebased approach to consensus algorithm performance_analysis,we study the well known linear consensus algorithm by means of a lqtype performance cost we want to understand how the communication topology influences this algorithm in order to do this we recall the analogy between markov_chains and electrical resistive networks by exploiting this analogy we are able to rewrite the performance cost as the average effective resistance on a suitable network we use this result to show that if the communication graph fulfills some local properties then its behavior can be approximated with that of a suitable grid over which the behavior of the cost is known
energy introspector a parallel composable framework for integrated powerreliabilitythermal modeling for multicore_architectures,
on the use of the source reconstruction method for estimating radiated emi in electronic circuits,electromagnetic_interference emi regulations are a very important issue in the design of almost any electronic circuit over the years cutandtry procedures have been adopted by electronic designers to make circuits comply with these regulations mainly due to the lack of reliable theoretical models of radiated noise and clear design rules to gain new insight into this field a novel approach is presented in this paper based on a wellknown technique in the field of antenna_design ie the source reconstruction method srm its application allows a set of equivalent currents to be obtained that behave exactly like the circuit under consideration with regard to radiated noise from these currents magnetic and electric radiated fields can be obtained at any point in space even at 3 or 10 m away from the circuit where the regulations must be met moreover the equivalent currents accurately represent noise sources in the circuit thus permitting the elements responsible for generating radiated noise to be located the aforementioned method would enable designers to reduce the use of anechoicchamber facilities when testing their designs thereby leaving the chamber only for final certification purposes
mimo cooperative_diversity in a transmit_power limited environment,this paper considers a fading relay_channel where the total_transmit_power used is constrained to be equal to that of the standard singlehop channel the relay_channel used operates in what is termed as mimo cooperative_diversity mode where the source transmits to both relay and destination terminals in the first instance both the source and relay then transmit to the destination in the second instance initially the cooperative_diversity framework is introduced to consider system constraints so a direct and fair comparison with the singlehop case can be made inparticular a power constraint is placed on the system and the optimal transmit_power levels are derived and presented the derived technique for finding the optimal transmit_power levels is then used to demonstrate the advantages of using cooperative_diversity in a wireless_network the results presented show that mimo cooperative_diversity offers a 34 db increase in spectral_efficiency at 5  outage with no additional cost incurred in transmit time power or bandwidth
an exact_solution procedure for multiitem twoechelon spare parts inventory control problem with batch ordering in the central warehouse,we consider a multiitem twoechelon inventory system in which the central warehouse operates under a qr policy and the local warehouses implement basestock policy an exact_solution procedure is proposed to find the inventory control policy parameters that minimize the systemwide inventory holding and fixed ordering cost subject to an aggregate mean response time constraint at each facility
utilitybased admission_control for mobile wimax_networks,this paper presents a novel utilitybased connection_admission_control cac scheme for ieee 80216e broadband_wireless_access networks we develop specific utility_functions for realtime and nonrealtime services coupled with a handover_process given these utility_functions we characterize the network utility with respect to the allocated bandwidth and further propose a cac algorithm which admits a connection that conducts to the greatest utility so as to maximize the total resource utilization the simulation results demonstrate the effectiveness of the proposed cac algorithm in terms of network utility
planetaryscale terrain composition,many interrelated planetary height map and surface image map data sets exist and more data are collected each day broad communities of scientists require tools to compose these data interactively and explore them via realtime visualization while related these data sets are often unregistered with one another having different projection resolution format and type we present a gpucentric approach to the realtime composition and display of unregisteredbutrelated planetaryscale data this approach employs a gpgpu process to tessellate spherical height fields it uses a rendertovertexbuffer technique to operate upon polygonal surface meshes in image space allowing geometry processes to be expressed in terms of image_processing with height and surface map data processing unified in this fashion a number of powerful composition operations may be uniformly applied to both examples include adaptation to nonuniform sampling due to projection seamless blending of data of disparate resolution or transformation regardless of boundary and the smooth interpolation of levels of detail in both geometry and imagery issues of scalability and precision are addressed giving outofcore access to gigapixel data sources and correct rendering at scales approaching one meter
continuous signature monitoring lowcost concurrent detection of processor control errors,a lowcost approach to concurrent detection of processor control errors is presented that uses a simple hardware monitor and signatures embedded into the executing program existing signaturemonitoring techniques detect a large portion of processor control errors at a fraction of the cost of duplication analytical methods developed in this study show that the new approach continuous signature monitoring csm makes major advances beyond existing techniques csm reduces the fraction of undetected controlflow errors by orders of magnitude to less than 10sup 6 while the number of signatures reaches a theoretical minimum being lowered by as much as three times to a range of 411 signature cost is reduced by placing csm signatures at locations that minimize performance loss and for some architectures memory overhead csm exploits the program memorys secded code to decrease errordetection latency by as much as 1000 times to 0016 program memory cycles without increasing memory overhead this short latency allows transient_faults to be tolerated 
traffic monitoring techniques for measurement based flow acceptance control,in this paper we describe the development of a measurement based flow acceptance control mechanism that will support guaranteed services in multiservice packet switched networks using simulation models we present simulation experiments with two monitoring techniques one that monitors the percentile occupancy of queue and the other monitors the mean and variance of packet interarrival times and packet lengths of a continuous media packet stream both these techniques are considered in the development of the measurement based flow acceptance mechanism
using fibonacci compression codes as alternatives to dense codes,recent publications advocate the use of various variable length codes for which each codeword consists of an integral number of bytes in compression applications using large alphabets this paper shows that another tradeoff with similar properties can be obtained by fibonacci codes these are fixed codeword sets using binary representations of integers based on fibonacci numbers of order m ges 2 fibonacci codes have been used before and this paper extends previous work presenting several novel features in particular they compress better and are more robust at the price of being slower
interfinger coordination and postural synergies in robot hands via mechanical implementation of principal_components_analysis,human_hands employ characteristic patterns of actuation or synergies that contain much of the information required to describe an entire hand_shape in some cases 80 or more of the total information can be described with only two scalar component values robotic_hands however commonly only couple intrafinger joints and rarely take advantage of this interfinger coordination in this paper realworld data on a variety of human hand postures was collected using a data glove and principal_components_analysis was used to calculate these synergies resulting in what we call eigenpostures a novel mechanism_design is presented to combine the eigenpostures and drive a 17degreeoffreedom 5fingered robot_hand the hand uses only 2 dc motors to accurately recreate a wide range of hand shapes we also present a design improvement that allows us to distinguish between highprecision and lowprecision tasks as well as greatly reduce overall error
developing process mediator for web service interactions,web service interactions lie in the core of soa due to the autonomy heterogeneity and continuous evolution of web services mediators are usually needed to support service interactions to overcome possible mismatches that may exist among business_processes in this paper we introduce a spacebased architecture for process mediator which considers both controlflow and dataflow present possible mismatch patterns and suggest how they can be automatically mediated our work can be used to perform runtime mediation and thus to facilitate service interactions
multipleview object_recognition in bandlimited distributed camera networks,in this paper we study the classical problem of object_recognition in lowpower lowbandwidth distributed camera networks the ability to perform robust object_recognition is crucial for applications such as visual_surveillance to track and identify objects of interest and compensate visual nuisances such as occlusion and pose_variation between multiple camera views we propose an effective framework to perform distributed object_recognition using a network of smart cameras and a computer as the base_station due to the limited bandwidth between the cameras and the computer the method utilizes the available computational_power on the smart_sensors to locally extract and compress sifttype image_features to represent individual camera views in particular we show that between a network of cameras highdimensional sift histograms share a joint sparse pattern corresponding to a set of common features in 3d such joint sparse patterns can be explicitly exploited to accurately encode the distributed signal via random_projection which is unsupervised and independent to the sensor modality on the base_station we study multiple decoding schemes to simultaneously recover the multipleview object features based on the distributed compressive sensing theory the system has been implemented on the berkeley citric smart camera platform the efficacy of the algorithm is validated through extensive simulation and experiments
change_detection for bridges over water in airborne and spaceborne sar_data,the main advantages of sar are the capability of imaging large areas in short time and delivering data at any day time and under nearly all weather conditions this is especially important for disaster management and continuous long term monitoring applications key elements of manmade infrastructure are bridges especially for bridges over water the sar specific side looking imaging geometry can lead to special characteristics in the image in this paper the possibilities of extracting bridge features like width and height from sar_data especially for bridges over water are discussed the feature_extraction is based on the segmentation of parallel lines in an image an approach is presented to exploit this feature_extraction for change_detection the investigations are supported by sar simulations and real airborne and spaceborne data are presented
variationalbased speckle noise_removal of sar_imagery,in this paper we present a variational_method for synthetic_aperture_radar sar speckle removal variational_method is a newly developed technique for the removal of sars multiplicative_noise for an image we could define an energy functional the energy evolves as the original image changes and the minimum energy corresponds to the speckle reduced result partial_differential_equation pde technique is used to get the minimal solution our energy functional makes use of the statistical information of the multiplicative_noise since it follows a gamma law with mean mu  1 and variance sigma 2   1m for mlook sar our energy is a regularization term with two constraints the regularization term is the integral for the norm of image gradient two constraints are the mean of noise should be 1 and the variance of noise should be 1m we use the method of lagrange multipliers eulerlagrange equation and heat flow method to obtain the minimizer of the energy ers precision image pri data are to demonstrate our algorithm numerical result shows that the speckle reduced image preserves edges and point_targets while smoothes homogenous regions in the original image the algorithm is computationally efficient and easy to implement
clusterbased distributed consensus,in this paper we incorporate clustering_techniques into distributed consensus algorithms for faster convergence and better energy efficiency together with a simple distributed clustering_algorithm we design clusterbased distributed consensus algorithms in forms of both fixed linear iteration and randomized gossip the time complexity of the proposed algorithms is presented in terms of metrics of the original and induced graphs through which the advantage of clustering is revealed our clusterbased algorithms are also shown to achieve an omegalog n gain in message_complexity over the standard ones
xisuml profile for extreme modeling interactive systems,the first version of the xis profile addressed the development of interactive systems by defining models oriented only towards how the system should perform tasks however issues such as userinterface layouts or the capture of interaction patterns were not addressed by the profile but only by the sourcecode generation process this originated systems that although functional were considered by endusers as difficult to use in this paper we present the second version of the xis uml_profile which is now a crucial component of the projectit research project this profile follows the separation_of_concerns principle by proposing an integrated set of views that address the various issues detected with the previous version of xis in addition this profile also promotes the usage of extreme modeling by relying on the extensive use of modeltomodel transformation templates that are defined to accelerate the model development tasks
forward acknowledgement refining tcp_congestion_control,we have developed a forward acknowledgment fack congestion_control_algorithm which addresses many of the performance problems recently observed in the internet the fack algorithm is based on first principles of congestion_control and is designed to be used with the proposed tcp sack option by decoupling congestion_control from other algorithms such as data recovery it attains more precise control over the data flow in the network we introduce two additional algorithms to improve the behavior in specific situations through simulations we compare fack to both reno and reno with sack finally we consider the potential performance and impact of fack in the internet
inservice signal quality estimation for tdma cellular_systems,inservice interference plus noise_power in and signaltointerference plus noise_power siin estimation methods are examined for tdma cellular_systems a simple in estimator is developed whose accuracy depends on the channel and symbol estimate error statistics improved in and sin estimators are developed whose accuracy depends only on the symbol error statistics the proposed estimators are evaluated through software simulation with an is54 frame_structure for high speed mobiles it is demonstrated that sin can be estimated to within 2 db in less than a second
embedding agents within the intruder to detect parallel attacks,we carry forward the work described in our previous papers 51820 on the application of data independence to the model_checking of security_protocols using csp 19 and fdr 10 in particular we showed how techniques based on data independence 1219 could be used to justify by means of a finite fdr check systems where agents can perform an unbounded number of protocol runs whilst this allows for a more complete analysis there was one significant incompleteness in the results we obtained while each individual identity could perform an unlimited number of protocol runs sequentially the degree of parallelism remained bounded and small to avoid state_space explosion in this paper we report significant progress towards the solution of this problem by means anticipated in 5 namely by internalising protocol roles within the intruder process the internalisation of protocol roles initially only servertype roles was introduced in 20 as a statespace reduction technique for which it is usually spectacularly successful it was quickly noticed that this had the beneficial sideeffect of making the internalised server arbitrarily parallel at least in cases where it did not generate any new values of data independent type we now consider the case where internal roles do introduce fresh values and address the issue of capturing their state of mind for the purposes of analysis
can the web be made accessible for people with intellectual disabilities,this article presents the findings of a research project that aimed to contribute to the social inclusion of people with intellectual disabilities id in the world_wide_web the web the inclusive new media design inmd project brought together thirtyone web designers and developers with twentynine people with intellectual disabilities to explore the best practice for building web sites accessible to the id community specifically the project took accessibility techniques identified in id accessibility research and investigated what would or would not make it possible for web professionals to implement them this article suggests some tentative answers to the question of whether a fully accessible web can be built one that includes people with id while the article outlines simple steps that can be taken to facilitate accessibility for people at the mild end of the id spectrum it also highlights a number of barriers that exist to implementing id accessibility guidance most notably the power holders and decision makers with whom web designers work who may not share the designers commitment to accessibility
a parallel_implementation of a fast multipole based 3d capacitance extraction program on distributed_memory multicomputers,very fast and accurate 3d capacitance extraction is essential for interconnect optimization in ultra deep submicro designs udsm parallel_processing provides an approach to reducing the simulation turnaround time this paper examines the parallelization of the well known fast multipole based 3d capacitance extraction program fastcap which employs new preconditioning and adaptive techniques to account for the complicated data dependencies in the unstructured problems we propose a generalized cost function model which can be used to accurately measure the workload associated with each cube in the hierarchy we then present two adaptive partitioning schemes combined with efficient communication mechanisms with bounded buffer size to reduce the parallel_processing overhead the overall load balance is achieved through balancing the load at each level of the multipole computation we report detailed performance results using a variety of standard benchmarks on 3d capacitance extraction on an ibm sp2
camera parameters autoadjusting technique for robust robot vision,how to make vision_system work robustly under dynamic light conditions is still a challenging research focus in computerrobot vision community in this paper a novel camera parameters autoadjusting technique based on image entropy is proposed firstly image entropy is defined and its relationship with camera parameters is verified by experiments then how to optimize the camera parameters based on image entropy is proposed to make robot vision adaptive to the different light conditions the algorithm is tested by using the omnidirectional vision in indoor robocup middle size league environment and the perspective camera in outdoor ordinary environment and the results show that the method is effective and color_constancy to some extent can be achieved
new but not improved  a critical examination of revisions to the regulation of investigatory powers act 2000 encryption provisions,considering the criminal uses of encryption it has been asserted that national security and law enforcement endeavours must not be frustrated by potential evidence being hidden through digital encryption while the encryption_key is withheld the facilitation of state access to encryption keys through the regulation of investigatory powers act 2000 ripa 2000 was intended to address precisely such a danger however since this statutes enactment there have been significant shifts in law and policy relating to terrorism and child pornography as well as important technological developments this article critically examines changes to the ripa 2000 encryption provisions made by the terrorism act 2006 and the policing and crime act 2009 it also considers emerging statistical data and cases including r v sf 2008 it concludes that the underlying premises of the revised provisions are flawed the provisions themselves ineffectual in practice and in the longer term potentially open to mission creep to cover lesser offences
the metadesk models and prototypes for tangible_user_interfaces,the metadesk is a user_interface platform demonstrating new interaction_techniques we call tangible user inter faces we explore the physical instantiation of interface elements from the graphical_user_interface paradigm giving physical form to windows icons handles menus and controls the design and implementation of the metadesk display sensor and software_architectures is discussed a prototype application driving an interaction with geographi cal space tangible geospace is presented to demonstrate these concepts
a motivation for ubuntu to enhance elearning social_network_services in south africa,this paper acknowledges the move from the contentcentric to the networkcentric approach to teaching_and_learning using social_network_services snss in higher education the south african cultural context is explained with a view to reconciling this with current snss finally a way forward is proposed for developing snss particular to the southern african cultural context
recent experiences utilizing terrasarx for the monitoring of natural disasters in different parts of the world,pasco in approximately 4 years after the launch of terrasarx tsx has successfully carried out a total 22 studies of disaster response for the worldwide and domestic cases the outline and a part of the conducted cases are explained in detailed in this paper
ther an authorization management architecture for ubiquitous_computing ,the ubiquitous_computing paradigm suggests that we are going to be surrounded by countless wireless devices capable of providing services trans parently by definition the nature of ubiquitous computing_environments is open and extremely dynamic making difficult the establishment of predefined security relationships between all of the participating entities authentication_mechanisms can be employed to establish the identity of a pervasive_computing entity but they suffer from scalability problems and have limited value in defin ing authorization decisions among strangers in this paper we propose aether an authorization management architecture designed specifically to address trust establishment and access_control in ubiquitous computing_environments own ers define attribute authority sets and access_control_policy entries that are em bedded into their devices members of the attribute authority sets are trusted to issue credentials for the corresponding attributes that can then be used in order to gain access to protected resources our architecture supports dynamic mem bership in these sets facilitating distributed administration which is required in the context of the volatile nature of ubiquitous security relationships and at tribute mapping to allow roaming among authority domains moreover we pre sent the foundation of a logic model for our proposed architecture that is used to prove access_control_decisions
utilizing semantic caching in ubiquitous_environment,semantic caching is a dynamic caching strategy which deals with not only exact but also inexact similar queries in this manner each query will be carefully analyzed by the cache manager to identify the part that can be found in the cache from the part that needs to be retrieved from the server this trimming process not only speeds up information_retrieval but also saves on communication_cost especially for mobile and wireless devices therefore query trimming is a key problem in mobile and wireless environment and devices in this environment have limited connection time bandwidth and battery power however the existing methods for query trimming have a number of limitations such as inefficiency in time space and the complexity of the algorithm used for trimming these factors restrict the applicability of semantic caching for many applications in this paper we investigate the shortcomings of query trimming process and propose a new solution to improve this process
a study on using hierarchical basis error_estimates in anisotropic mesh adaptation for the finite_element_method,a common approach for generating an anisotropic mesh is the muniform mesh approach where an adaptive mesh is generated as a uniform one in the metric specified by a given tensor m a key component is the determination of an appropriate metric which is often based on some type of hessian recovery recently the use of a global hierarchical basis error estimator was proposed for the development of an anisotropic metric tensor for the adaptive finite element solution this study discusses the use of this method for a selection of different applications numerical_results show that the method performs well and is comparable with existing metric tensors based on hessian recovery also it can provide even better adaptation to the solution if applied to problems with gradient jumps and steep boundary layers for the poisson problem in a domain with a corner singularity the new method provides meshes that are fully comparable to the theoretically optimal meshes
believable judge bot that learns to select tactics and judge opponents,this paper describes our believable judge bot icecig2011 that has an ability to learn tactics from a judge player and an ability to judge an opponent character as a human or a bot we conjecture that a bot with these two abilities should be considered humanlike in a competition environment such as botprize where human players participate to compete not only for being the most humanlike player but also the best judge main contributions of this work lie in our mechanisms for achieving these two abilities to achieve the former ability we develop a system and gui that allow a selected judge player  whose role is to train icecig2011  to control his or her character by only deciding which tactic to use under a given situation we then obtain the judges tactic log and use it for training tactic selection of icecig2011 with neuro evolution of augmenting topologies to achieve the latter ability we acquire additional logs when the judge character interacts with other opponent characters in order to represent the play of a known bot or human character we train a neural gas  a kind of selforganizing neural_network  from its log for an unknown character once its neural gas is trained after a certain period of observation icecig2011 decides if it is a human or bot by using the ifnearestneighbor algorithm this algorithm considers the majority in the labels of the ifnearest neural gases of known characters to the neural gas of that unknown character experimental results are given and discussed concerning these two abilities of icecig2011
typebased parametric analysis of program families,previous research on static_analysis for program families has focused on lifting analyses for single plain programs to program families by employing idiosyncratic representations the lifting effort typically involves a significant amount of work for proving the correctness of the lifted algorithm and demonstrating its scalability in this paper we propose a parameterized static_analysis framework for program families that can automatically lift a class of typebased static analyses for plain programs to program families the framework consists of a parametric logical specification and a parametric variational constraint solver we prove that a lifted algorithm is correct provided that the underlying analysis algorithm is correct an evaluation of our framework has revealed an error in a previous manually lifted analysis moreover performance tests indicate that the overhead incurred by the general framework is bounded by a factor of 2
polar write once memory codes,a coding_scheme for write once memory wom using polar_codes is presented it is shown that the scheme achieves the capacity_region of noiseless woms when an arbitrary number of multiple writes is permitted the encoding and decoding complexities scale as on log n where n is the blocklength for n sufficiently large the error probability decreases subexponentially in n some simulation results with finite length codes are presented
tolerance towards sensor failures an application to a double inverted pendulum,in this paper we present a sensor fault_tolerance control scheme that is applied to a double inverted pendulum sensor faults will affect the system when it is used in closedloop feedback the scheme uses a linear observer reconstruct the sensor fault and to subtract the reconstruction from the faulty sensor the net result is then used for the closedloop feedback it was found that the scheme restored the performance to the faultfree scenario
on the expressiveness of coordination models,a number of different coordination models for specifying interprocess communication and synchronisation rely on a notion of shared dataspace many of these models are extensions of the linda coordination model which includes operations for adding deleting and testing the presenceabsence of data in a shared dataspacernrnwe compare the expressive power of three classes of coordination models based on shared dataspaces the first class relies on lindas communication primitives while a second class relies on the more general notion of multiset rewriting eg like bauhaus linda or gamma finally we consider a third class of models featuring communication transactions that consist of sequences of lindalike operations to be executed atomically eg like in shared prolog or polis
manifold alignment for multitemporal hyperspectral_image_classification,while spectral and temporal advantages of multitemporal hyperspectral_images provide opportunities for advancing classification of time varying phenomena significant challenges are associated with high_dimensionality and nonstationary signatures while manifold_learning retains critical geometry and develops a low dimension space where class clusters are recovered spectral changes in temporal imagery impact the fidelity of the geometric representation of class dependent data in this paper we investigate a manifold alignment framework that exploits prior_information while exploring similar local structures the aim is to make use of common underlying geometries of two multitemporal images and embed the resemblances in a joint data manifold for classification_tasks promising results support the advantages of the proposed manifold alignment approach
incremental rule learning based on example nearness from numerical data_streams,mining data_streams is a challenging task that requires online_systems based on incremental learning approaches this paper describes a classification_system based on decision_rules that may store uptodate border examples to avoid unnecessary revisions when virtual drifts are present in data consistent rules classify new test examples by covering and inconsistent rules classify them by distance as the nearest_neighbor algorithm in addition the system provides an implicit forgetting heuristic so that positive and negative examples are removed from a rule when they are not near one another
elements of prescribed order prescribed traces and systems of rational functions over finite_fields,let k  1 and f1   fr  fqk x be a system of rational functions forming a strongly linearly independent set over a finite_field fq let 1  r  fq be arbitrarily prescribed elements we prove that for all sufficiently large extensions fqkm there is an element   fqkm of prescribed order such that trfqkmfq fi  i for i  1  r where trfqkmfq is the relative trace map from fqkm onto fq we give some applications to bch codes finite_field_arithmetic and ordered orthogonal arrays we also solve a question of helleseth et al hypercubic 4 and 5designs from doubleerrorcorrecting codes des codes cryptgr 282003 pp 265282 completely
the experimental evaluation of knowledge_acquisition techniques and methods history problems and new directions,the special problems of experimentally evaluating knowledge_acquisition and knowledge_engineering tools techniques and methods are outlined and illustrated in detail with reference to two series of studies the first is a series of experiments undertaken at nottingham university under the aegis of the uk alvey initiative and the esprit project acknowledge the second is the series of sisyphus benchmark studies a suggested programme of experimental evaluation is outlined which is informed by the problems with using sisyphus for evaluation
a domainspecific modeling_language for scientific_data composition and interoperability,domainspecific modeling_languages dsmls can offer assistance to domain_experts who may not be computer scientists by providing notations and semantic constructs that align with abstractions from a particular domain in this paper we describe our design and application of a dsml in the area of data composition and interoperability in particular we introduce our recent effort to design a dsml to assist with interoperability issues across scientific software applications eg composing scientific_data in different file structures and integrating scientific_data with data_gathering devices currently several different scientific_data file specifications have been proposed eg cid netcdf and hdf each file specification is optimized to manage a specific data type efficiently thus each file specification has evolved with slightly different notions and implementation technologies these differences led to the need for an environment that provides interoperability among the different specification formats in this paper we introduce our framework supported by a dsml that provides functionality to visually model the data composition and integration concepts independent from a particular data file specification
migrating autonomic selftesting to the cloud,the cloud_computing model continues to gain much attention from software_industry practitioners as such leading companies are investing in the development packaging and delivery of cloud_services over the internet however although much work is being done to model and build cloud applications and services there is significantly less research devoted to testing them in this paper we describe our researchinprogress towards migrating autonomic selftesting ast to the cloud our approach combines the development of an automated test harness for a cloud_service with the delivery of test support asaservice tsaas both ast and tsaas are supported by a virtual test environment which utilizes the power of the cloud to enhance the selftesting process
a cmos fifthorder lowpass currentmode filter using a linear transconductor,in this paper the design and analysis of a cmos fifthorder lowpass gmc filter are presented it has a cutoff frequency of 43 mhz to accommodate the wideband cdma standard the transconductor used in this filter is based on a fourtransistor cell operating in triode or saturation mode it achieves high linearity range of spl plusmn 1 v at spl plusmn 15 v supply voltages pspice simulations show that total harmonic distortion at 1 vpp and 1 mhz is equal to 01 with 1234 mw standby power dissipation the proposed filter and the transconductor are simulated using 035 spl mum technology
inverse_problems theory and application analysis of the twotemperature method for landsurface temperature and emissivity estimation,the twotemperature method ttm allows the separation of landsurface temperature and landsurface emissivity information from radiance measurements and therefore the solution can be uniquely determined by the data however the inverse_problem is still an illposed problem since the solution does not depend continuously on the data accordingly we have used some mathematical tools which are suited for analyses of illposed problems in order to show ttm properties evaluate it and optimize its estimations related to this last point we have shown that it is necessary to constrain the problem either by defining a region of physically admissible solutions andor by using regularization_methods in order to obtain stable results besides the results may be improved by using ttm with systems that possess a high temporal resolution as well as by acquiring observations near the maximum and minimum of the diurnal temperature range
operating rules classification_system of water supply reservoir based on lcs,genetic algorithmbased learning classifier system lcs is a massively parallel messagepassing and rulebased machine_learning system but its potential selfadaptive learning capability has not been paid enough attention in reservoir operation research in this paper an operating rule classification_system based on lcs  which learns through credit assignment the bucket brigade algorithm and rule discovery the genetic_algorithm is established to extract watersupply reservoir operating rules the proposed system acquires the online identification rate 95 for training_samples and offline rate 85 for testing samples in a case study and further discussions are made about the impacts on the performances or behaviors of the rule classification_system from three aspects of obtained rules training or testing samples and the comparisons between the rule classification_system and the artificial_neural_network ann the results indicate the learning classifier system is feasible and effective for the system to obtain the reservoir supply operating rules
assembling 2d blocks into 3d chips,threedimensional ics promise to significantly extend the scale of system integration and facilitate newgeneration electronics however progress in commercial 3d ics has been slow in addition to technologyrelated difficulties industry experts cite the lack of a commercial 3d eda toolchain and design standards high risk associated with a new technology and high cost of transition from 2d to 3d ics to streamline the transition we explore design styles that reuse existing 2d intellectual property ip blocks in 3d ics currently these design styles severely limit the placement of throughsilicon vias tsvs and constrain the reuse of existing 2d ip blocks in 3d ics to overcome this problem we develop a methodology for using tsv islands and novel techniques for clustering nets to connect 2d ip blocks through tsv islands our empirical validation demonstrates 3d integration of traditional 2d circuit blocks without modifying their layout for this context
software_engineering issues for smallscale parallelism,the availability of lowcost commodity multiprocessor machines change the nature of mainstream programming this discipline is required to include smallscale dual and quadruple processor machines to remain competitive these smallscale parallel systems require software_engineering principles capable of encapsulating the complex parallel_programming issues this paper discusses a technique that provides a simple model for incorporating parallel_programming in a scheduler this model can dynamically adjust to single and smallscale multiple processor environments
pilotless frame synchronization for ldpccoded transmission systems,we present a pilotless frame synchronization approach that exploits feedback from a lowdensity paritycheck ldpc_code decoder the synchronizer is based on syndrome checks using hard_decisions from the channel observations the bandwidth overhead associated with pilot_symbols in conventional receiver architectures is eliminated while providing sufficient synchronization performance an ldpc_decoder coupled with our synchronizer exhibits negligible frame error rate degradation over a system with perfect synchronization the complexity of the frame synchronizer is kept relatively low due to its xorbased approach
thirdoctave analysis of multichannel amplitude compressed speech,multiband amplitude compression has been studied as a compensation for the reduced auditory dynamic range often associated with sensorineural hearing loss since it is capable of altering the dynamic range of speech as a function of frequency compression systems are generally characterized by their response to steady state tones and to simple dynamic stimuli such as tone bursts level distributions of unprocessed and compressed materials are presented to demonstrate that these descriptions are inadequate to predict the processing of speech a simple analysis of multiband compression which incorporates the interactions of static and dynamic properties is presented
depth image postprocessing method by diffusion,threedimensional 3d movies in theaters have become a massive commercial success during recent years and it is likely that with the advancement of display_technologies and the production of 3d contents tv broadcasting in 3d will play an important role in home entertainments in the not too distant future 3d video_contents contain at least two views from different perspectives for the left and the right eye of viewers the amount of coded information is doubled if these views are encoded separately moreover for multiview displays ie different perspectives of a scene in 3d are presented to the viewer at the same time through different angles either video_streams of all the required views must be transmitted to the receiver or the displays must synthesize the missing views with a subset of the views the latter approach has been widely proposed to reduce the amount of data being transmitted the virtual views can be synthesized by the depth image based rendering dibr approach from textures and associated depth images however it is still the case that the amount of information for the textures plus the depths presents a significant challenge for the network transmission_capacity an efficient compression will therefore increase the availability of content access and provide a better video_quality under the same network capacity constraintsin this thesis the compression of depth images is addressed these depth images can be assumed as being piecewise smooth starting from the properties of depth images a novel depth image model based on edges and sparse samples is presented which may also be utilized for depth image postprocessing based on this model a depth image coding_scheme that explicitly encodes the locations of depth edges is proposed and the coding_scheme has a scalable structure furthermore a compression_scheme for blockbased 3dhevc is also devised in which diffusion is used for intra_prediction in addition to the proposed schemes the thesis illustrates several evaluation methodologies especially the subjective test of the stimuluscomparison method it is suitable for evaluating the quality of two impaired images as the objective metrics are inaccurate with respect to synthesized viewsthe mpeg test sequences were used for the evaluation the results showed that virtual views synthesized from postprocessed depth images by using the proposed model are better than those synthesized from original depth images more importantly the proposed coding_schemes using such a model produced better synthesized views than the state of the art schemes as a result the outcome of the thesis can lead to a better quality of 3dtv experience
memory in the small an application to provide taskbased organizational_memory for a scientific community,many forms of organizational_memory must exist embedded within the organizational processes and tasks this paper argues that memoryinthe small memory utilized in the performance of an organizational task can serve as an effective performance support mechanism by basing organizational_memory upon organizational tasks and basing task support upon organizational_memory organizational_memory systems can provide additional and necessary support services for organizations and communities as an example of memoryinthesmall this paper describes a software application called the assist that combines organizational_memory with task performance for a scientific community the assist utilizes and stores the collective memory of astrophysicists about data analysis and is used worldwide by astrophysicists the paper also considers the theoretical and architectural issues involved when combining organizational_memory with task performance 
cylofold secondary structure prediction including pseudoknots,computational rna secondary structure prediction approaches differ by the way rna pseudoknot interactions are handled for reasons of computational_efficiency most approaches only allow a limited class of pseudoknot interactions or are not considering them at all here we present a computational method for rna secondary structure prediction that is not restricted in terms of pseudoknot complexity the approach is based on simulating a folding process in a coarsegrained manner by choosing helices based on established energy rules the steric feasibility of the chosen set of helices is checked during the folding process using a highly coarsegrained 3d_model of the rna structures using two data sets of 26 and 241 rna sequences we find that this approach is competitive compared to the existing rna secondary structure prediction programs pknotsrg hotknots and unafold the key advantages of the new method are that there is no algorithmic restriction in terms of pseudoknot complexity and a test is made for steric feasibility availability the program is available as web_server at the site httpcylofoldabccncifcrfgov
topological analysis of the power grid and mitigation strategies against cascading_failures,this paper presents a complex systems overview of a power grid network in recent years concerns about the robustness of the power grid have grown because of several cascading outages in different parts of the world in this paper cascading effect has been simulated on three different networks the ieee 300 bus test system the ieee 118 bus test system and the wscc 179 bus equivalent model power degradation has been discussed as a measure to estimate the damage to the network in terms of load loss and node loss a network generator has been developed to generate graphs with characteristics similar to the ieee standard networks and the generated graphs are then compared with the standard networks to show the effect of topology in determining the robustness of a power grid three mitigation strategies homogeneous load reduction targeted rangebased load reduction and use of distributed renewable sources in combination with islanding have been suggested the homogeneous load reduction is the simplest to implement but the targeted rangebased load reduction is the most effective strategy
development of architecture and software technologies in highperformance lowpower soc design,for the success of an soc design a design platform and some key design technologies are needed for this purpose a research project in technology development program for academia was granted recently in taiwan to develop the following technologies toward a highperformance lowpower soc design platform first a soft intellectual property soft ip and related rtos compiler and integrated design environment ide software of an advanced taiwan vltw dsp_processor core which can be used as the star ip of an soc design platform will be developed second lowpower and lowvoltage digital and mixedsignal circuit design technologies will be developed based on the advanced mtcmos process third some key multimedia and communication soft or hard ips will be developed the developed advanced key technologies can be used as the technology driver to facilitate the design of socbased products and they will also help to enhance the design capability of the taiwan soc industry in this paper we introduce the research project and focus on architecture and software technologies developing in one of the subitems
a complete axiomatization for branching bisimulation congruence of finitestate behaviours,
using information_systems to structurally map workplace injury,previous authors have identified mechanical human and work organisation factors that may contribute to accident occurrences and have noted the value of sitespecific data in accident analysis however major criticisms of site specific safety information_systems have focused on the difficulties using traditional approaches for the identification of critical paths involving processes related to injury and noninjury incidents this paper presents an idiographic approach to the study of accidents and through contextual analysis develops maps of work processes related to injury incidents in mining the information used in the contextual maps includes data related to work area activity and work role being performed and equipment being used at the time of the injury incident the computer algorithm consists of a series of contextual conditionals where the incidents and lost days recorded in the final category of each pathway meet all of the conditionals of the previous categories the order of selection indicates the criticality of the path the analysis resolves the processes related to the incident into their constitutive components and then redescribes these processes as paths in order to reveal associations these contextual paths illustrate the processes which are part of an established pattern of recurring regularities associated with injury incidents
a pramnuma model of computation for addressing lowtlp workloads,it is possible to implement the parallel random access machine pram on a chip_multiprocessor cmp efficiently with an emulated shared_memory esm architecture to gain easy parallel programmability crucial to wider penetration of cmps to general_purpose computing this implementation relies on exploitation of the slack of parallel_applications to hide the latency of the memory system instead of caches sufficient bisection bandwidth to guarantee high throughput and hashing to avoid hot spots in intercommunication unfortunately this solution can not handle workloads with low threadlevel parallelism tlp efficiently because then there is not enough parallel slackness available for hiding the latency in this paper we show that integrating nonuniform memory_access numa support to the pram implementation architecture can solve this problem the obtained pramnuma hybrid model is described and architectural implementation of it is outlined on our eclipse esm cmp framework
davim adaptable middleware for sensor_networks,middleware services facilitate sensornetwork application development davim is adaptable middleware that enables dynamic management of services and isolation between simultaneously running applications
a survey on optimization metaheuristics,metaheuristics are widely recognized as efficient approaches for many hard optimization_problems this paper provides a survey of some of the main metaheuristics it outlines the components and concepts that are used in various metaheuristics in order to analyze their similarities and differences the classification adopted in this paper differentiates between single solution based metaheuristics and population based metaheuristics the literature survey is accompanied by the presentation of references for further details including applications recent trends are also briefly discussed
query_processing in incomplete logical databases,
mining formal_concepts with a bounded number of exceptions from transactional data,we are designing new data_mining_techniques on boolean contexts to identify a priori interesting bisets ie sets of objects or transactions associated to sets of attributes or items a typical important case concerns formal concept mining ie maximal rectangles of true values or associated closed sets by means of the socalled galois_connection it has been applied with some success to eg gene expression data analysis where objects denote biological situations and attributes denote gene expression properties however in such reallife application domains it turns out that the galois association is a too strong one when considering intrinsically noisy data it is clear that strong associations that would however accept a bounded number of exceptions would be extremely useful we study the new pattern domain of  concepts ie consistent maximal bisets with less than  false values per row and less than  false values per column we provide a complete algorithm that computes all the  concepts based on the generation of concept unions pruned thanks to antimonotonic constraints an experimental validation on synthetic_data is given it illustrates that more relevant associations can be discovered in noisy data we also discuss a practical application in molecular biology that illustrates an incomplete but quite useful extraction when all the concepts that are needed beforehand can not be discovered
epitoolkita web_server for computational immunomics,predicting the tcellmediated immune response is an important task in vaccine design and thus one of the key problems in computational immunomics various methods have been developed during the last decade and are available online we present epitoolkit a web_server that has been specifically designed to offer a problemsolving environment for computational immunomics epitoolkit offers a variety of different prediction methods for major histocompatibility complex class i and ii ligands as well as minor histocompatibility antigens these predictions are embedded in a userfriendly interface allowing refining editing and constraining the searches conveniently we illustrate the value of the approach with a set of novel tumorassociated peptides epitoolkit is available online at www epitoolkitorg
a novel breath analysis system based on electronic olfaction,certain gases in the breath are known to be indicators of the presence of diseases and clinical conditions these gases have been identified as biomarkers using equipments such as gas chromatography and electronic nose enose gc is very accurate but is expensive time consuming and nonportable enose has the advantages of low cost and easy operation but is not particular for analyzing breath odor and hence has a limited application in diseases diagnosis this paper proposes a novel system that is special for breath analysis we selected chemical sensors that are sensitive to the biomarkers and compositions in human breath developed the system and introduced the odor signal preprocessing and classification method to evaluate the system performance we captured breath samples from healthy persons and patients known to be afflicted with diabetes renal disease and airway inflammation respectively and conducted experiments on medical treatment evaluation and disease identification the results show that the system is not only able to distinguish between breath samples from subjects suffering from various diseases or conditions diabetes renal disease and airway inflammation and breath samples from healthy subjects but in the case of renal failure is also helpful in evaluating the efficacy of hemodialysis treatment for renal failure
improving business_processes with business_process_modelling notation and business_process_execution_language an action research approach,the first purpose of this paper is to improve business_processes in an industrial firm with business_process_modelling notation bpmn and business_process_execution_language bpel the second purpose is to provide a scientific approach for bringing business_analysts and it professionals together in a framework of an action research ar the research is conducted in one of the biggest distribution companies in iran bpmn is applied to model asis and tobe situations of sale and distribution process both asis and tobe models were simulated to compare their performance indexes moreover models were implemented using bpel so that the model can be used for automating and improving the process in this study ar methodology was used to find a resolution of an organisational issue with those who experience this issue directly and to improve scientific knowledge and reallife contexts simultaneously
virtual marionettes a system and paradigm for realtime 3d animation,this paper describes a computer graphics system that enables users to define virtual marionette puppets operate them using relatively simple hardware input devices and display the scene from a given viewpoint on the computer screen this computerized marionette theater has the potential to become a computer_game for children an interaction tool over the internet enabling the creation of simultaneously viewed and operated marionette show by users on the world_wide_web and most importantly a versatile and efficient professional animation system
will this car change the lane  turn signal recognition in the frequency_domain,
a call_admission_control algorithm based on utility fairness for low earth orbit satellite_networks,a fair and adaptive call_admission_control algorithm for multimedia low earth orbit leo_satellite_networks was proposed based on current call_dropping_probability of destination cell this algorithm reserves bandwidth for handoff calls using double_threshold method to avoid the discrimination of quality_of_service qos caused by the allocation based on fair bandwidth this algorithm adopts the bandwidth_allocation rule based on fair qos simulation results show that the proposed algorithm can accurately and adaptively reserve bandwidth present satisfactory call_blocking_probability and greatly reduce handoff call_dropping_probability while guarantees the high_bandwidth utilization
cotraining for domain adaptation,domain adaptation algorithms seek to generalize a model trained in a source domain to a new target domain in many practical cases the source and target distributions can differ substantially and in some cases crucial target features may not have support in the source domain in this paper we introduce an algorithm that bridges the gap between source and target domains by slowly adding to the training_set both the target features and instances in which the current algorithm is the most confident our algorithm is a variant of cotraining 7 and we name it coda cotraining for domain adaptation unlike the original cotraining work we do not assume a particular feature split instead for each iteration of cotraining we formulate a single optimization problem which simultaneously learns a target predictor a split of the feature_space into views and a subset of source and target features to include in the predictor coda significantly outperforms the stateoftheart on the 12domain benchmark data set of blitzer et al 4 indeed over a wide range 65 of 84 comparisons of target supervision coda achieves the best performance
multiagent resource allocation algorithm based on the xsufferage heuristic for distributed_systems,distributed_computing_systems provide a highly dynamic behavior which originates from heterogeneous computing and storage_resources heterogeneous users and the variety of submitted applications and finally from the heterogeneous communication that takes part among the systems entities as such applying global optima oriented allocation algorithms usually produces poor results and heuristics are used instead we concentrated our experiments around the sufferage heuristic and its adaptive clusteraware version xsufferage both sufferage and xsufferage use a centralized design and produce good results for low levels of dynamism and deterministic environments in real life distributed_environments both heuristics produce poor results we expose the sufferage heuristic through a distributed_architecture based on a cooperative set of entities which form a multiagent_system such that the results could be improved we implemented a new algorithm based on this architecture called distributed xsufferage in order to test the new algorithm a series of experiments were developed by simulating two real life grid_environments a complex set of performance metrics were collected  flow time make span throughput  both resource and cluster level utilization  both resource and cluster level and resources and clusters mean loads algorithms produce their allocation solution based on estimates and modeling of systems resources and as such are sensitive to estimation errors throughout our experiments dx sufferage was more robust to such errors compared to the original sufferage and respectively xsufferage heuristics
hierarchical a parsing with bridge outside scores,hierarchical a ha uses of a hierarchy of coarse grammars to speed up parsing without sacrificing optimality ha prioritizes search in refined grammars using viterbi outside costs computed in coarser grammars we present bridge hierarchical a bha a modified hierarchial a algorithm which computes a novel outside cost called a bridge outside cost these bridge costs mix finer outside scores with coarser inside scores and thus constitute tighter heuristics than entirely coarse scores we show that bha substantially outperforms ha when the hierarchy contains only very coarse grammars while achieving comparable performance on more refined hierarchies
strings of congruent primes in short intervals,fix  0 and let p1 2 p 2 3  be the sequence of all primes we prove that if q a 1  then there are infinitely many pairs pr p r1 such that pr  pr1  a mod q and pr1  pr  log pr    
leveraging speculative architectures for runtime program validation,program_execution can be tampered with by malicious attackers through exploiting software_vulnerabilities changing the program behavior by compromising control data and decision data has become the most serious threat in computer_system security although several hardware approaches have been presented to validate program_execution they either incur great hardware overhead or introduce false alarms we propose a new hardwarebased approach by leveraging the existing speculative architectures for runtime program validation the onchip branch target buffer btb is utilized as a cache of the legitimate control flow transfers stored in a secure memory region in addition the btb is extended to store the correct program path information at each indirect branch site the btb is used to validate the decision history of previous conditional branches and monitor the following execution path at runtime implementation of this approach is transparent to the upper operating_system and programs thus it is applicable to legacy_code because of good code locality of the executable programs and effectiveness of branch prediction the frequency of controlflow validations against the secure offchip memory is low our experimental results show a negligible performance penalty and small storage_overhead
a proposal of cell selection algorithm for lte handover optimization,a large number of cells will be deployed to provide high speed services in any places using the longterm evolution lte system the management of such a large number of cells increases the operating expenditure opex selforganizing networks son attracted interest as an effective way to reduce opex one of the main targets in son is the selfoptimization of handover ho that realizes mobility robustness ho optimization_algorithms adjust ho parameters between the serving and the reconnected cells based on the ho failure logs and cell selection which is the procedure used to select a suitable reconnected cell is very important for ho optimization_algorithms in this paper we propose a cell selection_scheme to enhance the performance of ho optimization in the proposed scheme both the uplink and downlink channel quality is considered when selecting a suitable reconnected cell through the computer simulation we can see that the proposed scheme reduces the ho failure rate and the number of ho failures by 3 percentage points and 38 respectively compared to the conventional scheme based solely on downlink channel quality
clustering large optical_networks for distributed and dynamic multicast,for a largescale mesh_network with dynamic_traffic maintaining the global state information in a centralized fashion is impractical hence distributed schemes are needed to organize nodes and to manage state information in a more localized manner one such effective scheme for organizing nodes is to cluster the nodes into a hierarchical_structure in this paper we address the problem of determining the appropriate clustering of nodes for providing scalability in wavelength division multiplexed wdm_optical_networks with dynamic_traffic we present an online loadbased or bandwidth availability based clustering technique that determines clusters adaptively in response to current network conditions we also consider the problem of dynamic multicast on clustered networks with wavelength_conversion capability we introduce a heuristic using an auxiliary graph model to address routing wavelength_assignment and traffic_grooming jointly simulation results demonstrate the feasibility of our approach
detecting patterns of change using enhanced parallel_coordinates visualization,analyzing data to find trends correlations and stable patterns is an important problem for many industrial applications we propose a new technique based on parallel_coordinates visualization previous work on parallel_coordinates method has shown that they are effective only when variables that are correlated andor show similar patterns are displayed adjacently although current parallel_coordinates tools allow the user to manually rearrange the order of variables this process is very timeconsuming when the number of variables is large automated assistance is needed we propose an editdistance based technique to rearrange variables so that interesting patterns can be easily detected our system vminer includes both automated methods for visualizing common patterns and a query tool that enables the user to describe specific target patterns to be mineddisplayed by the system following an overview of the system a case study is presented to explain how motorola engineers have used vminer to identify significant patterns in their product test and design data
phase_diversity for an antennaarray system with a short interelement separation,this paper presents a simple procedure of obtaining a diversity_gain in an antennaarray system with a short interelement separation typically less than the carrier wavelength the new technique provides a diversity_gain through a noncoherent combination of received_signals at each antenna_element the diversity_gain arises because as the number of signal components of the received signal at each antenna_element becomes large enough and as the arrival angle of each signal component is distinct from one another which is a general signal circumstance in most practical code_division_multiple_access cdma signal environments the amplitudes of the received_signals become nearly independent due to the phase difference among the received_signals the diversity_gain was referred to as ldquophase diversityrdquo in this paper the proposed technique is first theoretically analyzed to estimate the performance in terms of pseudorandomnoisecode acquisition which is verified through extensive computer simulations then through the experimental results that are obtained from a cdma arrayantenna base_station system it has been shown that the performance of noncoherent_detection is proportionally improved to the number of antenna_elements
evaluating usability and efficaciousness of an elearning system a quantitative modeldriven approach,using an elearning system as a case example of complex information_systems we tested a conceptual framework that expands upon the technology acceptance model tam and incorporate measures of selfefficacy a survey instrument was developed to gather large samples of users perceptions on system_usability and usefulness advanced statistical tests such as structural equation modeling were carried out the paper concludes with a discussion on efficacy of evaluation techniques and design implications for elearning systems
conception dun environnement sous matlab pour le marquage dimages en jpeg2000,le standard de compression jpeg2000 se caracterise par la diversite des options dencodage menant a de bons compromis compressionqualite cependant une extension integrant des elements de securite est fortement attendue le marquage numerique watermarking de lexpression anglaise est une technologie prometteuse pour securiser la circulation des contenus multimedias lincorporation dun marquage dans la chaine de compression presente lavantage de prise en compte de la distorsion introduite par la marque dans le processus global de compression en plus de la possibilite de reduire la complexite dimplementation en exploitant les traitements deja effectues pour la compression le travail realise consiste a la conception et au developpement dun environnement sous matlab offrant differentes ouvertures sur larchitecture jpeg2000 et permettant dincorporer une ou plusieurs methodes de marquage lenvironnement a ete developpe a partir de jj2000 implementation du standard en langage java comme premiere experimentation un marquage robuste a ete incorpore au module de transformee en ondelettes du codeur et du decodeur jpeg2000
analyzing the energytime tradeoff in highperformance computing applications,although users of highperformance computing are most interested in raw performance both energy and power consumption has become critical concerns one approach to lowering energy and power is to use highperformance cluster_nodes that have several powerperformance states so that the energytime tradeoff can be dynamically adjusted this paper analyzes the energytime tradeoff of a wide range of applicationsserial and parallelon a powerscalable cluster we use a cluster of frequency and voltagescalable amd64 nodes each equipped with a power meter we study the effects of memory and communication bottlenecks via direct measurement of time and energy we also investigate metrics that can at runtime predict when each type of bottleneck occurs our results show that for programs that have a memory or communication bottleneck a powerscalable cluster can save significant energy with only a small time penalty furthermore we find that for some programs it is possible to both consume less energy and execute in less time by increasing the number of nodes while reducing the frequencyvoltage setting of each node
using evolutionary_algorithms for signal_integrity checks of highspeed data buses,todays high performance computer_systems must have fast reliable access to memory and io devices unfortunately intersymbol_interference transmission line effects and other noise sources can distort data transfers engineers must therefore determine if bus designs have signal_integrity   e the bus can transfer data with minimal amplitude or timing distortion one method of determining signal quality on buses is to conduct a set of data transfers and measure various signal parameters at the receiver end but the tests must be conducted with stressful test patterns that maximize intersymbol_interference to help identify any potential problems in this paper we describe how an evolutionary_algorithm was used to evolve such test patterns all test_results were obtained intrinsically
a binary communication channel with memory based on a finite queue,a model for a binary additive_noise communication channel with memory is introduced the channel noise process which is generated according to a ball sampling mechanism involving a queue of finite length m is a stationary ergodic mthorder markov source the channel properties are analyzed and several of its statistical and informationtheoretical quantities eg block transition distribution autocorrelation function acf capacity and error exponent are derived in either closed or easily computable form in terms of its four parameters the capacity of the queuebased channel qbc is also analytically and numerically compared for a variety of channel_conditions with the capacity of other binary models such as the wellknown gilbertelliott channel gec the fritchman channel and the finitememory contagion channel we also investigate the modeling of the traditional gec using this qbc model the qbc parameters are estimated by minimizing the kullbackleibler divergence rate between the probability of noise sequences generated by the gec and the qbc while maintaining identical biterror rates ber and correlation_coefficients the accuracy of fitting the gec via the qbc is evaluated in terms of acf channel_capacity and error exponent numerical_results indicate that the qbc provides a good approximation of the gec for various channel_conditions it thus offers an interesting alternative to the gec while remaining mathematically tractable
a new approach for a topographic featurebased characterization of digital elevation data,triangular irregular network tin and regular square grid rsg are widely used for representing 25 dimensional spatial_data however these models are not defined from the topographic properties of the terrain ie ridge lines valley lines saddle points etc this paper introduces a threestep featurebased approach for topographic properties extraction on scattered elevation data modeled by a tin firstly a segmentation_process extracts homogeneous morphological areas bounded by critical lines and points secondly these lines and points are displaced using a deformable process in order to derive the terrain feature points lines and areas thirdly a classification_process labels any topographic feature this threestep approach relies on the definition of an adapted model of representation spin and data structure dcfl2 the proposed approach is validated on a real case study seolak mountain in south korea consistent results with the morphology of terrain are displayed
ultrahigh dimensional feature_selection beyond the linear_model,variable selection in highdimensional space characterizes many contemporary problems in scientific discovery and decision making many frequentlyused techniques are based on independence screening examples include correlation ranking fan  lv 2008 or feature_selection using a twosample ttest in highdimensional classification tibshirani et al 2003 within the context of the linear_model fan  lv 2008 showed that this simple correlation ranking possesses a sure independence screening property under certain conditions and that its revision called iteratively sure independent screening isis is needed when the features are marginally unrelated but jointly related to the response variable in this paper we extend isis without explicit definition of residuals to a general pseudolikelihood framework which includes generalized linear_models as a special case even in the leastsquares setting the new method improves isis by allowing feature deletion in the iterative process our technique allows us to select important features in highdimensional classification where the popularly used twosample tmethod fails a new technique is introduced to reduce the false selection rate in the feature screening stage several simulated and two real data examples are presented to illustrate the methodology
dns resource record analysis of urls in email_messages for improving spam_filtering,in recent years spam mails intending for oneclick fraud or phishing have become increasing as one antispam technology dnsbl based on the urls or their corresponding ip addresses in the messages is well used however some spam mails that cannot be filtered by conventional dnsbls get appearing since the spammers create websites using various techniques such as botnet fastflux and wildcard dns record to improve the accuracy of filtering spam mails using these techniques we analyzed dns record features corresponding to the domain name from the urls in actual spam mails according to the result of this analysis we confirmed that abuse of wildcard dns record is one effective criterion for spam_filtering
recovery of digital information using bacterial foraging optimization based nonlinear channel equalizers,transmission and storing of high density digital information plays an important role in the present age of information_technology these binary data are distorted while reading out of the recording medium or arriving at the receiver end due to inter symbol interference in the channel the adaptive channel equalizer alleviates this distortion and reconstructs the transmitted data faithfully the bacterial foraging optimization bfo is a recently developed efficient and derivative free evolutionary computing tool used for optimization purpose in the present paper we propose a novel nonlinear channel equalizer using bfo algorithm the recovery performance of the new equalizer is obtained through computer simulation study using nonlinear channels it is shown that the proposed equalizer offers superior performance both in terms of biterrorrate and convergence speed compared to the ga based equalizers in addition it requires substantially less computation during training
a faulttolerant approach to secure information_retrieval,several private information_retrieval pir schemes were proposed to protect users privacy when sensitive information stored in database servers is retrieved however existing pir schemes assume that any attack to the servers does not change the information stored and any computational results we present a novel faulttolerant pir scheme called ftpir that protects users privacy and at the same time ensures service availability in the presence of malicious server faults our scheme neither relies on any unproven cryptographic assumptions nor the availability of tamperproof hardware a probabilistic verification function is introduced into the scheme to detect corrupted results unlike previous pir research that attempted mainly to demonstrate the theoretical feasibility of pir we have actually implemented both a pir scheme and our ftpir scheme in a distributed_database environment the experimental and analytical_results show that only modest performance overhead is introduced by ftpir while comparing with pir in the faultfree cases the ftpir scheme tolerates a variety of server faults effectively in certain failstop fault scenarios ftpir performs even better than pir it was observed that 3582 less processing time was actually needed for ftpir to tolerate one server fault
autonomous land vehicle_navigation using millimeter wave radar,this paper discusses the use of a 77 ghz millimeter wave radar as a guidance sensor for autonomous land vehicle_navigation a test vehicle has been fitted with a radar and encoders that give steer angle and velocity an extended kalman_filter optimally fuses the radar range and bearing measurements with vehicle_control signals to give estimated position and variance as the vehicle moves around a test site the effectiveness of this data_fusion is compared with encoders alone and with a satellite positioning_system consecutive scans have been combined to give a radar image of the surrounding environment data in this format are invaluable for future work on collision_detection and map building navigation
wibro netbased five senses multimedia_technology using mobile mashup,in this paper we suggest and implement an enhanced wireless broadband wibro netbased five senses multimedia_technology using webmaporiented mobile mashup the wibro net in this applicative technology supports and allows web 20oriented various issues such as usercentric multimedia individual multimedia message exchange between multiusers and a new mediabased information and knowledge_sharing  participation without spatiotemporaldependency to inspect applicability and usability of the technology we accomplish various experiments which include 1 wibro netbased realtime field tests and 2 iso 924111 and 10based surveys on the user satisfaction by relative experience in comparison with the apbased commercialized mobile_service as a result this application provides higher data rate transmission in updown airlink and wider coverage region also its average system_usability scale sus scores estimated at 8358 and it relatively held competitive advantage in the specific item scales such as system integration individualization and conformity with user expectations
generalizing analytic shrinkage for arbitrary covariance structures,analytic shrinkage is a statistical technique that offers a fast alternative to crossvalidation for the regularization of covariance_matrices and has appealing consistency properties we show that the proof of consistency requires bounds on the growth rates of eigenvalues and their dispersion which are often violated in data we prove consistency under assumptions which do not restrict the covariance structure and therefore better match real world data in addition we propose an extension of analytic shrinkage orthogonal complement shrinkage which adapts to the covariance structure finally we demonstrate the superior performance of our novel approach on data from the domains of finance spoken letter and optical_character_recognition and neuroscience
charitime  concepts of analysis and design of an agentoriented system for appointment management,conceptual modeling is an important basis for developing general_purpose systems for information and service_management in this paper we present concepts for the analysis and design of a system of agents in which agents represent interests of individuals or groups in particular we develop a semantic model of agent domain_knowledge by introducing analytical concepts for service scheduling which is the envisioned application of our prototypical system charitime
monitoring crosschannel correlation solar scan measurements using the iowa xband polarimetric_radars,the sun is a convenient and frequently employed external radiation source for calibrating weather_radar antenna and receiver characteristics however changes in solar activity can be a major source of error in interpreting the results of solar calibration for lower frequency_bands the crosscorrelation of horizontal and vertical polarization signals which is zero for perfectly unpolarized electromagnetic radiation could give nonzero estimates if a quiet sun is not observed by the radar in this paper solar scan measurements are made at xband to detect the effect of solar activity on this crosscorrelation coefficient to facilitate mitigation of instrumentwide errors we employ multiple xpol radars to simultaneously observe the sun though our experiments during a limited period show that the crosschannel correlation estimates obtained by xband weather_radars remain relatively unaffected by the variations in solar flux the paper makes suggestions on improving the results
a contextbased ontological structure for knowledge_sharing and customization,several research projects have recently surfaced to automate the discovery and acquisition of knowledge from information and to facilitate information_sharing and reusability in the global network however some of these research attempts are limited in scope with respect to proper identification and delivery of customized information this paper addresses some of these challenges by proposing a highlevel ontological and contextbased architecture that enables information customization and knowledge organization the proposed model aims at managing knowledge items through the use of standalone computational layers ontology is used in this research to describe knowledge_representation and structure whereas context reflects knowledge adaptability to its hosting environment
verification of security_properties of payment protocol using avispa,emerging ecommerce activity is giving scope for the design of many new protocols and to gain confidence these protocol need to be verified for its designed properties specifically protocol used in ecommerce transactions needs to be verified for their security_properties verification of these protocols is done using the formal_verification tools avispa is one of the evolving tools used mainly for verifying security_properties a newly designed electronic payment protocol is verified for its correctness and security_properties this paper presents the use of avispa for verifying the security_properties of the newly evolved electronic transaction protocol
on kalman filtering for detectable systems with intermittent observations,we consider the problem of kalman filtering when observations are available according to a bernoulli process it is known that there exists a critical probability  pc  such that if measurements are available with probability greater than  pc  then the expected prediction covariance is bounded for all initial conditions otherwise it is unbounded for some initial conditions we show that when the system observation matrix restricted to the observable subspace is invertible the known lower bound on  pc  is in fact the exact critical probability this result is based on a novel decomposition of positive semidefinite matrices
probabilistic temporal_databases i algebra,dyreson and snodgrass have drawn attention to the fact that in many temporal_database applications there is often uncertainty about the start time of events the end time of events and the duration of events  when the granularity of time is small eg milliseconds a statement such as packet  p  was shipped sometime during the first 5 days of january 1998 leads to a massive amount of uncertainty 52460601000 possibilities  as noted in zaniolo et al 1997 past attempts to deal with uncertainty in databases have been restricted to relatively small amounts of uncertainty in attributes  dyreson and snodgrass have taken an important first step towards solving this problem  in this article we first introduce the   syntax of temporalprobabilistic tp relations and then show how they can be converted to an explicit significantly more spaceconsuming form called annotated relations  we then present a  theoretical annotated temporal algebra  tata  being explicit tata is convenient for specifying how the algebraic operations should behave but is impractical to use because annotated relations are overwhelmingly large  next we present a  temporal probabilistic algebra  tpa  we show that our definition of the tpalgebra provides a correct implementation of tata despite the fact that it operates on implicit succinct tprelations instead of overwhemingly large annotated relations finally we report on timings for an implementation of the tpalgebra built on   top of odbc
a conceptual_design of an adaptive and collaborative ework environment,emerging work models increasingly take the form of loosely structured often selforganising networks of nimble and virtual knowledge work teams within and between organisations to model such work patterns requires a different approach from that of traditional workflow management systems this paper presents the conceptual_design of a prototype adaptive and collaborative ework environment  eworkbench which we are currently developing to enable future collaborative workspaces to adapt to emerging knowledge work models we argue that with appropriate knowledge of tasks workspaces will be able to adapt to work and automatically retrieve contextually relevant knowledge elements from the web in order to contribute creatively to problem solving and semantically manage shared information among collaborating workers our goal is to enable eworkbench to become not only a working environment but also a collaborator and a coworker as a result of its knowledge of work and creative participation in problem solving
statistical distributions of dct_coefficients and their application to an interframe compression algorithm for 3d medical_images,displacement estimated interframe dei coding a coding_scheme for 3d medical_image data sets such as xray computed tomography ct or magnetic resonance mr images is presented to take advantage of the correlation between contiguous slices a displacementcompensated difference image based on the previous image is encoded the best fitting distribution_functions for the discrete_cosine_transform dct_coefficients obtained from displacement compensated difference_images are determined and used in allocating bits and optimizing quantizers for the coefficients the dei scheme is compared with 2d block discrete_cosine_transform dct as well as a fullframe dct using the bit_allocation technique of s lo and hk huang 1985 for xray ct head images the present bit_allocation and quantizer design using an appropriate distribution model resulted in a 13db improvement in the snr compared to the fullframe dct using the bit_allocation technique for an image set with 5mm slice thickness the dei method gave about 5 improvement in the compression_ratio on average and less blockiness at the same distortion the performance gain increases to about 10 when the slice thickness decreases to 3 mm 
coderank a new family of software_metrics,the concept of pagerank has proved successful in allowing search_engines to identify important pages in the world_wide_web in this paper we describe the application of the pagerank concept to the domain of software in order to derive a new family of metrics coderank which captures aspects of software not readily obtainable from other metrics we have implemented a tool coderanker to compute values of coderank metrics using a full semantic model which we have developed we present some results and discuss the use of coderank metrics in their interpretation
new multiparty authentication services and key_agreement protocols,many modern computing_environments involve dynamic peer groups distributed simulation multiuser games conferencing applications and replicated servers are just a few examples given the openness of todays networks communication among peers group_members must be secure and at the same time efficient this paper studies the problem of authenticated_key_agreement in dynamic peer groups with the emphasis on efficient and provably_secure key authentication key confirmation and integrity it begins by considering twoparty authenticated_key_agreement and extends the results to group diffiehellman 1976 key_agreement in the process some new security_properties unique to groups are encountered and discussed
fusing asynchronous feature streams for online writer_identification,in this paper we present a new approach to improving the performance of a writer_identification system by fusing asynchronous feature streams different feature streams are extracted from online handwritten text acquired from a whiteboard the feature streams are used to train a text and language independent writer_identification system based on gaussian_mixture_models gmms from a stroke consisting of n points n pointbased feature_vectors and one strokebased feature_vector are extracted the resulting feature streams thus have an unequal number of feature_vectors we evaluate different methods to directly fuse the feature streams and show that by means of feature fusion we can improve the performance of the writer_identification system on a data set produced by 200 different writers
scalable bgp qos extension with multiple metrics,being an important and yet a challenging problem the qosbased routing in the converging internet has received significant attention from the research community however most of the qosbased routing research is conducted in the context of intradomain routing leaving qosbased interdomain_routing relatively open in this paper we specifically investigate how to extend the current interdomain_routing protocol bgp to support multiple qos metrics with multiple metrics a bgp speaker has to advertise multiple_routes for each destination to its peer this will increase the routing message overhead and make qosaware bgp unscalable to large networks therefore our focus in this paper is to bring qos extensions to the original bgp while simultaneously providing high performance and scalability in response to this we propose path reduction algorithms ie contribution based reduction cbr algorithms to reduce the number of routes advertised by bgp speakers while maximizing the routing success ratio extensive_simulations show that our schemes achieve high performance with low complexity in terms of message overhead and computation making the qos extension to bgp scalable
optimized submerged batch fermentation strategy for systems scale studies of metabolic switching in streptomyces coelicolor a32,background systems_biology approaches to study metabolic switching in streptomyces coelicolor a32 depend on cultivation conditions ensuring high reproducibility and distinct phases of culture growth and secondary metabolite production in addition biomass concentrations must be sufficiently high to allow for extensive timeseries sampling before occurrence of a given nutrient depletion for transition triggering the present study describes for the first time the development of a dedicated optimized submerged batch fermentation strategy as the basis for highly timeresolved systems_biology studies of metabolic switching in s coelicolor a32 results by a stepwise approach cultivation conditions and two fully defined cultivation media were developed and evaluated using strain m145 of s coelicolor a32 providing a high degree of cultivation reproducibility and enabling reliable studies of the effect of phosphate depletion and lglutamate depletion on the metabolic transition to antibiotic production phase interestingly both of the two carbon sources provided dglucose and lglutamate were found to be necessary in order to maintain high growth rates and prevent secondary metabolite production before nutrient depletion comparative analysis of batch cultivations with i both lglutamate and dglucose in excess ii lglutamate depletion and dglucose in excess iii lglutamate as the sole source of carbon and iv dglucose as the sole source of carbon reveal a complex interplay of the two carbon sources in the bacteriums central carbon metabolism conclusions the present study presents for the first time a dedicated cultivation strategy fulfilling the requirements for systems_biology studies of metabolic switching in s coelicolor a32 key results from labelling and cultivation experiments on either or both of the two carbon sources provided indicate that in the presence of dglucose lglutamate was the preferred carbon source while dglucose alone appeared incapable of maintaining culture growth likely due to a metabolic bottleneck at the oxidation of pyruvate to acetylcoa
scalable absolute position recovery for omnidirectional image networks,we describe a lineartime algorithm that recovers absolute camera positions for networks of thousands of terrestrial images spanning hundreds of meters in outdoor urban scenes under uncontrolled lighting the algorithm requires no human input or interaction for real data it recovers camera pose globally consistent on average to roughly five centimeters or about four pixels of epipolar alignment the papers principal contributions include an extension of markov_chain_monte_carlo estimation techniques to the case of unknown numbers of feature points unknown occlusion and deocclusion large scale thousands of images and hundreds of thousands of point features and large dimensional extent tens of meters of intercamera baseline and hundreds of meters of baseline overall also a principled method is given to manage uncertainty on the sphere a new use of the hough_transform is proposed and a method for aggregating local baseline constraints into a globally consistent pose set is described
last mile problem in overlay design,performance of overlay_networks is dependent on lastmile connections since they require that data traverse these lastmile bottlenecks at each forwarding step this requires several times more upstream bandwidth than downstream further exaggerating the asymmetry between downstream and upstream bandwidth in lastmile technologies this imbalance can cause packet queuing at the outgoing network_interface of forwarding nodes increasing latency and causing packet_losses we describe a model of a lastmile constrained overlay_network and formulate and use it to solve a simplified latency and bandwidthbounded overlay construction problem we observe that queueing delay may be a significant component of the endtoend delay and approaches ignoring this may potentially result in an overlay_network violating the delay andor loss bounds we observe that allowing a small amount of loss it is possible to support a significantly large number of nodes for a given end_to_end_delay and loss bound we identify feasible degree fan out of each nodes our study sheds insights which provide engineering guidelines for designing overlays accounting for last mile problem in the internet
a learning adaptive bollinger band system,this paper introduces a novel forecasting algorithm that is a blend of micro and macro modelling perspectives when using artificial_intelligence ai techniques the micro component concerns the finetuning of technical indicators with population based optimization_algorithms this entails learning a set of parameters that optimize some economically desirable fitness_function as to create a dynamic signal processor which adapts to changing market environments the macro component concerns combining the heterogeneous set of signals produced from a population of optimized technical indicators the combined signal is derived from a learning classifier system lcs framework that combines population based optimization and reinforcement_learning rl this research is motivated by two factors that of nonstationarity and cyclical profitability as implied by the adaptive market hypothesis 10 these two properties are not necessarily in contradiction but they do highlight the need for adaptation and creation of new models while synchronously being able to consult others which were previously effective the results demonstrate that the proposed system is effective at combining the signals into a coherent profitable trading system but that the performance of the system is bounded by the quality of the solutions in the population
pitch_estimation using models of voiced speech on three levels,we present an algorithm for estimating the fundamental frequency in speech_signals our approach incorporates models of voiced speech on three levels first we estimate the pitch for each time frame based on its harmonic structure using nonnegative_matrix_factorization the second level utilizes temporal pitch continuity to extract partial pitch_contours thirdly we incorporate statistics of the succession of voiced segments to aggregate partial contours to the final contour of an utterance we evaluate our approach on the keele database the experimental results show the robustness of our method for noisy_speech and the good performance for clean_speech in comparison with stateoftheart algorithms
on the family of ml spectral estimates for mixed spectrum identification,a recently developed point spectrum identification procedure based on a family of ar and ml spectral estimates is exploited to arrive at a mixed spectrum identification procedure to this end a variety of properties of the ar and ml estimates as a function of model order are described these properties relate to amplitude convergence resolution and a characterization of the ar spectral artifact which is used to arrive at improved continuous spectral estimates a variety of examples are presented 
using linda for supercomputing on a local area network,a distributed parallel_processing system based on the linda programming constructs has been implemented on a local area network of computers this system allows a single application program to utilize many machines on the network simultaneously several applications have been implemented on the network at sandia national laboratories and have achieved performances considerably faster than that of a cray1s several collections of machines have been used including up to eleven dec vaxes three sun3 workstations and a pc
fault modeling and patternsensitivity testing for a multilevel dram,multilevel dynamic randomaccess memory mldram attempts to increase the storage density of semiconductor memory without further reducing the lithographic dimensions it does so by using more than two possible signal voltages on each cell capacitor thus permitting more than one bit to be stored in each cell birks mldram scheme has several promising properties including robust locallygenerated data signal and reference signal generation and fast flashconversion sensing this paper describes a fault model for birks mldram that was developed by considering the behaviors produced by likely defects at the schematic level the resulting behaviors include faults that are detectable as observable logical errors faults that can be detected by current measurements and faults that in the worst case can only be detected by testing for degraded noise margins all boolean faults in the fault model can be detected by an efficient test whose length grows linearly in the number of cells the narrower noise margins in mldram will make it more vulnerable to pattern sensitivities we also developed a linear test that evaluates worstcase sensing conditions
bit_error_rate analysis in ieee 802153a uwb channels,in this paper we present a computable bit_error_rate ber expression for the binary signals in the ieee 802153a ultrawideband uwb_channel in the literature the impacts of the rake_receivers finger numbers and lognormal shadowing on the ber_performance have not been reported yet we propose a characteristic function cf based ber formula to overcome the convergence problem of the existing moment_generating_function mgf approach when the ber calculation takes account of the shadowing effect simulation results demonstrate that the proposed cfbased computable ber formula can accurately estimate the complete effects of the cluster and ray arrival processes the lognormal fading as well as shadowing and the finger numbers at rake_receivers
applicationdriven voltageisland partitioning for lowpower systemonchip design,among the different methods of reducing power for corebased systemonchip soc_designs the  voltageisland   technique  has gained in popularity assigning cores to the different supply voltages and floorplanning to create contiguous voltage islands are two important steps in the design process we propose a new applicationdriven approach to voltage partitioning and island creation with the objective of reducing overall soc power area and floorplanner runtime given an application powerstate machine psm we first identify the suitable range of supply voltages for each core then we generate the discrete voltage assignment table using a heuristic technique next we describe a methodology of reducing the large number of available choices from the voltage assignment table down to a useful set using the application psm we partition the cores into islands using a cost function that gradually shifts from a powerbased assignment to a connectivitybased one compared with previously reported techniques a 94 reduction in power and 87 reduction in area are achieved using our approach with an average runtime improvement of 24 times
query scheduling in multi query_optimization,complex_queries are becoming commonplace with the growing use of decision_support_systems decision support queries often have a lot of common subexpressions within each query and queries are often run as a batch multi query_optimization aims at exploiting common subexpressions to reduce the evaluation cost of queries by computing them once and then caching them for future use both within individual queries and across queries in a batch in case cache space is limited the total size of subexpressions that are worth caching may exceed available cache space prior work in multi query_optimization involves choosing a set of common subexpressions that fit in available cache space and once computed retaining their results across the execution of all queries in a batch such optimization_algorithms do not consider the possibility of dynamically changing the cache contents this may lead to subexpressions occupying cache space even if they are not used by subsequent queries the available cache space can be best utilized by evaluating the queries in an appropriate order and changing the cache contents as queries are executed we present several algorithms that consider these factors in order to reduce the cost of query_evaluation
ddos detection and traceback with decision_tree and grey_relational_analysis,in distributed denialofservice ddos_attack an attacker breaks into many innocent computers called zombies then the attacker sends a large number of packets from zombies to a server to prevent the server from conducting normal business operations we design a ddosdetection system based on a decisiontree technique and after detecting an attack to trace back to the attackers locations with a trafficflow patternmatching technique our system could detect ddos_attacks with the false positive ratio about 1224 false negative ratio about 210 and find the attack paths in traceback with the false negative rate 812 and false positive rate 1214
comparative interactomics analysis of protein family interaction networks using psimap protein structural interactome map,motivation many genomes have been completely sequenced however detecting and analyzing their proteinprotein interactions by experimental methods such as coimmunoprecipitation tandem affinity purification and y2h is not as fast as genome sequencing therefore a computational prediction method based on the known protein structural interactions will be useful to analyze largescale proteinprotein interaction rules within and among complete genomesrnrnresults we confirmed that all the predicted protein family interactomes the full set of protein family interactions within a proteome of 146 species are scalefree networks and they share a small core network comprising 36 protein families related to indispensable cellular functions we found two fundamental differences among prokaryotic and eukaryotic interactomes 1 eukarya had significantly more hub families than archaea and bacteria and 2 certain special hub families determined the topology of the eukaryotic interactomes our comparative analysis suggests that a very small number of expansive protein families led to the evolution of interactomes and seemed tohave played a key role in species diversificationrnrncontact jongkribbrekrrnrnsupplementary information httpinteractomicsorg
exploring board game_design using digital technologies,this talk presents results of a case study from a course called history of games offered at the school of art  design at new jersey institute of technology after analyzing various traditional board games and their mechanics students explore the possibility of producing their own original board games by altering various existing game structures through application of new technologies such as digital prototyping including laser cutting and 3d printing and microcontroller technologies in principle we can fully emulate the playing of a board game such as monopoly inside a computer digitally however there is a certain quality of physicality with traditional board games that cannot be experienced through games in fully digital environments the existence of tangible game pieces boards and real human players can produce cooperation engagement and tensions unlike those in video_games and arbased applications through this projectbased course students further explore how new technologies can help in developing new types of games
using callcontext to prevent the emergence of chaotic workflow behaviors in overload situations,by defining workflows existing services are put into novel contexts and exposed to different workloads which in turn can result in unexpected behaviors this paper examines the chaotic_behavior of sequential workflows in overload situations and discusses the use of callcontexts as a means of avoiding them
an analytical and experimental investigation of flexible_manipulator performance,a critical modeling of a flexible_manipulator will involve a thorough understanding of the issues specific to flexible_manipulator performance and an experimental investigation of a physical system a comprehensive dynamic model of a flexible_manipulator has been presented in this paper together with a preliminary experimental investigation that examines the performance of this comprehensive model the analytical model recognizes in particular the coupling characteristics of the deformations in a flexible_manipulator configuration and realizes an efficient and dedicated finite element scheme to model general spatial manipulator configurations with revolute and prismatic pairs the experimental results indicate a favorable level of performance for the application of the special finite element for a practical manipulator
biddle a bidirectional data driven lisp engine,the authors propose biddle a direct execution architecture for lisp based on both data and demanddriven principles a priority queuing mechanism is used to control the parallelism and the workload of the processing_elements also introduced is a novel mechanism whereby the sequential semantics of lisp can be enforced in such a way as not to reduce the parallelism too drastically biddle is therefore aimed at achieving a balance between eager and lazy_evaluation sequential semantics and parallelism at present biddle exists only on paper and is a long way from hard ware implementation 
ant algorithms for discrete optimization,this article presents an overview of recent work on ant algorithms that is algorithms for discrete optimization that took inspiration from the observation of ant colonies foraging behavior and introduces the ant_colony_optimization aco metaheuristic in the first part of the article the basic biological findings on real ants are reviewed and their artificial counterparts as well as the aco metaheuristic are defined in the second part of the article a number of applications of aco_algorithms to combinatorial_optimization and routing in communications networks are described we conclude with a discussion of related work and of some of the most important aspects of the aco metaheuristic
vehicular networking for intelligent and autonomous traffic management,traffic congestion has become a daily problem that most people suffer this not only impacts the productivity of the population but also poses a safety risk most of the technologies for intelligent highways focus on safety measures and increased driver awareness and expect a centralized management for the traffic flow this paper presents a new approach for enabling autonomous and adaptive traffic management through vehicular_networks by allowing data_exchange between vehicles about route choices congestions and traffic alerts a vehicle makes a decision on the best course of action unlike centralized schemes that provide recommendations our vanetbased autonomous management vam approach factors in the destination and routes of nearby vehicles in deciding on whether rerouting is advisable in addition vam leverages the presence of smart traffic lights and enables coordination between vehicles and lightcontrollers in order to ease congestion the collective effect of all vehicles will be an autonomous reshape of the traffic pattern based on their destinations and road conditions the simulation results demonstrate the advantage of vam
peertopeer support for lowlatency massively_multiplayer_online_games in the cloud,cloud gaming has recently been proposed as an alternative to traditional video_game distribution with this approach the entire game is stored run and rendered on a remote server player input is forwarded to the server via the internet and the games output is returned as a video stream this adds network delay which can negatively impact the gameplay the delay is acceptable as long as the user is located geographically close to the cloud servers however for massively_multiplayer_online_games mmogs this delay is added on top of the existing delay between mmog client and server as mmogs are highly delaysensitive this can significantly degrade their playability to deal with this issue we propose to use peertopeer techniques to distribute the mmog server functionality and place it at the cloud server centers this allows us to reduce the additional delay introduced by running the mmog clients in the cloud
estimating crowd densities and pedestrian flows using wifi and bluetooth,the rapid deployment of smartphones as allpurpose mobile computing_systems has led to a wide adoption of wireless_communication_systems such as wifi and bluetooth in mobile scenarios both communication_systems leak information to the surroundings during operation this information has been used for tracking and crowd density estimations in literature however an estimation of pedestrian flows has not yet been evaluated with respect to a known ground truth and thus a reliable adoption in real world scenarios is rather difficult with this paper we fill in this gap using ground truth provided by the security check process at a major german airport we discuss the quality and feasibility of pedestrian flow estimations for both wifi and bluetooth captures we present and evaluate three approaches in order to improve the accuracy in comparison to a naive count of captured mac addresses such counts only showed an impractical pearson_correlation of 053 for bluetooth and 061 for wifi compared to ground truth the presented extended approaches yield a superior correlation of 075 in best case this indicates a strong correlation and an improvement of accuracy given these results the presented approaches allow for a practical estimation of pedestrian flows
alleviating the constant stochastic variance assumption in decision research theory measurement and experimental test,analysts often rely on methods that presume constant stochastic variance even though its degree can differ markedly across experimental and field settings this reliance can lead to misestimation of effect sizes or unjustified theoretical or behavioral inferences classic utilitybased discretechoice theory makes sharp testable predictions about how observed choice patterns should change when stochastic variance differs across items brands or conditions we derive and examine the implications of assuming constant stochastic variance for choices made under different conditions or at different times in particular whether substantive effects can arise purely as artifacts these implications are tested via an experiment designed to isolate the effects of stochastic variation in choice behavior results strongly suggest that the stochastic component should be carefully modeled to differ across both available brands and temporal conditions and that its variance may be relatively greater for choices made for the future the experimental design controls for several alternative mechanisms eg flexibility seeking and a series of related models suggest that several econometrically detectable explanations like correlated error state dependence and variety seeking add no explanatory power a series of simulations argues for appropriate flexibility in discretechoice specification when attempting to detect temporal stochastic inflation effects
the design of a lowpower asynchronous des coprocessor for sensor_network encryption,sensor_network nodes have a very tight power budget and the power efficiency is the biggest design concern in sensor_network circuits a generalpurpose processor eg an arm processor is not efficient to execute encryption_algorithms because it has no special instructions to support encryption operations for example very oftenused permutation operations in the paper we propose a lowpower asic encryption coprocessor for sensor_network nodes a des algorithm is used because the algorithm does not include powerhungry and complex mathematic operations such as multiplication division and addition an asynchronous logic style is used to design the coprocessor with an asynchronous controller a global clock is not necessary when idle resulting in zero standby dynamic power using the des coprocessor the power_consumed by encryption can be saved by 4 orders of magnitude than a pure software calculation
the effect of a standalone ethics course in chilean engineering students attitudes,engineering ethics education is taking on increasing importance worldwide but in chile the percentage of universities that have a mandatory course concerning ethics is still small traditionally chilean universities with existing ethics courses teach them using a philosophical or theological perspective limited to occidental theories and usually from a christian point of view this article studies the impact of a new methodology and technique to teach ethics in chile casebased nonnormative and with a criticaldescriptive approach an empirical study is conducted to assess the relative impact of an ethics class on students individual and inherent moral values and attitudes and understand the factors that contribute to this impact results indicate that even though the importance of religion in chile is decreasing it is still a major source of students ethical principles and moral values in addition results suggest that a change in moral values develops when discussions among groups with different points of view occur
functional validation of faulttolerant asynchronous algorithms,the paper presents an alternative approach to the formal_specification and validation of distributed asynchronous algorithms it begins with a syntactically correct description of the algorithm whose correctness is then to be validated the validation of the algorithm is based on the processoriented discrete simulation and permits a partial correctness validation of the algorithm implemented by a program the suggested method enables to model independent activity of several processors using pseudoparallel processes in simulation time and to model communication_channels with defined time behavior and failure semantics using the approach it is easy to add other processes like model of systems environment fault injector and state observer the method is described with the aid of a simple cbased validation tool called csim the utilization of csim requires only slight changes in ccoded implementation of the verified algorithm an example of validation of distributed election algorithm with the presence of faults is presented
an eventdriven sir model for topic diffusion in web forums,social_media is being increasingly used as a communication channel among social_media web forums where people in online_communities disseminate and receive information by interaction provide a good environment to examine information_diffusion in this research we aim to understand the mechanisms and properties of the information_diffusion in the web forum for that we model topiclevel information_diffusion in web forums using the baseline epidemic model the sirsusceptible infective and recovered model frequently used in previous research to analyze disease outbreaks and knowledge diffusion in addition we propose an eventdriven sir model that reflects the event effect on information_diffusion in the web forum the proposed model incorporates the effect of news postings on the web forum we evaluate two models using a large longitudinal dataset from the web forum of a major company the eventsir model outperforms the sir model in fitting on major spikey topics that have peaks of author participation
an intersection algorithm based on delaunay triangulation,a robust method for finding points of intersection of line segments in a 2d plane is presented the plane is subdivided by delaunay triangulation to localize areas where points of intersection exist and to guarantee the topological consistency of the resulting arrangement the subdivision is refined by inserting midpoints recursively until the areas containing points of intersection are sufficiently localized the method is robust in the sense that it does not miss points of intersection that are easily detectable when costly linepair checking is performed the algorithm is adaptive in the sense that most of the computational cost is incurred for the areas where finding points of intersection is difficult 
earth system science workbench a data_management infrastructure for earth science products,the earth system science workbench essw is a nonintrusive data_management infrastructure for researchers who are also data publishers an implementation of essw to track the processing of locally received satellite imagery is presented demonstrating the workbenchs transparent and robust support for archiving and publishing data products essw features a lab notebook metadata service an ndworm no duplicatewrite once read many storage service and web user_interface tools the lab notebook logs processes experiments and their relationships via a custom api to xml_documents stored in a relational_database the ndworm provides a managed storage archive for the lab notebook by keeping unique file digests and namespace metadata also in a relational_database essw notebook tools allow project searching and ordering and file and metadata management
highlyavailable services using the primarybackup approach,the authors derive lower bounds and the corresponding optimal protocols for three parameters for synchronous primarybackup systems they compare their results with similar results for active replication in order to determine whether the common folklore on the virtues of the two approaches can be shown formally they also extend some of their results to asynchronous primarybackup systems they implement an important subclass of primarybackup protocols that they call 0blocking these protocols are interesting because they introduce no additional protocol related delay into a failurefree service request through implementing these protocols the authors hope to determine the appropriateness of their theoretical system model and uncover other practical advantages or limitations of the primarybackup approach 
efficient composition of media object for multimedia scene rendering,in an audiovisual scene consisting of high_capacity multimedia objects when a scene change which an object is inserted deleted or replaced in real time by user interaction is required the object priority order compositor for an multimedia player according to the present invention performs rendering of objects based on priority order of objects the compositor searches priority order of objects and makes it possible to arbitrarily change order of objects and further to perform rendering of only objects requiring reconstruction thus the object priority order compositor provides reusability and availability of multimedia_data further the object priority order compositor renders efficient multimedia_data processing possible by providing a user with a dynamic scene
percolation based synthesis,a new approach called percolation based synthesis for the scheduling phase of high_level_synthesis hls is presented we discuss some new techniques which are implemented in our tools for compaction of flow graphs beyond basic blocks limits which can produce order of magnitude speed ups versus serial execution our algorithm applies to programs with  conditional jumps loops  and  multicycle pipelined operations  in order to schedule under resource constraints we start by first finding the  optimal schedule  without constraints and then add heuristics to map the optimal schedule onto the given system we argue that starting from an optimal schedule is one of the most important factors in scheduling because it offers the user flexibility to tune the heuristics and gives him a good bound for the resource constrained schedule this scheduling algorithm is integrated with synthesis tool which uses vhdl as input description and produces a  structural netlist  of generic registertransfer components and a  unit based control table  as output we show that our algorithm obtains better results than previously published algorithms
using sdl for implementing a wireless medium_access_control protocol,specification and description language sdl is a high abstraction level system design language with a clear graphical notation because of formal presentation an sdl model can be automatically converted into source c code for implementation however the high abstraction level creates a conceptual gap between a general sdl model and its implementation in a final operational platform the paper studies the sdl development of an embedded medium_access_control mac_protocol for a wireless local area network wlan demonstrator the sdl design_flow for the protocol is first started by architectural design without target platform dependencies functionality is added to the model using the topdown design approach functional simulations are used for verifying the operation of the protocol next the performance is estimated using performance simulations in a workstation environment performance improvements can be achieved by optimising the sdl model
aguia agents guidance for intelligence amplification in goal oriented tasks,all i know is that i know nothing socratic ignorance the world is an increasingly complex with problems that require swift resolution although knowledge is widely available be it stored in companies databases or spread over the internet humans have intrinsic limitations for handling very large volumes of information or keeping track of frequent updates in a constantly changing world moreover human reasoning is highly intuitive and potentially biased decisionmaking is often based on rules of thumb instead of systematic analysis with full understanding of decisions context computer_systems that manage knowledge to thoroughly explore the context and the range of alternatives may improve human decisionmaking by making people aware of possible misconceptions and biases computer_systems are also limited in their potential usage due to the frame problem systems are not aware of their ignorance thus they cannot surplus human intelligence but they may be useful complement to humans intelligence the objective of this paper is to present the aguia model for amplifying human intelligence based on agents technology for taskoriented contexts aguia uses domain_ontology and task scripts for handling formal and semiformal knowledge_bases thereby helping to systematically 1 explore the range of alternatives 2 interpret the problem and the context and 3 maintain awareness of the problem as for humans knowledge is a fundamental resource for aguia performance aguias knowledge_base keeps updating its content in the background during interaction with humans either through identified individuals or through anonymous mass contribution the feasibility and benefits of aguia were demonstrated in many different fields such as engineering_design fault diagnosis accident investigation and online interaction with the government the experiments considered a set of criteria including product cost number of explored alternatives users problem understanding and users awareness of problem context changes results indicate aguia can actually improve human problem solving capacity in many different areas
globally asymptotically_stable filters for source localization and navigation aided by direction measurements,this paper presents a set of filters with globally asymptotically_stable error dynamics for source localization and navigation in 3d based on direction measurements from the agent or vehicle to the source in addition to relative velocity readings of the agent both the source and the agent are allowed to have constant unknown drift velocities and the relative drift velocity is also explicitly estimated the observability of the system is studied and realistic simulation results are presented in the presence of measurement_noise that illustrate the performance of the achieved solutions comparison results with the extended kalman_filter are also provided and similar performances are achieved
learning and leveraging the relationship between architecturelevel measurements and individual user satisfaction,the ultimate goal of computer design is to satisfy the enduser in particular computing domains such as interactive applications there exists a variation in user expectations and user satisfaction relative to the performance of existing computer_systems in this work we leverage this variation to develop more efficient architectures that are customized to endusers we first investigate the relationship between microarchitectural parameters and user satisfaction specifically we analyze the relationship between hardware performance counter hpc readings and individual satisfaction levels reported by users for representative applications our results show that the satisfaction of the user is strongly correlated to the performance of the underlying hardware more importantly the results show that user satisfaction is highly userdependent to take advantage of these observations we develop a framework called individualized dynamic_voltage_and_frequency_scaling idvfs we study a group of users to characterize the relationship between the hpcs and individual user satisfaction levels based on this analysis we use artificial_neural_networks to model the function from hpcs to user satisfaction for individual users this model is then used online to predict user satisfaction and set the frequency level accordingly a second set of user studies demonstrates that idvfs reduces the cpu power consumption by over 25 in representative applications as compared to the windows_xp dvfs algorithm
performance of different mobile_payment service concepts compared with a nfcbased solution,the paper compares the performance of different traditional mobile_payment service concepts with a state of the art nfcbased mobile_payment solution the goal is to evaluate the different mobile_payment concepts not their software implementation from a performance and endtoend service duration time point of view overall there have been five different mobile_payment services developed implemented and benchmarked for the concept comparison namely interactive voice response short_message_service wireless application protocol one time password generator as well as a solution based on near_field_communication
electrical power monitoring system using thermochron sensor and 1wire communication_protocol,maintaining quality and reliability of operation of remotely located electrical machines as well as power supply equipment by monitoring their load is an important task in modern practical electrical engineering this paper presents a lowcost efficient robust and relatively simple load monitoring electronic system based on the use of special encapsulated temperature sensorsdata loggers socalled thermochron ibuttons combined with application of 1wire data communications implemented on the power supply line dedicated data communication line with offline data transfer
undergraduate students gender differences in it skills and attitudes,the worldwide concern about the gender gap in information_technology and the lack of woman participation in computer_science has been attributed to the different cultural influences to which boys and girls are subject in the university of hong kong girls achieved greater improvements in their computer skills than their male counterparts after completing one year of studies recognising their own progress has in turn boosted their confidence in using it the young womens estimates of their skill levels have doubled over the years from 1998 to 2000 despite this recorded acceleration at the end of the academic years girls were less confident of their abilities and possessed lower it skill levels than boys before starting their university education as found in surveys of freshmens computer skills this study compares the responses of student participants of the hkuibm notebook computer programme which started in 1998 in the selfreported it skills and attitudes of male and female students in surveys conducted both at the beginning and again at the end of the freshman year it also examines the achievement scores of the it proficiency tests and the foundations to information_technology courses administered for the student it requirement for graduation
construction and completion of flux balance models from pathway databases,motivation flux balance analysis fba is a wellknown technique for genomescale modeling of metabolic flux typically an fba formulation requires the accurate specification of four sets biochemical reactions biomass metabolites nutrients and secreted metabolites the development of fba models can be time consuming and tedious because of the difficulty in assembling completely accurate descriptions of these sets and in identifying errors in the composition of these sets for example the presence of a single nonproducible metabolite in the biomass will make the entire model infeasible other difficulties in fba modeling are that model distributions and predicted fluxes can be cryptic and difficult to understandrnrnresults we present a multiple gapfilling method to accelerate the development of fba models using a new tool called metaflux based on mixed integer linear programming milp the method suggests corrections to the sets of reactions biomass metabolites nutrients and secretions the method generates fba models directly from pathwaygenome databases thus fba models developed in this framework are easily queried and visualized using the pathway tools software predicted fluxes are more easily comprehended by visualizing them on diagrams of individual metabolic pathways or of metabolic maps metaflux can also remove redundant highflux loops solve fba models once they are generated and model the effects of gene knockouts metaflux has been validated through construction of fba models for escherichia coli and homo sapiensrnrnavailability pathway tools with metaflux is freely available to academic users and for a fee to commercial users download from biocycorgdownloadshtmlrnrncontact mariolatendressesricomrnrnsupplementary informationsupplementary data are available at bioinformatics online
practice prize reportthe power of clv managing customer lifetime value at ibm,customer management activities at firms involve making consistent decisions over time about a which customers to select for targeting b determining the level of resources to be allocated to the selected customers and c selecting customers to be nurtured to increase future profitability measurement of customer profitability and a deep understanding of the link between firm actions and customer profitability are critical for ensuring the success of the above decisions we present the case study of how ibm used customer lifetime value clv as an indicator of customer profitability and allocated marketing resources based on clv clv was used as a criterion for determining the level of marketing contacts through direct mail telesales email and catalogs for each customer in a pilot study implemented for about 35000 customers this approach led to reallocation of resources for about 14 of the customers as compared to the allocation rules used previously which were based on past spending history the clvbased resource reallocation led to an increase in revenue of about 20 million a tenfold increase without any changes in the level of marketing investment overall the successful implementation of the clvbased approach resulted in increased productivity from marketing investments we also discuss the organizational and implementation challenges that surrounded the adoption of clv in this firm
multiuser joint txiterative rx mmsefde and successive mui cancellation for uplink dscdma,uplink multiuser direct sequencecode division multiaccess dscdma suffers from strong multiuser_interference mui and self interchip interference ici caused by severe frequencyselective fading in this paper we propose a joint txiterative rx frequencydomain equalization fde based on minimum_mean_square_error mmse criterion and successive mui cancellation muic for dscdma uplink in the proposed scheme each user applies onetap tx fde before transmitting signal at the base_station joint onetap rx fde and successive muic is iteratively performed the fde weights of users and base_station are jointly optimized based on the mmse criterion in order to reduce mui and ici while exploiting channel frequencyselectivity computer simulation results show that the proposed scheme provides much improved bit_error_rate ber_performance than the conventional iterative rx mmsefde with successive muic
basic primitives for molecular diagram sketching,a collection of primitive operations for molecular diagram sketching has been developed these primitives compose a concise set of operations which can be used to construct publicationquality 2 d coordinates for molecular structures using a bare minimum of input bandwidth the input requirements for each primitive consist of a small number of discrete choices which means that these primitives can be used to form the basis of a user_interface which does not require an accurate pointing device this is particularly relevant to software designed for contemporary mobile_platforms the reduction of input bandwidth is accomplished by using algorithmic methods for anticipating probable geometries during the sketching process and by intelligent use of template grafting the algorithms and their uses are described in detail
efficient and complete remote authentication_scheme with smart_cards,a complete remote authentication_scheme should provide the following security_properties 1 mutual_authentication 2 session_key exchange 3 protection of user_anonymity 4 support of immediate revocation capability 5 low communication and computation cost 6 resistance to various kinds of attacks 7 freely choosing and securely changing passwords by users and 8 without storing password or verification tables in servers however none of the existing schemes meets all the requirements in this paper along the line of cost effective approach using hash_functions for authentication we propose an efficient and practical remote_user_authentication scheme with smart_cards to support the above complete security_properties
multimonostatic shape reconstruction of twodimensional dielectric cylinders by a kirchhoffbased approach,the inverse_problem of reconstructing the shape of dielectric cylinders by aspectlimited multimonostatic multifrequency electromagnetic_scattering data is dealt with the problem is formulated as a linear one by means of the physicaloptics approximation distributional approach the difference with respect to the case of perfectly electrical conducting scatterers is pointed out since the penetrability of the scatterers is taken into account by considering the contribution of the shadowed side to the local reflection coefficient the adopted model allows one to predict that both the illuminated and shadowed sides of the scatterer provide contribution to the reconstructed_image but with a delocalization depending on the relative dielectric permittivity the numerical_results confirm this expectation and show the effectiveness of the approach
robust and low complexity packet detector design for mbofdm uwb,multiband orthogonal_frequency_division_multiplexing mbofdm ultra_wideband uwb_systems have drawn much attention for its high spectrum_efficiency and multiple_access capability however its large throughput requirement and low_power spectral density result in high hardware complexity and high power consumption which are challenges of designing the packet detector in this paper a novel detection method is proposed with very good detection_performance in low_snr low cost and low_power schemes are also introduced in circuit design to save 70 area and 71 power the proposed packet detector is synthesized with umc 013m library at 132mhz clock_frequency the hardware cost is 569k gates and the power consumption is only 117mw
microfluidic device for continuous magnetophoretic separation of red blood cells,this paper presents a microfluidic device for magnetophoretic separation red blood cells from blood under continuous flow the separation method consist of continuous flow of a blood sample diluted in pbs through a microfluidic channel which presents on the bottom ldquodotsrdquo of ferromagnetic layer by applying a magnetic field perpendicular on the flowing direction the ferromagnetic ldquodotsrdquo generate a gradient of magnetic field which amplifies the magnetic force as a result the red blood cells are captured on the bottom of the microfluidic channel while the rest of the blood is collected at the outlet experimental results show that an average of 95  of red blood cells is trapped in the device
polynomial time construction algorithm of bcnc for network_coding in cyclic networks,network_coding in cyclic networks meets more problems than in acyclic networks recently syrli et alproposed a framework of convolutional network_coding cnc as well as its four properties for cyclic networks with theoretic fundamentals of discrete valuation ring dvr the four properties convolutional multicast cm convolutional broadcast cb convolutional dispersion cd and basic convolutional networkcode bcnc are notions of increasing strength in this order with regard to linear independence among the global encoding kernels the existence of a bcnc then implies the existence ofthe rest that is bcnc is the best convolutional network code in terms of linear independence however the code construction algorithm of bcnc was not presented explicitly to the best of our knowledge this is the first paper to propose a polynomial time construction algorithm of bcnc for network_coding in cyclic networks which can deal with different characteristics of cycles in terms of topology including link cycles but flow acyclic simpleflow cycles and knots finally polynomial_time_complexity of the algorithm was proved as well as its effectiveness
using context ontologies for addressing and routing in mobile_ad_hoc_networks,this paper presents a new way of addressing and routing for mobile_ad_hoc_networks on the basis of contextual information such as air pressure brightness wind direction and strength or gps position the most common use case of contextbased addressing is group_communication a participant sends a message to an a priori unspecified set of recipients but indicates the context in which the message could be useful for a potential receiver in contrast to infrastructure networks the sender no longer designates the receiver of its message with a distinct identifier instead each recipient using his local context decides by himself whether the message is useful for him and whether it should be sent out again the modeling of the necessary application knowledge is done as ontologies in owl web_ontology_language as an example scenario a wind gust warning on highways using a vehicular_ad_hoc_network vanet is described the warning message should be sent to all vehicles on the same route containing the place where the wind was detected the models are applied in a prototypical example scenario in order to show the performance of the approach through a simulation using the jistswans simulator for mobile_ad_hoc_networks the results show that the number of messages that are necessary to warn all vehicles in a given environment of the wind danger can be reduced by half  as opposed to a simple flooding of the network
local histogram of figureground segmentations for dynamic background_subtraction,we propose a novel feature local histogram of figureground segmentations for robust and efficient background_subtraction bgs in dynamic scenes eg waving trees ripples in water illumination changes camera jitters etc we represent each pixel as a local histogram of figureground segmentations which aims at combining several candidate solutions that are produced by simple bgs algorithms to get a more reliable and robust feature for bgs the background_model of each pixel is constructed as a group of weighted adaptive local histograms of figureground segmentations which describe the structure properties of the surrounding region this is a natural fusion because multiple complementary bgs algorithms can be used to build background models for scenes moreover the correlation of image variations at neighboring pixels is explicitly utilized to achieve robust detection_performance since neighboring pixels tend to be similarly affected by environmental effects eg dynamic scenes experimental results demonstrate the robustness and effectiveness of the proposedmethod by comparing with four representatives of the state of the art in bgs
search space boundary extension method in realcoded genetic_algorithms,in realcoded genetic_algorithms gas some crossover_operators do not work well on functions which have their optimum at the corner of the search space to cope with this problem we have proposed a boundary extension method which allows individuals to be located within a limited space beyond the boundary of the search space in this paper we give an analysis of the boundary extension methods from the viewpoint of sampling bias and perform a comparative study on the effect of applying two boundary extension methods namely the boundary extension by mirroring bem and the boundary extension with extended selection bes we were able to confirm that to use sampling methods which have smaller sampling bias had good performance on both functions which have their optimum at or near the boundaries of the search space and functions which have their optimum at the center of the search space the bessda bes by shortest distance selection with aging had good performance on functions which have their optimum at or near the boundaries of the search space we also confirmed that applying the bessda did not cause any performance degradation on functions which have their optimum at the center of the search space
a new implementation of a halfduplex decodeandforward cooperative algorithm using complete complementary code sets,this paper introduces an implementation of a halfduplex decodeandforward cooperative algorithm using the complete complementary code ccc sets these code sets have an impulsive autocorrelation sum among each set and a cross correlation sum along the set size that vanishes for all shifts each user is assigned a set of spreading_codes to spread his data and send each of the resulting signals on a different subband the decodeandforward cooperative procedure is applied and a receiver consisting of parallel branches matched filter followed by a mrc combiner is used for detection it is demonstrated that the probability of error_performance of the proposed algorithm is very close to the performance of the analytical closedform probability of error in similar transmission conditions the algorithm is also compared with a noncooperative multiband directsequence code_division_multiple_access mb dscdma algorithm using the complete complementary sets the simulation results illustrate the enhanced performance of the proposed algorithm under different channel assumptions
efficient lower bounds and heuristics for the variable cost and size bin_packing_problem,we consider the variable cost and size bin_packing_problem a generalization of the wellknown bin_packing_problem where a set of items must be packed into a set of heterogeneous bins characterized by possibly different volumes and fixed selection costs the objective of the problem is to select bins to pack all items at minimum total binselection cost the paper introduces lower bounds and heuristics for the problem the latter integrating lower and upper bound techniques extensive numerical tests conducted on instances with up to 1000 items show the effectiveness of these methods in terms of computational effort and solution quality we also provide a systematic numerical_analysis of the impact on solution quality of the bin selection costs and the correlations between these and the bin volumes the results show that these correlations matter and that solution_methods that are unbiased toward particular correlation values perform better
an efficiently revised sustain driver for ac plasma display,a new sustain driver employing a stepup function is presented to achieve the faster risetime of sustain voltage which ls suitable for widely used addressanddisplayperiodseparated ads driving method and most of all for cost saving the proposed sustain driver can reduce the number of switching devices by 25  compared to the prior approach improving the overall system efficiency about 10  and brightness decrease problem resulted from the lack of the number of the sustain pulse can be solved without a limitation to sustain pulse_width operational principle and its features are illustrated comparing with conventional approaches the validity of the proposed sustain driver is verified through experiments using a prototype equipped with a 75 inch diagonal panel which is operated at 200 khz switching_frequency
short note an automated_system for the statistical_analysis of sediment texture and structure at the micro scale,a macro has been developed that allows for automated statistical analyses of particles it can be used with binary images from any source and is developed for use with lacustrine marine aeolian and marine sediments the macro code is freely available and runs on opensource software it has been specially designed to accommodate very small regions of interest relative to particle size and to produce continuous downcore stratigraphies the macro runs quickly requires no userinteraction is easily modifiable saves raw data for algorithm checking and further analyses and can run in a mode that quantifies cracks and other disturbances on images the macro allows for rapid analyses of relatively long sedimentary sequences and provides relevant sedimentary interpretations of transport and depositional processes examples of output from two lacustrine sediment records and one marine record are shown
development of a hierarchical saving power system for campus,saving power is the major goal in every area either in industry or in campus however there are more uncertainties in campus for instances the usages of air_conditioners in the classroom are irregular events in the other hand the lighting in the parking area is the regular event for the regular event we use the unstable green energy like wind or solar to save the electric cost for the irregular event we use the stable electricity but managed by dynamic power dispatch system to efficiently handle usages of air_conditioners in the classroom in order to reduce cost this dispatch system is the integration of internet network coded radio frequency database interfacing technology and campus administration computerization servers by using the existing media and devices the setup of this system is without extra expensive cost this hybrid strategy is simulated in a university with about 15000 students and more than 100 classrooms equipped with air_conditioners we can estimate to save more than 10 power than usual
architectural adaptation addressing the criteria of multiple quality_attributes in missioncritical systems,missioncritical software claims safe and robust adaptations that comply with rigorous criteria of multiple critical quality_attributes existing adaptation approaches pay little attention to comprehensively capture mission goals and explicitly specify adaptation requirements we propose an approach to using scenariobased analysis to elicit and specify the criteria of multiple quality_attributes as adaptation invariants and design corresponding architecture variants as facilities implementing adaptations we also present how to make adaptation decisions at runtime
simulationbased atpg for low_power testing of crosstalk delay_faults in asynchronous_circuits,a new multiobjective genetic_algorithm has been proposed for testing crosstalk delay_faults in asynchronous sequential_circuits that reduces average power dissipation during test application the proposed elitist nondominated sorting genetic_algorithm engabased automatic_test_pattern_generation atpg for crosstalk induced delay_faults generates test_pattern set that has high fault_coverage and low switching activity redundancy is introduced in engabased atpg by modifying the fault dropping phase and hence a very good reduction in transition activity is achieved tests are generated for several asynchronous sis benchmark_circuits experimental results demonstrate that enga gives higher fault_coverage reduced transitions and compact test_vectors for most of the asynchronous benchmark_circuits when compared with those generated by weighted sum genetic_algorithm wsga
field measurements of a hybrid dvbsh single_frequency network with an inclined satellite orbit,field measurements of a dvbsh network with both satellite and terrestrial transmitters are presented the system is a single_frequency network transmitting video_streams to vehicular terminals with up to 4 branches of receiver antenna_diversity the signal is transmitted in the sband at 21859 ghz with both time and frequency_synchronization of the terrestrial repeaters with the satellites signal the time and frequency variations are cancelled out at the satellite gateway but because these can not be canceled at all locations and because the satellites position in space is variable in an inclined orbit the terrestrial repeaters are made to cancel the residual time and frequency variation the field measurements include data taken in late 2008 through 2009 with multiple repeaters located in several cities including las vegas nv raleigh and durham nc as well as various morphologies and with elevation angles to the satellite ranging from 25   to 52    this paper presents coverage data for these various morphologies modulation and code_rates as well as various mpeifec interleaver settings used to ameliorate the effects of long shadowed intervals such as when the vehicle goes under highways or bridges where there signal is obscured for several seconds we observed excellent performance in hybrid mode with better than 99 of the measured seconds error free in satelliteonly mode the mpeifec interleaver raised the performance from 81 to 91 averaged over all environments including dense urban
assessing the feasibility of selforganizing maps for data_mining financial information,analyzing financial performance in todays informationrich society can be a daunting task with the evolution of the internet access to massive amounts of financial data typically in the form of financial statements is widespread managers and stakeholders are in need of a datamining tool allowing them to quickly and accurately analyze this data an emerging technique that may be suited for this application is the selforganizing map the purpose of this study was to evaluate the performance of selforganizing maps for analyzing financial performance of international pulp and paper companies for the study financial data in the form of seven financial ratios was collected using the internet as the primary source of information a total of 77 companies and six regional averages were included in the study the time frame of the study was the period 199500 an example analysis was performed and the results analyzed based on information contained in the annual reports the results of the study indicate that selforganizing maps can be feasible tools for the financial analysis of large amounts of financial data
an energyaware video_streaming system for portable computing devices,in this demonstration we show an energyaware video_streaming system which allows users to play back video for the specified duration within the remaining battery amount in the system we execute a proxy server on an intermediate_node in the network it receives the video stream from a content server transcodes it to the videos with appropriate quality and forwards it to a pda or a laptop pc here suitable parameter values of the video such as picture size frame rate and bitrate which enable playback for the specified duration are automatically calculated on the proxy using our battery consumption model the system also allows users to play back video segments with different qualities based on the importance specified to each video segment
a tool kit for lexicon building,this paper describes a set of interactive routines that can be used to create maintain and update a computer lexicon the routines are available to the user as a set of commands resembling a simple operating_system the lexicon produced by this system is based on lexicalsemantic relations but is compatible with a variety of other models of lexicon structure the lexicon builder is suitable for the generation of moderatesized vocabularies and has been used to construct a lexicon for a small medical expert_system a future version of the lexicon builder will create a much larger lexicon by parsing definitions from machinereadable dictionaries
exact phase transition of backtrackfree search with implications on the power of greedy algorithms,backtracking is a basic strategy to solve constraint satisfaction problems csps a satisfiable csp instance is backtrackfree if a solution can be found without encountering any deadend during a backtracking search implying that the instance is easy to solve we prove an exact phase transition of backtrackfree search in some random csps namely in model rb and in model rd this is the first time an exact phase transition of backtrackfree search can be identified on some random csps our technical results also have interesting implications on the power of greedy algorithms on the width of superlinear dense random hypergraphs and on the exact satisfiability threshold of random csps
autonomous climbing of spiral staircases with humanoids,in this paper we present an approach to enable a humanoid_robot to autonomously climb up spiral staircases this task is substantially more challenging than climbing straight stairs since careful repositioning is needed our system globally estimates the pose of the robot which is subsequently refined by integrating visual observations in this way the robot can accurately determine its relative position with respect to the next step we use a 3d_model of the environment to project edges corresponding to stair contours into monocular camera images by detecting edges in the images and associating them to projected model edges the robot is able to accurately locate itself towards the stairs and to climb them we present experiments carried out with a nao humanoid equipped with a 2d laser_range_finder for global localization and a lowcost monocular camera for shortrange sensing as we show in the experiments the robot reliably climbs up the steps of a spiral staircase
lightweight map_matching for indoor localisation using conditional_random_fields,indoor tracking and navigation is a fundamental need for pervasive and contextaware smartphone applications although indoor maps are becoming increasingly available there is no practical and reliable indoor map_matching solution available at present we present mapcraft a novel robust and responsive technique that is extremely computationally efficient running in under 10 ms on an android smartphone does not require training in different sites and tracks well even when presented with very noisy sensor_data key to our approach is expressing the tracking_problem as a conditional_random_field crf a technique which has had great success in areas such as natural_language_processing but has yet to be considered for indoor tracking unlike directed graphical_models like hidden_markov_models crfs capture arbitrary constraints that express how well observations support state transitions given map constraints extensive experiments in multiple sites show how mapcraft outperforms stateofthe art approaches demonstrating excellent tracking_error and accurate reconstruction of tortuous trajectories with zero training effort as proof of its robustness we also demonstrate how it is able to accurately track the position of a user from accelerometer and magnetometer measurements only ie gyro and wififree we believe that such an energyefficient approach will enable alwayson background localisation enabling a new era of locationaware applications to be developed
medialguided fuzzy segmentation,segmentation is generally regarded as partitioning space at the boundary of an object so as to represent the objects shape pose size and topology some images however contain so much noise that distinct boundaries are not forthcoming even after the object has been identified we have used statistical_methods based on medial features in real time 3d echocardiography to locate the left ventricular axis even though the precise boundaries of the ventricle are simply not visible in the data we then produce a fuzzy labeling of ventricular voxels to represent the shape of the ventricle without any explicit boundary the fuzzy segmentation permits calculation of total ventricular volume as well as determination of local boundary equivalencies both of which are validated against manual tracings on 155 left ventricles the method uses a medialbased compartmentalization of the object that is generalizable to any shape
cryptographically secure identity certificates,we present facecerts a simple inexpensive and cryptographically secure identity certification system a facecert is a printout of persons portrait photo an arbitrary textual message and a 2d color barcode which encodes an rsa signature of the message hash and the compressed representation of the face encompassed by the photo the signature is created using the private_key of the party issuing the id verification is performed by a simple intelligent and offline scanning device that contains the public_key of the issuer the system does not require smart_cards more interestingly the id does not need to be printed by a highend printer it can be printed anywhere we present a novel algorithm for compressing faces and investigate the reliability of the crucial components of the system
use of communication technologies in south korean universities,south korean universities are currently at various stages of advancement in their use of electronic communication networking provision for universities is expected to improve considerably over the next few years there is therefore a need to examine their present usage of communication technologies how this depends on the level of facilities available and what the implications are for their future development the study described here investigates usage at a stratified sample of south korean universities via a questionnaire survey it also examines the factors affecting use via a series of interviews the results suggest that though infrastructural limitations are important optimal deployment of communication technologies will require organisational changes within the university system
a 625 ns holographic reconfiguration of an optically differential reconfigurable gate array,to date holographic configuration speeds have remained limited to 16 mus because of issues related to the architecture of optically reconfigurable gate array vlsis orgavlsis therefore to improve the issue optically differential reconfigurable gate array vlsis odrgavlsis have been developed and have achieved zerooverhead and nanosecond optical reconfiguration moreover the architecture of an odrgavlsi has the advantage of accelerating the reconfiguration speed compared to that of other orgas however nanosecond holographic configurations and in particular rapid holographic reconfiguration exploiting the advantages of odrgavlsis have not been reported therefore this paper presents results of the worlds fastest 625 ns holographic reconfiguration exploiting advantages of the odrgavlsi
performance of coherent ask lightwave systems with finite intermediate frequency,the impact of finite intermediate frequency if on the performance of heterodyne ask lightwave systems is examined and quantified in the presence of laser phase_noise and shot noise for negligible linewidths it is shown that certain finite choices of if rsub b3rsub b22rsub b5rsub b2 etc lead to the same ideal biterrorrate ber_performance as infinite choices of if results indicate that for negligible linewidths the worst case sensitivity penalty is 09 db for proper heterodyne_detection and occurs when fsub if125 rsub b for nonnegligible linewidths eg when spl deltaspl nutspl ges004 the sensitivity penalty is always less than 09 db for finite choices of if the analysis presented does lead to a closedform signaltonoise ratio snr expression at the decision gate of the receiver which can readily be used for ber and sensitivity penalty computations the snr expression provided includes all the key system parameters of interest such as system bit rate rsub b the peak if snr spl xi laser linewidth spl deltaspl nu and the if filter expansion factor spl alpha the findings of this work suggest that the number of channels in a multichannel heterodyne ask lightwave system can be increased substantially by properly choosing a small value for the if at the expense of a small penalty 1 db on the negative side if frequency stabilization becomes a more critical requirement in multichannel systems employing small values of if
simulation_and_measurement of narrowband antennas for small terminals,in this paper both normalband and narrowband pifa antennas for small terminals are compared through numerical simulations and measurements for different umts bandwidths it is found that using different antennas for the transmitting and receiving regions of the frequency duplex it is possible to achieve a significant improvement in terms of isolation measurement results show also that ohmic losses lower the total efficiency in the narrowband case especially at low frequencies
an extension to the ordered subcarrier selection algorithm ossa,in this letter we propose an extension to the ordered subcarrier selection algorithm ossa for orthogonal_frequency_division_multiplexing ofdm_systems the result is a simple algorithm for minimizing the bit_error_rate of the ofdm system at a fixed throughput the proposed algorithm employs multiple modulations nonuniform bit loading within an ofdm symbol however unlike existing bit loading algorithms that have a very high computational complexity the proposed algorithm is based only on the ordered statistics of the subcarrier gains and is consequently very simple after ordering the subcarriers based on their gains progressively higher order modulations are used with increasing gains the key aspect here that greatly simplifies the algorithm is that the modulation used on a subcarrier depends only on the position of its gain in the ordered set and not on the actual values of the gains we show an analytical approach for determining the parameters of the algorithm
classifiers for motion,in this paper we present a supervised_learning based approach for subpixel motion_estimation the novelty of this work is the learning based method itself which tries to learn the shifts from a large training database integer pixel shift is subdivided and discretized to levels in both the horizontal and vertical direction we pose the problem of motion_estimation in a polar coordinate system shift estimation in the x and y direction has been posed as a problem of estimating r and thetas the ordinal property of r has been used and consequently we employ a ranking based approach for estimating r for thetas estimation we employ multiclass_classification techniques we demonstrate how very simplistic features can be used to differentiate between different subpixel shifts
performance_analysis of a novel traffic scheduling algorithm in slotted optical_networks,this paper considers the scheduling problem in a new slotted optical_network called timedomain wavelength interleaved network twin the twin architecture possesses interesting properties which may offer solutions for nextgeneration optical_networks besides better quality_of_service qos could be achieved in twin by minimizing two parameters queueing delay and delay variance however to the best of our knowledge most of the existing scheduling algorithms in twin ignored consideration of qos and focused mainly on maximizing the throughput in this paper we formulate the scheduling problem into an integer linear programming ilp problem and propose a novel heuristic  destination slot set dss algorithm to solve it fast and efficiently besides we derive an analytical model for twin and investigate the performance of dss in it by means of simulations we demonstrate that our analytical model approximates the twin network very well moreover dss incurs smaller queueing delay and delay variance which ensures better qos
motion filter vector_quantization,motioncompensated prediction of video is formulated as a novel vector_quantization scheme called motion filter vector_quantization mfvq in mfvq the motion_vector and the pixelintensity interpolation filter are combined into a motion filter and the entire filter is vector quantized a codebook_design algorithm is proposed for designing unit gain and entropy constrained mfvq codebooks the algorithm is tested under two application configurations mfvq with static codebook and mfvq with forwardadaptive codebook and is shown to furnish up to a db of psnr gain
towards a realtime navigation strategy for a mobile_robot,describes the design of a realtime obstacle_avoidance and navigation strategy for a mobile_robot using simulated time of flight infrared data algorithms have been developed in order to overcome the undesirable effect of potential traps within the field and a new approach for dealing with sensing whilst moving is demonstrated the authors show simulated results of a vehicle moving under artificial potential force equations which is designed to achieve modification of behaviour at a speed close to the normal operational speed of a real mobile 
development of a telepresence controlled ambidextrous robot for space applications,a masterslave system is developed to evaluate the effectiveness of telepresence in space telerobotics applications an operator uses the masters telepresence and virtual_reality equipment to control the slave the slave is a dualarm dualhand robot equipped with a stereo_camera platform designed to provide an operatorcentered perspective of the remote_environment the initial integration and tests of the system presented several operational issues that are resolved through a variety of shared control techniques and optimized algorithms the resulting system provides a very flexible capability and is used to perform a range of tasks grasping and handling toots manipulating electronics controls manipulating soft flexible material and performing planetary geology tasks that involve a variety of manipulation and tooluse skills using this system an operator is able to complete most of these tasks in less than 2 minutes
natural hand_posture_recognition based on zernike moments and hierarchical classifier,viewindependence and userindependence are two fundamental requirements for hand_posture_recognition during natural humanrobot interaction however only a few research concerns on the two issues simultaneously the difficulty for natural gesturebased humanrobot interaction lies in that appearances of the same hand_posture vary with different users from different viewing directions in this paper we propose a systematic feature_selection approach based on zernike moments and isomap dimensionality_reduction a hierarchical classifier based on multivariate decision_tree and piecewise linearization is developed to deal with the irregular distribution of the same hand postures the proposed method is compared with other commonly used ones in hand_posture_recognition experimental results indicate that the proposed method can effectively identify different hand postures irrespective of viewing directions and users
unsupervised feature_learning for visual sign language_identification,prior research on language_identification focused primarily on text and speech in this paper we focus on the visual modality and present a method for identifying sign languages solely from short video samples the method is trained on unlabelled video data unsupervised feature_learning and using these features it is trained to discriminate between six sign languages supervised_learning we ran experiments on short video samples involving 30 signers about 6 hours in total using leaveonesignerout crossvalidation our evaluation shows an average best accuracy of 84 given that sign languages are underresourced unsupervised feature_learning techniques are the right tools and our results indicate that this is realistic for sign language_identification
association control in mobile wireless_networks,as mobile_nodes roam in a wireless_network they continuously associate with different access_points and perform handoff operations however frequent handoffs can potentially incur unacceptable delays and even interruptions for interactive applications to alleviate these negative impacts we present novel association control algorithms that can minimize the frequency of handoffs occurred to mobile_devices specifically we show that a greedy lookahead algorithm is optimal in the offline setting where the users future mobility is known inspired by such optimality we further propose two online_algorithms namely lookback and track that operate without any future mobility information instead they seek to predict the lifetime of an association using randomization and statistical approaches respectively we evaluate the performance of these algorithms using both analysis and tracedriven simulations the results show that the simple lookback algorithm has surprisingly a competitive_ratio of log k  2 where k is the maximum number of aps that a user can hear at any time and the track algorithm can achieve nearoptimal performance in practical scenarios
debugging the internet_of_things the case of wireless_sensor_networks,the internet_of_things iot has the strong potential to support a human society interacting more symbiotically with its physical environment indeed the emergence of tiny devices that sense environmental cues and trigger actuators after consulting logic and human preferences promises a more environmentally aware and less wasteful society however the iot inherently challenges software development_processes particularly techniques for ensuring software_reliability researchers have developed debugging_tools for wireless_sensor_networks wsns which can be viewed as the enablers of perception in the iot these tools gather runtime information on individual sensor_node executions and node interactions and then compress that information
two empirical studies of computersupported collaborative_learning in science methodological and affective implications,in this paper the results and implications of two studies of computersupported collaborative_learning are presented and implications discussed the first study was an experimental study in a british secondary school while the second study followed a group of primary school children in a naturalistic context assessing learning situations is discussed with an emphasis on the affective factors the differences between the products the interactions and the outcomes of learning situations are discussed along with the research methodology there is an emphasis on pre and posttesting naturalistic and experimental studies and timebased analyses
acquisition of projectspecific assets with bayesian updating,we study the impact of learning on the optimal_policy and the timetodecision in an infinitehorizon bayesian sequential decision_model with two irreversible alternatives exit and expansion in our model a firm undertakes a smallscale pilot project to learn via bayesian updating about the projects profitability which is known to be in one of two possible states the firm continuously observes the projects cumulative profit but the true state of the profitability is not immediately revealed because of the inherent noise in the profit stream the firm bases its exit or expansion decision on the posterior probability_distribution of the profitability the optimal_policy is characterized by a pair of thresholds for the posterior_probability we find that the timetodecision does not necessarily have a monotonic relation with the arrival rate of new information
a rule repository for active database_systems,active database_systems adbss provides a good infrastructure to define and execute active rules nevertheless this infrastructure offered by adbss does not completely satisfy the necessities of rules management that demands current business applications rules also need to be stored in appropriate structures to facilitate their management as the existing structures for data in these systems this work proposes a rule repository composed by structures that allow the storage and organization of rules in order to facilitate their management for this purpose a rule classification with the main rule types existing in the literature is presented and then it represents the characteristics and anatomy of each type in a metamodel with the goal of analyzing the data that must be stored about rules the rule repository proposed in this paper has been built based on this metamodel
decentralized group formation,imagine a network of entities being it replica servers aiming to minimize the probability of data loss players of online teambased games and tournaments or companies that look into cobranding opportunities the objective of each entity in any of these scenarios is to find a few suitable partners to help them achieve a shared goal replication of the data for fault_tolerance winning the game successful marketing campaign all information attainable by the entities is limited to the profiles of other entities that can be used to assess the pairwise fitness how can they create teams without help of any centralized component and without going into each others way we propose a decentralized algorithm that helps nodes in the network to form groups of a specific size the protocol works by finding an approximation of a weighted kclique matching of the underlying graph of assessments we discuss the basic version of the protocol and explain how dissemination via gossiping helps in improving its scalability we evaluate its performance through extensive_simulations
efficient blind compressed sensing using sparsifying transforms with convergence guarantees and application to mri,natural signals and images are wellknown to be approximately sparse in transform domains such as wavelets and dct this property has been heavily exploited in various applications in image_processing and medical_imaging compressed sensing exploits the sparsity of images or image_patches in a transform_domain or synthesis dictionary to reconstruct images from undersampled measurements in this work we focus on blind compressed sensing where the underlying sparsifying transform is a priori unknown and propose a framework to simultaneously reconstruct the underlying image as well as the sparsifying transform from highly undersampled measurements the proposed block coordinate_descent type algorithms involve highly efficient optimal updates importantly we prove that although the proposed blind compressed sensing formulations are highly nonconvex our algorithms are globally convergent ie they converge from any initialization to the set of critical points of the objectives defining the formulations these critical points are guaranteed to be at least partial global and partial local minimizers the exact points of convergence may depend on initialization we illustrate the usefulness of the proposed framework for magnetic resonance image_reconstruction from highly undersampled kspace measurements as compared to previous methods involving the synthesis dictionary model our approach is much faster while also providing promising reconstruction_quality
optimizing local pickup_and_delivery with uncertain loads,the local pickup_and_delivery problem lpdp is an essential operational problem in intermodal industry while the problem with deterministic settings is already difficult to solve in reality there exist a set of loads called uncertain loads which are unknown at the beginning of the day but customers may call in during the day to materialize these loads in this paper we call the lpdp considering these uncertain loads as the stochastic lpdp the problem description and the mathematical modeling of stochastic lpdp are discussed then a simulationbased optimization approach is proposed to solve the problem which features in a fast solution generation procedure and an intelligent simulation budget allocation framework the numerical_examples show the best strategy to consider the stochastic loads in the planning process and validate the benefits compared to its deterministic counterpart
objectbased coding for plenoptic videos,a new objectbased coding system for a class of dynamic imagebased representations called plenoptic videos pvs is proposed pvs are simplified dynamic light fields where the videos are taken at regularly spaced locations along line segments instead of a 2d plane in the proposed objectbased approach objects at different depth values are segmented to improve the rendering quality by encoding pvs at the object level desirable functionalities such as scalability of contents error_resilience and interactivity with an individual imagebased rendering ibr object can be achieved besides supporting the coding of texture and binary shape maps for ibr objects with arbitrary shapes the proposed system also supports the coding of grayscale alpha maps as well as depth maps geometry information to respectively facilitate the matting and rendering of the ibr objects both temporal and spatial redundancies among the streams in the pv are exploited to improve the coding_performance while avoiding excessive complexity in selective decoding of pvs to support fast rendering speed advanced spatialtemporal prediction methods such as global disparitycompensated prediction as well as direct prediction and its extensions are developed the bit_allocation and rate_control_scheme employing a new convex optimizationbased approach are also introduced experimental results show that considerable improvements in coding_performance are obtained for both synthetic and real_scenes while supporting the stated objectbased functionalities
adaptive reconstruction method of missing textures based on inverse projection via sparse representation,this paper presents an adaptive reconstruction method of missing textures based on an inverse projection via sparse representation the proposed method approximates original and corrupted textures in lowerdimensional subspaces by using the sparse representation technique then this approach effectively solves problems of not being able to directly estimate an inverse projection for reconstructing missing textures furthermore even if target textures contain missing areas the proposed method enables adaptive generation of the subspaces by monitoring errors caused in their known neighboring textures by the estimated inverse projection consequently since the optimal inverse projection is adaptively estimated for each texture successful reconstruction of the missing areas can be expected experimental results show impressive improvement of the proposed reconstruction technique over previously reported reconstruction techniques
large margin nonlinear embedding,it is common in classification_methods to first place data in a vector space and then learn decision boundaries we propose reversing that process for fixed decision boundaries we learn the location of the data this way we i do not need a metric or even stronger structure  pairwise dissimilarities suffice and additionally ii produce lowdimensional embeddings that can be analyzed visually we achieve this by combining an entropybased embedding method with an entropybased version of semisupervised logistic_regression we present results for clustering and semisupervised classification
scientific misconduct three forms that directly harm others as the modus operandi of mills tyranny of the prevailing opinion,scientific misconduct is usually assumed to be selfserving this paper however proposes to distinguish between two types of scientific misconduct type one scientific misconduct is selfserving and leads to falsely positive conclusions about ones own work while type two scientific misconduct is otherharming and leads to falsely negative conclusions about someone elses work the focus is then on the latter type and three known issues are identified as specific forms of such scientific misconduct biased quality assessment smear and officially condoning scientific misconduct these concern the improper ways how challenges of the prevailing opinion are thwarted in the modern world the central issue is pseudoskepticism uttering negative conclusions about someone elses work that are downright false it is argued that this may be an emotional response rather than a calculated strategic action recommendations for educative and punitive measures are given to prevent and to deal with these three forms of scientific misconduct
yield curve shapes and the asymptotic short rate distribution in affine onefactor models,we consider a model for interest rates where the short rate is given under the riskneutral measure by a timehomogeneous onedimensional affine process in the sense of duffie filipovic and schachermayer we show that in such a model yield curves can only be normal inverse or humped ie endowed with a single local maximum each case can be characterized by simple conditions on the present short rate rt we give conditions under which the short rate process converges to a limit distribution and describe the riskneutral limit distribution in terms of its cumulant generating function we apply our results to the vasicek model the cir model a cir model with added jumps and a model of ornsteinuhlenbeck type
multimedia descriptions based on mpeg7 extraction and applications,the amount of digital multimedia_content available to consumers is growing because of the existence of digital capturing devices such as digital cameras camcorders and the advent of digital_video broadcast with this increase in content it becomes important for users to be able to browse and search for content in a timely manner descriptions and annotations of the content are needed to enable searching and browsing of content mpeg7 is a recent iso standard for multimedia_content description in this paper we present three descriptors which we had proposed to mpeg7 and have been accepted to the standard in addition we describe algorithms for automatically extracting these descriptors we also present the indexing and retrieval_algorithms that we developed for these descriptors these algorithms are fast and scalable for large databases the results presented in the paper for image and video segment matching using these descriptors show their usefulness in real applications
3d shape reconstruction using volume intersection techniques,volume intersection algorithms are used to reconstruct incomplete objects from their silhouettes an imagined light source is moved about the data and the cumulative amount of light seen at each point an space is interpreted as indicating the likelihood that the point is inside the object the object data need not be uniformly distributed nor exclusively surface data explicit distinction between noise surface and interior data is avoided the novel concept of a localised viewing region is introduced to overcome the inherent inability of volume intersection algorithms to reconstruct concave surfaces algorithms for 2d pixel and 3d voxel data are described and applied to 3d ultrasound data
characterizing and mitigating interdomain policy violations in overlay routes,the internet is a complex structure arising from the interconnection of numerous autonomous systems as each exercising its own administrative policies to reflect the commercial agreements behind the interconnection however routing in service_overlay_networks is quite capable of violating these policies to its advantage to prevent these violations we see an impending drive in the current internet to detect and filter overlay traffic in this paper we first present results from a case study overlay_network constructed on top of planetlab that helps us gain insights into the frequency and characteristics of the different interdomain policy violations we further investigate the impact of two types of overlay traffic filtering that aim to prevent these routing policy violations blind filtering and policy aware filtering we show that such filtering can be detrimental to the performance of overlay_routing we next consider two approaches that allow the overlay_network to realize the full advantage of overlay_routing in this context in the first approach overlay_nodes are added so that good overlay paths do not represent interdomain policy violations in the second approach the overlay acquires transit permits from certain ases that allow certain policy violations to occur we develop a single costsharing framework that allows the incorporation of both approaches into a single strategy we formulate and solve an optimization problem that aims to determine how the overlay_network should allocate a given budget between paying for additional overlay_nodes and paying for transit permits to ases we illustrate the use of this approach on our case study overlay_network and evaluate its performance under varying network characteristics
a sensor_fusion framework using multiple particle filters for videobased navigation,this paper presents a sensorfusion framework for videobased navigation videobased navigation offers the advantages over existing approaches with this type of navigation road signs are directly superimposed onto the video of the road scene as opposed to those superimposed onto a 2d map as is the case with conventional navigation_systems drivers can then follow the virtual signs in the video to travel to the destination the challenges of videobased navigation require the use of multiple sensors the sensorfusion framework that we propose has two major components 1 a computer_vision module for accurately detecting and tracking the road by using partition sampling and auxiliary variables and 2 a sensorfusion module using multiple particle filters to integrate vision global positioning_systems gpss and geographical_information_systems giss gps and gis provide prior knowledge about the road for the vision module and the vision module in turn corrects gps errors
an evolution programme for the resourceconstrained project scheduling problem,this paper describes an implementation of an evolution programme for the resourceconstrained project scheduling problem in essentials the problem consists of two issues a to determine the order of activities without violating precedence constraints and b subsequently to determine earliest start time for each activity according to available resources how to determine the order of activation is critical to the problem because if the order is determined a schedule can be easily constructed with some determining procedures the basic ideas of the proposed approach are a using an evolution programme to evolve an appropriate order of activities and b using a fitinbest procedure to calculate the earliest start times of activities a new approach is addressed to guide how to design genetic_operators one operator is designed to perform a wide spread search to try to explore the area beyond local optima whereas the other is designed to perform an intensive search to try to find an improved solut
a methodology for evaluating the accuracy of wave field rendering techniques,in this paper we propose a methodology for assessing the accuracy of techniques of wave field rendering through loudspeaker arrays in order to measure the rendered wave field we adopt a solution based on a circular harmonic_analysis of the sound field captured by a virtual microphone_array as a result of this analysis stage we are able to compare the target the theoretical and the measured wave fields which may differ due to the nonideality in the loudspeaker array or in the environment that generates some spurious reverberations moreover in order to quantify the error between target theoretical and measured wave fields we define some evaluation metrics based on rmse and modal analysis of the acquired wave fields we show some experimental results on real data
rectangular vsplines,this article describes and presents examples of some techniques for the representation and interactive design of surfaces based on a parametric surface representation that user vspline curves these vspline curves similar in mathematical structure to vsplines were developed as a more computationally efficient alternative to splines in tension although splines in tension can be modified to allow tension to be applied at each control_point the procedure is computationally expensive the vspline curve however uses more computationally tractable piecewise cubic curves segments resulting in curves that are just as smoothly joined as those of a standard cubic_spline after presenting a review of vsplines and some new properties this article extends their application to a rectangular grid of control_points three techniques and some application examples are presented
downlink joint basestation assignment and packet_scheduling_algorithm for cellular cdmatdma networks,in this paper using a utilitybased approach downlink packet transmission in a cdmatdma cellular_network is formulated as an optimization problem a utility_function corresponds to each packet served by a basestation that is an increasing function of the packet experienced delay and the channel gain and a decreasing function of the basestation load unlike previous works in this paper the optimization objective is to maximize the total network utility instead of the basestation utility we show that this optimization results in joint basestation assignment and packet_scheduling therefore in addition to multiuser_diversity the proposed method also exploits multiaccesspoint diversity and soft capacity a polynomial time heuristic algorithm is then proposed to solve the optimization problem simulation results indicate a significant performance improvement in terms of packetdropratio and achieved throughput
enhanced routingaware adaptive mac with traffic differentiation and smoothed contention_window in wireless adhoc networks,in wireless mobile adhoc networks data packets have to be relayed hop by hop from a given source node to a destination node this means that some or all of the mobile_nodes must accept to forward information for the benefit of other nodes it has been shown by f naitabdesselam et al 2003 that this ability of forwarding packets leads to a new unfairness problem in wireless adhoc networks where a node which is forwarding other nodes packets gets less bandwidth for its own use than a node which is not participating to the routing service the proposed ramac scheme by f naitabdesselam et al 2003 has shown all its effectiveness to cope with this unfairness problem however an extra bandwidth is sometimes allowed to the routing nodes own traffic comparing to other nonrouting nodes own traffics and the routing nodes routed traffic gets much less bandwidth this is explained by the fact that at the upper layer for instance the ip layer does not differentiate between the routing and the own traffics and that the multiplicative factor used to compute the new contention_window is too aggressive in this paper an enhanced ramac scheme is proposed by taking into account the differentiation on top of the mac_layer between the and routed traffics within a routing node and by smoothing the multiplicative factor used to compute the new contention_window the simulation results obtained showed a good improvement of the original ramac scheme leading to better approximate equal bandwidth share among all the mobile_nodes in the wireless adhoc network
meaning of pearson residuals linear algebra view,marginal distributions play an central role in statistical_analysis of a contingency table however when the number of partition becomes large the contribution from marginal distributions decreases this paper focuses on a formal analysis of marginal distributions in a contingency table the main approach is to take the difference between two matrices with the same sample size and the same marginal distributions which we call difference matrix the important nature of the difference matrix is that the determinant is equal to 0 when the rank of a matrix is r the difference between a original matrix and the expected matrix will become r  1 at most since the sum of rows or columns of the will become zero which means that the information of one rank corresponds to information on the frequency of a contingency matrix interestingly if we take an expected matrix whose elements are the expected values based on marginal distributions the difference between an original matrix and expected matrix can be represented by linear combination of determinants of 2 times 2 submatrices
an experimental study of quartets maxcut and other supertree methods,backgroundrnsupertree methods represent one of the major ways by which the tree of life can be estimated but despite many recent algorithmic innovations matrix representation with parsimony mrp remains the main algorithmic supertree method
online knowledgebased simulation for fms a state of the art survey,simulation modeling has been widely used to study many aspects of fms design planing and control yet simulation modeling still offers other capabilities that are proven to be effective for the online control of fms an example of it is the training of neural nets offline so that it will later control the decision making process when the fms is in operation if the neural net runs out of knowledge it turns back to a simulation model to learn new situations in this paper we review the various ways in which online knowledgebased simulation for fms has been approached this paper represents the early stages of ongoing research efforts at florida international university
study on the management models of urban multiethnic community in northwestern cities of china  the case of lanzhou,urban multiethnic community is a special type of community which has a high degree of heterogeneity thus a study on the management of these communities will be of theoretical and practical significance this paper took an urban multiethnic community of lanzhou as example investigating on the management of multiethnic communities in northwestern cities based on the theory of social_relationship using qualitative and case study methods this paper has probed into the management models causes further development and other issues s in multiethnic communities in the northwestern cities of china
a crosslayer adaptation for voip over infrastructure mesh_network,the deployment of wireless_mesh paradigm was meant to extend internet access without a consideration of delay_sensitive applications none the less since voice_over_ip voip services are rapidly increasing in popularity ieee 80211 based wireless_mesh_networks are challenged with the provision of guaranteed quality voip calls in this paper the disquiet on voip_systems caused by physical phy and medium_access_control mac anomaly in the current wireless_mesh deployment is addressed through a crosslayer scheme the scheme is aimed at enhancing voip call capacity by mitigating phy and mac overheads through aggregation of packets of the same next hop through simulations it is shown that the proposed scheme has significant performance improvements while leaving the ieee 80211 standard intact
combining variants of iterative flattening search,iterative flattening search ifs is an iterative improvement heuristic schema for makespan minimization in scheduling problems given an initial_solution ifsiteratively interleaves a relaxationstep which randomly retracts some search decisions and an incremental solving step or flatteningstep to recompute a new solution the process continues until a stop condition is met and the best solution found is returned in recent work we have created a uniform software_framework to analyze component techniques that have been proposed in ifsapproaches in this paper we combine basic components to obtain hybrid variants and perform a detailed experimental evaluation of their performance specifically we examine the utility of 1 operating with different relaxation strategies and 2 using different searching strategies to built a new solution we present a twostep experimental evaluation a an extensive explorative evaluation with a spectrum of parameter combination b a timeintensive evaluation of the best ifscombinations emerged from the previous the experimental results shed light on weaknesses and strengths of the different variants improving the current understanding of this family of metaheuristics
a multielement tactile_feedback system for robotassisted minimally invasive surgery,a multielement tactile_feedback mtf system has been developed to translate the force distribution in magnitude and position from 3times2 sensor arrays on surgical robotic endeffectors to the fingers via 3times2 balloon tactile_displays high detection accuracies from perceptual tests  96 suggest that mtf may be an effective means to improve robotic control
can brand extension signal product quality,this paper asks whether brand extension can serve as a signal of product quality given that it costs less than a new brand existing literature has assumed either that brand extension is costneutral or that it costs more i show that it can as a perfect bayesian equilibrium but the argument is unconvincing for one thing the separating equilibrium is not unique a pooling equilibrium also exists in which brand extension signals nothing for another the separating equilibrium relies on offequilibrium beliefs that are poorly motivated in the model i propose a refinement of the perfect bayesian equilibrium that resolves both issues empirical offequilibrium beliefs require that consumers offequilibrium beliefs be justifiable on the basis of their prior beliefs and product performance observations with empirical offequilibrium beliefs two necessary conditions for brand extension to signal product quality are identified i consumers must perceive old and new products of the firm to be positively correlated in quality and ii at least some consumers must identify with brands and not the firm behind the brands even with these conditions in place the signaling argument is fragile firm observability of past performance diminishes brand extensions signaling capability an arbitrarily small probability of failure for good products eliminates it my results suggest that going forward the case for brand extension must rest on foundations other than signaling product quality
community_detection using a neighborhood strength driven label_propagation algorithm,studies of community structure and evolution in large social_networks require a fast and accurate algorithm for community_detection as the size of analyzed communities grows complexity of the community_detection algorithm needs to be kept close to linear the label_propagation algorithm lpa has the benefits of nearlylinear running time and easy implementation thus it forms a good basis for efficient community_detection methods in this paper we propose new update rule and label_propagation criterion in lpa to improve both its computational_efficiency and the quality of communities that it detects the speed is optimized by avoiding unnecessary updates performed by the original algorithm this change reduces significantly by order of magnitude for large networks the number of iterations that the algorithm executes we also evaluate our generalization of the lpa update rule that takes into account with varying strength connections to the neighborhood of a node considering a new label experiments on computer generated networks and a wide range of social_networks show that our new rule improves the quality of the detected communities compared to those found by the original lpa the benefit of considering positive neighborhood strength is pronounced especially on realworld networks containing sufficiently large fraction of nodes with high clustering_coefficient
semidefinite programming based algorithms for the sparsest cut problem,in this paper we analyze a known relaxation for the spars est cut problem based on positive semidefinite constraints and we present a branch and bound algorithm and heuristics based on this relaxation the relaxed formulation and the algorithms were tested on small and moderate sized instances it leads to values very close to the optimum solution values the exact_algorithm could obtain solutions for small and moderate sized instances and the best heuristics obtained optimum or near optimum solutions for all tested instances the semidefinite_relaxation gives a lower bound cw and each heuristic produces a cut s with a ratio c s w s  where either c s  is at most a factor of c or w s  is at least a factor of w we solved the semidefinite_relaxation using a semiinfinite cut generation with a commercial linear programming package adapted to the sparsest cut problem we showed that the proposed strategy leads to a better performance compared to the use of a known semidefinite programming solver
asynchronous comparisonbased decoders for delayinsensitive codes,a comparisonbased decoder detects the arrival of a code word by comparing the received checkbits with the checkbits computed using the received data implementation issues underlying comparisonbased decoders for systematic delayinsensitive di or unordered codes is the subject of this paper we show that if the decoder is to be implemented using asynchronous logic ie if the gate and wire delays are arbitrary unbounded but finite then it is impossible to design a comparisonbased decoder for any code that is more efficient than a dualrail code in other words the encoded word must contain at least twice as many bits as the data in addition the codes should satisfy two other properties called the initial condition and the allzero lower triangle azlt property for the realization of a delayinsensitive comparisonbased decoder the paper shows that comparisonbased decoders for codes that have the requisite level of redundancy and that satisfy the two properties can be implemented using asynchronous logic
analysis of an adaptive sic for nearfar resistant dscdma,a new multiuser_detection scheme is proposed which employs adaptive minimum_mean_square_error mmse detection in combination with successive_interference_cancellation sic through theoretical analysis and numerical_examples it is shown that the proposed detector provides superior performance to existing ones in terms of asymptotic multiuser efficiency ame and bit_error_rate ber
rex a randomized exclusive region based scheduling scheme for mmwave wpans with directional_antenna,millimeterwave mmwave transmissions are promising technologies for high data rate multigbps wireless_personal_area_networks wpans in this paper we first introduce the concept of exclusive region er to allow concurrent transmissions to explore the spatial_multiplexing gain of wireless_networks considering the unique characteristics of mmwave communications and the use of omnidirectional or directional antennae we derive the er conditions which ensure that concurrent transmissions can always outperform serial tdma transmissions in a mmwave wpan we then propose rex a randomized er based scheduling scheme to decide a set of senders that can transmit simultaneously in addition the expected number of flows that can be scheduled for concurrent transmissions is obtained analytically extensive_simulations are conducted to validate the analysis and demonstrate the effectiveness and efficiency of the proposed rex scheduling scheme the results should provide important guidelines for future deployment of mmwave based wpans
year 5 pupils reading an interactive storybook on cdrom losing the plot,the use of interactive storybooks in the primary classroom may facilitate small group and individual reading with minimal teacher intervention this smallscale study examines whether small groups of year 5 pupils without teacher supervision progress linearly through an interactive storybook and whether such diversions as cued animations affect pupil comprehension the study finds that more intensive choice of diversions affects some pupils comprehension
using texture mapping with mipmapping to render a vlsi layout,this paper presents a method of using texture mapping with mipmapping to render a vlsi layout  texture mapping is used to save already rasterized areas of the layout from frame to frame and to take advantage of any hardware accelerated capabilities of the host platform mipmapping is used to select which textures to display so that the amount of information sent to the display is bounded and the image rendered on the display is filtered correctly  additionally two caching schemes are employed  the first used to bound memory consumption is a general_purpose cache that holds textures spatially close to the users current viewpoint the second used to speed up the rendering process is a cache of heavily used subdesigns that are precomputed so rasterization on the fly is not necessary  an experimental implementation shows that realtime navigation can be achieved on arbitrarily large designs results also show how this technique ensures that image_quality does not degrade as the number of polygons drawn increases avoiding the aliasing artifacts common in other layout systems
matrix inversion on cpugpu platforms with applications in control_theory,in this paper we tackle the inversion of largescale dense_matrices via conventional matrix_factorizations lu cholesky ldl t  and the gaussjordan method on hybrid platforms consisting of a multicore cpu and a manycore graphics_processor gpu specifically we introduce the different matrix inversion algorithms using a unified framework based on the notation from the flame project we develop hybrid implementations for those matrix operations underlying the algorithms alternative to those in existing libraries for singlegpu systems and we perform an extensive experimental study on a platform equipped with stateoftheart generalpurpose architectures from intel and a fermi gpu from nvidia that exposes the efficiency of the different inversion approaches our study and experimental results show the simplicity and performance advantage of the gjebased inversion methods and the difficulties associated with the symmetric indefinite case
opportunistic_relaying in inhome plc networks,we consider the use of a relay to provide capacity improvements and range extension for inhome power line communication_networks in particular we focus on opportunistic_relaying where the relay is exploited only if it provides improved capacity wrt the use of direct transmission between the source and the destination the relay applies a decode_and_forward scheme and the channel is shared in a time_division_multiple_access mode the performance is studied in statistically representative inhome power line communication plc networks via the use of a statistical topology model together with the application of transmission line theory for the computation of the channel transfer function among network nodes the statistical topology model allows determining the capacity improvements as a function of the relay position furthermore we determine the optimal time slot duration for each considered relay configuration as well as we propose the use of a globally optimal time slot duration that maximizes the average network capacity the numerical_results show that significant capacity improvement can be obtained via opportunistic_relaying in inhome plc networks the gains are more significant for low_snr scenarios and for networks composed by subnetworks each connected to the main panel via a circuit breaker that introduces signal attenuation
speech ogle indexing uncertainty for spoken document search,the paper presents the position specific posterior lattice pspl a novel lossy representation of automatic_speech_recognition lattices that naturally lends itself to efficient indexing and subsequent relevance ranking of spoken documentsin experiments performed on a collection of lecture recordings  mit icampus data  the spoken document ranking accuracy was improved by 20 relative over the commonly used baseline of indexing the 1best output from an automatic speech recognizerthe inverted index built from pspl lattices is compact  about 20 of the size of 3gram asr lattices and 3 of the size of the uncompressed speech  and it allows for extremely fast retrieval furthermore little degradation in performance is observed when pruning pspl lattices resulting in even smaller indexes  5 of the size of 3gram asr lattices
performance evaluation for different suggested wireless_atm systems deployed at a wireless_channel with abrupt fading conditions,in this paper we suggest and compare the performance of different systems that can be used for wireless asynchronous_transfer_mode watm to improve performance in terms of cell loss ratio and throughput under a certain fading wireless_channel for a certain period of time the type of fading considered in this paper is a result of a sudden and a sharp change in the signal level due to a sudden change in the weather condition eg lightning or due to a multipath effect caused by a mobile obstacle it was found that compressing the atm cell header and then applying the bosechaudhurihocquenghem bch i18i3 code to the compressed header during the fading period when the bit_error_rate ranges between 10sup 3 and 10sup 1 results in an optimum performance in terms of cell loss ratio and throughput
iminer a web_usage_mining framework using hierarchical intelligent_systems,recently web_mining has become a hot research topic which combines two of the prominent research areas comprising of data_mining and the world_wide_web www web_usage_mining attempts to discover useful knowledge from the secondary data obtained from the interactions of the users with the web web_usage_mining has become very critical for effective web site management business and support services personalization network_traffic flow analysis and so on our previous study on web_usage_mining using a concurrent neurofuzzy approach has shown that the usage trend analysis very much depends on the performance of the clustering of the number of requests in this paper a novel approach intelligentminer iminer is introduced to optimize the concurrent architecture of a fuzzy clustering_algorithm to discover data clusters and a fuzzy_inference_system to analyze the trends in the concurrent neurofuzzy approach selforganizing maps were used to cluster the web user requests a hybrid evolutionary fcm approach is proposed in this paper to optimally segregate similar user_interests the clustered data is then used to analyze the trends using a takagisugeno fuzzy_inference_system learned using a combination of evolutionary_algorithm and neural_network learning empirical results clearly shows that the proposed technique is efficient with lesser number of ifthen rules and improved accuracy at the expense of complicated algorithms and extra computational cost
preceding car tracking using belief functions and a particle filter,this article presents a preceding car rear view tracking_algorithm which utilizes a particle filter and belief function data_fusion most of tracking applications resort to only one source of information making the system dependent on the source reliability to achieve more robust and longer tracking multiple source data_fusion is a solution belief functions are a powerful tool for data_fusion using bridges between probability theory and belief function theory data_fusion information can be incorporated inside a particle filter the efficiency of the proposed method is demonstrated on natural onroad sequences
performance_analysis for bicm transmission over gaussian_mixture noise fading_channels,bitinterleaved coded_modulation bicm has been adopted in many systems and standards for spectrally efficient coded transmission the analytical evaluation of bicm performance parameters in particular biterror rate ber has received considerable attention in the recent past in this paper we derive ber approximations for bicm transmission over general fading_channels impaired by gaussian_mixture noise gmn to this end we build upon the saddlepoint approximation of the pairwise error probability pep and a recently established approximation for the probability_density_function pdf of bitwise reliability metrics for nonfading additive_white_gaussian_noise awgn channels we extend this pdf approximation to the case of gmn and obtain closedform expressions for its laplace transform for fading gmn channels the latter allows us to express the pep and thus ber via the saddlepoint approximation for the special case of fading awgn channels the presented approximations are closed_form since the saddlepoint is well approximated by 12 for bicm decoding furthermore we derive closedform pep expressions also for gmn channels in the high signaltonoise ratio regime and establish the diversity and coding_gain for bicm transmission over fading gmn channels selected numerical_results for the ber of convolutional coded bicm highlight the usefulness of the proposed approximations and the differences between awgn and gmn channels
a staggered fec system for seamless handoff in wireless_lans implementation experience and experimental study,we report the implementation experience and experimental evaluation of a staggered adaptive forward error correction fec system for video multicast over wireless_lans in the system the parity packets generated by a crosspacket fec code are transmitted at a time_delay from the original video_packets ie staggercasting video stream and fec stream in different multicast_groups the delay provides temporal diversity to improve the robustness of video multicast especially to enable the clients to correct burst packet_loss using fec and to achieve seamless handoff a wireless client dynamically joins the fec multicast_groups based upon its channel_conditions and handoff events we have implemented the system including the streaming server and client proxy a novel software_architecture is designed to integrate the fec functionality in the clients without requirement for changing the existing video player software we conduct extensive experiments to investigate the impact of fec overhead and the delay between the video stream and fec stream to the video_quality under different interference levels and mobile handoff durations the efficacy of staggered adaptive fec system on improving video multicast quality is demonstrated in real system implementation
an adaptive feedforward compensation algorithm for active vibration control,adaptive feedforward broadband vibration or noise compensation is currently used when an image of the disturbance is available however in most of the systems there is a positive feedback coupling between the compensator system and the measurement of the image of the disturbances the paper proposes a new algorithm taking in account this coupling effect and the corresponding analysis the algorithm has been applied to an active vibration control avc system and real time results are presented
explaining robust additive utility models by sequences of preference swaps,as decisionaiding tools become more popular everydaybut at the same time more sophisticatedit is of utmost importance to develop their explanatory capabilities some decisions require careful explanations which can be challenging to provide when the underlying mathematical model is complex this is the case when recommendations are based on incomplete expression of preferences as the decisionaiding tool has to infer despite this scarcity of information this step is key in the process but hardly intelligible for the user the robust additive utility model is a necessary preference relation which makes minimal assumptions at the price of handling a collection of compatible utility_functions virtually impossible to exhibit to the user this strength for the model is a challenge for the explanation in this paper we come up with an explanation engine based on sequences of preference swaps that is pairwise_comparison of alternatives the intuition is to confront the decision maker with elementary comparisons thus building incremental explanations elementary here means that alternatives compared may only differ on two criteria technically our explanation engine exploits some properties of the necessary preference relation that we unveil in the paper equipped with this we explore the issues of the existence and length of the resulting sequences we show in particular that in the general case no bound can be given on the length of explanations but that in binary domains the sequences remain short
extending the applicability of recommender_systems a multilayer framework for matching human resources,recommender_systems rs so far have been applied to many fields of ecommerce in order to assist users in finding the products that best meet their preferences however while the application of rs to the search for objects is well established this is not the case for the search for subjects this is astonishing as a growing number of people make personal and professional information digitally available to others by managing profiles in cv databases social networking platforms and other online services in order to address this new field of application for rs we integrate own prior research into a unified multilayer framework supporting the matching of individuals for recruitment and team staffing processes by this means we enhance rs research and make a next step towards the development of empirically and theoretically grounded decision support for the human resources function
multicast for small conferences,this paper describes a concept to support scalable multicast communications for small audiovideo conferencing groups on the internet the solution presented in this paper is based on extensions of ipv6 and the session description protocol sdp a goal of the concept called multicast for small conferences msc is the smooth deployment in the internet
transformational construction of correct pointer algorithms,this paper shows how to use the transformation of paterson and hewitt to improve the memory and operations used in a pointer algorithm that transformation scheme normally is only of theoretical interest because of the inefficient performance of the transformed function however we present a method how it can be used to decrease the amount of selective updates in memory while preserving the original runtime performance this leads to a general transformation framework for the derivation of a class of pointer algorithms
network and content aware information_management,the presented approach addresses the problem of query and persistent query subscription resolution taking into consideration distribution across multidomains network infrastructure and content_management this approach is particularly suitable for informationcentric and cloud_computing applications based around a mobiledevice infrastructure
outsourcing decisions of small and mediumsized enterprises a multiplecase study approach in the german software_industry,outsourcing of software_development tasks has become a major issue for large software enterprises over the last decades nowadays small and mediumsized enterprises smes follow this trend and outsource parts of their software_development as well however most of the existing literature deals with large enterprises whereas the situation of smes is being neglected especially sourcing decisions and the organizational as well as operational setup may differ between large enterprises and smes we choose an exploratory multiplecase study approach focusing on the german software market in order to shed light on these aspects of the sourcing behavior of smes this paper addresses three complementing research questions besides the question of which phases of the software_development_process qualify for outsourcing we explore the organizational setup of smes outsourcing scenarios in addition we seek to find out which characteristics a software_component has to fulfill in order to qualify for outsourcing
efficient incremental analysis of onchip power grid via sparse_approximation,in this paper a new sparse_approximation technique is proposed for incremental power grid analysis our proposed method is motivated by the observation that when a power grid network is locally updated during circuit design its response changes locally and hence the incremental change of the power grid voltage is almost zero at many internal nodes resulting in a unique sparse pattern an efficient orthogonal_matching_pursuit omp algorithm is adopted to solve the proposed sparse_approximation problem in addition several numerical techniques are proposed to improve the numerical stability of the proposed solver while simultaneously maintaining its high efficiency several industrial circuit examples demonstrate that when applied to incremental power grid analysis our proposed approach achieves up to 130 runtime speedup over the traditional algebraic multigrid amg method without surrendering any accuracy
lcars the next generation programming context,in this paper we present a highlevel graphical language to develop pervasive_applications based on a unique interface design the language supports a wide range of programming constructs its graphical notation is based on the lcars design which is appealing to different target groups based on their specific interests and requirements we show that users can easily create pervasive_applications using an lcarsbased user_interface the first step is to describe the technical context in which the application will execute based on this technical context the ui offers a contextspecific set of visual primitives by composing these visual primitives on the screen the user can specify the behavior of the application
hierarchical parttemplate matching for human detection and segmentation,local partbased human detectors are capable of handling partial_occlusions efficiently and modeling shape articulations flexibly while global shape templatebased human detectors are capable of detecting and segmenting human shapes simultaneously we describe a bayesian_approach to human detection and segmentation combining local partbased and global templatebased schemes the approach relies on the key ideas of matching a parttemplate tree to images hierarchically to generate a reliable set of detection hypotheses and optimizing it under a bayesian map framework through global likelihood reevaluation and fine occlusion analysis in addition to detection our approach is able to obtain human shapes and poses simultaneously we applied the approach to human detection and segmentation in crowded scenes with and without background_subtraction experimental results show that our approach achieves good performance on images and video sequences with severe occlusion
using google drive to facilitate a blended approach to authentic learning,abstractrnrnwhile technology has the potential to create opportunities for transformative learning in higher education it is often used to merely reinforce didactic teaching that aims to control access to expert_knowledge instead educators should consider using technology to enhance communication and provide richer more meaningful platforms for the social construction of knowledge by using technology to engage in shared learning_experiences that extend beyond the walls of the classroom we can create opportunities to develop the patterns of thinking that students need to participate in complex real world situations we used authentic learning as a framework to guide the implementation of a casebased blended module in a south african physiotherapy department google drive was used as a collaborative online authoring environment in which small groups of students used clinical cases to create their own content guided by a team of facilitators this paper describes an innovative approach to clinical education using authentic learning as a guiding framework and google drive as an implementation platform we believe that this approach led to the transformation of student_learning practices altered power relationships in the classroom and facilitated the development of critical attitudes towards knowledge and authority
quantitative comparison of the errorcontainment capabilities of a bus and a star_topology in can networks,there has been an increasing interest in using star topologies in fieldbus communications eg in time triggered protocol for sae classc applications ttpc flexray or controller_area_network can due to increased fault resilience and potential errorcontainment advantages in this context an innovative cancompliant star_topology cancentrate has been developed whose hub includes enhanced faulttreatment mechanisms however despite this interest toward stars it is still necessary to quantify their real dependability benefits for this purpose and for the particular case of can this paper presents models for the dependability features of can and cancentrate using stochastic activity networks sans it quantitatively compares their reliability and errorcontainment capabilities under permanent hardware_faults these models rely on assumptions that ensure that results are not biased toward cancentrate which in some cases is too detrimental for it thus despite not reflecting the full cancentrate potential results quantitatively confirm the improvement of errorcontainment it achieves over can additionally the way in which the nodes ability to contain their own errors affects the relevance of using a star_topology has been quantified although this paper refers to the case of can conclusions regarding the justification of using a star_topology depending on this ability can be extrapolated to other fieldbus technologies
analysis and design of singlestage acdc llc resonant converter,analysis and design of a singlestage  llc  resonant converter are proposed a singlestage converter uses only one control signal to drive two power converters a power factor corrector pfc converter and a dcdc converter for reducing the cost of the system however this simplicity induces power imbalance between two converters and then the bus voltage between two converters drifts and becomes unpredictable to ensure that the bus capacitor voltage can be kept in a tolerable region the characteristics of a pfc converter and an  llc  tank are investigated and then a design procedure is proposed correspondingly finally a singlestage  llc  resonant converter is implemented to verify the analysis
agentbased petroleum offshore monitoring using sensor_networks,this paper investigates the architecture and design of agentbased sensor_networks for petroleum offshore monitoring a few challenges to monitor the reservoir wellbore and wellhead are identified moreover the necessary components for a reliable precise and accurate monitoring are suggested the paper describes the architecture of the routing agent and discusses the cross layer optimization issues for query_processing the paper also provides the software_design and components for a webbased continuous monitoring application
the study of 3d_reconstruction method based on dynamic threshold method and improved ray casting algorithm,this article carries on a new method which can improve quality and speed of 3d_visualization this method first based on dynamic threshold method divides region of interest roi from the original image then extends bounding box algorithm to 3d space and combines it with ray casting algorithm we validate the validity and robustness of this method in 3d_reconstruction experimenting with 3d lung parenchymas image vascular image and bones image which used to be 2d medical_images about chest ct
cael an ontology modelling cultural behaviour in adaptive education,the presentation of learning materials in adaptive education hypermedia is influenced by several factors such as learning_style background_knowledge and cultural_background to name a few in this paper we introduce the notion of the cael ontology for modelling stereotype cultural artefacts in adaptive education the ontology_design is based on the user study gathered from the respondents to the cae questionnaire which determines the cultural artefacts that influence a learners behaviour within an educational environment we present a brief overview of the implementation and discuss the stereotype presentation styles from three different countries namely china ireland and uk
solar powered unmanned aerial vehicle for continuous flight conceptual overview and optimization,an aircraft that is capable of continuous flight offers a new level of autonomous capacity for unmanned aerial vehicles we present an overview of the components and concepts of a small scale unmanned aircraft that is capable of sustaining powered flight without a theoretical time limit we then propose metrics that quantify the robustness of continuous flight achieved and optimization criteria to maximize these metrics finally the criteria are applied to a fabricated and flight tested small scale high efficiency aircraft prototype to determine the optimal battery and photovoltaic array mass for robust continuous flight
measuring oscillating walking paths with a lidar,this work describes the analysis of different walking paths registered using a light_detection_and_ranging lidar laser range sensor in order to measure oscillating trajectories during unsupervised walking the estimate of the gait and trajectory parameters were obtained with a terrestrial lidar placed 100 mm above the ground with the scanning plane parallel to the floor to measure the trajectory of the legs without attaching any markers or modifying the floor three different large walking experiments were performed to test the proposed measurement system with straight and oscillating trajectories the main advantages of the proposed system are the possibility to measure several steps and obtain average gait parameters and the minimum infrastructure required this measurement system enables the development of new ambulatory applications based on the analysis of the gait and the trajectory during a walk
identification of normalised coprime plant factors from closedloop experimental data,recently introduced methods of iterative identification and control_design are directed towards the design of high performing and robust control_systems these methods show the necessity of identifying approximate models from closed loop plant experiments in this paper a method is proposed to approximately identify normalized coprime plant factors from closed loop data the fact that normalized plant factors are estimated gives specific advantages both from an identification and from a robust_control design point of view it will be shown that the proposed method leads to identified models that are specifically accurate around the bandwidth of the closed loop system the identification procedure fits very naturally into a recently developed the iterative identificationcontrol design scheme based on h robustness optimization
current situation of digitalized ship navigation_system for safety,automatic_identification_system ais has to be on board in every ship over 500 gross tonnages gt in japan from july 1 2008 this is a response of amendment of maritime relating laws in accordance with solas international convention for the safety of life at sea convention and imo international maritime organization requires all ships over 500 gt or upward are fitted with ship borne ais ais is a system which makes it possible to get precise online information from a large area about ships and their movements ais is based on a ship borne radio wave of vhf it continuously and automatically transmits fixed dynamic and voyagerelated information and receives corresponding information from other ships near by vts vessel transport service is provided by collection of ais information from ships most of the vts stations around the area of gulf of finland have equipped with coastal surveillance_system with radar for early warnings to endangering ships combinations of ais and vts in addition practical laneseparation system have also been activated under international agreement of russia estonia and finland these three systems are well combined and organised then significant effects of reducing risks of ship borne accidents have been observed practices of gulf of finland should be introduced other congested sea lanes
inference of gene predictor set using boolean_satisfiability,the inference of gene predictors in the gene regulatory network grn has become an important research area in the genomics and medical disciplines accurate predicators are necessary for constructing the grn model and to enable targeted biological experiments that attempt to validate or control the regulation process in this paper we implement a satbased algorithm to determine the gene predictor set from steady state gene expression data attractor states using the attractor states as input the states are ordered into attractor cycles for each attractor cycle ordering all possible predictors are enumerated and a conjunctive normal form cnf expression is generated which encodes these predictors and their biological constraints each cnf is solved using a sat solver to find candidate predictor sets statistical_analysis of the resulting predictor sets selects the most likely predictor set of the grn corresponding to the attractor data we demonstrate our algorithm on attractor state data from a melanoma study 1 and present our predictor set results
integration of chemical and visual sensors for identifying an odor source in near shore ocean conditions,this paper develops new multiple sensorbased algorithms for identifying a chemical odor source in a nearshore and ocean environment via an autonomous_underwater_vehicle auv those algorithms implement two modules source declaration and source verification which are embedded in a subsumption architecture for chemical plume tracing cpt the source declaration module is based on chemical events detected by a chemical sensor in combination with measured vehicle locations and fluid flow directions while the source verification module uses a fuzzy color_segmentation algorithm to process an image taken when the odor source is declared
web design and maintenance,
physical properties of lyso scintillator for nnpet detectors,
hivsetsubtype software for subtype classification of hiv1 sequences,an automated web based tool for assigning hiv1 pure and recombinant subtypes within unaligned sequences is presented the system combines the blast search algorithm and the recombination identification program for genetic subtyping of hiv1 the software was validated through combined analysis of simulated and other hiv1 real data
robust intensitybased 3d2d registration of ct and xray images for precise estimation of cup alignment after total hip arthroplasty,the widely used procedure of evaluation of cup orientation following tha using single standard anteroposterior radiograph is known inaccurate largely due to the wide variability in individual pelvic position relative to xray plate 3d2d image_registration methods have been introduced to estimate the transformation between a ct volume and the radiograph for an accurate estimation of the cup orientation relative to an anatomical reference extracted from the ct_data however the robustness of these methods is questionable this paper presents a robust image similarity measure which is derived from a variational_approximation to mutual_information and allows for incorporation of spatial information via energy minimization experimental results on estimating cup alignment from single xray radiograph with gonadal shielding demonstrate the robustness and the accuracy of the present approach
image_denoising with shrinkage and redundant representations,shrinkage is a well known and appealing denoising technique the use of shrinkage is known to be optimal for gaussian_white_noise provided that the sparsity on the signals representation is enforced using a unitary transform still shrinkage is also practiced successfully with nonunitary and even redundant representations in this paper we shed some light on this behavior we show that simple shrinkage could be interpreted as the first iteration of an algorithm that solves the basis_pursuit denoising bpdn problem thus this work leads to a novel iterative shrinkage algorithm that can be considered as an effective pursuit method we demonstrate this algorithm both on synthetic_data and for the image_denoising problem where we learn the image prior parameters directly from the given image the results in both cases are superior to several popular alternatives
global tracking for robot_manipulators using a simple causal pd controller plus feedforward,this paper shows that a wellknown causal pd controller plus feedforward solves the global output_feedback tracking_control_problem of robot_manipulators by requiring only the existence of the robot natural damping no matter how small to this end we first demonstrate that a robot controlled by a causal pd is globally inputtostate stable iss with respect to a bounded input disturbance then we prove that the addition of a feedforward compensation renders the error system uniformly globally asymptotically_stable furthermore we present a possible extension to more general nonlinear systems and also to uncertain_systems
prototypeoriented development of highperformance systems,we discuss the problem of developing performanceoriented software and the need for methodologies we then present the edpepps environment for design and performance evaluation of portable parallel software approach to the problem of designing and evaluating highperformance parallel_applications the edpepps toolset is based on a rapid_prototyping philosophy where the designer synthesises a model of the intended software which may be simulated and the performance is subsequently analysed using visualisation the toolset combines a graphical design tool a simulation facility and a visualisation tool the same design is used to produce a code suitable for simulation and real execution
a strategy for visionbased controlled pushing of microparticles,in this paper a strategy for controlled pushing is presented for microassembly of 45 mum polystyrene particles on a flat glass substrate using an atomic force microscope probe tip realtime vision based feedback from a ccd camera mounted to a high resolution optical microscope is used to track particle positions relative to the tip and target_position tipparticle system is modeled in 2d as a nonholonomic differential drive robot effectiveness of the controller is demonstrated through experiments performed using a single goal position as well as linking a series of target positions to form a single complex trajectory cell decomposition and wavefront expansion algorithms are implemented to autonomously locate a navigable path to a specified target position_control strategy alleviates problem of slipping and spinning during pushing
crowd character complexity on big hero 6,on disneys  big hero 6  we needed to create the city of  san fransokyo  with unparalleled levels of visual complexity the cityscape has more buildings and more geometry than any prior disney film inhabiting this city are hundreds of unique characters each performing a high caliber of animation individually and as a group these challenges prompted a major upgrade to our existing crowd pipeline and the development of several new technologies in authoring crowd characters generating crowd animation cycles and instancing crowds for rendering
evaluating realtime java features and performance for realtime embedded_systems,this paper provides two contributions to the study of programming_languages and middleware for realtime and embedded applications first we present the empirical results from applying the rtjperf benchmarking suite to evaluate the efficiency and predictability of several implementations of the realtime specification for java rtsj second we describe some of the techniques used to develop jrate which is an opensource aheadoftimecompiled implementation of rtsj we are developing our results indicate that rtsj implementations are maturing to the point where they can be applied to a variety of realtime embedded applications
biologically inspired design principles for scalable robust adaptive decentralized search and automated response radar,distributed search problems are ubiquitous in artificial life alife many distributed search problems require identifying a rare and previously unseen event and producing a rapid response this challenge amounts to finding and removing an unknown needle in a very large haystack traditional computational search models are unlikely to find nonetheless appropriately respond to novel events particularly given data distributed across multiple platforms in a variety of formats and sources with variable and unknown reliability biological systems have evolved solutions to distributed search and response under uncertainty immune systems and ant colonies efficiently scale up massively parallel search with automated response in highly dynamic_environments and both do so using distributed coordination without centralized control these properties are relevant to alife where distributed autonomous robust and adaptive_control is needed to design robot_swarms mobile_computing networks computer_security systems and other distributed intelligent_systems they are also relevant for searching tracking the spread of ideas and understanding the impact of innovations in online_social_networks we review design principles for scalable robust adaptive decentralized search with automated response scalable radar in biology we discuss how biological radar scales up efficiently and then discuss in detail how modular search in the immune system can be mimicked or built upon in alife such search mechanisms are particularly useful when components have limited capacity to communicate and when physical distance makes communication more costly
combining grid and cloud resources by use of middleware for spmd applications,distributed computing_environments have evolved from inhouse clusters to grids and now cloud platforms we as others provide hpc benchmarks results over amazon_ec2 that show a lower performance of cloud resources compared to private resources so it is not yet clear how much of impact clouds will have in high performance computing hpc but hybrid gridcloud computing may offer opportunities to increase overall applications performance while benefiting from inhouse computational_resources extending them by cloud ones only whenever needed in this paper we advocate the usage of proactive a well established middleware in the grid community for mixed gridcloud computing extended with features to address gridcloud issues with little or no effort for application developers we also introduce a framework developed in the context of the disco grid project based upon the proactive middleware to couple hpc domaindecomposition spmd applications in heterogeneous multidomain environments performance results coupling grid and cloud resources for the execution of such kind of highly communicating and processing intensive applications have shown an overhead of about 15 which is a nonnegligible value but lower enough to consider using such environments to achieve a better costperformance tradeoff than using exclusively cloud resources
the evolution of a hierarchical partitioning algorithm for largescale scientific_data three steps of increasing complexity,as scientific_data sets grow exponentially in size the need for scalable algorithms that heuristically partition the data increases in this paper we describe the threestep evolution of a hierarchical partitioning algorithm for largescale spatiotemporal scientific_data sets generated by massive simulations the first version of our algorithm uses a simple topdown partitioning technique which divides the data by using a fourway bisection of the spatiotemporal space the shortcomings of this algorithm lead to the second version of our partitioning algorithm which uses a bottomup approach in this version a partition hierarchy is constructed by systematically agglomerating the underlying cartesian grid that is placed on the data finally the third version of our algorithm utilizes the intrinsic topology of the data given in the original scientific problem to build the partition hierarchy in a bottomup fashion specifically the topology is used to heuristically agglomerate the data at each level of the partition hierarchy despite the growing complexity in our algorithms the third version of our algorithm builds partition hierarchies in less time and is able to build trees for larger size data sets as compared to the previous two versions
determinantal equations for secant varieties and the eisenbudkohstillman conjecture,we address special cases of a question of eisenbud on the ideals of secant varieties of veronese reembeddings of arbitrary varieties eisenbuds question generalizes a conjecture of eisenbud koh and stillman eks for curves we prove that settheoretic equations of small secant varieties to a high degree veronese reembedding of a smooth variety are determined by equations of the ambient veronese variety and linear_equations however this is false for singular varieties and we give explicit counterexamples to the eks conjecture for singular curves the techniques we use also allow us to prove a gap and uniqueness theorem for symmetric tensor rank we put eisenbuds question in a more general context about the behaviour of border rank under specialisation to a linear subspace and provide an overview of conjectures coming from signal_processing and complexity theory in this context
an approach to supply simulations of the functional environment of ecus for hardwareintheloop test systems based on eearchitectures conform to autosar,todays vehicles include a complex_network of programmableelectronic control units with software_components avehicles electric and electronic ee architecture has to bemodeled in an early design phase to evaluate design alternativesthe tool preevision offers possibilities to modeleearchitectures considering feature_function networksfunction networks component networks as well as wiringharness and the respective mappingsthe software_architecture specified by autosar separateshardware dependent and hardware independent softwaremodules this allows the mapping of hardware independentsoftware applications to different hardware platformshardwareintheloop hil is an established technologyfor testing electronic control units ecu and to assurequality hiltestsystems hilts simulate the ecusfunctional environment car driver road tires etc andadditionally offers the possibility to insert logical faults aswell as electrical faults short circuit open load etcmostly this hilsimulation is individually engineered forevery single ecuthis paper introduces a concept for the automated supportof such simulations this includes the derivation of relevantinformation from the model of the eearchitecture as wellas the portation of the autosar software_architecture tothe hilts following this concept engineering costs canbe reduced and the quality and correctness of the simulationincreased
constriction of mutual_information based matchingsuitable features for sar_image aided navigation,mutual_information based matchingsuitable features are studied in this paper for the selection of good matching areas with high success probability in sar matching aided navigation_system based on the analysis of sar_imaging and multisource matching four feature constructing guide lines are proposed first then ten candidate features are designed under mutual_information measurement effectiveness of these features is tested and compared through experiments under real sar_data and features such as reference_image complexity and absolute value roughness etal which show stable monotony and good convergence can be use as good matchingsuitable features in practices
blockwise zero mapping image_coding,a coding algorithm of low addressing and implementation complexity is proposed it is based on partitioning uniformly quantized wavelet_coefficients into multiscale blocks with each block classified as either an allzero block or a nonzero block the nonzero blocks are coded by a new method called zeromapping whose outputs are further compressed by a firstorder arithmetic coder the performance of the proposed coding algorithm compares favorably with that of some wellknown coding algorithms particularly for images with considerable high frequency components
demo distributed_video_coding applications in wireless multimedia sensor_networks,novel distributed_video_coding dvc architectures developed by the ibbt dvc group realize stateoftheart video coding_efficiency under stringent energy restrictions while supporting errorresilience and scalability therefore these architectures are particularly attractive for application scenarios involving lowcomplexity energyconstrained wireless visual sensors this demo presents the scenarios which are considered to be the most promising areas of integration for ibbts dvc systems considering feasibility and commercial applicability
a saturation algorithm for homogeneous binomial ideals,
hpceuropa towards uniform access to european hpc infrastructures,one of the goals of the hpceuropa project is to provide users with a single point of access spa to the resources of hpc centers in europe to this end the hpceuropa portal is being built to provide transparent uniform flexible and intuitive user access to hpceuropa resources in this paper we present a mechanism that enables endusers to transparently access the diverse services available in the hpceuropa environment the uniform job submission interface that uses this mechanism utilizing the job specification description language jsdl is described we also present the architecture of the spa based on the gridsphere portal framework finally we discuss the various interoperability problems encountered in particular those concerning job submission security and accounting
modeling the faulty behaviour of digital designs using a feed forward neural_network approach,cosmic rays lead to soft_errors and faulty behavior in electronic circuits knowing about their faulty behavior before fabrication would be helpful this research proposes an approach for modeling the faulty behaviour of digital circuits it could be applied in a design_flow before circuit fabrication this is achieved by extracting_information about faulty behaviour of circuits from lowlevel models expressed in the vhdl_language afterwards the extracted information is used to train highlevel artificial_neural_networks models expressed in cc or matlab tm  languages the trained neural_network models are able to replicate the behaviour of circuits in presence of faults the methodology is based on experiments done with two benchmarks the iscasc17 and a 4bit multiplier results show that the neural_network approach leads to models that are more accurate than a previously reported signature generation method for the c17 using only 30 of the dataset generated with the lifting fault simulator the neural_network is able to replicate the output of the circuit in presence of faults with a mean absolute modeling error below 6
optimalnearoptimal dimensionality_reduction for distributed estimation in homogeneous and certain inhomogeneous scenarios,we consider distributed estimation of a deterministic vector parameter from noisy sensor observations in a wireless_sensor_network wsn the observation noise is assumed uncorrelated across sensors to meet stringent power and bandwidth budgets inherent in wsns local data dimensionality_reduction is performed at each sensor to reduce the number of messages sent to a fusion_center fc the problem of interest is to jointly design the compression matrices associated with those sensors aiming at minimizing the estimation error at the fc such a dimensionality_reduction problem is investigated in this paper specifically we study a homogeneous environment where all sensors have identical noise covariance_matrices and an inhomogeneous environment where the noise covariance_matrices across the sensors have the same correlation structure but with different scaling factors given a total number of messages sent to the fc theoretical lower bounds on the estimation error of any compression strategy are derived for both cases compression strategies are developed to approach or even attain the corresponding theoretical lower bounds performance_analysis and simulations are carried out to illustrate the optimality and effectiveness of the proposed compression strategies
incentive_compatible configuration for wireless_multicast a game theoretic approach,multicast and broadcast service mbs is a service offered by the base_station bs to multiple receivers requesting the same information such a bs must be kept updated with the receivers feedback information eg packet_loss_rates to configure the mbs we propose mbs operation schemes that are dominantstrategy incentive_compatible in game_theory ie the schemes induce dominantstrategy equilibria where all selfish receivers reveal their true information moreover the induced equilibria are pareto efficient and maxmin fair to conclude the proposed schemes can elicit true feedback information from the receivers avoiding any manipulation and thereby ensuring an efficient and fair system operation
what computing students can learnby developing their own serious games,
quantization channel compensation and energy allocation for estimation in wireless_sensor_networks,in clustered networks of wireless_sensor motes each mote collects noisy observations of the environment quantizes these observations into a local estimate of finite length and forwards them through one or more noisy wireless channels to the cluster_head ch the measurement_noise is assumed to be zeromean and have finite variance each wireless hop is assumed to be a binary_symmetric_channel bsc with a known crossover probability we propose a novel scheme that uses dithered quantization and channel compensation to ensure that each motes local estimate received by the ch is unbiased the ch then fuses these unbiased local estimates into a global one using a best linear unbiased estimator blue the energy allocation problem at each mote and among different sensor motes are also discussed simulation results show that the proposed scheme can achieve much smaller mean_square_error mse than two other common schemes while using the same amount of energy the sensitivity of the proposed scheme to errors in estimates of the crossover probability of the bsc channel is studied by both analysis and simulation
a direct algorithm to compute rational solutions of first order linear qdifference systems,we present an algorithm to compute rational function solutions to a first order system of linear qdifference equations with rational coefficients we make use of the fact that qdifference equations bear similarity to differential_equations at the point 0 and to difference_equations at other points this allows the combining of known algorithms for the differential and the difference cases this algorithm does not require preliminary uncoupling of the given system
product model derivation by model_transformation in software_product_lines,product_derivation is an essential part of the software_product_line spl development process the paperproposes a model_transformation for deriving automatically a uml_model of a specific product from the uml_model of a product_line this work is a part of a larger project aiming to integrate performance_analysis in the spl modeldriven development the spl source model is expressed in uml extended with two separate profiles a product_line profile from literature for specifying the commonality and variability between products and the marte profile recently standardized by omg for performance annotations the automatic derivation of a concrete product model based on a given feature configuration is enabled through the mapping between features from the feature_model and their realizations in the design model the paper proposes an efficient mapping technique that aims to minimize the amount of explicit feature annotations in the uml design model of spl implicit feature mapping is inferred during product_derivation from the relationships between annotated and nonannotated model elements as defined in the uml metamodel and well formedness rules the transformation is realized in the atlas transformation language atl and illustrated with an ecommerce case study that models structural and behavioural spl views
web_browsing mobile_computing and academic performance,students in two different courses at a major research university one a communication course the other a computer_science_course were given laptop computers with wireless_network access during the course of a semester students web_browsing on these laptops including urls dates and times was recorded 24 hoursday 7 daysweek in a log file by a proxy server during most of a semester about 15 weeks for each student browsing behavior was quantified and then correlated with academic performance the emergence of statistically significant results suggests that quantitative characteristics of browsing behavioreven prior to examining browsing contentcan be useful predictors of meaningful behavioral outcomes variables such as number of browsing sessions and length of browsing sessions were found to correlate with students final grades the valence and magnitude of these correlations were found to interact with course ie whether student was enrolled in the communication or computer_science_course browsing context ie setting in which browsing took place during class on the wireless_network between classes or at home and gender the implications of these findings in relation to previous studies of laptop use in education settings are discussed
optimization of transceivers with bit_allocation to maximize bit rate for mimo_transmission,there have been many results on designing transceivers for mimo_channels in early results the transceiver is designed for a given bit_allocation in this paper we will jointly design the transceiver and bit_allocation for maximizing bit rate by using a high bit rate assumption we will see that the optimal transceiver and bit_allocation can be obtained in a closed_form using simple hadamard inequality and the poincare separation theorem in the simulation we will demonstrate the usefulness of the joint design simulation results in which a high bit rate assumption is not used in allocating bits show that a higher bit rate can be achieved compared to previously reported methods
morepriv mobile os support for application personalization and privacy,privacy and personalization of mobile experiences are inherently in conflict better personalization demands knowing more about the user potentially violating user_privacy a promising approach to mitigate this tension is to migrate personalization to the client an approach dubbed  clientside personalization  this paper advocates for  operating_system support  for clientside personalization and describes morepriv an operating_system  service  implemented in the windows phone os we argue that personalization support should be as ubiquitous as location support and should be provided by a unified system within the os instead of by individual apps   we aim to provide a solution that will stoke innovation around mobile personalization to enable easy application personalization morepriv approximates users interests using  personae  such as technophile or business executive using a number of case studies and crowdsourced user studies we illustrate how more complex personalization tasks can be achieved using morepriv   for privacy_protection morepriv distills sensitive user_information to a coarsegrained  profile  which limits the potential damage from information leaks we see morepriv as a way to increase enduser privacy by enabling clientside computing thus minimizing the need to share user data with the server as such morepriv shepherds the ecosystem towards a better privacy stance by  nudging  developers away from todays privacyviolating practices furthermore morepriv can be  combined  with privacyenhancing technologies and is complimentary to recent advances in data leak detection
hierarchical implicit deregistration with forced registrations in 3g wireless_networks,deregistration due to the departures of mobile_users from their current visiting registration area may cause significant traffic in the wireless_cellular_networks in this paper we propose a hierarchical implicit deregistration scheme with forced registration in thirdgeneration wireless_cellular_networks to reduce the remoteinternational roaming signaling traffic when homelocation registers hlrs gatewaylocation registers glrs and the visitorlocation registers vlrs form a threelevel database hierarchy in this scheme if a mobile_phone arrives and the glrvlr is full a random record is deleted and the reclaimed storage is reassigned to the new arriving mobile_phone when a call arrives and the callees record is missing in the glrvlr forced registration is executed to restore the glrvlr record before the callsetup operation proceeds an analytic model is proposed to carry out the performance evaluation for the proposed scheme our results show that the proposed scheme not only reduces the local deregistration traffic between the glr and the vlr but also reduces the remoteinternational deregistration traffic between the hlr and the glr especially when the ratio of the cost of the remoteinternational traffic between glr and hlr to the cost of local traffic between the vlr and the glr is high
global optimization of extended handeye calibration,this paper introduces simultaneous global optimization of both camera orientation and vehicle wheel circumference without requiring any information about the translations in the system the main contribution are new objective function bounds to integrate this problem into a branchandbound parameter space search the presented method constitutes the first guaranteed globally optimal estimator for both components of the problem with respect to a cost function based on reprojection errors the algorithm operates directly on image measurements and does not depend on any structure and motion preprocessing to estimate camera poses the complete system is implemented and validated on both synthetic and real automotive datasets
blog popularity mining using social interconnection analysis,analyzing interconnections among blog communities reveals blogger behaviors that could help in assessing blog quality a new approach to ranking blogs uses a social blog network model based on blogs interconnection structure and a popularity ranking method called brank experiments show that the proposed method can discriminate blogs with various degrees of popularity in the blogosphere
do changes in movements after tool use depend on body schema or motor learning,in a recent study cardinali et al 2009 showed that training the use of a tool affected kinematic characteristics of subsequent free movements ie movement were slower for instance which they interpreted as that the use of a tool affects the body schema the current study examined whether these results can also be explained in terms of motor learning where movement characteristics during tool use persist in the free movements using a different tool we replicated parts of the study of cardinali et al as did cardinali et al we found that tool use aftereffects can be found in subsequent free movements importantly we showed that the tooling movement was very slow compared to the free hand movement we concluded that it can not be ruled out yet that aftereffects of tool use originate from a general slowing down of movement speed that persists in free hand movements
discovering golden nuggets data_mining in financial application,with the increase of economic globalization and evolution of information_technology financial data are being generated and accumulated at an unprecedented pace as a result there has been a critical need for automated approaches to effective and efficient utilization of massive amount of financial data to support companies and individuals in strategic_planning and investment decisionmaking data_mining_techniques have been used to uncover hidden patterns and predict future trends and behaviors in financial_markets the competitive advantages achieved by data_mining include increased revenue reduced cost and much improved marketplace responsiveness and awareness there has been a large body of research and practice focusing on exploring data_mining_techniques to solve financial problems in this paper we describe data_mining in the context of financial application from both technical and application perspectives in addition we compare different data_mining_techniques and discuss important data_mining issues involved in specific financial applications finally we highlight a number of challenges and trends for future research in this area
fault and simple power attack resistant rsa using montgomery modular_multiplication,side_channel_attacks and more specifically fault simple power attacks constitute a pragmatic potent mean of braking a cryptographic algorithm like rsa for this reason many researchers have proposed modifications on the arithmetic operation functions required for rsa in order to thwart those attacks however these modifications are applied on theoretic  algorithmic level and do not necessary result in high performance rsa designs this paper constitute the first complete attempt for an efficient design approach on a fault and simple power attack resistant rsa based on the well known for its high performance montgomery_multiplication algorithm to achieve this a fault and simple power attack resistant modular_exponentiation algorithm is proposed that is based on the montgomery modular_multiplication in order to optimize this algorithms performance we also propose a modified version of montgomery modular_multiplication algorithm that employs value precomputation and carry save logic in all input output and intermediate values we introduce a hardware_architecture based on the proposed montgomery modular_multiplication algorithm and use it as a building block for the design of a fault and simple power attack resistant modular_exponentiation unit this unit is optimized by taking advantage of the inherit parallelism in the proposed fault and simple power attack resistant modular_exponentiation algorithm realizing the proposed unit in fpga technology very advantageous results are found when compared against other well known designs even though our design bears an extra computation cost due to its fault and simple power attack resistance characteristic
a fast and efficient_algorithm to map prerequisites of landslides in sensitive clays based on detailed soil and topographical information,we present an algorithm developed for gisapplications in order to produce maps of landside susceptibility in postglacial and glacial sediments in sweden the algorithm operates on detailed topographic and quaternary deposit data we compare our algorithm to two similar computational schemes based on a global visibility operator and a shadowcasting algorithm we find that our algorithm produces more reliable results in the vicinity of stable material than the global visibility algorithm we also conclude that our algorithm is more computationally efficient than the other two methods which is important when we may want to assess the effects of uncertainty in the data by evaluating many different models our method also provides the possibility to take other data into account we show how different soil types with different geotechnical properties may be modelled our algorithm may also take depth information ie the thicknesses of the deposits into account we thus propose that our method may be used to provide more refined maps than the overview maps in areas where more detailed geotechnicalgeological data have been acquired the efficiency of our algorithm suggests that it may replace any global visibility operators used in other applications or processing schemes of gridded map data
assessing the value of enterprise identity_management eidm towards a generic evaluation approach,the introduction of enterprise identity_management_systems eldms in organisations is a costly and challenging endeavour where organisations have to face various costs for the planning the implementation and the operation of such systems in the planning phase it is important that organisational aspects are incorporated into the development of an enterprise identity_management eldm solution instead of purely focussing on the technological or financial issues indeed without a proper assessment of the costs and the organisational settings such as stakeholders processes companies will not see the benefit for introducing eldm as additional layer into their it_infrastructure and their business_processes this paper proposes initial ideas for a generic approach based on the balanced scorecard for assessing the value of investing in the introduction of eldms in the decision process such an instrument can be used for decision support purposes and the planning phase on a tactical level furthermore the organisational aspects are discussed and possible solutions for integrating all relevant parties into the planning process are presented
reactive numa a design for unifying scoma and ccnuma,this paper proposes and evaluates a new approach to directorybased cache_coherence_protocols called  reactive numa  rnuma an rnuma system combines a conventional ccnuma coherence_protocol with a morerecent simplecoma scoma protocol what makes rnuma novel is the way it dynamically reacts to program and system behavior to switch between ccnuma and scoma and exploit the best aspects of both protocols this reactive behavior allows each node in an rnuma system to independently choose the best protocol for a particular page thus providing much greater performance stability than either ccnuma or scoma alone our evaluation is both qualitative and quantitative we first show the theoretical_result that rnumas worstcase performance is bounded within a small constant factor ie two to three times of the best of ccnuma and scoma we then use detailed executiondriven simulation to show that in practice rnuma usually performs better than either a pure ccnuma or pure scoma protocol and no more than 57 worse than the best of ccnuma and scoma for our benchmarks and base system assumptions
robust_exponential_stability of markovian jump impulsive stochastic cohengrossberg neural_networks with mixed time_delays,this paper is concerned with the problem of exponential_stability for a class of markovian jump impulsive stochastic cohengrossberg neural_networks with mixed time_delays and known or unknown_parameters the jumping parameters are determined by a continuoustime discretestate markov_chain and the mixed time_delays under consideration comprise both timevarying delays and continuously distributed_delays to the best of the authors knowledge till now the exponential_stability problem for this class of generalized neural_networks has not yet been solved since continuously distributed_delays are considered in this paper the main objective of this paper is to fill this gap by constructing a novel lyapunovkrasovskii functional and using some new approaches and techniques several novel sufficient_conditions are obtained to ensure the exponential_stability of the trivial solution in the mean_square the results presented in this paper generalize and improve many known results finally two numerical_examples and their simulations are given to show the effectiveness of the theoretical results
the type and effect discipline,the type and effect discipline a framework for reconstructing the principal type and the minimal effect of expressions in implicitly typed polymorphic functional_languages that support imperative constructs is introduced the type and effect discipline outperforms other polymorphic type_systems just as types abstract collections of concrete values effects denote imperative operations on regions regions abstract sets of possibly aliased memory_locations effects are used to control type generalization in the presence of imperative constructs while regions delimit observable side effects the observable effects of an expression range over the regions that are free in its type environment and its type effects related to local data structures can be discarded during type reconstruction the type of an expression can be generalized with respect to the variables that are not free in the type environment or in the observable effect 
acoustictoarticulatory inversion using an episodic memory,this paper presents a new acoustictoarticulatory inversion methodbased on an episodic memory which is an interesting model for two reasons first it does not rely on any assumptions about the mapping function but rather it relies on real synchronized acoustic and articulatory data_streams second the memory structurally embeds the naturalness of the articulatory dynamics in addition we introduce the concept of generative episodic memory which enables the production of unseen articulatory trajectories according to the acoustic_signals to be inverted the proposed memory is evaluated on the mocha corpus the results show its effectiveness and are very encouraging since they are comparable to those of recently proposed methods
home_network framework based on osgi service platform using ssl component bundle,it becomes increasingly common to use distributed services according to development of wirewireless network home_network area is also based on distributed_environment which use a lot of services various technologies already have been used in home_network however the research of information_security part is insufficient thus in this paper we enhance our previous work by applying ssl component as a bundle format this framework includes efficient user access_control based on osgi service platform from the former researches this paper can cover the venerability between home_gateway and authorization server in existing osgi service platform therefore we provide the advantages of prestudied framework and user_information protection simultaneously
analyzing an embedded sensor with timed_automata in uppaal,an infrared_sensor is modeled and analyzed in uppaal the sensor typifies the sort of component that engineers regularly integrate into larger systems by writing interface hardware and software   in all three main models are developed in the first model the timing diagram of the sensor is interpreted and modeled as a timed safety automaton this model serves as a specification for the complete system a second model that emphasizes the separate roles of driver and sensor is then developed it is validated against the timing diagram model using an existing construction that permits the verification of timed trace inclusion for certain models by reachability_analysis ie model_checking a transmission correctness property is also stated by means of an auxiliary automaton and shown to be satisfied by the model   a third model is created from an assembly language driver program using a direct translation from the instruction set of a processor with simple timing behavior this model is validated against the driver component of the second timing diagram model using the timed trace inclusion validation technique the approach and its limitations offer insight into the nature and challenges of programming in real time
social motivations to use gamification an empirical study of gamifying exercise,this paper investigates how social factors predict attitude toward gamification and intention to continue using gamified services as well as intention to recommend gamified services the paper employs structural equation modelling for analyses of data n107 gathered through a survey that was conducted among users of one of the worlds largest gamification applications for physical exercise the results indicate that social factors are strong predictors for attitudes towards gamification and further continued use intentions and intentions to recommend the related service
a global test for groups of genes testing association with a clinical outcome,motivation this paper presents a global test to be used for the analysis of microarray data using this test it can be determined whether the global expression pattern of a group of genes is significantly related to some clinical outcome of interest groups of genes may be any size from a single gene to all genes on the chip eg known pathways specific areas of the genome or clusters from a cluster analysisrnrnresult the test allows groups of genes of different size to be compared because the test gives one pvalue for the group not a pvalue for each gene researchers can use the test to investigate hypotheses based on theory or past research or to mine gene ontology databases for interesting pathways multiple testing problems do not occur unless many groups are tested special attention is given to visualizations of the test result focussing on the associations between samples and showing the impact of individual genes on the test resultrnrnavailability an rpackage globaltest is available from httpwwwbioconductororg
information_security governance control through comprehensive policy architectures,information_security governance has become one of the key focus areas of strategic_management due to its importance in the overall protection of the organizations information_assets a properly implemented information_security governance framework should ideally facilitate the implementation of directing and compliance to control strategic level management directives these strategic level management directives are normally interpreted disseminated and implemented by means of a series of information_security related policies these policies should ideally be disseminated and implemented from the strategic_management level through the tactical level to the operational level where eventual execution takes place control is normally exercised by capturing data at the lowest levels of execution and measuring compliance against the operational level policies through statistical and summarized analyses of the operational level data into higher levels of extraction compliance at the tactical and strategic levels can be facilitated this scenario of directing and controlling defines the basis of sound information_security governance unfortunately information_security_policies are normally not disseminated onto the operational level as a result proper controlling is difficult and therefore compliance measurement against all information_security_policies might be problematic the objective of this paper is to argue towards a more complete information_security policy architecture that will facilitate complete control and therefore compliance to ensure sound information_security governance
darwin customizable resource management for valueadded network_services,the internet is rapidly changing from a set of wires and switches that carry packets into a sophisticated infrastructure that delivers a set of complex valueadded services to end users services can range from bit transport all the way up to distributed valueadded services like video teleconferencing data_mining and distributed interactive simulations before such services can be supported in a general and dynamic manner we have to develop appropriate resource management mechanisms these resource management mechanisms must make it possible to identify and allocate resources that meet service or application requirements support both isolation and controlled dynamic sharing of resources across organizations sharing physical_resources and be customizable so services and applications can tailor resource usage to optimize their performance the darwin project is developing a set of customizable resource management mechanisms that support valueadded services in this paper we present these mechanisms describe their implementation in a prototype system and describe the results of a series of proofofconcept experiments
blockquantized support_vector ordinal regression,support_vector ordinal regression svor is a recently proposed ordinal regression or algorithm despite its theoretical and empirical success the method has one major bottleneck which is the high computational complexity in this brief we propose a both practical and theoretical guaranteed algorithm blockquantized support_vector ordinal regression bqsvor where we approximate the kernel matrix  k  with  k tilde that is composed of  k   2  constant blocks we provide detailed theoretical justification on the approximation accuracy of bqsvor moreover we prove theoretically that the or problem with the blockquantized kernel matrix  k tilde could be solved by first separating the data samples in the training_set into  k  clusters with kernel  k means and then performing svor on the  k  cluster representatives hence the algorithm leads to an optimization problem that scales only with the number_of_clusters instead of the data set size finally experiments on several realworld data sets support the previous analysis and demonstrate that bqsvor improves the speed of svor significantly with guaranteed accuracy
formality agility security and evolution in software_development,combining formal and agile techniques in software_development has the potential to minimize changerelated problems
nonnegative preimage in machine_learning for pattern_recognition,moreover in order to have a physical interpretation some constraints should be incorporated in the signal or image_processing technique such as the nonnegativity of the solution this paper deals with the nonnegative preimage problem in kernel machines for nonlinear pattern_recognition while kernel machines operate in a feature_space associated to the used kernel_function a preimage technique is often required to map back features into the input space we derive a gradientbased algorithm to solve the preimage problem and to guarantee the nonnegativity of the solution its convergence speed is significantly improved due to a weighted stepsize approach the relevance of the proposed method is demonstrated with experiments on real datasets where only a couple of iterations are necessary
a signbit autocorrelation architecture for fractional frequency_offset_estimation in ofdm,this paper presents an architecture of an autocorrelator for orthogonal_frequency_division_multiplexing_systems the received signal is quantized to only the signbit which dramatically simplifies the frequency_offset_estimation hardware cost is reduced under the assumption that synchronization during acquisition does not have to be very accurate but sufficient for coarse estimation the architecture is synthesized towards a 65nm lowleakage high threshold standard cell cmos library the proposed architecture results in area reduction of 93 if compared to typical 8bit implementation the area occupied by the architecture is 0063 mm 2  the architecture is evaluated for wlan lte and dvbh power simulations for dvbh transmission shows a power consumption of 48w per symbol
