A new approach of 3D watermarking based on image_segmentation
In this paper a robust 3D triangular mesh watermarking algorithm based on 3D segmentation is proposed In this algorithm three classes of watermarking are combined First we segment the original image to many different regions Then we mark every type of_region with the corresponding algorithm based on their curvature value The experiments show that our watermarking is robust against numerous attacks including RST transformations smoothing additive random noise cropping simplification and remeshing
Attractor neural_networks with activity-dependent synapses The role of synaptic facilitation
We studied an autoassociative neural_network with dynamic synapses which include a facilitating mechanism We have developed a general mean-field framework to study the relevance of the different parameters defining the dynamics of the synapses and their influence on the collective properties of the network Depending on these parameters the network shows different types of behaviour including a retrieval phase an oscillatory regime and a non-retrieval phase In the oscillatory phase the network activity continously jumps between the stored patterns Compared with other activity-dependent mechanisms such as synaptic depression synaptic facilitation enhances the network ability to switch among the stored patterns and therefore its adaptation to external stimuli A detailed analysis of our system reflects an efficient-more rapid and with lesser errors-network access to the stored information with stronger facilitation We also present a set of Monte Carlo simulations confirming our analytical_results
A characterization of balanced episturmian sequences
It is well-known that Sturmian sequences are the non ultimately periodic_sequences that are balanced over a 2-letter alphabet They are also characterized by their complexity they have exactly n 1 distinct factors of length n A natural generalization of Sturmian sequences is the set of infinite episturmian sequences These sequences are not necessarily balanced over a k -letter alphabet nor are they necessarily aperiodic In this paper we characterize balanced episturmian sequences periodic or not and prove Fraenkel s conjecture for the special case of episturmian sequences It appears that balanced episturmian sequences are all ultimately periodic and they can be classified in 3 families
Exploring the space of a human action
One of the fundamental challenges of recognizing actions is accounting for the variability that arises when arbitrary cameras capture humans performing actions In this paper we explicitly identify three important sources of variability 1 viewpoint 2 execution rate and 3 anthropometry of actors and propose a model of human_actions that allows us to investigate all three Our hypothesis is that the variability associated with the execution of an action can be closely approximated by a linear combination of action bases in joint spatio-temporal space We demonstrate that such a model bounds the rank of a matrix of image measurements and that this bound can be used to achieve recognition of actions based only on imaged data A test employing principal angles between subspaces that is robust to statistical fluctuations in measurement data is presented to find the membership of an instance of an action The algorithm is applied to recognize several actions and promising results have been obtained
Generalized upper bounds on the minimum distance of PSK block_codes
This paper generalizes previous optimal upper bounds on the minimum Euclidean distance for phase_shift_keying PSK block_codes that are explicit in three parameters alphabet size block length a
Applying BCMP multi-class queueing_networks for the performance evaluation of hierarchical and modular software_systems
queueing_networks with multiple classes of customers play a fundamental role for evaluating the performance of both software and hardware_architectures The main strength of product form models in particular of BCMP queueing_networks is that they combine a flexible formalism with efficient analysis techniques and solution algorithms In this paper we provide an algorithm that starting from a high level description of a system and from the definition of its components in terms of interacting sub systems computes a multiple class and multiple chain BCMP queueing_network We believe that the strength of this approach is twofold First the modeller deals with simplified models which are defined in a modular and hierarchical way Hence we can carry on sensitivity analysis that may easily include structural changes and not only on the time parameters Second maintaining the product form property allows one to derive the average system performance indices very efficiently The paper also discusses the
A Push Pull Class-C CMOS VCO
A CMOS oscillator employing differential transistor pairs working in Class-C in push-pull configuration is presented The oscillator exhibits the same advantages enjoyed by complementary topologies on oscillators based on a single differential pair while yielding a substantial power consumption reduction thanks to the Class-C operation The phase-noise performance and the fundamental conditions required to keep the transistors working in Class-C are analyzed in detail It is shown that for an optimal performance both nMOS and pMOS transistors should not be pushed into the deep triode region by the instantaneous resonator voltage and a simple circuit solution is proposed to accommodate a large oscillation swing A 0 18- Î¼m CMOS prototype of the voltage-controlled oscillator displays an oscillation_frequency from 6 09 to 7 50 GHz The phase_noise at 2-MHz offset is below -120 dBc Hz with a power dissipation of 2 2 mW for a state-of-the-art figure-of-merit ranging from 189 to 191 dBc Hz
On computability of pattern_recognition problems
In statistical setting of the pattern_recognition problem the number of examples required to approximate an unknown labelling function is linear in the VC dimension of the target learning class In this work we consider the question whether such bounds exist if consider only computable pattern_recognition methods assuming that the unknown labelling function is also computable We find that in this case the number of examples required for a computable method to approximate the labelling function not only is not linear but grows faster in the VC dimension of the class than any computable function No time or space constraints are put on the predictors or target functions the only resource we consider is the training examples R N R N The task of pattern_recognition is considered in conjunction with another learning problem data_compression An impossibility result for the task of data_compression allows us to estimate the sample complexity for pattern_recognition
Manipulating biological and mechanical micro-objects using LIGA-microfabricated end-effectors
We first discuss some general aspects of micromanipulation and possible different approaches Then we present new results in the micromanipulation of mechanical and biological objects The apparatus we use is a purposely developed workstation comprising macro- and micro-manipulators The most innovative component of the workstation is a micro-gripper fabricated using LIGA technology and actuated by piezoelectric actuators We describe the design fabrication and performance of a few prototypes of LIGA micro-grippers Results are presented which demonstrate the ability of the system to manipulate effectively both micro-mechanical and biological micro-objects
An abundance of invariant polynomials satisfying the Riemann hypothesis
In 1999 Iwan Duursma defined the zeta function for a linear_code as a generating function of its hamming_weight enumerator It can also be defined for other homogeneous polynomials not corresponding to existing codes If the homogeneous polynomial is invariant under the MacWilliams transform then its zeta function satisfies a functional equation and we can formulate an analogue of the Riemann hypothesis As far as existing codes are concerned the Riemann hypothesis is believed to be closely related to the extremal property In this article we show there are abundant polynomials invariant by the MacWilliams transform which satisfy the Riemann hypothesis The proof is carried out by explicit construction of such polynomials To prove the Riemann hypothesis for a certain class of invariant polynomials we establish an analogue of the Enestrom-Kakeya theorem
Entity resolution with iterative blocking
Entity Resolution ER is the problem of identifying which records in a database refer to the same real-world entity An exhaustive ER process involves computing the similarities between pairs of records which can be very expensive for large datasets Various blocking techniques can be used to enhance the performance of ER by dividing the records into blocks in multiple ways and only comparing records within the same block However most blocking techniques process blocks separately and do not exploit the results of other blocks In this paper we propose an iterative blocking framework where the ER results of blocks are reflected to subsequently processed blocks Blocks are now iteratively processed until no block contains any more matching records Compared to simple blocking iterative blocking may achieve higher accuracy because reflecting the ER results of blocks to other blocks may generate additional record matches Iterative blocking may also be more efficient because processing a block now saves the processing time for other blocks We implement a scalable iterative blocking system and demonstrate that iterative blocking can be more accurate and efficient than blocking for large datasets
Evaluating the accuracy of Java profilers
Performance analysts profile their programs to find methods that are worth optimizing the hot methods This paper shows that four commonly-used Java profilers xprof hprof jprofile and yourkit often disagree on the identity of the hot methods If two profilers disagree at least one must be incorrect Thus there is a good chance that a profiler will mislead a performance analyst into wasting time optimizing a cold method with little or no performance improvement This paper uses causality analysis to evaluate profilers and to gain insight into the source of their incorrectness It shows that these profilers all violate a fundamental requirement for sampling based profilers to be correct a sampling-based profilermust collect samples randomly We show that a proof-of-concept profiler which collects samples randomly does not suffer from the above problems Specifically we show using a number of case studies that our profiler correctly identifies methods that are important to optimize in some cases other profilers report that these methods are cold and thus not worth optimizing
Robust group-of-picture architecture for video_transmission over error-prone_channels
In motion-compensated video-coding_schemes such as MPEG an I frame is normally followed by several P frames and possibly B frames in a group-of-picture GOP In error-prone environments errors happening in the previous frames in a GOP may propagate to all the following frames until the next I frame which is the beginning of the next GOP In this paper we propose a novel GOP structure for robust transmission of MPEG video bitstream By selecting the optimal position of the I frame in a GOP robustness can be achieved without reducing any coding_efficiency Experimental results demonstrate the robustness of the proposed GOP structure
Useful computations need useful numbers
Most of us have taken the exact rational and approximate numbers in our computer algebra systems for granted for a long time not thinking to ask if they could be significantly better With exact rational arithmetic and adjustable-precision floating-point_arithmetic to precision limited only by the total computer memory or our patience what more could we want for such numbers It turns out that there is much more that can be done that permits us to obtain exact results more often more intelligible results approximate results guaranteed to have requested error bounds and recovery of exact results from approximate ones
Fast Content Aware Image Retargeting
This paper addresses the problem of retargeting namely adapting large source_images for effective viewing at a smaller size with possible applications to PDAs or dynamic page layouts Instead of extracting regions of interest for retargeting the uninteresting parts are removed from the scene in Shai Avidan and Shamir A 2007 This is done by computing the RGB variance within non-overlapping 3times3 blocks and removing the block path with minimal variance cost using dynamic programming It is shown that transformation to CIELAB space is more effective for visual interpretation of image content The implementations are shown to be much faster than the seam carving approach of Shai Avidan and Shamir A 2007 Schemes are also presented for speeding up the seam carving scheme itself
SCADDAR an efficient randomized technique to reorganize continuous media blocks
Scalable storage architectures allow for the addition of disks to increase storage capacity and or bandwidth In its general form disk scaling also refers to disk removals when either capacity needs to be conserved or old disk drives are retired Assuming random placement of blocks on multiple nodes of a continuous media server our optimization objective is to redistribute a minimum number of media blocks after disk scaling This objective should be met under two restrictions First uniform distribution and hence a balanced load should be ensured after redistribution Second the redistributed blocks should be retrieved at the normal mode of operation in one disk access and through low complexity computation We propose a technique that meets the objective while we prove that it also satisfies both restrictions The SCADDAR approach is based on using a series of REMAP functions which can derive the location of a new block using only its original location as a basis
Importance sampling in Markovian settings
Rare event simulation for stochastic_models of complex systems is still a great challenge even for Markovian models We review results in importance sampling for markov_chains provide new viewpoints and insights and we pose some future research directions
Identification of cold-induced genes in cereal crops and arabidopsis through comparative analysis of multiple EST sets
Freezing tolerance in plants is obtained during a period of low nonfreezing temperatures before the winter sets on through a biological process known as cold acclimation Cold is one of the major stress factors that limits the growth productivity and distribution of plants and understanding the mechanism of cold tolerance is therefore important for crop improvement Expressed sequence tags EST analysis is a powerful economical and time-efficient way of assembling information on the transcriptome To date several EST sets have been generated from cold-induced cDNA libraries from several different plant species In this study we utilize the variation in the frequency of ESTs sampled from different cold-stressed plant libraries in order to identify genes preferentially expressed in cold in comparison to a number of control sets The species included in the comparative study are oat Avena sativa barley Hordeum vulgare wheat Triticum aestivum rice Oryza sativa and Arabidopsis thaliana However in order to get comparable gene expression estimates across multiple species and data sets we choose to compare the expression of tentative ortholog groups TOGs instead of single genes as in the normal procedure We consider TOGs as preferentially expressed if they are detected as differentially expressed by a test statistic and up-regulated in comparison to all control sets and or uniquely expressed during cold stress i e not present in any of the control sets The result of this analysis revealed a diverse representation of genes in the different species In addition the derived TOGs mainly represent genes that are long-term highly or moderately expressed in response to cold and or other stresses
Real-time distributed_computing
This position paper concerns itself with real-time safety critical distributed_systems It presents a computational model that is appropriate for this type of application and architecture It then defines a resource allocations scheme based upon fixed_priority_scheduling Such a scheme has the advantage over purely static schedules of supporting greater levels of flexibility and non-determinism whilst still providing static guarantees of necessary timing behaviour i e end-to-end deadlines through the systems Priority based communication_protocols are investigated with possible future techniques reviewed
On the impact of using volume as an independent variable for the solution of P-T fluid-phase equilibrium with equations of state
a b s t r a c t The constant pressure temperature P T flash plays an important role in the modelling of fluid-phase behaviour and its solution is especially challenging for equations of state in which the volume is expressed as an implicit function of the pressure We explore the relative merits of solving the P T flash in two ensembles mole numbers pressure and temperature in which each free-energy evaluation requires the use of a numerical solver and mole numbers volume and temperature in which a direct evaluation of the free-energy is possible We examine the performance of two algorithms HELD Helmholtz free energy Lagrangian dual introduced in Pereira et al 2012 and GILD Gibbs free energy Lagrangian dual introduced here for the fluid-phase equilibria of 8 mixtures comprising up to 10 components using two equations of state While the reliability of both algorithms is comparable the computational cost of HELD is consistently lower this difference becomes increasingly pronounced as the number of components is increased 2014 The Authors Published by Elsevier Ltd This is an open_access article under the CC BY license
Semantic Processing of natural_language_queries in the OntoNL Framework
The OntoNL Framework provides an architecture and re-usable components for automating as much as possible the building of natural_language_interfaces to information_systems In addition to the syntactic_analysis components OntoNL has semantic_analysis components which exploit domain_ontologies to provide better disambiguation of the user input We present in this paper the algorithms used for semantic processing of the natural_language_queries as well as an ontology-driven semantic_relatedness measure developed for this purpose We also present extensive evaluation results with different ontologies using human subjects
A Comparison of Linear Keyword and Restricted natural_language Data Base Interfaces for Novice Users
This study compares a linear keyword language interface and a restricted natural_language_interface for data retrieval by a novice user The comparison focuses on the effect of different data base interfaces on user performance as measured by query correctness and query writing time in a query writing task across varying query types and training levels To accomplish this objective a laboratory experiment was conducted using a split-plot factorial design using two between-subjects factors and one within-subjects factor The results indicate that the restricted natural_language subjects performed significantly better than the linear keyword language subjects in terms of both query correctness and query writing time
Reactive Virtual Position-Based Routing in wireless_sensor_networks
Virtual position-based routing_protocols have many attractive characteristics for wireless_sensor_networks Typically such protocols use a proactive scheme for updating routing_tables Because sensor_networks can have very low data rate sending periodic beacons to update routing_tables can be very expensive Instead reactive approaches might be more appropriate in such scenarios MANET-inspired reactive_routing_protocols do not scale well because of the effort in the order of O n for each routing_information update In this paper we present Reactive Virtual Cord Protocol RVCP a data-centric reactive virtual position based routing_protocol for use in sensor_networks route_discovery is directed towards the destination and hence there is no need to flood the entire network to discover a route Our approach is based on Virtual Cord Protocol VCP an efficient virtual relative position based routing_protocol that also provides support for data_management as known from typical distributed_hash_table DHT services To minimize the end-to-end_delay and energy consumption we used adaptive techniques for the development of RVCP
A note on the complexity of scheduling coupled tasks on a single processor
This paper considers a problem of coupled task_scheduling on one processor where all processing times are equal to 1 the gap has exact length h precedence constraints are strict and the criterion is to minimise the schedule length This problem is introduced e g in systems controlling radar operations We show that the general problem is NP-hard
Novel Weighting-Delay-Based stability_criteria for recurrent_neural_networks With time-varying_delay
In this paper a weighting-delay-based method is developed for the study of the stability_problem of a class of recurrent_neural_networks RNNs with time-varying_delay Different from previous results the delay interval 0 d t is divided into some variable subintervals by employing weighting delays Thus new delay-dependent_stability_criteria for RNNs with time-varying_delay are derived by applying this weighting-delay method which are less conservative than previous results The proposed stability_criteria depend on the positions of weighting delays in the interval 0 d t which can be denoted by the weighting-delay parameters Different weighting-delay parameters lead to different stability margins for a given system Thus a solution based on optimization_methods is further given to calculate the optimal weighting-delay parameters Several examples are provided to verify the effectiveness of the proposed criteria
cross-language_information_retrieval
Search for information is no longer exclusively limited within the native language of the user but is more and more extended to other languages This gives rise to the problem of cross-language_information_retrieval CLIR whose goal is to find relevant information written in a different language to a query In addition to the problems of monolingual information_retrieval IR translation is the key problem in CLIR one should translate either the query or the documents from a language to another However this translation problem is not identical to full-text machine_translation MT the goal is not to produce a human-readable translation but a translation suitable for finding relevant documents Specific translation methods are thus required The goal of this book is to provide a comprehensive description of the specifi c problems arising in CLIR the solutions proposed in this area as well as the remaining problems The book starts with a general description of the monolingual IR and CLIR problems Different classes of approaches to translation are then presented approaches using an MT system dictionary-based translation and approaches based on parallel and comparable corpora In addition the typical retrieval_effectiveness using different approaches is compared It will be shown that translation approaches specifically designed for CLIR can rival and outperform high-quality MT systems Finally the book offers a look into the future that draws a strong parallel between query_expansion in monolingual IR and query_translation in CLIR suggesting that many approaches developed in monolingual IR can be adapted to CLIR The book can be used as an introduction to CLIR Advanced readers can also find more technical details and discussions about the remaining research challenges in the future It is suitable to new researchers who intend to carry out research on CLIR
Best case energy analysis of localized euclidean minimum_spanning_tree based multicasting in ad hoc and sensor_networks
I consider the known localized multicast_protocol MSTEAM and derive the energy consumed by the multicast_tree constructed by this protocol in the best case Moreover I show that the length of multicast links connecting into a multicast branch can not be bounded from above For typical wireless_networks where links have a limited communication range however I can show that asymptotically the relation between the derived best case energy consumption of MSTEAM and a known lower bound on multicast energy consumption is limited by a factor of 2
DQAINF an algorithm for automatic integration of infinite oscillating tails
We describe an automatic quadrature routine which is specifically designed for real functions having a certain type of infinite oscillating tails The algorithm is designed to integrate a vector function over an infinite interval A FORTRAN implementation of the algorithm is included
An Interoperability Framework for Pan-European E-Government Services PEGS
Interoperability between public administrations receives nowadays a lot of attention Also in the European Union interworking is high on the priority list but the challenges to achieve the European administrative space is enormous Many research projects are undertaken especially in the domain of semantic_interoperability Many of these efforts seem to start from a technical solution rather than from an actual business problem By taking a narrow view on the problem space they only promise limited support for the many challenges in the domain of interoperability and innovation of e-government services In this paper we present a business driven approach that looks promising in enabling entire classes of interoperability solutions
Performance optimization of interference-limited multihop_networks
The performance of a multihop_wireless_network is typically affected by the interference caused by transmissions in the same network In a statistical fading environment the interference effects become harder to predict Information sources in a multihop_wireless_network can improve throughput and delay performance of data_streams by implementing interference-aware packet injection mechanisms Forcing packets to wait at the head of queues and coordinating packet injections among different sources enable effective control of copacket interference In this paper throughput and delay performance in interference-limited multihop_networks is analyzed Using nonlinear probabilistic hopping models waiting times which jointly optimize throughput and delay performances are derived Optimal coordinated injection strategies are also investigated as functions of the number of information sources and their separations The resulting analysis demonstrates the interaction of performance constraints and achievable capacity in a wireless multihop network
Action Reaction Learning Automatic visual_analysis and Synthesis of Interactive Behaviour
We propose Action-Reaction Learning as an approach for analyzing and synthesizing human_behaviour This paradigm uncovers causal mappings between past and future events or between an action and its reaction by observing time sequences We apply this method to analyze human interaction and to subsequently synthesize human_behaviour Using a time series of perceptual measurements a system automatically discovers correlations between past gestures from one human participant action and a subsequent gesture reaction from another participant A probabilistic_model is trained from data of the human interaction using a novel estimation technique Conditional Expectation Maximization CEM The estimation uses general bounding and maximization to monotonically find the maximum conditional likelihood solution The learning system drives a graphical interactive character which probabilistically predicts a likely response to a user s behaviour and performs it interactively Thus after analyzing human interaction in a pair of participants the system is able to replace one of them and interact with a single remaining user
Circuit Delay Models and Their Exact Computation Using Timed boolean_functions
We propose a general circuit delay model that unifies all previous delay models e g floating viability and transition delays and models introduced in this paper e g delays by sequences of vectors and minimum delays Then we formulate the computation of the exact circuit delays under both bounded and unbounded gate delay models as a mixed Boolean linear programming using a new formulation technique called Timed boolean_function Next we compute the exact delays of combinational_circuits for transition delay and delay by sequences of vectors We show that delays by sequences of vectors and floating or viability delays are invariant under both bounded and unbounded gate delay models Finally we address the effect of gate delay lower bounds on delays of circuits We demonstrate the effectiveness of the method by giving exact delay results for all ISCAS benchmark_circuits except C6188
An anisotropic evolution formulation applied in 2-D unwrapping of discontinuous phase surfaces
In this paper a new method to reconstruct piecewise continuous phase estimates using inphase and quadrature components acquired from interferometry measurements is derived and discussed The method based on the concept of anisotropic evolution formulations is shown to be far less noise sensitive than similar methods operating on modulo-mapped data i e traditional phase_unwrapping methods The method is able to produce reliable phase estimates from data containing complex sheared structures in combination with high noise content without relying on user-defined weights
A score function of splitting band for two-band speech model
Two-band speech model which assumes lower band is a quasi-periodic component and upper band is a non-periodic component is widely used due to its natural and simple framework In this paper a score function is defined for splitting lower and upper band of two-band speech model and estimation_method of band-splitting frequency which is the boundary of the two bands is proposed The score function is calculated for each harmonic frequency using the normalized autocorrelation function of the time signal corresponding to the each sub-band divided by the given frequency By using the score function tracking technique is applied to the band-splitting frequency_estimation procedure to reflect the continuity between neighboring frames Experimental tests confirm that the proposed score function is effective for estimation of the band-splitting frequency and produces better results compared with the previous other methods
The Reliability Study of the Single Hydraulic Prop Based on finite_element_analysis
In allusion to the reliability which exists in the parametric design and optimizing process of the single hydraulic prop this paper presents the new method in comparison with the traditional calculating and checking method for the reliability The geometry model of the hydraulic prop is built firstly based on the 3D software then analyzed and optimized by the finite_element_software-ANSYS Results show that the method presented for the reliability is efficient and accurate
Fuzzy Capacitated Location-allocation Problem with Minimum Risk Criteria
Based on credibility theory a new class of two-stage minimum risk location-allocation model is first proposed Then we deal with the approximation of the location and allocation problem after that a hybrid_algorithm which integrates the approximation approach neural_network and simulated_annealing is designed to solve the proposed location-allocation problem and a numerical_example is provided to test the effectiveness of the hybrid_algorithm
Towards End User service_composition
The popularity of service_oriented_computing SOC brings a large number of distributed well-encapsulated and reusable services all over Internet and makes it possible to create value-added services by means of service_composition Current composition styles are too professional to those end users when building their own applications Actually the end user would prefer rapidly discovering the best-of-breed services to assemble as well as visually personalizing the presentation to enjoy rich experiences We propose an end user service_composition approach for reducing the composition complexity and difficulty from the end user perspective In our approach similar candidate services are aggregated together as a unified resource whose wide QoS spectrum can be easily manipulated by the end users to satisfy their requirements Then they can personalize the services and the composition occurs only at the presentation layer The main contributions of the approach are i enabling the end users to personalize the composite application with more powerful presentation ii supporting the end users to dynamically customize the service_composition in terms of QoS iii alleviating the end users from the time-consuming task of selecting service to compose
Optimal detection of functional connectivity from high-dimensional EEG synchrony data
article i nfo Computing phase-locking values between eeg_signals is a popular method for quantifying functional connectivity However this method involves large-scale high-resolution datasets which impose a serious multiple testing problem Standard multiple testing methods fail to exploit the information from the complex dependence structure that varies across hypotheses in spectral temporal and spatial dimensions and result in a severe loss of power They tend to control the false positives at the cost of hiding true positives We introduce a new approach called optimal discovery procedure ODP for identifying synchrony that is statistically significant ODP maximizes the number of true positives for a given number of false positives and thus offers a theoretical optimum for detecting significant synchrony in a multiple testing situation We demonstrate the utility of this method with PLV data obtained from a visual search study We also present simulation analysis to confirm the validity and relevance of using ODP in comparison with the standard FDR method for given configurations of true synchrony We also compare the effectiveness of ODP with our previously published investigation of hierarchical FDR method Singh and Phillips 2010
starBase v2 0 decoding miRNA-ceRNA miRNA-ncRNA and protein RNA interaction networks from large-scale CLIP-Seq data
Although microRNAs miRNAs other non-coding RNAs ncRNAs e g lncRNAs pseudogenes and circRNAs and competing endogenous RNAs ceRNAs have been implicated in cell-fate determination and in various human diseases surprisingly little is known about the regulatory interaction networks among the multiple classes of RNAs In this study we developed starBase v2 0 http starbase sysu edu cn to systematically identify the RNA RNA and protein RNA interaction networks from 108 CLIP-Seq PAR-CLIP HITS-CLIP iCLIP CLASH data sets generated by 37 independent studies By analyzing millions of RNA-binding protein binding sites we identified 9000 miRNA-circRNA 16 000 miRNApseudogene and 285 000 protein RNA regulatory relationships Moreover starBase v2 0 has been updated to provide the most comprehensive CLIP-Seq experimentally supported miRNA-mRNA and miRNAlncRNA interaction networks to date We identified 10 000 ceRNA pairs from CLIP-supported miRNA target sites By combining 13 functional genomic annotations we developed miRFunction and ceRNAFunction web_servers to predict the function of miRNAs and other ncRNAs from the miRNAmediated regulatory networks Finally we developed interactive web implementations to provide visualization analysis and downloading of the aforementioned large-scale data sets This study will greatly expand our understanding of ncRNA functions and their coordinated regulatory networks
A low_phase_noise 100MHz Silicon BAW Reference Oscillator
The paper presents a temperature compensated 100MHz reference oscillator based on a capacitive silicon Bulk Acoustic Wave BAW resonator interfaced with a CMOS amplifier The resonator is optimized for high quality factor 92000 and low impedance The CMOS IC comprises of a trans-impedance amplifier to sustain oscillations and an oven control mechanism for temperature control A phase_noise floor of 136dBc Hz was measured for the oscillator and the temperature drift of frequency was measured to be 56ppm over 100 C
Performance Bounds for MLSD Reception of ofdm_signals in Fast Fading
For ofdm_systems in fast fading it is difficult to obtain a closed_form expression for the OFDM symbol error probability Thus tight analytical bounds on actual performance are extremely useful for performance prediction and verification Additionally the structure of the bound provides insight into system behavior A new expurgated 2-dimensional union bound is proposed and applied to an OFDM system in fast fading This bound becomes extremely tight as the rate of fading L increases Performance comparison of the new bound to a lower bound on a known expurgated union bound demonstrates a gain of up to 1 5 dB at P e 10 2 for channel implicit diversity_order of L 4 A new simulated upper bound that uses a reduced state vector in a modified trellis search algorithm and a simulated lower bound that assumes limited knowledge of the ICI are also presented Both bounds are derived from a time-varying finite_state_machine model of the received signal Performance results for these bounds are extremely tight for small to large values of N where N is the number of ofdm_signal tones Also the reduction in computational complexity achieved for N 512 is from 2 512 to 2 6 for both bounds
Creating GIS-based spatial interaction models for retail centres in Jeddah City
Spatial interaction models are used today in facilities planning research for predicting and for allocating flows of demand between origin and destination areas based on the attractiveness of each facility and based on the distance between facilities and demand areas These models have been adapted to a wide range of application areas including predicting flows of people to shops offices schools and hospitals The aim of this paper is to use GIS for producing spatial interaction models for two retail centres Jeddah City Saudi Arabia These models are created using ArcGIS software and using the interaction function which is available within the network_analysis module To produce these models detailed geo-database was created that covers location of retail centres the capacity of each centre the size of centres demand at the study area and road network coverage for Jeddah City The created models can be used by city planners for identifying areas of the city that are poorly served by existing retail centres In addition these models can be used to define the impacts of expanding retail supply and or retail demand at the study area
Surface_models of tube trees
This paper describes a new method for generating surfaces of branching tubular structures with given center-lines and radii As the centerlines are not straight lines the cross-sections are not parallel and well-known algorithms for surface tiling from parallel cross-sections cannot be used Nonparallel cross-sections can be tiled by means of the maximal-disc interpolation method special methods for branching-structures modeling by means of convolution surfaces produce excellent results but these methods are more complex than our approach The proposed method tiles nonparallel circular cross-sections and constructs a topologically-correct surface mesh The method is not artifact-free but it is fast and simple The surface mesh serves as a data representation of a vessel tree suitable for real-time virtual_reality operation planning and operation support within a medical application Proposed method extracts a classical polygonal representation which can be used in common surface-oriented graphic accelerators
Node-to-Set Disjoint-path Routing in Metacube
The metacube interconnection_network introduced a few years ago has some very interesting properties it has a short diameter similar to the hypercube and its degree is much lower than that of a hypercube of the same size In this paper we describe an efficient_algorithm for finding disjoint_paths between one source node and at most m k target nodes in a metacube MC k m excluding MC 1 MC 2 2 MC 3 2 and MC 3 3 We show that we can find m k disjoint_paths between the source node and the m k targets of length at most metacube diameter plus k 4 with time complexity of order of metacube degree times its diameter
Systematic error in the organization of physical action
Current views of the control of complex purposeful movements acknowledge that organizational processes must reconcile multiple concerns The central priority is of course accomplishing the actor s goal But in specifying the manner in which this occurs the action plan must accommodate such factors as the interaction of mechanical forces associated with the motion of a multilinked system classical mechanics and in many cases intrinsic bias toward preferred movement patterns characterized by so-called coordination dynamics The most familiar example of the latter is the symmetry constraint where spatial trajectories and or temporal landmarks e g reversal points of concurrentlymoving body segments limbs digits etc exhibit mutual attraction The natural coordination tendencies that emerge through these constraints can facilitate or hinder motor control depending on the degree of congruency with the desired movement pattern Motor control theorists have long recognized the role of classical mechanics in theories of movement organization but an appreciation of the importance of intrinsic interlimb bias has been gained only recently Although detailed descriptions of temporal coordination dynamics have been provided systematic attempts to identify additional salient dimensions of interlimb constraint have been lacking We develop and implement here a novel method for examining this problem by exploiting two robust principles of psychomotor behavior the symmetry constraint and the Two-Thirds Power Law Empirical evidence is provided that the relative spatial patterns of concurrently moving limbs are naturally constrained in much the same manner as previously identified temporal constraints and further that apparent velocity interference is an indirect secondary consequence of primary spatial assimilation The theoretical implications of spatial interference are elaborated with respect to movement organization and motor learning The need to carefully consider the appropriate dimensions with which to characterize coordination dynamics is also discussed 2001 cognitive_science
Modeling the performance of evolutionary_algorithms on the root identification problem A case study with pbil and chc algorithms
The availability of a model to measure the performance of evolutionary_algorithms is very important especially when these algorithms are applied to solve problems with high computational requirements That model would compute an index of the quality of the solution reached by the algorithm as a function of run-time Conversely if we fix an index of quality for the solution the model would give the number of iterations to be expected In this work we develop a statistical_model to describe the performance of PBIL and CHC evolutionary_algorithms applied to solve the root identification problem This problem is basic in constraint-based geometric parametric modeling as an instance of general constraint-satisfaction problems The performance model is empirically validated over a benchmark with very large search spaces
Towards a Probabilistic Calculus for mobile_ad_hoc_networks
In this paper we present a probabilistic calculus for formally modeling and reasoning about mobile_ad_hoc_networks MANETs with unreliable connections and mobility of nodes In our calculus a MANET node can locally broadcast messages to a group of nodes within its physical transmission_range The group probability is also introduced since two distinct nodes within different groups should receive messages from the same sender with different possibilities Our calculus naturally captures essential features of MANETs i e local broadcast mobility and probability Moreover we give a formal operational_semantics of the calculus in terms of the labeled transition_system and define the notion of open bisimulation Finally we illustrate our calculus with a toy example
Slow motion replay of video sequences using fractal zooming
Slow motion replay is a special effect used in the video entertainment field It consists in a presentation of a video scene at a rate display lower than the original Already consolidated as a commercial feature of analog video players today slow motion is likely to be extended to the digital environment Purpose of this paper is to present a technique combining fractals I F S and wavelets to obtain a subjectively pleasant zoom and slow motion of digital_video sequences Active scene detection and post processing techniques are used to reduce computational cost and improve visual_quality respectively This study shows that the proposed technique produces better results than the state of the art techniques based either on data replication or classical interpolation
Joint Semiblind frequency_offset and channel_estimation for multiuser_mimo OFDM Uplink
A semiblind method is proposed for simultaneously estimating the carrier_frequency_offsets CFOs and channels of an uplink multiuser multiple-input_multiple-output orthogonal frequency-division multiplexing MIMO-OFDM system By incorporating the CFOs into the transmitted symbols and channels the MIMO-OFDM with CFO is remodeled into an MIMO-OFDM without CFO The known blind method for channel_estimation Zeng and Ng in 2004 Y H Zeng and T S Ng ldquoA semi-blind_channel_estimation_method for multi-user multi-antenna ofdm_systems rdquo IEEE Trans Signal Process vol 52 no 5 pp 1419-1429 May 2004 is then directly used for the remodeled system to obtain the shaped channels with an ambiguity matrix A pilot OFDM block for each user is then exploited to resolve the CFOs and the ambiguity matrix Two dedicated pilot designs periodical and consecutive pilots are discussed Based on each pilot design and the estimated shaped channels two methods are proposed to estimate the CFOs As a result based on the second-order_statistics SOS of the received signal and one pilot OFDM block the CFOs and channels are found simultaneously Finally a fast equalization method is given to recover the signals corrupted by the CFOs
The cluster density of a distributed clustering_algorithm in ad_hoc_networks
Given is a wireless multihop network whose nodes are randomly distributed according to a homogeneous poisson_point_process of density spl rho in nodes per unit area The network employs Basagni s distributed mobility-adaptive clustering DMAC algorithm to achieve a self-organizing network structure We show that the cluster density i e the expected number of cluster- heads per unit area is spl rho sub c spl rho spl divide 1 spl mu spl divide 2 where spl mu denotes the expected number of neighbors of a node Consequently a clusterhead is expected to incorporate half of its neighboring nodes into its cluster This result also holds in a scenario with mobile_nodes and serves as a bound for inhomogeneous spatial node distributions
Fault-tolerant ring embedding in faulty arrangement graphs
The arrangement graph A sub n k which is a generalization of the star_graph n-t 1 presents more flexibility than the star_graph in adjusting the major design parameters number of nodes degree and diameter Previously the arrangement graph has proven hamiltonian In this paper we further show that the arrangement graph remains hamiltonian even if it is faulty Let F sub e and F sub v denote the numbers of edge faults and vertex faults respectively We show that A sub n k is hamiltonian when 1 k 2 and n-k spl ges 4 or k spl ges 3 and n-k spl ges 4 k 2 and F sub e spl les k n-k-2 -1 or 2 k spl ges 2 n-k spl ges 2 k 2 and F sub e spl les k n-k-3 -1 or 3 k spl ges 2 n-k spl ges 3 and F sub 3 spl les k
The Local Structure of a Bipartite Distance-regular Graph
In this paper we consider a bipartite distance-regular graph X E with diameter d 3 We investigate the local structure of focusing on those vertices with distance at most 2 from a given vertex x To do this we consider a subalgebra R R x ofMat0307a0x gif X C where 0307a1x gifX denotes the set of vertices in X at distance 2 from x R is generated by matrices A 0307a2x gif J and 0307a3x gif D defined as follows For all y z 0307a4x gif X the y z -entry of A is 1 if y z are at distance 2 and 0 otherwise The y z -entry of 0307a5x gif J equals 1 and the y z -entry of 0307a6x gif D equals the number of vertices of X adjacent to each ofx y and z We show that R is commutative and semisimple with dimension at least 2 We assume thatdimR is one of 2 3 or 4 and explore the combinatorial implications of this We are motivated by the fact that if has a Q-polynomial structure thendimR 4
Prerequesites for symbiotic brain-machine_interfaces
Recent advancements in the neuroscience and engineering of brain-machine_interfaces are providing a blueprint for how new co-adaptive designs based on reinforcement_learning change the nature of a user s ability to accomplish tasks that were not possible using static methodologies By designing adaptive_controls and artificial_intelligence into the neural interface computers can become active assistants in goal-directed behavior and further enhance human performance This paper presents a set of minimal prerequisites that enable a cooperative symbiosis and dialogue between biological and artificial systems
Computation and analysis of natural compliance in fixturing and grasping arrangements
This paper computes and analyzes the natural compliance of fixturing and grasping arrangements Traditionally linear-spring contact_models have been used to determine the natural compliance of multiple contact arrangements However these models are not supported by experiments or elasticity theory We derive a closed-form formula for the stiffness matrix of multiple contact arrangements that admits a variety of nonlinear contact_models including the well-justified Hertz model The stiffness matrix formula depends on the geometrical and material properties of the contacting bodies and on the initial loading at the contacts We use the formula to analyze the relative influence of first- and second-order geometrical effects on the stability of multiple contact arrangements Second-order effects i e curvature effects are often practically beneficial and sometimes lead to significant grasp stabilization However in some contact arrangements curvature has a dominant destabilizing influence Such contact arrangements are deemed stable under an all-rigid body model but in fact are unstable when the natural compliance of the contacting bodies is taken into account We also consider the combined influence of curvature and contact preloading on stability Contrary to conventional wisdom under certain curvature conditions higher preloading can increase rather than decrease grasp stability Finally we use the stiffness matrix formula to investigate the impact of different choices of contact model on the assessment of the stability of multiple contact arrangements While the linear-spring model and the more realistic Hertz model usually lead to the same stability conclusions in some cases the two models lead to different stability results
Adapting Information Theoretic Clustering to Binary Images
We consider the problem of finding points_of_interest along local curves of binary images Information theoretic vector_quantization is a clustering_algorithm that shifts cluster_centers towards the modes of principal curves of a data set Its runtime characteristics however do not allow for efficient processing of many data_points In this paper we show how to solve this problem when dealing with data on a 2D lattice Borrowing concepts from signal_processing we adapt information theoretic clustering to the quantization of binary images and gain significant speedup
Construction of robust prognostic predictors by using projective adaptive resonance theory as a gene filtering method
Motivation For establishing prognostic predictors of various diseases using DNA microarray analysis technology it is desired to find selectively significant genes for constructing the prognostic model and it is also necessary to eliminate non-specific genes or genes with error before constructing the model R N R N Results We applied projective adaptive resonance theory PART to gene screening for DNA microarray data Genes selected by PART were subjected to our FNN-SWEEP modeling method for the construction of a cancer class prediction model The model performance was evaluated through comparison with a conventional screening signal-to-noise S2N method or nearest shrunken centroids NSC method The FNN-SWEEP predictor with PART screening could discriminate classes of acute leukemia in blinded data with 97 1 accuracy and classes of lung cancer with 90 0 accuracy while the predictor with S2N was only 85 3 and 70 0 or the predictor with NSC was 88 2 and 90 0 respectively The results have proven that PART was superior for gene screening R N R N Availability The software is available upon request from the authors R N R N Contact honda nubio nagoya-u ac jp
The relationship between minimum entropy control and risk-sensitive control for time-varying systems
The connection between minimum entropy control and risk-sensitive control for linear time-varying systems is investigated For time-invariant_systems the entropy functional and the linear exponential quadratic Gaussian cost are the same In this paper it is shown that this is not true for general time varying systems It does hold however when the system admits a state-space representation
A Novel Hybrid Parallel-Prefix Adder Architecture With Efficient Timing-Area Characteristic
Two-operand binary addition is the most widely used arithmetic operation in modern datapath designs To improve the efficiency of this operation it is desirable to use an adder with good performance and area tradeoff characteristics This paper presents an efficient carry-lookahead adder architecture based on the parallel-prefix computation graph In our proposed method we define the notion of triple-carry-operator which computes the generate and propagate signals for a merged block which combines three adjacent blocks We use this in conjunction with the classic approach of the carry-operator to compute the generate and propagate signals for a merged block combining two adjacent blocks The timing-driven nature of the proposed design reduces the depth of the adder In addition we use a ripple-carry type of structure in the nontiming critical portion of the parallel-prefix computation network These techniques help produce a good timing-area tradeoff characteristic The experimental results indicate that our proposed adder is significantly faster than the popular Brent-Kung adder with some area_overhead On the adder hand the proposed adder also shows marginally faster performance than the fast Kogge-Stone adder with significant area savings
Control of Doubly-Fed Induction Generator System Using PIDNNs
An intelligent_control stand-alone doubly-fed induction generator DFIG system using proportional-integral-derivative neural_network PIDNN is proposed in this study This system can be applied as a stand-alone power supply system or as the emergency power system when the electricity grid fails for all sub-synchronous synchronous and super-synchronous conditions The rotor side converter is controlled using the field-oriented control to produce three-phase stator voltages with constant magnitude and frequency at different rotor speeds Moreover the stator side converter which is also controlled using field-oriented control is primarily implemented to maintain the magnitude of the DC-link voltage Furthermore the intelligent PIDNN controller is proposed for both the rotor and stator side converters to improve the transient and steady-state responses of the DFIG system_for different operating conditions Both the network structure and on-line_learning algorithm are introduced in detail Finally the feasibility of the proposed control scheme is verified through experimentation
Password-based tripartite key exchange protocol with forward secrecy
A tripartite authenticated_key_agreement protocol is designed for three entities to communicate securely over an open network particularly with a shared key password-authenticated_key_exchange PAKE allows the participants to share a session_key using a human memorable password only In this paper A password-based authenticated tripartite key exchange protocol 3-PAKE is presented in the_standard_model The security of the protocol is reduced to theDecisional Bilinear Diffie-Hellman DBDH problem and the protocol provides not only the properties of forward secrecy but also resistance against known key attacks The proposed protocol is more efficient than the similar protocols in terms of both communication and computation
Evaluation of high-altitude balloons as a learning_technology
The utility of high-altitude balloons as a learning_technology to facilitate education in several disciplines is considered in this paper The role that a high-altitude balloon can play as a learning_technology is discussed and its utility in this role is considered The need for a formal design framework for high-altitude ballooning is also discussed A framework for the assessment of high-altitude ballooning in supporting an undergraduate-level course is evaluated and an assessment using this framework is conducted The paper concludes with a discussion of techniques that can be used to broaden access to high-altitude ballooning in education
Automatic Discovery of Action Taxonomies from multiple_views
We present a new method for segmenting actions into primitives and classifying them into a hierarchy of action classes Our scheme learns action classes in an unsupervised manner using examples recorded by multiple cameras Segmentation and clustering of action classes is based on a recently proposed motion descriptor which can be extracted efficiently from reconstructed volume sequences Because our representation is independent of viewpoint it results in segmentation and classification_methods which are surprisingly efficient and robust Our new method can be used as the first step in a semi-supervised action_recognition system that will automatically break down training examples of people performing sequences of actions into primitive actions that can be discriminatingly classified and assembled into high-level recognizers
Dr VIS a database of human disease-related viral integration sites
Viral integration plays an important role in the development of malignant diseases Viruses differ in preferred integration site and flanking sequence Viral integration sites VIS have been found next to oncogenes and common fragile sites Understanding the typical DNA features near VIS is useful for the identification of potential oncogenes prediction of malignant disease development and assessing the probability of malignant transformation in gene therapy Therefore we have built a database of human disease-related VIS Dr VIS http www scbit org dbmi drvis to collect and maintain human disease-related VIS data including characteristics of the malignant disease chromosome region genomic position and viral host junction sequence The current build of Dr VIS covers about 600 natural VIS of 5 oncogenic viruses representing 11 diseases Among them about 200 VIS have viral host junction sequence
Research on Internet marketing relationship model
The present models about Internet marketing to a certain extent have some limits which cannot systematically reveals the process and characteristics of Internet marketing Based on the theories of traditional marketing and Internet marketing the paper builds the model of Internet marketing relationship that describes consumer s purchase decision process and firm s Internet marketing process and correlation among consumer firm and bank and logistics firm Through systematically analyzing the two processes the model reveals intrinsic characteristics and essences of Internet marketing that are all different from traditional marketing The model provides a researchful platform for the researchers and a fundamental basis for further researching Internet marketing
Expanded rectangles a new VLSI data structure
A data structure derived from corner stitching which allows efficient representation of VLSI layouts is presented While each entry in the expanded rectangle database is larger than the corresponding corner-stitched entry generally fewer entries are required to represent the same VLSI layout The data structure has two important features first the vlsi_design is represented as a slicing structure in which each slice contains a portion of the solid material and second corner stitches are used to provide two-dimensional nearness information Initial measurements indicate that expanded rectangles is a viable data structure for use in a complete VLSI layout system
Predicting opponent actions by observation
In competitive domains the knowledge about the opponent can give players a clear advantage This idea lead us in the past to propose an approach to acquire models of opponents based only on the observation of their input-output behavior If opponent outputs could be accessed directly a model can be constructed by feeding a machine_learning method with traces of the opponent However that is not the case in the Robocup domain To overcome this problem in this paper we present a three phases approach to model low-level behavior of individual opponent agents First we build a classifier to label opponent actions based on observation Second our agent observes an opponent and labels its actions using the previous classifier From these observations a model is constructed to predict the opponent actions Finally the agent uses the model to anticipate opponent reactions In this paper we have presented a proof-of-principle of our approach termed OMBO Opponent Modeling Based on Observation so that a striker agent can anticipate a goalie Results show that scores are significantly higher using the acquired opponent s model of actions
web_accessibility compliance of government web sites in Korea
This paper introduces Korean web_accessibility activities such as relational laws ordinances policies guidelines It also presents analytical result of the investigation on web-contents accessibilities of the 39 Korean government agencies The result shows that only one agency provides web_contents satisfying all the minimum requirements while 97 of the agencies does not satisfy all the minimum requirements Unfortunately 6 agencies do not satisfy any
Free-Viewpoint Video Sequences a New Challenge for Objective Quality Metrics
Free-viewpoint television is expected to create a more natural and interactive viewing experience by providing the ability to interactively change the viewpoint to enjoy a 3D scene To render new virtual_viewpoints free-viewpoint systems rely on view_synthesis However it is known that most objective metrics fail at predicting perceived quality of synthesized views Therefore it is legitimate to question the reliability of commonly used objective metrics to assess the quality of free-viewpoint video FVV sequences In this paper we analyze the performance of several commonly used objective quality metrics on FVV sequences which were synthesized from decompressed depth data using subjective scores as ground truth Statistical analyses showed that commonly used metrics were not reliable predictors of perceived image_quality when different contents and distortions were considered However the correlation improved when considering individual conditions which indicates that the artifacts produced by some view_synthesis algorithms might not be correctly handled by current metrics
Dealing with Hardware in embedded_software A General Framework Based on the Devil Language
Writing code that talks to hardware is a crucial part of any embedded project Both productivity and quality are needed but some flaws in the traditional development process make these requirements difficult to meet We have recently introduced a new approach of dealing with hardware based on the Devil language Devil allows to write a high-level formal definition of the programming interface of a peripheral circuit A compiler automatically checks the consistency of a Devil specification from which it generates the low-level hardware-operating code In our original framework the generated code is dependent of the host architecture CPU buses and bridges Consequently any variation in the hardware environment requires a specific tuning of the compiler Considering the variability of embedded architectures this is a serious drawback In addition this prevents from mixing different buses in the same circuit interface In this paper we remove those limitations by improving our framework in two ways i We propose a better isolation between the Devil compiler and the host architecture ii We introduce Trident a language extension aimed at mapping one or several buses to each peripheral circuit
Uniprocessor Scheduling Under Precedence Constraints
In this paper we present a novel approach to the constrained scheduling problem while addressing a more general class of constraints that arise from the timing requirements on real-time embedded controllers and from the implementation of mixed data-flow event-driven real-time_systems We provide general necessary and sufficient_conditions for scheduling under precedence constraints and derive sufficient_conditions for two well-known scheduling policies We define mathematical problems that provide optimum priority and deadline assignments while ensuring both precedence constraints and system s schedulability We show how these problems can be relaxed to corresponding ilp_formulations leveraging on available solvers
Adaptive Multi-Layer traffic_engineering with Shared Risk Group Protection
In this paper we propose a new traffic_engineering scheme to be used jointly with protection in multi-layer grooming-capable optical-beared networks To make the working and protection paths of demands better adapt to changing traffic and network conditions we propose the adaptive multi-layer traffic_engineering AMLTE scheme that tailors i e fragments and de-fragments wavelength paths in a fully automatic distributed way
Endowing Spoken Language dialogue_systems with Emotional Intelligence
While most dialogue_systems restrict themselves to the adjustment of the propositional contents our work concentrates on the generation of stylistic va- riations in order to improve the user s perception of the interaction To accomplish this goal our approach integrates a social theory of politeness with a cognitive theory of emotions We propose a hierarchical selection process for politeness behaviors in order to enable the refinement of decisions in case additional context_information becomes available
A multiple subset sum formulation for feedback implosion suppression over satellite_networks
In this paper we present a feedback implosion suppression FIS algorithm that reduces the volume of feedback information transmitted through the network without relying on any collaboration between users or on any infrastructure other than the satellite_network Next generation satellite_systems that utilize the Ka frequency band are likely to rely on various fade mitigation techniques in order to guarantee a service_quality that is comparable to other broadband technologies User feedback would be a valuable input for a number of such components however collecting periodic feedback from a large number of users would result in the well-known feedback implosion problem Feedback implosion is identified as a major problem when a large number of users try to transmit their feedback messages through the network holding up a significant portion of the uplink resources and clogging the shared uplink medium In this paper we look at a system where uplink channel_access is organized in time-slots The goal of the FIS algorithm is to reduce the number of uplink time-slots hold up for the purpose of feedback transmission Our analysis show that the FIS algorithm effectively suppresses the feedback messages of 95 of all active users but still achieves acceptable performance results when the ratio of available time-slots to number of users is equal to or higher than 5
Computing subgraph probability of random geometric graphs with applications in quantitative analysis of ad_hoc_networks
Random geometric graphs RGG contain vertices whose points are uniformly distributed in a given plane and an edge between two distinct nodes exists when their distance is less than a given positive value RGGs are appropriate for modeling ad_hoc_networks consisting of n mobile_devices that are independently and uniformly distributed randomly in an area To the best of our knowledge this work presents the first paradigm to compute the subgraph probability of RGGs in a systematical way In contrast to previous asymptotic bounds or approximation which always assume that the number of nodes in the network tends to infinity the closed-form formulas we derived herein are fairly accurate and of practical value Moreover computing exact subgraph probability in RGGs is shown to be a useful tool for counting the number of induced_subgraphs which explores fairly accurate quantitative property on topology of ad_hoc_networks
unequal_error_protection for video_streaming Over wireless_lans using Content-Aware Packet Retry Limit
In this paper we propose a content-aware retry limit adaptation scheme for video_streaming over IEEE 802 11 wireless_lans WLANs video_packets of different importance are unequally protected with different retry limits at the mac_layer The loss impact of each packet is estimated to guide the selection of its retry limit More retry numbers are allocated to packets of higher loss impact to achieve unequal_error_protection Experimental results show that the proposed adaptation scheme can effectively mitigate the error propagation due to packet_loss and assure the on-time arrival of packets for presentation thereby improving video_quality significantly
Linearization of ancestral multichromosomal genomes
Background R N Recovering the structure of ancestral genomes can be formalized in terms of properties of binary matrices such as the Consecutive-Ones Property C1P The Linearization Problem asks to extract from a given binary matrix a maximum weight subset of rows that satisfies such a property This problem is in general intractable and in particular if the ancestral genome is expected to contain only linear chromosomes or a unique circular chromosome In the present work we consider a relaxation of this problem which allows ancestral genomes that can contain several chromosomes each either linear or circular
OdinTools--model-driven_development of Intelligent mobile_services
Today s computationally able mobile_devices are capable of acting as service providers as opposed to their traditional role as consumers To address the challenges associated with the development of these mobile_services we have developed Odin a middleware which masks complexity allowing rapid development of mobile_services Odin however does not allow cross-platform development which is an important concern with today s wide variety of mobile_devices To solve this problem we have designed Odin Tools - a model-driven toolkit for cross-platform development of mobile_services Leveraging appropriate metamodels a prototype has been implemented in Eclipse and Marama that allows developers to model mobile_services in a platform-independent manner We are currently working on transformations between levels of the model hierarchy which will allow full Odin-based service implementations to be generated automatically
A socio political model of the relationship between it_investments and business performance
In recent years many studies have been published on the assessment of payoffs from investments in IT The research has produced mainly mixed results Different explanations can be given for these mixed results One possible explanation might be the dominance of the rational perspective in previous research The consequence of this overemphasis on rationality is that the social political nature of the it_investment process has largely been neglected in previous research on IT business value This omission produces an incomplete picture and might contribute to the conflicting empirical results In this research we studied whether and how the socio political perspective can be used to explain the mixed results Case study research was used to test whether the attitude towards the value of IT destructive conflict and a low level of trust influence the relationship between it_investments and business performance
A new metrics set for evaluating testing efforts for object-oriented programs
software_metrics proposed and used for procedural paradigm have been found inadequate for object_oriented_software_products mainly because of the distinguishing features of the object_oriented paradigm such as inheritance and polymorphism Several object_oriented_software_metrics have been described in the literature These metrics are goal driven in the sense that they are targeted towards specific software qualities We propose a new set of metrics for object_oriented programs this set is targeted towards estimating the testing efforts for these programs The definitions of these metrics are based on the concepts of object orientation and hence are independent of the object_oriented_programming_languages The new metrics set has been critically compared with three other metrics sets published in the literature
Towards a universal client for grid monitoring systems_design and implementation of the Ovid browser
In this paper we present the design and implementation of Ovid a browser for grid-related information The key goal of Ovid is to support the seamless navigation of users in the grid information space Key aspects of Ovid are i a set of navigational primitives which are designed to cope with problems such as network disorientation and information_overloading ii a small set of Ovid views which present the end-user with high-level visual abstractions of grid information these abstractions correspond to simple models that capture essential aspects of a grid infrastructure iii support for embedding and implementing hyperlinks that connect related entities represented within different information views iv a plug-in mechanism which enables the seamless integration with Ovid of third-party software that retrieves and displays data from various grid information sources and v a modular software_design which allows the easy integration of different visualization algorithms that support the graphical_representation of large amounts of grid-related information in the context of Ovid s views
A trainable single-pass algorithm for column segmentation
Column segmentation logically precedes OCR in the document_analysis process The trainable algorithm XYCUT relies on horizontal and vertical binary profiles to produce an XY-tree representing the column structure of a page of a technical document in a single pass through the bit image Training against ground truth adjusts a single resolution independent parameter using only local information and guided by an edit distance function The algorithm correctly segments the page image for a fairly wide range of parameter values although small local and repairable errors may be made an effect measured by a repair cost function
Hybrid Wordlength optimization_methods of Pipelined fft_processors
Quickly and accurately predicting the performance based on the requirements for IP-based system implementations optimizes the design and reduces the design time and overall cost This study describes a novel hybrid method for the word-length optimization of pipelined fft_processors that is the arithmetic kernel of OFDM-based systems This methodology utilizes the rapid computing of statistical_analysis and the accurate evaluation of simulation-based analysis to investigate a speedy optimization flow A statistical error model for varying word-lengths of PE stages of an FFT processor was developed to support this optimization flow Experimental results designate that the word-length optimization employing the speedy flow reduces the percentage of the total area of the FFT processor that increases with an increasing FFT length Finally the proposed hybrid method requires a shorter prediction time than the absolute simulation-based method does and achieves more accurate outcomes than a statistical calculation does
Generating organic textures with_controlled anisotropy and directionality
This article presents a method for generating organic textures by tessellating a region into a set of pseudo-Voronoi polygons using a particle model and then generating the detailed geometry each of the polygons with fractal noise
Guidelines for reporting an fMRI study
In this editorial we outline a set of guidelines for the reporting of methods and results in functional magnetic resonance imaging studies and provide a checklist to assist authors in preparing manuscripts that meet these guidelines
On Load Regulated CSMA
In this paper we derive throughput of a threshold-based transmission policy namely load-regulated CSMA taking into account the propagation delay of the medium and the offered load at different probability of the fading_channel In case of the saturated load regulated CSMA a trivial relationship between deterministic offered load to the channel at a particular fading_channel condition and the maximum possible offered load has been shown We further extend the load regulation concept into multi-channel domain Both single and multi-channel load regulated CSMA improves the throughput of the system compared to the existing CSMA system which does not consider channel fading to control the packet_transmissions
component_based_design using constraint_programming for module placement on FPGAs
Constraint satisfaction modeling is both an efficient and an elegant approach to model and solve many real world problems In this paper we present a constraint solver targeting module placement in static and partial run-time_reconfigurable_systems We use the constraint solver to compute feasible placement positions Our placement model incorporates communication implementation variants and device configuration granularity In addition we model heterogeneous resources such as embedded_memory multipliers and logic Furthermore we take into account that logic resources consist of different types including logic only LUTs arithmetic LUTs with carry chains and LUTs with distributed_memory Our work targets state of the art field-programmable_gate_arrays FPGAs in both design-time and run-time applications In order to evaluate our placement model and module placer implementation we have implemented a repository containing 200 fully functional placed and routed relocatable modules The modules are used to implement complete systems This validates the feasibility of both the model and the module placer Furthermore we present simulated results for run-time applications and compare this to other state of the art research In run-time applications the results point to improved resource utilization This is a result of using a finer tile grid and complex module shapes
E-BRAINSTORMING OPTIMIZATION OF collaborative_learning THANKS TO ONLINE QUESTIONNAIRES
The purpose of this article is to present a methodology and tools allowing the use of online multiple-choice questionnaires to enhance collaborative_work The first goal is to allow the questionnaires generation and setting with a simple and ergonomic manner but also to let questioned people making comments and proposing new questions to other contributors The developed system provides a visualization of a synthesis of the questionnaire results that is also accessible by the mean of external applications through standard Web services These principles were developed and tested on a sample of users
Bioinformatics integration framework for metabolic pathway data-mining
A vast amount of bioinformatics information is continuously being introduced to different databases around the world Handling the various applications used to study this information present a major data_management and analysis challenge to researchers The present work investigates the problem of integrating heterogeneous applications and databases towards providing a more efficient data-mining environment for bioinformatics research A framework is proposed and GeXpert an application using the framework towards metabolic pathway determination is introduced Some sample implementation results are also presented
schedulability_analysis of fixed priority real-time_systems with offsets
For a number of years work has been performed in collaboration with industry to establish improved techniques for achieving and proving the system timing_constraints The specific requirements encountered during the course of this work for both uniprocessor and distributed_systems indicate a need for an efficient mechanism for handling the timing_analysis of task sets which feature offsets Little research has been performed on this subject The paper describes a new technique tailored to a set of real world problems so that the results are effective and the complexity is manageable
change_impact_analysis for Generic Libraries
Since the Standard Template Library STL generic libraries in C rely on concepts to precisely specify the requirements of generic algorithms function templates on their parameters template arguments Modifying the definition of a concept even slightly can have a potentially large impact on the interfaces of the entire library In particular the non-local effects of a change however make its impact difficult to determine by hand In this paper we propose a conceptual change_impact_analysis CCIA which determines the impact of changes of the conceptual specification of a generic library The analysis is organized in a pipe-and-filter manner where the first stage finds any kind of impact the second stage various specific kinds of impact Both stages describe reachability algorithms which operate on a conceptual dependence graph In a case study we apply CCIA to a new proposal for STL iterator concepts which is under review by the C standardization committee The analysis shows a number of unexpected incompatibilities and for certain STL algorithms a loss of genericity
A model and case study for efficient shelf usage and assortment analysis
In the rapidly changing environment of Fast Moving Consumer Goods sector where new product launches are frequent retail channels need to reallocate their shelf spaces intelligently while keeping up their total profit margins and to simultaneously avoid product pollution In this paper we propose an optimization model which yields the optimal product mix on the shelf in terms of profitability and thus helps the retailers to use their shelves more effectively The model is applied to the shampoo product class at two regional supermarket chains The results reveal not only a computationally viable model but also substantial potential increases in the profitability after the reorganization of the product list
Multiprocessors May Reduce System Dependability under File-Based Race Condition Attacks
Attacks exploiting race conditions have been considered rare and low risk However the increasing popularity of multiprocessors has changed this situation instead of waiting for the victim process to be suspended to carry out an attack the attacker can now run on a dedicated processor and actively seek attack opportunities This change from fortuitous encountering to active exploiting may greatly increase the success probability of race condition attacks This point is exemplified by studying the TOCTTOU Time-of- Check-to-Time-of-Use race condition attacks in this paper We first propose a probabilistic_model for predicting TOCTTOU attack success rate on both uniprocessors and multiprocessors Then we confirm the applicability of this model by carrying out TOCTTOU attacks against two widely used utility programs vi and gedit The success probability of attacking vi increases from low single digit percentage on a uniprocessor to almost 100 on a multiprocessor Similarly the success rate of attacking gedit jumps from almost zero to 83 These case studies suggest that our model captures the sharply increased risks and hence the decreased dependability of our systems represented by race condition attacks such as TOCTTOU on the next generation multiprocessors
Members of Random Closed Sets
The members of Martin-Lof random closed sets under a distribution studied by Barmpalias et al are exactly the infinite paths through Martin-Lof random Galton-Watson trees with survival parameter frac 2 3 To be such a member a sufficient condition is to have effective Hausdorff dimension strictly greater than gamma log_2 frac 3 2 and a necessary condition is to have effective Hausdorff dimension greater than or equal to
information_extraction from nanotoxicity related publications
High-quality experimental data are important when developing predictive models for studying nanomaterial environmental impact NEI Given that raw data from experimental laboratories and manufacturing workplaces are usually proprietary and small-scaled extracting_information from publications is an attractive alternative for collecting data We developed an information_extraction system that can extract useful information from full-text nanotoxicity related publications This information_extraction system consists of five components raw data transformation into machine readable format data preprocessing ontology-based named_entity_recognition rule-based numerical attribute extraction from both tables and unstructured text and relation_extraction among entities and attributes The information_extraction system is applied on a dataset made of 94 publications and results in an acceptable accuracy By storing extracted data into a table according to relations among the data a dataset that can be used to predict nanomaterial environmental impact is obtained Such a system is unique in current nanomaterial community and can help nanomaterial scientists and practitioners quickly locate useful information they need without spending lots of time reading articles
CMOS body-enhanced cascode current mirror
A cascode current mirror with auxiliary body-driven feedback loop is proposed Main performance parameters are analytically evaluated and compared to those of a conventional high-swing cascode and of a recently-proposed body-driven topology Simulations are also provided confirming improvements in the achievable output resistance important for short channel technologies DC accuracy and input dynamic range Linearity bandwidth noise and voltage requirements are substantially the same of the conventional high-swing cascode solution
Performance of the beacon-less routing_protocol in realistic scenarios
The beacon-less routing_protocol BLR is a position-based routing_protocol for mobile_ad-hoc_networks that makes use of location_information to reduce routing overhead Unlike other position-based routing_protocols BLR does not require nodes to periodically broadcast hello messages This avoids drawbacks such as extensive use of scarce battery-power interferences with regular data transmission and outdated position information in case of high_mobility This paper discusses the behavior and performance of BLR in realistic scenarios in particular with irregular transmission_ranges BLR has been implemented using appropriate simulation models and in an out-door test-bed consisting of GNU Linux laptops with wireless_lan network_interfaces and gps_receivers
Two-dimensional orthogonal tiling from theory to practice
In pipelined parallel_computations the inner loops are often implemented in a block fashion In such programs an important compiler_optimization involves the need to statically determine the grain size This paper presents extensions and experimental validation of the previous results of Andonov and Rajopadhye 1994 on optimal grain size determination
A two-level ECN marking for fair bandwidth_allocation between HSTCP and tcp_reno
Many versions of TCP have been proposed for transmitting data and among them tcp_reno is most widely used today However the problem that it is difficult to use the wide_bandwidth efficiently with tcp_reno has been pointed out HSTCP is one of the several new versions of TCP that are proposed to address this problem but when its flows compete with tcp_reno flows at the same link HSTCP gains most of the bandwidth and it is impossible to conduct fair transfer In order to address this problem we propose a two-level ECN marking to increase the frequency of congestion_controls of HStcp_flows holding its throughput We evaluate our proposal through computer simulations and the results show that our proposal mitigates bandwidth_allocation to HSTCP promoting fair transfer with tcp_reno
A distributed simulation based monitoring
MSS a computer-based monitoring system with integrated cooperative objects is proposed MSS uses an object-based framework to interface with the user to guide a specific system evolution MSS espouses a blackboard architecture and runs according a cooperating objects model To achieve monitoring tasks MSS selects the appropriate technique s within a set of a high performance algorithms From the user viewpoint MSS has been developed as a control assistant featuring different levels of interactivity a hierarchical design style and fully embedded algorithmic tools Virtually MSS is able to design a monitoring board for any dynamic system
Leakage power reduction in dual-Vdd and dual-Vth designs through probabilistic analysis of Vth variation
The noise sensitivity of low_power circuits is rapidly increasing with the increasing levels of process_variability and uncertainty In this work we study the problem of leakage power minimization in dual-Vdd and dual-Vth designs in the presence of significant Vth variation The impact of the uncertainty in Vth on leakage power and timing are studied through probabilistic analytical models We develop probabilistic_models for timing slack and leakage power considering threshold variations with the objective of achieving an optimal selection of Vth An analysis of the models indicate that in the presence of variability the value of the second Vth must be about 30mV higher than the Vth value obtained without considering variability We show that our proposed method for the selection of Vth yields the lowest leakage power ratio of the dual-Vdd and dual-Vth versus the single-Vdd and single-Vth designs In addition the proposed models can be used to determine the ideal values for the second Vdd and Vth values in the context of variability for a variety of process conditions
Probabilistic Cluster Signature for Modeling Motion Classes
In this paper a novel 3-D motion trajectory signature is introduced to serve as an effective description to the raw trajectory More importantly based on the trajectory signature a probabilistic_model-based cluster signature is further developed for modeling a motion class The cluster signature is a mixture model-based motion description that is useful for motion class perception recognition and to benefit a generalized robot task representation The signature modeling process is supported by integrating the EM and IPRA algorithms The conducted experiments verified the cluster signature s effectiveness
Secure cross-domain positioning architecture for autonomic_systems
Positioning as one of the prime components of context has been a driving factor in the development of ubiquitous_computing applications throughout the past two decades Based on the redundant positioning architecture this paper discusses the issues of exchanging positioning data between applications and between different administrative domains with a focus on implementing security_and_privacy concepts in a self-learning and self-adapting autonomic_systems environment
Refinement of medical knowledge_bases a neural_network approach
One important issue in designing medical knowledge-based_systems is the management of uncertainty Among the schemes that have been developed for this purpose probability and CF certainty factor are the most widely used If rules are organized according to a connectionist model then neural_network learning suggests a promising solution to this problem When most rules are correct semantically incorrect rules can be recognized if their associated certainty factors are weakened or change signs after training with correct samples The techniques for rule_base refinement are examined under this approach The concept has been implemented and tested in an actual medical expert_system
An IT appliance for remote collaborative review of mechanisms of injury to children in motor vehicle crashes
This paper describes the architecture and implementation of a Java-based appliance for collaborative review of crashes involving injured children in order to determine mechanisms of injury The multidisciplinary expertise needed for such reviews is not available at any one institution resulting in the need for remote collaboration while the sensitive nature of the information requires secure transmission and controlled access of data The intended users of the appliance are researchers engineers medical doctors government regulators automobile and restraint manufacturers insurance company representatives and others who are interested in understanding the types and causes of injuries to children involved in motor vehicle crashes The ultimate goal is to devise engineering solutions that prevent similar injuries from occurring in the future The collaboration appliance called Telecenter enables the following activities 1 the distributed asynchronous collection of digital_content needed for each crash case review under a scheme that consistently organizes content across multiple cases 2 the secure Web-based remote participation of users in case-review meetings that involve viewing of case-specific content live communication written or verbal multimedia access and sharing slide presentations images and use of web_resources and 3 archival and post-review access of case reviews for follow-up activities and other functions e g statistics search and networking The Telecenter design supports audio conferencing remote delivery and viewing of slide presentations and other collaboration features also available in commercial and public-domain collaboration middleware products However it goes beyond existing solutions by also embedding a specific workflow and content organization suited for traffic injury reviews supporting spatio-temporal role-based_access_control distributed management of content and seamless integration of existing services The current status and experience from using an early prototype of the Telecenter in actual case reviews are discussed along with planned extensions to its functionality
Strategic Design of the Purchase System Toward R D Supply Chain Based on SNA
In order to make the strategy for research and development R D purchase system better serve the personalization of material requirements in R D process the authors propose to develop a strategy set which will satisfy the internal as well as external constraints simultaneously social_network_analysis is used to analyze the vertical and horizontal relationships among the project department and enterprise layers Through a case study the authors display the regulatory relationship of participants under given organization pattern and supply chain configuration To disclose the restricted equilibrium mechanism of participants involved changes under different strategies are compared which can assist enterprises to enforce the decision making and to improve the R D purchase system ability The authors outline some of the managerial implications arising from the research findings at the end of this paper
Exploring early usage patterns of mobile data services
In this paper we study the nature of factors that facilitate mobile data services use as well as the characteristics of early adopters to shed light into diffusion patterns and inform predictions for future growth We advocate that the use of mobile data services can be associated with one s level of satisfaction with his her life Based on the findings of a questionnaire-based survey N 388 we have found that users satisfied with their personal life use information mobile e-mail and stock broking services more frequently than dissatisfied ones while users satisfied with their professional life tend to use financial information and mobile e-mail services more heavily Furthermore we identify early adopters profiles in terms of their demographic characteristics gender age education and income to inform the design of effective target marketing strategies
A Fast Fixed Point Iteration Algorithm for Sparse channel_estimation
Channels with a long but sparse impulse_response arise in a variety of wireless_communication applications such as high_definition_television HDTV terrestrial transmission and underwater_acoustic_communications By adopting the ell_1 -norm as the sparsity metric of the channel response the channel_estimation is formulated as a complex-valued convex optimization problem A fast fixed point iteration algorithm is developed to solve the resultant complex-valued ell_1 -minimization problem The proposed fast channel_estimation algorithm is easy to implement and has a low computational complexity of O N log N per iteration with N the signal length Simulation results are provided to demonstrate the performance of the proposed fixed point algorithm
Model-independent recovery of object orientations
A novel algorithm is presented for determining the orientation of road vehicles in traffic scenes using video images The algorithm requires no specific 3-D vehicle models and only uses local image gradient values It may easily be implemented in real-time Experimental results with a variety of vehicles in routine traffic scenes are included to demonstrate the effectiveness of the algorithm
Linear Scale and Rotation Invariant Matching
Matching visual patterns that appear scaled rotated and deformed with respect to each other is a challenging problem We propose a linear formulation that simultaneously matches feature points and estimates global geometrical transformation in a constrained linear space The linear scheme enables search space reduction based on the lower convex hull property so that the problem size is largely decoupled from the original hard combinatorial_problem Our method therefore can be used to solve large scale problems that involve a very large number of candidate feature points Without using prepruning in the search this method is more robust in dealing with weak features and clutter We apply the proposed method to action detection and image_matching Our results on a variety of images and videos demonstrate that our method is accurate efficient and robust
A built-in_self-testing approach for minimizing hardware overhead
A built-in_self-test BIST hardware insertion technique is addressed Applying to register_transfer_level designs this technique utilizes not only the circuit structure but also the module functionality in reducing test hardware overhead Experimental results have shown up to 38 reduction in area_overhead over other system level BIST techniques
Multi-Stage TR Scheme for papr_reduction in ofdm_signals
In the tone_reservation TR scheme of the orthogonal_frequency_division_multiplexing ofdm_systems there exists a trade-off between the peak to average power ratio papr_reduction performance and the peak reduction tone PRT set size In this paper we propose a multi-stage TR scheme for papr_reduction which adaptively selects one of several PRT sets according to the PAPR of ofdm_signal while the PRT set is fixed for the conventional TR scheme It is shown that the papr_reduction performance of the proposed scheme is better than that of the conventional TR scheme when the tone_reservation rate TRR is the same
Random key_predistribution_schemes for sensor_networks
key_establishment in sensor_networks is a challenging problem because asymmetric key cryptosystems are unsuitable for use in resource constrained sensor_nodes and also because the nodes could be physically compromised by an adversary We present three new mechanisms for key_establishment using the framework of pre-distributing a random set of keys to each node First in the q-composite keys scheme we trade off the unlikeliness of a large-scale network_attack in order to significantly strengthen random key_predistribution s strength against smaller-scale attacks Second in the multipath-reinforcement scheme we show how to strengthen the security between any two nodes by leveraging the security of other links Finally we present the random-pairwise_keys scheme which perfectly preserves the secrecy of the rest of the network when any node is captured and also enables node-to-node authentication and quorum-based revocation
Quantitative assessment of image_noise and streak artifact on ct_image comparison of z-axis automatic tube current modulation technique with fixed tube current technique
Abstract The purpose of our study is to quantitatively assess the effects of z -axis automatic tube current modulation technique on image_noise and streak artifact by comparing with fixed tube current technique Standard deviation of CT-values was employed as a physical index for evaluating image_noise and streak artifact was quantitatively evaluated using our devised Gumbel evaluation method z -Axis automatic tube current modulation technique will improve image_noise and streak artifact compared with fixed tube current technique and will make it possible to significantly reduce radiation doses at lung levels while maintaining the same image_quality as fixed tube current technique
Coordination policy for a two-stage supply chain considering quantity discounts and overlapped delivery with imperfect quality
Unlike the traditional integrated supplier-buyer coordination model this research incorporates overlapped delivery and imperfect items into the production-distribution model This model improves the observable fact that the system might experience shortage during the screening duration and also takes quantity discount into account This approach has not been discussed in previous integrated supplier-buyer coordination models The expected annual integrated total cost function is derived and properties and theorems are explored to help develop an algorithm A solution procedure free from the convexity associated with an algorithm is established to find the optimal solution A numerical_example is given to illustrate the proposed procedure and algorithm A sensitivity analysis is made to investigate the effects of five important parameters the inspect rate the annual demand the defective rate the holding cost and the receiving cost on the optimal solution Managerial insights are also discussed
Monte Carlo modeling for implantable fluorescent analyte sensors
A Monte Carlo simulation of photon propagation through human skin and interaction with a subcutaneous fluorescent sensing layer is presented The algorithm will facilitate design of an optical probe for an implantable fluorescent sensor which holds potential for monitoring many parameters of biomedical interest Results are analyzed with respect to output light intensity as a function of radial distance from source angle of exit for escaping photons and sensor fluorescence SF relative to tissue autofluorescence AF A sensitivity study was performed to elucidate the effects on the output due to changes in optical properties thickness of tissue layers thickness of the sensor layer and both tissue and sensor quantum yields The optical properties as well as the thickness of the stratum corneum epidermis tissue layers through which photons must pass to reach the sensor and the papillary dermis tissue distal to sensor are highly influential The spatial emission profile of the SF is broad compared that of the tissue fluorescence and the ratio of sensor to tissue fluorescence increases with distance from the source The angular distribution of escaping photons is more concentrated around the normal for SF than for tissue AF The information gained from these simulations will he helpful in designing appropriate optics for collection of the signal of interest
Towards a data_publishing framework for primary biodiversity data challenges and potentials for the biodiversity informatics community
Background Currently primary scientific_data especially that dealing with biodiversity is neither easily discoverable nor accessible Amongst several impediments one is a lack of professional recognition of scientific_data_publishing efforts A possible solution is establishment of a data_publishing Framework which would encourage and recognise investments and efforts by institutions and individuals towards management and publishing of primary scientific_data potentially on a par with recognitions received for scholarly publications Discussion This paper reviews the state-of-the-art of primary biodiversity data_publishing and conceptualises a data_publishing Framework that would help incentivise efforts and investments by institutions and individuals in facilitating free and open_access to biodiversity data It further postulates the institutionalisation of a Data Usage Index DUI that would attribute due recognition to multiple players in the data collection creation management and publishing cycle Conclusion We believe that institutionalisation of such a data_publishing Framework that offers socio-cultural legal technical economic and policy environment conducive for data_publishing will facilitate expedited discovery and mobilisation of an exponential increase in quantity of fit-for-use primary biodiversity data much of which is currently invisible
On transistor level gate_sizing for increased robustness to transient_faults
In this paper we present a detailed analysis on how the critical charge Q sub crit of a circuit node usually employed to evaluate the probability of transient fault TF occurrence as a consequence of a particle hit depends on transistors sizing We derive an analytical model allowing us to calculate a node s Q sub crit given the size of the node s driving gate and fan-out gate s thus avoiding time costly electrical level simulations We verified that such a model features an accuracy of the 97 with respect to electrical level simulations performed by HSPICE Our proposed model shows that Q sub crit depends much more on the strength conductance of the gate driving the node than on the node total capacitance We also evaluated the impact of increasing the conductance of the driving gate on TFs propagation hence on soft_error susceptibility SES We found that such a conductance increase not only improves the TF robustness of the hardened node but also that of the whole circuit
Design-inclusive UX research design as a part of doing user_experience research
Since the third wave in human_computer_interaction HCI research on user_experience UX has gained momentum within the HCI community The focus has shifted from systematic usability_requirements and measures towards guidance on designing for experiences This is a big change since design has traditionally not played a large role in HCI research Yet the literature addressing this shift in focus is very limited We believe that the field of UX research can learn from a field where design and experiential aspects have always been important design research In this article we discuss why design is needed in UX research and how research that includes design as a part of research can support and advance UX design practice We do this by investigating types of design-inclusive UX research and by learning from real-life cases of UX-related design research We report the results of an interview study with 41 researchers in three academic research units where design research meets UX research Based on our interview findings and building on existing literature we describe the different roles design can play in research projects We also report how design research results can inform designing for experience methodologically or by providing new knowledge on UX The results are presented in a structured palette that can help UX researchers reflect and focus more on design in their research projects thereby tackling experience_design challenges in their own research
Multi-factory optimization enables kit reconfiguration in semiconductor manufacturing
To enable the huge saving of the kit-breakdown we developed MaxIt v1 2 to generate an optimal capacity plan at the kit component level for the mid-range build plan in multi-factory environment We describe the MILP mixed integer linear programming_model and system_architecture of MaxIt v1 2 We also conduct detailed sensitivity analysis on parameter setting and objective prioritizing With the implementation in the Intel Shanghai and Manila sites we have significantly improved data integrity and enabled a -US spl ges cost_savings
Eudaemonic computing underwearables
This paper presents a framework for wearable_computing based on the principle that it be unobtrusive and that it be integrated into ordinary clothing This design philosophy called eudaemonic computing named in honor of the group of physicists who designed the first truly unobtrusive wearable_computers with vibrotactile_displays is reduced to practice through the underwearable_computer underwearable for short The underwearable is a computer_system that is meant to be worn within or under ordinary clothing The first underwearables were built in the early 1980s and have evolved into a form that very much resembles a tank-top There were three reasons for the tank structure 1 weight is evenly and comfortably distributed over the body and bulk is distributed unobtrusively 2 it provides privacy by situating the apparatus within the corporeal boundary we consider our own personal space and others also so-regard and 3 proximity to the body affords capability to both sense biological signal quantities such as respiration and heart signals which are both accessible to a vest-based device as well as produce output that we can sense unobtrusively The vibrotactile output modality VibraVest was explored as a means of assisting the visually challenged to avoid bumping into objects through an ability to feel objects at a distance The success of VibraVest suggests other possibilities for similar unobtrusive devices that can be worn over an extended period of time in all facets of day-to-day life
information_systems for the age of consequences
This paper discusses what kinds of computer information_systems might be of broad social value in the context of the increasingly severe ecological and social consequences of economic_growth and how they might be built and maintained The paper has two parts The first offers a particular understanding of the ecological and social limits to economic_growth The second considers how this understanding can inform computer information_systems_design and operation and characterizes good limits-aware computing research
Active Selection of Training Examples for Meta-Learning
Meta-learning has been used to relate the performance of algorithms and the features of the problems being tackled The knowledge in meta-learning is acquired from a set of meta-examples which are generated from the empirical evaluation of the algorithms on problems in the past In this work active_learning is used to reduce the number of meta-examples needed for meta-learning The motivation is to select only the most relevant problems for meta-example generation and consequently to reduce the number of empirical evaluations of the candidate algorithms Experiments were performed in two different case studies yielding promising results
Engineering semantic_web_information_systems
web_information_systems WIS use the Web paradigm and technologies to retrieve information from sources connected to the Web and present the information in a web or hypermedia presentation to the user Hera is a design methodology that supports the design of WIS It is a model -driven method that distinguishes integration data_gathering and presentation generation In this paper we address the Hera methodology and specifically explain the integration model that covers the different aspects of integration and the adaptation model that specifies how the generated presentations are adaptable e g device capabilities user_preferences The Hera software_framework provides a set of transformations that allow a WIS to go from integration to presentation generation These transformations are based on RDF S and we show how RDF S has proven its value in combining all relevant aspects of WIS design In this way RDF S being the foundation of the semantic_web Hera allows the engineering of semantic_web_information_systems SWIS
Optimal wiresizing for interconnects with multiple sources
The optimal wiresizing problem for nets with multiple sources is studied under the distributed Elmore delay model We decompose such a net into a source subtree SST and a set of loading subtrees LSTs and show the optimal wiresizing solution satisfies a number of interesting properties including the LST separability the LST monotone property the SST local monotone property and the general dominance property Furthermore we study the optimal wiresizing problem using a variable grid and reveal the bundled refinement property These properties lead to efficient_algorithms to compute the lower and upper bounds of the optimal_solutions Experiment results on nets from an Intel processor layout show an interconnect delay reduction of up to 35 9 when compared to the minimum-width solution In addition the algorithm based on a variable grid yields a speedup of two orders of magnitude without loss of accuracy when compared with the fixed grid based methods
Magneto- and electroencephalographic manifestations of reward anticipation and delivery
article i nfo Article history Accepted 19 April 2012 Available online 26 April 2012 The monetary incentive delay task was used to characterize reward anticipation and delivery with concur- rently acquired evoked magnetic fields EEG potentials and EEG MEG oscillatory responses obtaining a pre- cise portrayal of their spatiotemporal evolution In the anticipation phase differential activity was most prominent over midline electrodes and parieto-occipital sensors Differences between non-reward- and reward-predicting cues were localized in the cuneus and later in the dorsal PCC suggesting a modulation by potential reward information during early visual processing followed by a coarse emotional evaluation of the cues Oscillatory analysis revealed increased theta power after non-reward cues over fronto-central sites In the beta range power decreased with the magnitude of the potential reward and increased with re- action time probably reflecting the influence of the striatal response to potential reward on the sensorimotor cortex At reward delivery negative prediction errors led to a larger mediofrontal negativity The spatiotem- poral evolution of reward processing was modulated by prediction error whereas differences were located in PCC and putamen in the prediction error comparison in the case of expected outcomes they were located in PCC ACC and parahippocampal gyrus In the oscillatory realm theta power was largest following rewards and in the case of non-rewards was largest when these were unexpected Higher beta activity following re- wards was also observed in both modalities but MEG additionally showed a significant power decrease for this condition over parieto-occipital sensors Our results show how visual limbic and striatal structures are involved in the different stages of reward anticipation and delivery and how theta and beta oscillations have a prominent role in the processing of these stimuli
Ranking Weblogs by Analyzing Reading and Commenting Activities
In this paper we analyze people s reading and commenting behaviors in blogspace and proposed an algorithm for blog ranking Upon two selected communities AI and Medical we show how comments reading records active browsing and multi time browsing can help to construct the weblog graph and reflect a blog s popularity Based on these analysis we propose cRank a graph based algorithm to rank blog among community members Finally we divide our dataset temporally and present how the proposed algorithm can make prediction on blogs rankings The experiment shows that cRank has a better performance upon several baseline systems
Robust lip region segmentation for lip images with complex background
Robust and accurate lip region segmentation is of vital importance for lip image_analysis However most of the current techniques break down in the presence of mustaches and beards With mustaches and beards the background region becomes complex and inhomogeneous We propose in this paper a novel multi-class shape-guided FCM MS-fcm_clustering_algorithm to solve this problem For this new approach one cluster is set for the object i e the lip region and a combination of multiple clusters for the background which generally includes the skin region lip shadow or beards The proper number of background clusters is derived automatically which maximizes a cluster_validity_index A spatial penalty term considering the spatial location_information is introduced and incorporated into the objective function such that pixels having similar color but located in different regions can be differentiated This facilitates the separation of lip and background_pixels that otherwise are inseparable due to the similarity in color Experimental results show that the proposed algorithm provides accurate lip-background partition even for the images with complex background features like mustaches and beards
First formant difference for i and u A cross-linguistic study and an explanation
The value of the first formant of high back and high front vowels u and i has been determined for near minimal pairs in a 30-language sample It is found that for 29 out of 30 languages the average of the first formant is higher for high back vowels than for high front vowels and that for 26 out of 28 languages the majority of minimal pairs has a high back vowel with a higher first formant than that of the high front vowel A trend towards smaller differences was found in women but this is not significant in the present data set R N R N Two factors may explain this observation Firstly the human vocal tract can only vary the position of gradual and not abrupt transitions of cross-sectional area Secondly there is a narrow tube just above the glottis the epilarynx tube Both factors cause the first formant of high back vowels to be raised but neither is sufficiently important to explain the observed differences on its own
An e-Learning Library on the Web
The main topic addressed in this paper is how to help learners select some instructive hypermedia-based learning resources according to their learning contexts from the Web Our approach is to provide a digital_library for web-based_learning called e-Learning Library which includes learning resource repository local indexing and adaptive navigation support This aims to promote their learning with diverse learning resources involving a certain topic
Stability of a class of linear switching_systems with applications to two consensus_problems
In this paper we first establish a stability result for a class of linear switching_systems involving Kronecker product The problem is intriguing in that the system matrix does not have to be Hurwitz in any time instant We have established the main result by a combination of the lyapunov_stability analysis and a generalized Barbalat s Lemma applicable to piecewise continuous linear_systems As applications of this stability result we study both the leaderless consensus problem and the leader-following consensus problem for general marginally stable linear multi-agent_systems under switching network_topology In contrast with many existing results our result only assume that the dynamic graph is uniformly connected
Factoring nonnegative matrices with linear programs
This paper describes a new approach based on linear programming for computing nonnegative_matrix_factorizations NMFs The key idea is a data-driven model for the factorization where the most salient features in the data are used to express the remaining features More precisely given a data matrix X the algorithm identifies a matrix C that satisfies X CX and some linear constraints The constraints are chosen to ensure that the matrix C selects features these features can then be used to find a low-rank NMF of X A theoretical analysis demonstrates that this approach has guarantees similar to those of the recent NMF algorithm of Arora et al 2012 In contrast with this earlier work the proposed method extends to more general noise models and leads to efficient scalable algorithms Experiments with synthetic and real datasets provide evidence that the new approach is also superior in practice An optimized C implementation can factor a multigigabyte matrix in a matter of minutes
Three-dimensional subband coding of video
We describe and show the results of video_coding based on a three-dimensional 3-D spatio-temporal subband_decomposition The results include a 1-Mbps coder based on a new adaptive differential pulse code modulation scheme ADPCM and adaptive bit_allocation This rate is useful for video storage on CD-ROM Coding results are also shown for a 384-kbps rate that are based on ADPCM for the lowest frequency band and a new form of vector_quantization geometric vector_quantization GVQ for the data in the higher frequency_bands GVQ takes advantage of the inherent structure and sparseness of the data in the higher bands Results are also shown for a 128-kbps coder that is based on an unbalanced tree-structured vector quantizer UTSVQ for the lowest frequency band and GVQ for the higher frequency_bands The results are competitive with traditional video_coding_techniques and provide the motivation for investigating the 3-D subband framework for different coding_schemes and various applications
Translation of uml_state_machines to Modelica Handling semantic issues
ModelicaML is a uml_profile that enables modeling and simulation of systems and their dynamic behavior ModelicaML combines the power of the OMG UML standardized graphical notation for systems and software modeling and the simulation power of Modelica This addresses the increasing need for precise and integrated modeling of products containing both software and hardware This article discusses the usage of executable uml_state_machines for system modeling i e usage of the same formalism for describing the state-based dynamic behavior of physical system components and software Moreover it points out that the usage of Modelica as an action_language enables an integrated simulation of continuous-time and reactive event-based system dynamics The main purpose of this article is however to highlight issues that are identified regarding the UML specification which are experienced with typical executable implementations of uml_state_machines The issues identified are resolved and rationales for the taken design_decisions are provided
Compressed sensing with linear correlation between signal and measurement_noise
Existing convex relaxation-based approaches to reconstruction in compressed sensing assume that noise in the measurements is independent of the signal of interest We consider the case of noise being linearly correlated with the signal and introduce a simple technique for improving compressed sensing reconstruction from such measurements The technique is based on a linear_model of the correlation of additive_noise with the signal The modification of the reconstruction_algorithm based on this model is very simple and has negligible additional computational cost compared to standard reconstruction_algorithms but is not known in existing literature The proposed technique reduces reconstruction error considerably in the case of linearly correlated measurements and noise numerical_experiments confirm the efficacy of the technique The technique is demonstrated with application to low-rate quantization of compressed measurements which is known to introduce correlated_noise and improvements in reconstruction error compared to ordinary basis_pursuit De-Noising of up to approximately 7dB are observed for 1bit sample quantization Furthermore the proposed method is compared to Binary Iterative Hard Thresholding which it is demonstrated to outperform in terms of reconstruction error for sparse signals with a number of non-zero coefficients greater than approximately 1 10th of the number of compressed measurements
Architecture optimization of a 3-DOF translational parallel_mechanism for machining applications the orthoglide
This paper addresses the architecture optimization of a three-degree-of-freedom translational parallel_mechanism_designed for machining applications The design optimization is conducted on the basis of a prescribed Cartesian workspace with prescribed kinetostatic performances The resulting machine the Orthoglide features three fixed parallel linear joints which are mounted orthogonally and a mobile_platform which moves in the Cartesian x-y-z space with fixed orientation The interesting features of the Orthoglide are a regular Cartesian workspace shape uniform performances in all directions and good compactness A small-scale prototype of the Orthoglide under development is presented at the end of this paper
Prototype learning with margin-based conditional log-likelihood loss
The classification_performance of nearest prototype_classifiers largely relies on the prototype learning algorithms such as the learning_vector_quantization LVQ and the minimum_classification_error MCE This paper proposes a new prototype learning algorithm based on the minimization of a conditional log-likelihood loss CLL called log-likelihood of margin LOGM A regularization term is added to avoid over-fitting in training The CLL loss in LOGM is a convex function of margin and so gives better convergence than the MCE algorithm Our empirical study on a large suite of benchmark datasets demonstrates that the proposed algorithm yields higher accuracies than the MCE the generalized LVQ GLVQ and the soft nearest prototype_classifier SNPC
smart_sensor architecture customized for image_processing applications
A system_level_design methodology is applied to the embedded_system_design for a typical sensor_network application face_detection for security purpose The tradeoff analysis is performed for hardware and software implementations of the tasks in this application The best system design is achieved with limited hardware_resources
A problem-driven collaborative approach to eliciting requirements of internetwares
In the software_development most stakeholders cannot clearly and objectively express their needs for the envisioned software_systems In this paper we propose a problem-driven collaborative requirements_elicitation approach with the purpose of helping identify and extract the requirements of the Internetwares a complex and new software paradigm The basic idea of our approach is that the requirements of the software_systems should be stated by stakeholders in an objective way i e problem-identifying-solving way That is first identify the problems existed in the as-is problem domain and then find the solutions to the problems The solutions to the problems are the requirements of the envisioned software_systems To this end we propose the structure of problems and a collaborative process for achieving the solutions
A diversity-based method for infrequent purchase decision support in e-commerce
In this paper we propose a method for supporting consumer buying decisions in e-commerce We are advocating the diversity-driven approach to generating alternatives for infrequently purchased products i e computers vehicles etc Our method is based upon the well-known divergence convergence principle of problem solving The paper discusses the method based on fuzzy weighted-sum model and cluster_analysis the architecture and the operation of the decision_support_system_for generating product alternatives The preliminary experiments with the prototype for notebook selection provide some support in favor of our approach over the catalog-based systems
Heuristic scheduling of jobs on parallel batch machines with incompatible job families and unequal ready times
This research is motivated by a scheduling problem found in the diffusion and oxidation areas of semiconductor wafer fabrication where the machines can be modeled as parallel batch processors We attempt to minimize total weighted tardiness on parallel batch machines with incompatible job families and unequal ready times of the jobs Given that the problem is NP-hard we propose two different decomposition approaches The first approach forms fixed batches then assigns these batches to the machines using a genetic_algorithm GA and finally sequences the batches on individual machines The second approach first assigns jobs to machines using a GA then forms batches on each machine for the jobs assigned to it and finally sequences these batches Dispatching and scheduling rules are used for the batching phase and the sequencing phase of the two approaches In addition as part of the second decomposition approach we develop variations of a time window heuristic based on a decision_theory approach for forming and sequencing the batches on a single machine
BrainKnowledge A Human Brain Function Mapping Knowledge-Base System
Associating fMRI image datasets with the available literature is crucial for the analysis and interpretation of fMRI data Here we present a human brain function mapping knowledge-base system BrainKnowledge that associates fMRI data analysis and literature search functions BrainKnowledge not only contains indexed literature but also provides the ability to compare experimental data with those derived from the literature BrainKnowledge provides three major functions 1 to search for brain activation models by selecting a particular brain function 2 to query functions by brain structure 3 to compare the fMRI data with data extracted from the literature All these functions are based on our literature extraction and mining module developed earlier Hsiao Chen Chen Journal of Biomedical_informatics 42 912 922 2009 which automatically downloads and extracts information from a vast amount of fMRI literature and generates co-occurrence models and brain association patterns to illustrate the relevance of brain structures and functions BrainKnowledge currently provides three co-occurrence models 1 a structure-to-function co-occurrence model 2 a function-to-structure co-occurrence model and 3 a brain structure co-occurrence model Each model has been generated from over 15 000 extracted Medline abstracts In this study we illustrate the capabilities of BrainKnowledge and provide an application example with the studies of affect BrainKnowledge which combines fMRI experimental results with Medline abstracts may be of great assistance to scientists not only by freeing up resources and valuable time but also by providing a powerful tool that collects and organizes over ten thousand abstracts into readily usable and relevant sources of information for researchers
radio_resource_allocation for cellular_networks based on OFDMA with QoS guarantees
In this paper we address the problem of radio_resource_allocation for qos_support in the downlink of a cellular OFDMA system The major impairments considered are cochannel_interference CCI and frequency_selective_fading The allocation problem involves assignment of base_stations and subcarriers bit loading and power_control for multiple users We propose a three-stage low-complexity heuristic algorithm to distribute radio_resources among multiple users according to their individual qos_requirements while at the same time maintaining the QoS of already established links in all the cochannel cells The allocation objective is to minimize the total_transmit_power which adds to reducing CCI Simulation results show a superior performance of the proposed method when compared to classical radio_resource_management techniques Our scheme allows us to achieve almost 6 times higher capacity sum data rate than the method based on FDMA with power_control at a blocking_probability of 0 02
Spectral texturing for real-time_applications
In this sketch we present a new method for rendering large-scale high-resolution non-repetitive textures in real-time using multi layer texturing The basic idea of spectral texturing is to construct the nal texture by multiple texture layers where each layer provides a certain range of the spectrum of the texture s spatial frequencies Alpha channels are used to introduce statistical dependencies between the frequency_bands This approach extends a method called detail texturing which does not use alpha channels to model higher statistical properties of the resulting texture Other approaches to generate textures with specied statistical properties like DeBonet 1997 are not suitable for real-time use and would require storage of the generated texture Spectral texturing is very easy to implement runs on all contemporary 3d_graphics_cards and is especially suitable for naturalistic textures in real-time_applications which are viewed from a large range of distances
Streams on wires a query compiler for FPGAs
Taking advantage of many-core heterogeneous hardware for data processing tasks is a difficult problem In this paper we consider the use of FPGAs for data_stream_processing as coprocessors in many-core_architectures We present Glacier a component library and compositional compiler that transforms continuous_queries into logic circuits by composing library components on an operator-level basis In the paper we consider selection aggregation grouping as well as windowing operators and discuss their design as modular elements R N R N We also show how significant performance improvements can be achieved by inserting the FPGA into the system s data path e g between the network_interface and the host CPU Our experiments show that queries on the FPGA can process streams at more than one million tuples per second and that they can do this directly from the network removing much of the overhead of transferring the data to a conventional CPU
Exploring component-based approaches in forest landscape modeling
Forest management issues are increasingly required to be addressed in a spatial context which has led to the development of spatially explicit forest landscape models The numerous processes complex spatial interactions and diverse applications in spatial modeling make the development of forest landscape models difficult for any single research group New developments in componentbased modeling approaches provide a viable solution Component-based modeling breaks a monolithic model into small interchangeable and binary components They have these advantages compared to the traditional modeling work 1 developing a component is a much smaller task than developing a whole model 2 a component can be developed using most programming_languages since the interface format is binary and 3 new components can replace the existing ones under the same model framework this reduces the duplication and allows the modeling community to focus resources on the common products and to compare results In this paper we explore the design of a spatially explicit forest landscape model in a component-based modeling framework based on our work on object-oriented forest landscape modeling We examine the representation of the major components and the interactions between them Our goal is to facilitate the use of the component-based modeling approach at the early stage of spatially explicit landscape modeling 2002 Elsevier Science Ltd All rights reserved
Extensive feature detection of N-terminal protein sorting signals
Motivation The prediction of localization sites of various proteins is an important and challenging problem in the field of molecular biology TargetP by Emanuelsson et al J Mol Biol 300 1005 1016 2000 is a neural_network based system which is currently the best predictor in the literature for N-terminal sorting signals One drawback of neural_networks however is that it is generally difficult to understand and interpret how and why they make such predictions In this paper we aim to generate simple and interpretable rules as predictors and still achieve a practical prediction accuracy We adopt an approach which consists of an extensive search for simple rules and various attributes which is partially guided by human intuition Results We have succeeded in finding rules whose prediction accuracies come close to that of TargetP while still retaining a very simple and interpretable form We also discuss and interpret the discovered rules Availability An experimental web service using rules obtained by our method is provided at http
Optimum color masking matrix determination for digital color platemaking using virtual color samples
Abstract R N R N For high-fidelity color_reproduction with digital color platemaking systems it is the most important to determine the color masking matrix which converts the red green and blue intensities of the monitor to cyan magenta yellow and black inks halftone dot area rates With regard to this determination process the author previously developed a color_difference least square method by which the optimum matrix is determined using a simulation with the Neugebauer equation and virtual color samples First this paper evaluates the method with actual images Next the method is extended for adaptive masking matrix optimization By adapting the matrix to each color_image using black ink it was shown that the average color_differences of reproduced images could be reduced to below six when black ink is used as much as possible i e in the case of achromatic printing This masking matrix adaption is a new concept which was impossible by conventional color scanner processing but became possible by using virtual color samples
Downlink Optimization with Interference Pricing and Statistical CSI
In this paper we propose a downlink transmission strategy based on intercell_interference pricing and a distributed_algorithm that enables each base_station BS to design locally its own beamforming vectors without relying on downlink channel_state_information of links from other BSs to the users This algorithm is the solution to an optimization problem that minimizes a linear combination of data transmission_power and the resulting weighted intercell_interference with pricing factors at each BS and maintains the required signal-to-interference-plus-noise_ratios SINR at user terminals We provide a convergence analysis for the proposed distributed_algorithm and derive conditions for its existence We characterize the impact of the pricing factors in expanding the operational range of SINR targets at user terminals in a power-efficient manner Simulation results confirm that the proposed algorithm converges to a network-wide equilibrium_point by balancing and stabilizing the intercell_interference levels and assigning power optimal_beamforming vectors to the BSs The results also show the effectiveness of the proposed algorithm in closely following the performance limits of its centralized coordinated_beamforming counterpart
Secure authentication watermarking for localization against the Holliman---Memon attack
Authentication watermarking_schemes using block-wise watermarks for tamper localization are vulnerable to the Holliman---Memon attack In this paper we propose a novel method based on the Wong s localization scheme Proceedings of the IS T PIC Portland to resist this attack A unique image index scheme is used for computing the authentication signature that is embedded in the least significant bit-plane of the block The informed detector estimates the correct_image index by using the side information about the watermarked image The image index estimation from the fake image can definitely be an alternative to keeping a directory of image indices So it is not necessary to manage the database of image indices for the verification purpose The authenticity measure is defined to quantify the attack severity by taking the connectivity among possible authentic blocks into consideration There are more blocks verified as authentic when this measure is high for a fake image constructed using this attack As such the blocks for a fake image can be chosen from a reduced number of database_images The blocks from any such image are to be connected with each other to maximize the authenticity measure Thus the attacker s task to generate a fake image of reasonable perceptual quality becomes increasingly difficult With the proposed method there is no loss or ambiguity in localization after the Holliman---Memon attack and content tampering in an image The localization_accuracy in the proposed method is demonstrated by the simulation results and is equal to the chosen block size similar to the Wong s scheme
Directions of external knowledge search investigating their different impact on firm performance in high-technology industries
Purpose The aim of the paper is to identify the different directions of external knowledge search and to investigate their individual effect on performance at the firm level Design methodology approach The empirical study is based on survey data gathered from two distinct informants of 248 large- and medium-sized high-tech manufacturing Spanish firms In dealing with concerns on simultaneity and reverse causality perceived time-lags among dependent and independent variables were introduced Quantitative methods based on questionnaire answers were used Findings Findings reveal six distinct external search_patterns and indicate that while market sources such as customers and competitors are positively associated with performance knowledge acquired from general information sources other firms beyond the core business and patents and databases have no significant effect Moreover knowledge obtained from science and technology organizations and from suppliers displays an inversed U-shaped effect o
An Empirical Evaluation of the Student-Net delay_tolerant_network
Radio equipped mobile_devices have enjoyed tremendous growth in the past few years We observe that in the near future it might be possible to build a network that routes delay-tolerant packets by harnessing user mobility and the pervasive availability of wireless devices Such a delay-tolerant_network could be used to supplement wireless infrastructure or provide service where none is available Since mobile_devices in a delay-tolerant_network forward packets to nearby users the devices can use short-range radio which potentially reduces device power consumption and radio contention The design of a user mobility based delay-tolerant_network raises two key challenges determining the connectivity of such a network and determining the latency characteristics and replication requirements of routing_algorithms in such a network To determine realistic contact patterns we collected user mobility data by conducting two user studies We outfitted groups of students with instrumented wireless-enabled PDAs that logged pairwise contacts between study participants over a period of several weeks Experiments conducted on these traces show that it is possible to form a delay-tolerant_network based on human_mobility The network has good connectivity so that routes exist between almost all study participants via some multi-hop path Moreover it is possible to effectively route packets with modest replication
Local Strategy Improvement for Parity Game Solving
Theproblemofsolvinga paritygameis atthecore ofmanyproblemsin modelchecking satisfiability checking and program synthesis Some of the best algorithms for solving parity game are strategy improvement algorithms These are global in nature since they require the entire parity game to be present at the beginning This is a distinct disadvantagebecause in many applications one only needs to know which winning region a particular node belongs to an daw itnessing winning strategy may cover only a fractional part of the entire game graph We present a local strategy improvement algorithm which explores the game graph on-the-fly whilst performing the improvement steps We also compare it empirically with existing global strategy improvementalgorithms and the currently only other local algorithm for solving parity games It turnsout that local strategy improvementcan outperformthese othersby severalordersof magnitude
White matter integrity fiber count and other fallacies The do s and don ts of diffusion_mri
Diffusion-weighted MRI DW-MRI has been increasingly used in imaging neuroscience over the last decade An early form of this technique diffusion tensor imaging DTI was rapidly implemented by major MRI scanner companies as a scanner selling point Due to the ease of use of such implementations and the plausibility of some of their results DTI was leapt on by imaging neuroscientists who saw it as a powerful and unique new tool for exploring the structural connectivity of human brain However DTI is a rather approximate technique and its results have frequently been given implausible interpretations that have escaped proper critique and have appeared misleadingly in journals of high reputation In order to encourage the use of improved DW-MRI methods which have a better chance of characterizing the actual fiber structure of white matter and to warn against the misuse and misinterpretation of DTI we review the physics of DW-MRI indicate currently preferred methodology and explain the limits of interpretation of its results We conclude with a list of Do s and Don ts which define good practice in this expanding area of imaging neuroscience
The International Technology Alliance in Network and Information Sciences
In May 2006 the US Army Research Laboratory and UK Ministry of Defense created the international technology alliance The consortium of 26 partners including the ARL and MoD offers an open research environment in which leading US and UK companies and universities can collaborate see table 1 It will also fuse the best aspects of the US Army s Collaborative Technology Alliances and UK MoD s Defense Technology Centers on an international scale The ITA aims to develop flexible distributed and secure decision-making procedures to improve networked coalition operations Network science is a young discipline we have limited information models and network theories to describe the behavior and scaling of large complex mobile_ad_hoc_networks 1 moreover you can t understand a coalition network s performance without understanding its cognitive and sociocultural aspects and physical characteristics A key ITA goal is to perform basic research in network-centric coalition decision making across four technical areas network theory security across a system of systems sensor information processing and delivery and distributed coalition planning and decision making 2 we focus on the last area because this is where intelligent_systems will play the biggest role
Standoff Detection Using Millimeter and Submillimeter Wave Spectroscopy
The millimeter MM wave and sub-MM wave 30-600 GHz frequency band contains fundamental rotational and vibrational resonances of many molecular gases composed of carbon nitrogen oxygen and sulphur The high specificity of rotational spectra to organic molecules affords MM wave spectroscopy having potential use in remotely sensing atmospheric pollutants and the detection of airborne chemicals is important for arms control treaty verification intelligence collection and environmental monitoring This paper considers the sensitivity requirements of radiofrequency receiver systems for measuring MM wave absorption emission signatures The significance of receiver sensitivity and material optical depth to sensing is highlighted A background to the technology needed for sensing at MM and sub-MM wavelengths then provides the basis for a review of MM wave spectroscopy and its role on profiling the concentrations of trace polar molecules and ionized radicals in the high altitude atmosphere The application of the MM wave spectroscopic technique in ambient conditions is then reviewed and the issues associated with developing the technique for standoff remote sensing is discussed
Let s talk about rings
Defining the ring topology of a molecule belongs to the central and elementary problems of cheminformatics Questions like How many rings does a molecule contain or In how many rings is a specific atom involved are based on such a definition Obviously the ring topology must be unique i e it does not depend on atom order chemical meaningful and of reasonable size at most polynomial in the number of atoms For a long period the smallest set of smallest rings SSSR was used in cheminformatics applications ignoring a very critical flaw namely that it is not unique even for simple structures Other definitions like the set of relevant cycles heal this flaw however they are sometimes chemically not meaningful and can become exponential in size Among all attempts made none fulfils all three criteria at the same time 1 R N R N Recently we developed a ring definition named unique ring family URF and a corresponding algorithm for calculating it in polynomial time 2 URFs match the common chemical sense of a molecule s ring topology The definition results in a unique ring description consisting of a number of ring prototypes which is at most quadratic in the number of atoms In this talk we will present the algorithm benchmarks as well as several examples demonstrating the usefulness of URFs
Instability of submicron anisotropic liquid cylinders and jets in magnetic field
The capillary instability of a magnetically anisotropic liquid cylinder and jets such as Nematic liquid crystals LC in magnetic fields is considered using an energy approach The boundary problem is solved in the linear approximation of the anisotropy spl chi sub a of the magnetic susceptibility spl chi The effect of the anisotropy in the region 1 spl chi spl chi sub a spl chi sup 2 can be strong enough to counteract and even reverse the tendency of the field to enhance stabilization by increasing the cut-off wave number k sub s beyond the conventional one set by Rayleigh It is shown that the elastic effect which is typical of LC is significant on the scale of nano-jets where it prevails over the magnetic effect The jet instability is determined by surface tension elasticity and magnetic permeability and anisotropy The relative influence of the elasticity and permeability on the jet stability depends on its radius This is particularly true on the nano-scale
information_retrieval_methods for automatic_speech_recognition
In this paper we use information_retrieval IR techniques to improve a speech_recognition ASR system The potential benefits include improved speed accuracy and scalability Where conventional HMM-based speech_recognition_systems decode words directly our IR-based system_first decodes subword units These are then mapped to a target word by the IR system In this decoupled system the IR serves as a lightweight data-driven pronunciation model Our proposed method is evaluated in the Windows Live Search for Mobile WLS4M task and our best system has 12 fewer errors than a comparable HMM classifier We show that even using an inexpensive IR weighting scheme TF-IDF yields a 3 relative error rate reduction while maintaining all of the advantages of the IR approach
A decision making model using soft set and rough set on fuzzy approximation spaces
In modern era of computing there is a need of development in data analysis and decision making Most of our tools are crisp deterministic and precise in character But general real life situations contains uncertainties To handle such uncertainties many theories are developed such as fuzzy set rough set rough set on fuzzy approximation spaces etc But all these theories have their own limitations To overcome the limitations the concept of soft set is introduced But soft set also fails if the attributes in the information_system are almost identical rather exactly identical In this paper we propose a decision making model that consists of two processes such as preprocess and postprocess to mine decisions In preprocess we use rough set on fuzzy approximation spaces to get the almost equivalence_classes whereas in postprocess we use soft set techniques to obtain decisions The proposed model is tested over an institutional dataset and the results show practical viability of the proposed research
The influence of parental and peer attachment on Internet usage motives and addiction
The impact of parental and peer attachment on four Internet usage motives and Internet addiction was compared using path modelling of survey data from 1 577 adolescent Malaysian school students The model accounted for 31 percent of Internet addiction score variance Lesser parental attachment was associated with greater Internet addiction risk Psychological escape motives were more strongly related to Internet addiction than other motives and had the largest mediating effect upon the parental attachment addiction relationship Peer attachment was unrelated to addiction risk its main influence on Internet usage motives being encouragement of use for social interaction It is concluded that dysfunctional parental attachment has a greater influence than peer attachment upon the likelihood of adolescents becoming addicted to Internet related activities It is also concluded that the need to relieve dysphoria resulting from poor adolescent parent relationships may be a major reason for Internet addiction and that parents fostering of strong bonds with their children should reduce addiction risk
An integer programming based approach for verification and diagnosis of workflows
Workflow analysis is indispensable to capture modeling errors in workflow designs While several workflow analysis approaches have been defined previously these approaches do not give precise feedback thus making it hard for a designer to pinpoint the exact cause of modeling errors In this paper we introduce a novel approach for analyzing and diagnosing workflows based on integer programming IP Each workflow model is translated into a set of IP constraints Faulty control flow connectors can be easily detected using the approach by relaxing the corresponding constraints We have implemented this diagnosis approach in a tool called DiagFlow which reads and diagnoses XPDL models using an existing open source IP solver as a backend We show that the diagnosis approach is correct and illustrate it with realistic examples Moreover the approach is flexible and can be extended to handle a variety of new constraints as well as to support new workflow_patterns Results of testing on large process_models show that DiagFlow outperforms a state of the art tool like Woflan in terms of the solution time
A hybrid wireless_network enhanced with multihopping for emergency communications
This paper proposes a hybrid wireless_network scheme enhanced with ad_hoc_networking for disaster damage assessment and emergency communications The network aims to maintain the connection between a base_station BS and nodes by way of multihopping In the event that a direct link between BS and a node is disconnected the node switches modes from cellular to ad hoc in order to access BS via neighboring nodes A routing_protocol proposed in this paper is capable of building a route using unicast-based route_discovery_process without route request flooding A proposed mac_protocol satisfies the requirement of maintaining accessibility and a short delay even in emergency circumstances We discuss an analytical model based on a markov_process Experimental results are shown regarding reachability throughput and delay
A wavelet-like filter based on neuron action potentials for analysis of human scalp electroencephalographs
This paper describes the development and testing of a wavelet-like filter named the SNAP created from a neural activity simulation and used in place of a wavelet in a wavelet_transform for improving EEG wavelet_analysis intended for brain-computer_interfaces The hypothesis is that an optimal wavelet can be approximated by deriving it from underlying components of the EEG The SNAP was compared to standard wavelets by measuring support_vector_machine-based eeg_classification accuracy when using different wavelets filters for eeg_analysis When classifying P300 evoked potentials the error as a function of the wavelet filter used ranged from 6 92 to 11 99 almost twofold Classification using the SNAP was more accurate than that with any of the six standard wavelets tested Similarly when differentiating between preparation for left- or right-hand movements classification using the SNAP was more accurate 10 03 error than for four out of five of the standard wavelets 9 54 to 12 00 error and internationally competitive 7 error on the 2001 NIPS competition test_set Phenomena shown only in maps of discriminatory EEG activity may explain why the SNAP appears to have promise for improving EEG wavelet_analysis It represents the initial exploration of a potential family of EEG-specific wavelets
Distributed Multi Class SVM for Large Data Sets
data_mining_algorithms are originally designed by assuming the data is available at one centralized site These algorithms also assume that the whole data is fit into main memory while running the algorithm But in today s scenario the data has to be handled is distributed even geographically Bringing the data into a centralized site is a bottleneck in terms of the bandwidth when compared with the size of the data In this paper for multiclass SVM we propose an algorithm which builds a global svm_model by merging the local SVMs using a distributed approach DSVM And the global SVM will be communicated to each site and made it available for further classification The experimental analysis has shown promising results with better accuracy when compared with both the centralized and ensemble method The time complexity is also reduced drastically because of the parallel construction of local SVMs The experiments are conducted by considering the data sets of size 100s to hundred of 100s which also addresses the issue of scalability
Across boundaries of influence and accountability the multiple scales of public sector information_systems
The use of ICTs in the public sector has long been touted for its potential to transform the institutions that govern and provide social services The focus however has largely been on systems that are used within particular scales of the public sector such as at the scale of state or national government the scale of_regional or municipal entity or at the scale of local service providers The work presented here takes aim at examining ICT use that crosses these scales of influence and accountability We report on a year long ethnographic investigation conducted at a variety of social service outlets to understand how a shared information_system crosses the boundaries of these very distinct organizations We put forward that such systems are central to the work done in the public sector and represent a class of collaborative_work that has gone understudied
Symmetric Exponential Integrators with an Application to the Cubic SchrÃ¶dinger_equation
In this article we derive and study symmetric exponential integrators numerical_experiments are performed for the cubic Schrodinger_equation and comparisons with classical exponential integrators and other geometric methods are also given Some of the proposed methods preserve the L 2-norm and or the energy of the system
Limits of homology detection by pairwise sequence comparison
Motivation Noise in database_searches resulting from random_sequence similarities increases as the databases expand rapidly The noise problems are not a technical shortcoming of the database_search programs but a logical consequence of the idea of homology searches The effect can be observed in simulation experiments Results We have investigated noise levels in pairwise alignment based database_searches The noise levels of 38 releases of the SwissProt database display perfect logarithmic growth with the total length of the databases Clustering of real biological sequences reduces noise levels but the effect is marginal
business_process Development in Semantically-Enriched Environment
Middleware support for business_process_management BPM has met some of the challenges with respect to encoding_performance and maintenance of workflows A remaining challenge is complexity business_processes are becoming widely distributed interoperating across a range of inter- and intra-organizational behaviours vocabularies and semantics It is important that this semantic complexity is checked and analyzed for optimality and trustworthiness prior to deployment petri_nets are a formal_method that successfully provides behavioural analysis A shortcoming of petri_nets is that the data_exchanged between business_activities abstract too far away from the importance of data in actual business_processes This paper addresses this abstraction gap via additional semantic enrichment through a two stage model-driven approach
Distributed dynamic scheduling for end-to-end rate guarantees in wireless_ad_hoc_networks
We present a framework for the provision of deterministic end-to-end bandwidth guarantees in wireless_ad_hoc_networks Guided by a set of local feasibility conditions multi-hop sessions are dynamically offered allocations further translated to link demands Using a distributed time_division_multiple_access TDMA protocol nodes adapt to the demand changes on their adjacent links by local conflict-free slot reassignments As soon as the demand changes stabilize the nodes must incrementally converge to a TDMA schedule that realizes the global link and session demand allocation We first derive sufficient local feasibility conditions for certain topology classes and show that trees can be maximally utilized We then introduce a converging distributed link_scheduling algorithm that exploits the logical tree structure that arises in several ad_hoc_network applications Decoupling bandwidth_allocation to multi-hop sessions from link_scheduling allows support of various end-to-end quality_of_service QoS objectives We focus on the max-min fairness MMF objective and design an end-to-end asynchronous distributed_algorithm for the computation of the session MMF rates Once the end-to-end algorithm converges the link_scheduling algorithm converges to a TDMA schedule that realizes these rates We demonstrate the applicability of this framework through an implementation over an existing wireless_technology This implementation is free of restrictive assumptions of previous TDMA approaches it does not require any a-priori knowledge on the number of nodes in the network nor even network-wide slot synchronization
Evaluating Various Branch-Prediction Schemes for Biomedical-Implant Processors
This paper evaluates various branch-prediction schemes under different cache configurations in terms of performance power energy and area on suitably selected biomedical workloads The benchmark suite used consists of compression encryption and data-integrity algorithms as well as real implant applications all executed on realistic biomedical input datasets Results are used to drive the micro architectural design of a novel microprocessor targeting microelectronic implants Our profiling study has revealed that under strict or relaxed area constraints and regardless of cache_size the ALWAYS TAKEN and ALWAYS NOT-TAKEN static prediction schemes are in almost all cases the most suitable choices for the envisioned implant processor It is further shown that bimodal predictors with small Branch-Target-Buffer BTB tables are suboptimal yet also attractive solutions when processor I D-cache_sizes are up to 1024KB 512KB respectively
A New user_authentication_protocol for mobile_terminals in wireless_network
For constructing a ubiquitous network the highspeed wireless_lan WLAN attracts attention as an infrastructure for global access However some issues are impeding further adoption of the technology in particular security_problems including user_authentication message compromising password theft connection hijacking etc In this paper we discuss a fast authentication method of mobile ubiquitous terminals in WLAN To achieve an efficient access_control between access_points APs and mobile_terminals and sharing of a session_key between terminals we propose a new user secure authentication method and a session_key_distribution protocol based on service ticket issuing system
On the Basis of the Generated Foundation of Blended E-learning - Transcendence and Integration
Learning theory develops in the constant process of transcendence and integration Not only from knowledge to people but also its theory approaches are all beyond each other and integration Therefore the emergence of blended e_learning is inevitable which is based on the theoretical study s transcendence and integration and as a learning concepts and theories blended e_learning is also bound to each other than with the integration Such mutual transcendence and integration of learning theory is just an important basis for the starting point
An Improved Signcryption Scheme and Its Variation
Signcryption is a new cryptographic primitive which simultaneously provides both confidentiality and authenticity This paper proposes an improved signcryption scheme and a variant scheme providing message_recovery The first scheme is revised from an authenticated encryption scheme which has been found to have a security-flaw Our scheme solves the security-flaw and provides an additional property called the public_verifiability of the signature The second scheme is a message_recovery type It surpasses most of the current signcryption_schemes on the size of the signcrypted ciphertext That is in our second scheme we require only two parameters r s with r epsi Z p and s epsi Z q while most signcryption_schemes require three parameters c r s with the additional parameter c epsi Z p This second scheme is modified from an authenticated encryption scheme with message_recovery and surpasses the based authenticated encryption scheme on the property of non-repudiation of the origin
A new network_architecture with intelligent node IN to enhance IEEE 802 14 HFC networks
In the hybrid fiber coax HFC architecture over several hundreds subscribers in CATV community antenna TV network may cause serious collisions In this paper we propose a new network_architecture which using an intelligent node IN to stand for a group of subscribers to request the demand resources The IN has the ability to reduce the collision probability as well as the collision resolving period The simulation results show that the proposed architecture in terms of throughput buffer delay and fairness outperforms the standard architecture
dynamic_load_balancing schemes for computing accessible surface area of protein molecules
This paper presents an experimental study of dynamic_load_balancing methods for a parallelized solution to a well-known problem in computational molecular biology computing the accessible surface areas ASA of proteins The main contribution is a better understanding of how certain techniques for load estimation and redistribution must be combined carefully for effectiveness and how these combinations need to change during the course of a computation In particular the Shrake-Rupley Asa_algorithm is implemented and three aspects of dynamic_load_balancing are studied how to estimate load_imbalance the estimation problem when to invoke load redistribution the invocation problem and how to load balance the mapping problem The results in this paper show that a dynamically-selected mix of algorithms in each category that adapts to changing structure within the protein works better than a static periodic application of a static mix of algorithms
Level crossing rate and average fade duration of MRC and EGC diversity in Ricean fading
The average level crossing rate and average fade duration of the output signal of a maximal ratio combiner MRC and equal gain combiner EGC operating on independent Ricean fading input branch signals are derived Exact closed-form results are obtained for MRC diversity while precise expressions for EGC diversity are presented with an infinite series method The results are valid for an arbitrary number of independent identically distributed diversity branches isotropic scattering and a specular component perpendicular to the line of motion of the mobile
Design and implementation of WIRE Diameter
This paper presents the design and implementation of WIRE Diameter The WIRE Diameter is an open source implementation of Diameter Based Protocol and Diameter EAP application developed by the Wireless Internet Research_engineering WIRE Laboratory Research has shown that traditional RADIUS protocol may suffer performance degradation and data loss in a large system Diameter thus was proposed to address the deficiencies in RADIUS Both 3GPP and 3GPP2 have adopted Diameter as their AAA protocol The WIRE Diameter could be used to authenticate and authorize 802 1x supplicant It provides various authentication_schemes including EAP-MD5 EAP-TLS EAP-TTLS and PEAP The WIRE Diameter is developed to be independent of OS as much as possible Currently it supports Linux FreeBSD and various versions of MS Windows It is believed that the WIRE Diameter is the first open source implementation of Diameter EAP Application in the world The source_code can be downloaded freely The WIRE Diameter should be useful for the research community This paper demonstrates the design and implementation of the WIRE Diameter
Stability and performance of intersecting aircraft flows under decentralized conflict avoidance rules
This paper considers the problem of two intersecting aircraft flows under decentralized conflict resolution rules Considering aircraft flowing through a fixed control volume new air traffic control models and scenarios are defined that enable the study of long-term aircraft flow stability For a class of two intersecting aircraft flows this paper considers conflict scenarios involving arbitrary encounter angles It is shown that aircraft flow stability defined both in terms of safety and performance is preserved under the decentralized conflict resolution algorithm considered It is shown that the lateral deviations experienced by aircraft in each flow are bounded
Informational acquisition and cognitive_models
Abstract R N R N Life forms must organize information into cognitive_models reflecting the outside environment and in a complex and changing environment a life form must constantly select and organize this mass of information to avoid slipping into a chaotic cognitive state The task of developing and maintaining adaptive cognitive_models can be understood through two processes crucial to regulating the interconnections between environmental elements The inclusion and exclusion of information follows a process designated by P and the process by which cognitive_models change is designated by K Higher order concepts are created by reducing the interconnections between elements to a minimal number to avoid cognitive chaos 2004 Wiley Periodicals Inc Complexity 9 31 37 2004
Contextual motion field-based distance for video analysis
In this work we propose a general method for computing distance between video frames or sequences Unlike conventional appearance-based methods we first extract motion_fields from original videos To avoid the huge memory requirement demanded by the previous approaches we utilize the bag of motion_vectors model and select gaussian_mixture_model as compact representation Thus estimating distance between two frames is equivalent to calculating the distance between their corresponding gaussian_mixture_models which is solved via earth mover distance EMD in this paper On the basis of the inter-frame distance we further develop the distance measures for both full video sequences R N R N Our main contribution is four-fold Firstly we operate on a tangent vector field of spatio-temporal 2D surface manifold generated by video motions rather than the intensity gradient space Here we argue that the former space is more fundamental Secondly the correlations between frames are explicitly exploited using a generative_model named dynamic conditional_random_fields DCRF Under this framework motion_fields are estimated by Markov volumetric regression which is more robust and may avoid the rank deficiency problem Thirdly our definition for video distance is in accord with human intuition and makes a better tradeoff between frame dissimilarity and chronological ordering Lastly our definition for frame distance allows for partial distance
Minimum effort inverse_kinematics for redundant_manipulators
This paper investigates the use of an infinity norm in formulating the optimization measures for computing the inverse_kinematics of redundant arms The infinity norm of a vector is its maximum absolute value component and hence its minimization implies the determination of a minimum effort solution as opposed to the minimum-energy criterion associated with the Euclidean norm In applications where individual magnitudes of the vector components are of concern this norm represents the physical requirements more closely than does the Euclidean norm We first study the minimization of the infinity-norm of the joint velocity vector itself and discuss its physical interpretation Next a new method of optimizing a subtask criterion defined using the infinity-norm to perform additional tasks such as obstacle_avoidance or joint limit avoidance is introduced Simulations illustrating these methods and comparing the results with the Euclidean norm solutions are presented
A Traceability Technique for Specifications
Traceability in software involves discovering links between different artifacts and is useful for a myriad of tasks in the software life cycle We compare several different information_retrieval techniques for this task across two datasets involving real-world software with the accompanying specifications and documentation The techniques compared include dimensionality_reduction_methods probabilistic and information theoretic approaches and the standard vector_space_model
A generic tension-closure analysis method for fully-constrained cable-driven parallel_manipulators
Cable-driven parallel_manipulators CDPMs are a special class of parallel_manipulators that are driven by cables instead of rigid links Due to the unilateral property of the cables all the driving cables in a fully-constrained CDPM must always maintain positive tension As a result tension analysis is the most essential issue for these CDPMs By drawing upon the mathematical theory from convex analysis a sufficient and necessary tension-closure condition is proposed in this paper The key point of this tension-closure condition is to construct a critical vector that must be positively expressed by the tension vectors associated with the driving cables It has been verified that such a tension-closure condition is general enough to cater for CDPMs with different numbers of cables and DOFs Using the tension-closure condition a computationally efficient_algorithm is developed for the tension-closure pose analysis of CDPMs in which only a limited set of deterministic linear equation systems need to be resolved This algorithm has been employed for the tension-closure workspace_analysis of CDPMs and verified by a number of computational examples The computational_time required by the proposed algorithm is always shorter as compared to other existing algorithms
Parallel support_vector_machines The Cascade SVM
We describe an algorithm for support_vector_machines SVM that can be parallelized efficiently and scales to very large problems with hundreds of thousands of training vectors Instead of analyzing the whole training_set in one optimization step the data are split into subsets and optimized separately with multiple SVMs The partial results are combined and filtered again in a Cascade of SVMs until the global optimum is reached The Cascade SVM can be spread over multiple processors with minimal communication_overhead and requires far less memory since the kernel_matrices are much smaller than for a regular SVM Convergence to the global optimum is guaranteed with multiple passes through the Cascade but already a single pass provides good_generalization A single pass is 5x - 10x faster than a regular SVM for problems of 100 000 vectors when implemented on a single processor parallel_implementations on a cluster of 16 processors were tested with over 1 million vectors 2-class problems converging in a day or two while a regular SVM never converged in over a week
Estimation of heart-surface potentials using regularized multipole sources
Direct inference of heart-surface potentials from body-surface potentials has been the goal of most recent work on electrocardiographic inverse solutions We developed and tested indirect methods for inferring heart-surface potentials based on estimation of regularized multipole sources Regularization was done using Tikhonov constrained-least-squares and multipole-truncation techniques These multipole-equivalent methods MEMs were compared to the conventional mixed boundary-value method BVM in a realistic torso model with up to 20 noise added to body-surface potentials and spl plusmn 1 cm error in heart position and size Optimal regularization was used for all inverse solutions The relative error of inferred heart-surface potentials of the MEM was significantly less p 0 05 than that of the BVM using zeroth-order Tikhonov regularization in 10 of the 12 cases tested These improvements occurred with a fourth-degree 24 coefficients or smaller multipole moment From these multipole coefficients heart-surface potentials can be found at an unlimited number of heart-surface locations Our indirect methods for estimating heart-surface potentials based on multipole inference appear to offer significant improvement over the conventional direct approach
Towards runtime testing in automotive embedded_systems
Runtime testing is a common way to detect faults during normal system operation To achieve a specific diagnostic coverage runtime testing is also used in safety critical automotive embedded_systems In this paper we propose a test architecture to consolidate the hardware resource consumption and timing needs of runtime tests and of application and system tasks in a hard_real-time embedded_system as applied to the automotive domain Special emphasis is put to timing requirements of embedded_systems with respect to hard_real-time and concurrent hardware resource accesses of runtime tests and tasks running on the target system
An Environment for re configuration and Execution Managenment of Flexible Radio Platforms
This paper presents the Flexible Radio Kernel FRK a configuration and execution management environment for hybrid hardware software flexible radio platform The aim of FRK is to manage platform reconfiguration for multi-mode multi-standard operation with different levels of abstraction A high level framework is described to manage multiple mac_layers and to enable MAC cooperation algorithms for cognitive_radio A low-level environment is also available to manage platform reconfiguration for radio operations Radio can be implemented using hardware or software elements Configuration state is hidden to the high-level layers offering pseudo concurrency time sharing properties This study presents a global view of FRK with details on some specific parts of the environment A practical study with algorithmic description is presented
A simple approach to evaluate the ergodic_capacity and outage_probability of correlated Rayleigh diversity channels with unequal signal-to-noise_ratios
In this article we propose a novel method to derive exact closed-form ergodic_capacity and outage_probability expressions for correlated rayleigh_fading_channels with receive_diversity Unlike the existing works the proposed method employ a simple approach for the capacity and outage_analysis for receiver diversity channels operating at different signal-to-noise_ratios depicted in the diagonal elements of matrix Î© With x being the channel gain vector random_variable of the form Y a a x Î© x is considered Novelty of the work resides in the fact that the distribution of Y a is accurately determined by employing Fourier representation of unit step function followed by complex integration in a straight forward way The ergodic channel_capacity is thus calculated by using the first-order moment N N N N N N E N N N N log N N N 2 N N N N Y N N 1 N N N N N N while the outage_probability for a certain threshold Î³0is evaluated using N N N N N N N N N N N 0 N N N N N Î³ N N N 0 N N N N N N N f N N N Y N N 0 N N N N N y N N dy N N N Extensive experiments have been conducted demonstrating the accuracy of the proposed approach
On the feasibility of synthesizing CAD software from specifications generating maze router tools in ELF
The application of program synthesis techniques to the generation of technology-sensitive VLSI physical design tools is described The architecture and implementation of a particular software generator called ELF targeted at the generation of maze routing software is described ELF strives to meet the demands of the target technology by automatically generating maze router implementations to match the application requirements ELF has three key features First a very high level language lacking data structure implementation specifications is used to describe algorithm design styles Second application-specific expertise about routing and application independent code synthesis techniques are used to guide search among alternative design styles for algorithms and data structures Third code_generation is used to transform the resulting abstract descriptions of selected algorithms and data structures into final executable code code_generation is an incremental stepwise refinement process Experimental results are presented covering several correct fully functional routers synthesized by ELF from varying high-level specifications Results from synthetic and industrial benchmarks are examined to illustrate ELF s capabilities
Canonical Sequence Directed Tactics Analyzer for Computer Go Games
We present an approach used in CSDTA canonical sequence directed tactics analyzer that uses canonical sequences Joseki in hoping to improve computer Go programs We collect 1278 canonical sequences and their deviations in our system Instead of trivially matching the current game to the collected sequences we define a notion of similarity to extract the most suitable move from the candidate sequences for the next move The simplicity of our method and its positive outcome make our approach a promising tool to be integrated into a complete computer Go program for a foreseeable improvement
An improved approximation to the estimation of the critical F values in best subset regression
Variable selection methods are routinely applied in regression_modeling to identify a small number of descriptors which best explain the variation in the response variable Most statistical packages that perform regression have some form of stepping algorithm that can be used in this identification process Unfortunately when a subset of p variables measured on a sample of n objects are selected from a set of k p to maximize the squared sample multiple_regression coefficient the significance of the resulting regression is upwardly biased The extent of this bias is investigated by using Monte Carlo simulation and is presented as an inflation factor which when multiplied by the usual tabulated F ratio gives an estimate of the true 5 critical value The results show that selection bias can be very high even for moderate-size data sets Selecting three variables from 50 generated at random with 20 observations will almost certainly provide a significant result if the usual tabulated F values are used An interpolation formula is provided for the calculation of the inflation factor for different combinations of n p k Four real_data_sets are examined to illustrate the effect of correlated descriptor variables on the degree of inflation
MultiK-MHKS A Novel Multiple kernel_learning Algorithm
In this paper we develop a new effective multiple kernel_learning algorithm First we map the input data into m different feature_spaces by m empirical kernels where each generated feature_space is taken as one view of the input space Then through borrowing the motivating argument from Canonical correlation_analysis CCA that can maximally correlate the m views in the transformed coordinates we introduce a special term called Inter-Function Similarity Loss R IFSI into the existing regularization framework so as to guarantee the agreement of multiview outputs In implementation we select the Modification of Ho-Kashyap algorithm with Squared approximation of the misclassification errors MHKS as the incorporated paradigm and the experimental results on benchmark data sets demonstrate the feasibility and effectiveness of the proposed algorithm named MultiK-MHKS
On the rank of random sparse_matrices
We investigate the rank of random symmetric sparse_matrices Our main finding is that with high probability any dependency that occurs in such a matrix is formed by a set of few rows that contains an overwhelming number of zeros This allows us to obtain an exact estimate for the co-rank
Maximum Utility peer_selection for p2p_streaming in wireless_ad_hoc_networks
In the recent years the peer-to-peer P2P overlay_network has been a promising architecture for multimedia_streaming_services besides its common use for efficient file_sharing By simply increasing the number of peers the P2P overlay_network can meet the high bit rate requirements of multimedia applications Optimal peer_selection for newly joining peers is one of the important problems especially in wireless_networks which have limited resources and capacity since the peer_selection process has a direct impact on the throughput of the underlay network and the co-existing unicast_traffic In this paper we tackle the problem of peer_selection for streaming_applications over wireless_ad_hoc_networks We devise a novel peer_selection algorithm which maximizes the throughput of the underlay network and at the same time makes p2p_streaming friendly towards the co-existing data traffic The proposed receiver based rate allocation and peer_selection RPS algorithm is derived using the network_utility_maximization NUM framework The algorithm solves the peer_selection and rate allocation problem distributedly while optimally adapting the medium_access_control mac_layer parameters and is easily extensible to large p2p_networks Simulation results show that by using the proper price exchange mechanism the peer receivers can effectively maximize the throughput of the underlay network by intelligently selecting its source peers
Time dependent groundwater modeling using spreadsheet
Abstract R N R N Time-dependent groundwater modeling using spreadsheet simulation TGMSS model is developed as solution technique It is a practical method that uses spreadsheets instead of the conventional solution_methods All of the aquifer parameters can easily be described in TGMSS model The results of TGMSS are validated with MODFLOW Results showed that TGMSS and MODFLOW results were in good agreement in terms of resulting values of hydraulic heads 2005 Wiley Periodicals Inc Comput Appl Eng Educ 13 192 199 2005 Published online in Wiley InterScience www interscience wiley com DOI 10 1002 cae 20048
A Simulation Tool to Study High-Frequency Chest Compression Energy Transfer Mechanisms and Waveforms for Pulmonary Disease Applications
High-frequency chest compression HFCC can be used as a therapeutic intervention to assist in the transport and clearance of mucus and enhance water secretion for cystic fibrosis patients An HFCC pump-vest and half chest-lung simulation with 23 lung generations has been developed using inertance compliance viscous friction relationships and Newton s second law The simulation has proven to be useful in studying the effects of parameter variations and nonlinear_effects on HFCC system performance and pulmonary system response The simulation also reveals HFCC waveform structure and intensity changes in various segments of the pulmonary system The HFCC system simulation results agree with measurements indicating that the HFCC energy transport mechanism involves a mechanically induced pulsation or vibration waveform with average velocities in the lung that are dependent upon small air displacements over large areas associated with the vest-chest interface In combination with information from lung physiology autopsies and a variety of other lung modeling efforts the results of the simulation can reveal a number of therapeutic implications
Distributed-information neural control the case of dynamic routing in traffic_networks
Large-scale traffic_networks can be modeled as graphs in which a set of nodes are connected through a set of links that cannot be loaded above their traffic capacities Traffic flows may vary over time Then the nodes may be requested to modify the traffic flows to be sent to their neighboring nodes In this case a dynamic routing problem arises The decision makers are realistically assumed 1 to generate their routing decisions on the basis of local information and possibly of some data received from other nodes typically the neighboring ones and 2 to cooperate on the accomplishment of a common goal that is the minimization of the total traffic cost Therefore they can be regarded as the cooperating members of informationally distributed organizations which in control engineering and economics are called team organizations Team optimal_control_problems cannot be solved analytically unless special assumptions on the team model are verified In general this is not the case with traffic_networks An approximate resolutive method is then proposed in which each decision maker is assigned a fixed-structure routing function where some parameters have to be optimized Among the various possible fixed-structure functions feedforward_neural_networks have been chosen for their powerful approximation capabilities The routing functions can also be computed or adapted locally at each node Concerning traffic_networks we focus attention on store-and-forward packet_switching_networks which exhibit the essential peculiarities and difficulties of other traffic_networks Simulations performed on complex communication_networks point out the effectiveness of the proposed method
Matching of PDB chain sequences to information in public databases as a prerequisite for 3D functional site visualization
The 3D structures of biomacromolecules stored in the protein_data_bank 1 were correlated with different external biological information from public databases We have matched the feature table of SWISS-PROT 2 entries as well InterPro 3 domains and function sites with the corresponding 3D-structures OMIM 4 Online Mendelian Inheritance in Man records containing information of genetic disorders were extracted and linked to the structures The exhaustive all-against-all 3D structure comparison of protein structures stored in DALI 5 was condensed into single files for each PDB entry Results are stored in xml_format facilitating its incorporation into related software The resulting annotation of the protein structures allows functional sites to be identified upon visualization Availability http leger gbf de PDBXML
L 2 Optimization in Discrete FIR Estimation Exploiting State-Space Structure
This paper studies the L 2 mean-square optimal_design of discrete-time FIR estimators A solution procedure which reduces the problem to a static matrix optimization problem admitting a closed-form_solution is proposed In the latter solution a special state-space structure of the associated matrices is exploited to obtain efficient formulae with the computational complexity proportional to the length of the impulse_response of the estimator Unlike previously available least-square FIR results our treatment does not impose unnecessarily restrictive assumptions on the process dynamics and can handle interpolation constraints on the unit circle which facilitates the inclusion of steady-state performance requirements
Application of plane waves for accurate measurement of microwave scattering from geophysical surfaces
The authors utilized the concept of a compact antenna range to obtain plane-wave illumination to accurately measure scattering properties of simulated sea ice They also made simultaneous_measurements using conventional antennas Measured scattering coefficients obtained with the plane-wave system at 10 GHz decreased by about 35 dB when the incidence angle increased from 0 spl deg to 10 spl deg Scattering coefficients derived from data collected with the radar_system at 13 5 GHz using conventional far-field antennas decreased by about 20 dB over the same angular region This demonstrates that the far-field properties of a widebeam antenna are inadequate for measuring the angular scattering response of smooth surfaces They believe that application of the compact antenna range concept for scattering measurements has a wide range of applications and is the solution to the long-standing problem of how to directly measure scattering consisting of coherent and incoherent components
A Parallelized Surface Extraction Algorithm for Large Binary Image Data Sets Based on an Adaptive 3-D Delaunay Subdivision Strategy
In this paper we describe a novel 3D subdivision strategy to extract the surface of binary image data This iterative approach generates a series of surface meshes that capture different levels of detail of the underlying structure At the highest level of detail the resulting surface mesh generated by our approach uses only about 10 percent of the triangles in comparison to the Marching Cube MC algorithm even in settings where almost no image_noise is present Our approach also eliminates the so-called staircase effect which voxel-based algorithms like the MC are likely to show particularly if nonuniformly sampled images are processed Finally we show how the presented algorithm can be parallelized by subdividing 3D image space into rectilinear blocks of subimages As the algorithm scales very well with an increasing number of processors in a multithreaded setting this approach is suited to process large image data sets of several gigabytes Although the presented work is still computationally more expensive than simple voxel-based algorithms it produces fewer surface triangles while capturing the same level of detail is more robust toward image_noise and eliminates the above-mentioned staircase effect in anisotropic settings These properties make it particularly useful for biomedical applications where these conditions are often encountered
AutoVision - flexible processor architecture for video-assisted driving
Summary form only given Future automotive security_systems will benefit from visual scene analysis based on a fusion of video infrared and radar images Today we have already functions like lane departure warning and automatic cruise control ACC for pretty well defined driving environments such as highways and primary roads Recent research activities concentrate on more complex environments such as city traffic with a wide variety of traffic participants moving in an unpredictable manner e g bikes pedestrians children and even animals and under changing weather and lighting_conditions The ITRS semiconductor roadmap for microelectronics forecasts a continued doubling of transistor capacity per chip every 2 to 2 5 years enabling billion transistor ASIC designs in the near future Multi processor system_on_chip MPSoC solutions with 8 16 or even more standard RISC CPU cores mega-bytes of fast ns access latencies on-chip SRAM memories giga-byte per second interconnect buses or NoC network_on_chip meshes high-speed serial I Os and last but not least million gate equivalent dedicated hardware_accelerator functions in eFPGA embedded field_programmable_gate_array logic are becoming reality on a single silicon substrate Examples of current research projects shall illustrate our perception on how this tremendous increase in functionality and computational performance per chip area may impact automotive control unit ACU architectures for driver assistance applications The AutoVision processor is a dynamically reconfigurable MPSoC prototype where video-specific pixel processing engines are on-the-fly loaded or exchanged without interrupting regular_system operations For the time being pixel processing engines cover functions such as object edge_detection or luminance segmentation and are implemented as dedicated hardware_accelerators to ensure real-time frame processing capabilities of the AutoVision processor Dynamic replacement of processing engines ensures an automatic and area efficient adaptation to various driving conditions Segmented objects are in a subsequent step characterized by means of standard MPEG-7 descriptors and entered as search criteria into traffic scene analysis databases Goal is to obtain a clean distinction between passenger cars trucks and big rectangular traffic signs and to identify pedestrians or bikers in complex traffic situations The AutoVision processor project is supported by the German Research Foundation DFG in the special emphasis research programme reconfigurable_computing
TALP at GikiCLEF 2009
This paper describes our experiments in Geographical information_retrieval with the Wikipedia collection in the context of our participation in the GikiCLEF 2009 Multilingual task in English and Spanish Our system_called gikiTALP follows a very simple approach that uses standard information_retrieval with the Sphinx full-text search_engine and some natural_language_processing techniques without Geographical Knowdledge
transmit_beamforming for frequency-selective_channels
In this paper we propose beamforming schemes for frequency-selective_channels with decision-feedback equalization DFE at the receiver We consider both finite impulse_response FIR and infinite_impulse_response IIR beamforming filters BFFs In case of IIR beamforming we are able to derive closed-form_expressions for the optimum BFFs In addition we provide an efficient numerical_method for recursive calculation of the optimum FIR BFFs Simulation and numerical_results for typical GSM EDGE channels confirm the significant performance gains achievable with beamforming compared to single-antenna transmission and optimized delay diversity
All possible second-order four-impedance two-stage Colpitts oscillators
The authors report all the possible four-impedance settings that yield a valid second-order two-stage Colpitts oscillator These settings are obtained following an exhaustive search conducted on two possible structures of the oscillator modelled through two-port network transmission parameters Only valid second-order cases with a maximum of three reactive elements are reported Experimental and Spice verification of a selected example using both MOS and BJT transistors is given
Mining Frequent Sequential Patterns under a Similarity Constraint
Many practical applications are related to frequent sequential pattern mining ranging from web_usage_mining to Bioinformatics To ensure an appropriate extraction cost for useful mining tasks a key issue is to push the user-defined constraints deep inside the mining_algorithms In this paper we study the search for frequent sequential patterns that are also similar to an user-defined reference pattern While the effective processing of the frequency constraints is well-understood our contribution concerns the identification of a relaxation of the similarity constraint into a convertible anti-monotone constraint Both constraints are then used to prune the search space during a levelwise search Preliminary experimental validations have confirmed the algorithm efficiency
Dispatching Petroleum Products
Petroleum products are distributed worldwide from refineries and lube plants to retail outlets and industrial customers Proper dispatching of shipments of such products packaged and in bulk may result in significant transportation and inventory cost_savings This work examines the variety of operational environments which exist in dispatching petroleum products and the operations research tools used by oil companies to dispatch such products In addition it identifies gaps where additional research is needed
CLASSIFICATION AVERAGING AND RECONSTRUCTION OF MACROMOLECULES IN electron_tomography
electron_tomography provides opportunities to determine three-dimensional cellular architecture at resolutions high enough to identify individual macromolecules such as proteins image_analysis of such data poses a challenging problem due to the extremely low_signal-to-noise_ratios that makes individual volumes simply too noisy to allow reliable structural interpretation This requires using averaging techniques to boost the signal-to-noise_ratios a common practice in electron microscopy single particle analysis where they have proven to be very powerful in elucidating high resolution structure Although there are significant similarities in the way data is processed several new problems arise in the tomography case that have to be properly dealt with Such problems involve dealing with the missing wedge characteristic of limited angle tomography the need for robust and efficient 3D alignment routines and design of methods that account for diverse conformations through the use of classification We present a framework for reconstruction via alignment classification and averaging of volumes obtained from limited angle electron_tomography providing a powerful tool for high resolution structure determination and description of conformational variability in a biological context
performance_analysis of the conventional complex LMS and augmented complex lms_algorithms
Recently the augmented complex LMS AClms_algorithm has been proposed for modeling complex-valued signal relationships in which a widely-linear_model can be more appropriate 1 It is not clear however how the behavior of ACLMS differs from that of the conventional complex LMS CClms_algorithm In this paper we leverage a recently-developed analysis for the complex lms_algorithm 2 to illuminate the performance relationships between the ACLMS and CClms_algorithms Our analysis shows that the AClms_algorithm can potentially achieve a lower steady-state mean-squared error as compared to that of CCLMS but the convergence speed of ACLMS is slowed in the presence of highly non-circular complex-valued input signals An adaptive_beamforming example indicates the utility of the results
Extending wireless_sensor_network_lifetime through order-based genetic_algorithm
Extending the lifetime is a key issue in wireless_sensor_networks An effective way to extend the lifetime is to partition the sensors into several covers and activate the covers one by one Thus the more the covers the longer the lifetime To find the maximum number of covers has been modeled as the Set K-Cover problem In this paper we propose using order-based genetic_algorithm to solve the Set K-Cover problem for extending the lifetime of wireless_sensor_networks The proposed algorithm needs neither an upper bound nor any assumption about the maximum number of covers Experimental results show that the order-based genetic_algorithm can achieve near-optimal_solutions efficiently
Robust neural predictor for noisy chaotic time series prediction
A robust neural predictor is designed for noisy chaotic time series prediction in this paper The main idea is based on the consideration of the bounded uncertainty in predictor input and it is a typical Errors-in-Variables problem The robust design is based on the linear-in-parameters ESN echo_state_network model By minimizing the worst-case residual induced by the bounded perturbations in the echo state variables the robust predictor is obtained in coping with the uncertainty in the noisy time series In the experiment the classical Mackey-Glass 84-step benchmark prediction task is investigated The prediction performance is studied for the nominal and robust design of ESN predictors
Minimization strategies for maximally parallel multiset rewriting systems
Maximally parallel multiset rewriting systems MPMRS give a convenient way to express relations between unstructured objects The functioning of various computational devices may be expressed in terms of MPMRS e g register machines and many variants of P systems In particular this means that MPMRS are Turing universal however a direct translation leads to quite a large number of rules Like for other classes of computationally complete devices there is a challenge to find a universal system having the smallest number of rules In this article we present different rule minimization strategies for MPMRS based on encodings and structural transformations We apply these strategies to the translation of a small universal register machine Korec 1996 9 and we show that there exists a universal MPMRS with 23 rules Since MPMRS are identical to a restricted variant of P systems with antiport rules the results we obtained improve previously known results on the number of rules for those systems
The Introduction of the OSCAR Database API ODA
The OSCAR 14 cluster installation toolkit was created by the Open Cluster Group OCG for one particular type of High Performance Computing HPC cluster OSCAR is currently one of the widely used cluster installation toolkits it boasts hundreds of thousands of downloads and active mailing lists OSCAR has expanded its area with several sub-projects targeting other types of hpc_clusters Each of these projects share a core set of OSCAR code including the OSCAR Database and its access API ODA OSCAR Database API The ODA abstraction layer consisting of a database_schema and corresponding API hides a commodity back-end database e g MySQL 15 Because OSCAR and its sub-projects are targeted at new innovative environments including non-HPC environments there are significant issues with managing various configurations of each project For example as we previously showed 8 previous versions of ODA were unable to represent the complex ever-growing set of data required to accurately describe the clusters that it manages Further its API was extremely complex requiring a steep learning curve for OSCAR developers Therefore we have designed and implemented a new database_schema to deal with these issues This new version of ODA has not only resolved the above problems but also as proposed in our previous paper enabled storage and retrieval of various configuration information and encouraged data re-use between the main OSCAR project and its derivative projects In addition the new version of ODA has sped up the OSCAR installation process This document presents a simpler highly flexible design and implementation of ODA slated to be included in OSCAR v5 0 It also suggests a blueprint for maintaining the database modules of ODA in a systematic organized way
Learning for Sustainability Transition through Bounded Socio-technical Experiments in Personal Mobility
Abstract A bounded socio-technical experiment BSTE attempts to introduce a new technology service or a social arrangement on a small scale Many such experiments in personal mobility are ongoing worldwide They are carried out by coalitions of diverse actors and are driven by long term and large scale visions of advancing society s sustainability agenda This paper focuses on the processes of higher-order learning that occur through BSTEs Based on the conceptual frameworks from theories of organizational_learning policy-oriented learning and diffusion of innovation we identify two types of learning the first type occurs among the participants in the experiment and their immediate professional networks the second type occurs in the society at large Both types play a key role in the societal transition towards sustainable mobility systems Two case studies in which the Design for Sustainability Group at Technical University of Delft has participated provide empirical data for the analysis One
attitude_control of a Quadruped Trot While Turning
During a complete running stride which involves significant periods of flight during which no legs are contacting the ground a quadruped cannot employ static stability techniques Instead the corrective forces necessary to maintain dynamic stability must be applied during the short stance intervals inherent to high-speed running Because of this complexity and the large coupled forces required to run much of the research on the control of quadruped running has focused on planar_systems which are not required to simultaneously control attitude in all three dimensions The 3D trot controller presented here overcomes these and other complexities to control a trot up to 3 75 m s approximately 3 body lengths per second and turning rates up to 20 deg s The biomimetic method of banking into a high-speed turn is also investigated here Along with the details of the attitude_control algorithm a set of control principles for high-speed legged motion is presented These principles such as the need to counteract the disturbance of swing leg return and the usefulness of force redistribution during stance are not dependent on a particular scale or actuation scheme and can be applied to a wider range of legged systems
Robust fuzzy and recurrent_neural_network motion_control among dynamic obstacles for robot_manipulators
An integration of a fuzzy controller and modified Elman neural_networks NN approximation-based computed-torque_controller is proposed for motion_control of autonomous manipulators in dynamic and partially known_environments containing moving_obstacles The navigation technique of robot_control using artificial_potential_fields is based on the fuzzy controller The NN controller can deal with unmodeled bounded disturbances and or unstructured unmodeled dynamics of the robot_arm The NN weights are tuned online with no off-line learning phase required The stability of the closed-loop system is guaranteed by the lyapunov_theory The purpose of the controller which is designed as a neuro-fuzzy_controller is to generate the commands for the servo-systems of the robot so it may choose its way to its goal autonomously while reacting in real-time to unexpected events The proposed scheme has been successfully tested The controller also demonstrates remarkable performance in adaptation to changes in manipulator dynamics Sensor-based motion_control is an essential feature for dealing with model uncertainties and unexpected obstacles in real-time world systems
Non binary protograph low_density_parity_check_codes for space_communications
SUMMARY R N R N Protograph-based non-binary low-density parity-check ldpc_codes with ultra-sparse parity-check matrices are compared with binary LDPC and turbo_codes TCs from space communication standards It is shown that larger coding_gains are achieved outperforming the binary competitors by more than 0 3 dB on the additive_white_gaussian_noise_channel AWGN In the short block length regime the designed codes gain more than 1 dB with respect to the binary protograph ldpc_codes recently proposed for the next generation up-link standard of the Consultative Committee for Space Data Systems Copyright 2012 John Wiley Sons Ltd
Coding theoretic approach to image_segmentation
This paper introduces multi-scale tree-based approaches to image_segmentation using Rissanen s coding theoretic minimum description length MDL principle to penalize overly complex segmentations Images are modelled as Gaussian random fields of independent pixels with piecewise constant mean and variance This model captures variations in both intensity mean value and texture variance Segmentation thus amounts to detecting changes in the mean and or variance One algorithm is based on an adaptive greedy rectangular recursive partitioning scheme The second algorithm is an optimally pruned wedgelet decorated dyadic partitioning We compare the two schemes with an alternative constant variance dyadic CART classification and regression tree scheme which accounts only for variations in mean and demonstrate their performance on sar_images
An empirical Bayes approach to inferring large-scale gene association networks
Motivation Genetic networks are often described statistically using graphical_models e g bayesian_networks However inferring the network structure offers a serious challenge in microarray analysis where the sample size is small compared to the number of considered genes This renders many standard algorithms for graphical_models inapplicable and inferring genetic networks an ill-posed_inverse_problem R N R N Methods We introduce a novel framework for small-sample inference of graphical_models from gene expression data Specifically we focus on the so-called graphical gaussian_models GGMs that are now frequently used to describe gene association networks and to detect conditionally dependent genes Our new approach is based on 1 improved regularized small-sample point estimates of partial correlation 2 an exact test of edge inclusion with adaptive estimation of the degree of freedom and 3 a heuristic network search based on false discovery rate multiple testing Steps 2 and 3 correspond to an empirical Bayes estimate of the network_topology R N R N Results Using computer simulations we investigate the sensitivity power and specificity true negative rate of the proposed framework to estimate GGMs from microarray data This shows that it is possible to recover the true network_topology with high accuracy even for small-sample datasets Subsequently we analyze gene expression data from a breast cancer tumor study and illustrate our approach by inferring a corresponding large-scale gene association network for 3883 genes R N R N Availability The authors have implemented the approach in the R package GeneTS that is freely available from http www stat uni-muenchen de strimmer genets from the R archive CRAN and from the Bioconductor website R N R N Contact korbinian strimmer lmu de
Analytical models for crosstalk excitation and propagation in vlsi_circuits
The authors develop a general methodology to analyze crosstalk effects that are likely to cause errors in deep submicron high-speed circuits They focus on crosstalk due to capacitive coupling between a pair of lines closed_form equations are derived that quantify the severity of these effects and describe qualitatively the dependence of these effects on the values of circuit parameters the rise fall times of the input transitions and the skew between the transitions For noise propagation they present a new way for predicting the output waveform produced by an inverter due to a nonsquare wave pulse at its input To expedite the computation of the response of a logic_gate to an input pulse the authors have developed a novel way of modeling such gates by an equivalent inverter The results of their analysis provide conditions that must be satisfied by a sequence of vectors used for validation of designs as well as post-manufacturing testing of devices in the presence of significant crosstalk They present data to demonstrate accuracy of their results including example runs of a test generator that uses these results
Land use Dynamic Monitoring using Multi-Temporal SPOT Data in Beijing City from 1986 to 2004
Remote sensing dynamic monitoring of land use can detect the change information of land use and update the current land use map which is important for rational utilization and scientific management to land resources This paper discussed the technological procedure of land use dynamic monitoring including the process of remote sensed images the information classification and extraction of remote sensed imagery and analysis of land use changes Based on SPOT imagery data in three periods the paper took Beijing city as an example extracted the land use information during 1986-2004 and the land use changes were required in the period The object-oriented method was used to extract information and contrastive method after classification was used to confirm change zones
VoiceXML and the W3C speech interface framework
VoiceXML is a markup language for creating voice-user_interfaces It uses speech and telephone touchtone recognition for input and prerecorded audio and text-to-speech_synthesis TTS for output It s based on the world_wide_web Consortium s W3C s extensible_markup_language XML and leverages the Web paradigm for application development and deployment By having a common language application developers platform vendors and tool providers all can benefit from code portability and reuse The paper discusses VoiceXML and the W3C speech interface framework
Flexible ASIC shared masking for multiple media processors
ASIC provides more than an order of magnitude advantage in terms of density speed and power requirement per gate However economic cost of masks and technological deep micron manufacturability trends favor FPGA as an implementation platform In order to combine the advantages of both platforms and alleviate their disadvantages recently a number of approaches such as structured ASIC regular fabrics have been proposed Our goal is to introduce an approach that has the same objective but is orthogonal to those already proposed The idea is to implement several ASIC designs in such a way that they share the datapath memory structure and several bottom layers of interconnect while each design has only a few unique metal layers We identified and addressed two main problems in our quest to develop a CAD flow for realization of such designs They are i the creation of the datapath and ii the identification of common and unique interconnects for each design Both problems are solved optimally using ilp_formulations We assembled a design_flow platform using two new programs and the Trimaran and Shade tools We quantitatively analyzed the advantages and disadvantages of the approach using the Mediabench benchmark suite
Buy-at-Bulk Network Design with Protection
We consider approximation_algorithms for buy-at-bulk network design with the additional constraint that demand pairs be protected against edge or node_failures in the network In practice the most popular model used in high speed telecommunication_networks for protection against failures is the so-called 1 1 model In this model two edge or node-disjoint_paths are provisioned for each demand pair We obtain the first non-trivial approximation_algorithms for buy-at-bulk network design in the 1 1 model for both edge and node-disjoint protection requirements Our results are for the single-cable cost model which is prevalent in optical_networks More specifically we present a constant-factor approximation for the single-sink case and an O log 3 n approximation for the multi-commodity case These results are of interest for practical applications and also suggest several new challenging theoretical problems
Software-defined infrastructure and the Future Central Office
This paper discusses the role of virtualization and software-defined infrastructure SDI in the design of future application platforms and in particular the Future Central Office CO A multi-tier computing cloud is presented in which resources in the Smart Edge of the network play a crucial role in the delivery of low-latency and data-intensive_applications Resources in the Smart Edge are virtualized and managed using cloud_computing principles but these resources are more diverse than in conventional data centers including programmable hardware GPUs etc We propose an architecture for future application platforms and we describe the SAVI Testbed TB design for the Smart Edge The design features a novel Software-Defined Infrastructure manager that operates on top of OpenStack and OpenFlow We conclude with a discussion of the implications of the Smart Edge design on the Future CO
Fisher Kernels for Handwritten Word-spotting
The Fisher kernel is a generic framework which combines the benefits of generative and discriminative approaches to pattern_classification In this contribution we propose to apply this framework to handwritten word-spotting Given a word image and a keyword generative_model the idea is to generate a vector which describes how the parameters of the keyword model should be modified to best fit the word image This vector can then be used as the input of a discriminative classifier We compare the performance of the proposed approach with that of a generative baseline on a challenging real-world dataset of customer letters When the kernel used by the classifier is linear the performance improvement is marginal but the proposed system is approximately 15 times faster than the baseline If we use a non-linear kernel devised for this task we obtain a 15 relative_reduction of the error but the detector is approximately 15 times slower
Nonintrusive Load Monitoring and Diagnostics in Power Systems
This paper describes a transient event classification scheme system identification techniques and implementation for use in nonintrusive load monitoring Together these techniques form a system that can determine the operating schedule and find parameters of physical models of loads that are connected to an AC or DC power distribution_system The monitoring system requires only off-the-shelf hardware and recognizes individual transients by disaggregating the signal from a minimal number of sensors that are installed at a central location in the distribution_system Implementation details and field tests for AC and DC systems are presented
False alarm reduction by improved filler model and post-processing in speech keyword_spotting
This paper proposes four methods for improving the performance of keyword_spotting KWS systems Keyword models are usually created by concatenating the phoneme HMMs and garbage models consist of all phonemes HMMs We present the results of investigations involving the use of skips in states of keyword HMMs and we focus on improving the hit ratio then for false alarm reduction in KWS we model the words that are similar to keywords and we create HMMs for highly frequent words These models help to improve the performance of the filler model Two post-processing steps based on phoneme and word probabilities are used on the results of KWS to reduce the false alarms We evaluate the performance of the improved keyword_spotting in FarsDat corpus and compare the approaches The presented techniques depict better performances than the popular KWS systems
Automated Pathfinding tool chain for 3D-stacked integrated_circuits Practical case study
New technologies for manufacturing 3D Stacked ICs offer numerous opportunities for the design of complex and effcient embedded_systems But these technologies also introduce many design options at system chip design level hard to grasp during the complete design cycle Because of the sequential nature of current design practices designers are often forced to introduce design margins to meet required specications resulting in sub-optimal_designs In this paper we introduce new design methodology and practical tool chain called PathFinding Flow that can help designers to easily trade-off between different system_level_design choices physical design and or technology options and understand their impact on typical design parameters such as cost performance and power Proposed methodology and the tool chain will be demonstrated on a practical case study involving fairly complex Multi-Processor System-on-Chip using Network-on-Chip for communication medium With this example we will show how high-level_synthesis can be used to quickly move from high-level to RTL models necessary for accurate physical prototyping for both computation and communication We will also show how the possibility of design iteration through the mechanism of feedback based on physical information from physical prototyping can improve design performance Finally we will show how we can move in no time from traditional 2D to 3D design and how we can measure benets of such design choice
Managing traceability information in manufacture
In this paper an approach to design information_systems for traceability is proposed The paper applies gozinto graph modelling for traceability of the goods flow A gozinto graph represents a graphical listing of raw materials parts intermediates and subassemblies which a process_transforms into an end product through a sequence of operations Next the graphical listing has been translated into a reference data model that is the basis for designing an information_system_for tracking and tracing Materials that are modelled this way represent production and or purchase lots or batches The composition of a certain end product is then represented through modelling all its constituent materials along with their intermediate relations By registering all relations between sub-ordinate and super-ordinate material lots a method of tracking the composition of the end product is obtained When the entire sequence of operations required for manufacturing an end product adheres to this registering of relations a multilevel bill of lots can be compiled That bill of lots then provides the necessary information to determine the composition of a material item out of component items These composition data can be used to recall any items having consumed a certain component of specific interest e g deficient but also to certify product quality or to pro-actively adjust production processes to optimise the product quality in relation to its production characteristics e g scarcity costs or time
PhD forum Calibrating and using the global network of outdoor webcams
The vast imaging resources available via the Internet are underutilized We propose to lay the foundation for the use of cameras attached to the Internet also known as webcams as free and flexible sensors Developing an understanding of the relationship between signals in the world and the image variations they cause is critical to this effort We use this understanding to develop methods to calibrate webcams to estimate scene properties and to report the weather
Acquiring a Holistic Picture The 4Screens Web-Based Simulator Helping Students to Unify Behaviours of electronic_systems
Engineering knowledge is complex Therefore discipline concepts cannot usually be introduced to students all at once Normally individual models and behaviours are taught in several courses by different teaching staff and in separate years of study This often results in inadequate comprehension of the disciplinepsilas ldquobig picturerdquo - that is the interrelationship of various concepts models and behaviours Students studying electronic circuit design for example often overlook important aspects of linear circuit performance Their knowledge of the effects of steady-state and transient responses requires improved unification Simple computer-based simulators can significantly help students to establish well-developed holistic views of systems in many engineering disciplines The 4Screens Web-based simulator has helped hundreds of students in becoming more proficient with electronics systems It has been developed by a group of undergraduates to make the Four screens model easy to utilise A student using the simulator can simultaneously display four important characteristics of an electronic system on a computer screen its algebraic transfer function H s systempsilas pole-zero plot its Bode plots of magnitude and phase as well as a graph of its time response to a unit step Questionnaire responses and test performances over the last seven years confirm the effectiveness of the Four screens and the 4Screens Web-based simulator in improving student_learning
Benefits derived from restructuring the practical component of an introductory course in electronic communication_systems
In an effort to address students complaints regarding the tedious and frustrating nature of the practical component of the department s introductory communication course we embarked on a restructuring exercise The restructuring of this component of the course shows promise based on preliminary results The students are obtaining higher grades and gave the practical component a more favorable rating Additionally the restructuring exercise allows for the addition of several more experiments enabling us to cover a broader portion of the course in the practical component This paper looks at the changes that were made in the restructuring exercise and present preliminary results on the improvement in student s grade which can be attributed to this exercise Over 16 of students are now obtaining grades over 80 compare to 0 before the restructuring exercise Results from students evaluations of the practical component of the course before and after the restructuring exercise is also presented which shows an effective 24 improvement in the student rating
Minimizing data and synchronization costs in one-way communication
Minimizing communication and synchronization costs is crucial to the realization of the performance potential of parallel computers This paper presents a general technique which uses a global data-flow framework to optimize communication and synchronization in the context of the one-way communication_model In contrast to the conventional send receive message-passing communication_model one-way communication is a new paradigm that decouples message transmission and synchronization In parallel_machines with appropriate low-level support this may open up new opportunities not only to further optimize communication but also to reduce the synchronization overhead We present optimization_techniques using our framework for eliminating redundant data communication and synchronization operations Our approach works with the most general data alignments and distributions in languages like High Performance Fortran HPF and uses a combination of the traditional data-flow analysis and polyhedral algebra Empirical results for several scientific benchmarks on a Cray T3E multiprocessor machine demonstrate that our approach is successful in reducing the number of data communication and synchronization messages thereby reducing the overall execution times
Pattern Mining in Visual Concept Streams
Pattern mining_algorithms are often much easier applied than quantitatively assessed In this paper we address the pattern evaluation problem by looking at both the capability of models and the difficulty of target concepts We use four different data_mining models frequent_itemset_mining k-means_clustering hidden_markov_model and hierarchical hidden_markov_model to mine 39 concept streams from the a 137-video broadcast_news collection from TRECVID-2005 We hypothesize that the discovered patterns can reveal semantics beyond the input space and thus evaluate the patterns against a much larger concept space containing 192 concepts defined by LSCOM Results show that HHMM has the best average prediction among all models however different models seem to excel in different concepts depending on the concept prior and the ontological relationship Results also show that the majority of the target concepts are better predicted with temporal or combination hypotheses and there are novel concepts found that are not part of the original lexicon This paper presents the first effort on temporal pattern mining in the large concept space There are many promising directions to use concept mining to help construct better concept detectors or to guide the design of multimedia ontology
Adaptive Goals for Self-Adaptive service_compositions
service_compositions need to continuously self- adapt to cope with unexpected failures In this context adaptation becomes a fundamental requirement that must be elicited along with the other functional and non functional_requirements Beside modelling effective adaptation also demands means to trigger it at runtime as soon as the actual behavior of the composition deviates from stated requirements This paper extends traditional goal_models with adaptive goals to support continuous adaptation Goals become live runtime entities whose satisfaction level is dynamically updated Furthermore boundary infringement triggers adaptation capabilities The paper also provides a methodology to trace goals onto the underlying composition assess goals satisfaction at runtime and activate adaptation consequently All the key elements are demonstrated on the definition of the process to control an advanced washing machine
Integrating Replenishment Decisions with Advance Demand Information
There is a growing consensus that a portfolio of customers with different demand lead times can lead to higher more regular revenues and better capacity utilization Customers with positive demand lead times place orders in advance of their needs resulting inadvance demand information This gives rise to the problem of finding effective inventory control policies under advance demand information We show that state-dependent s S and base-stock policies are optimal for stochastic inventory systems with and without fixed costs The state of the system reflects our knowledge of advance demand information We also determine conditions under which advance demand information has no operational value A numerical study allows us to obtain additional insights and to evaluate strategies to induce advance demand information
A new construction for n-track d k codes with redundancy
Digital magnetic and optical storage_systems employing NRZI recording use d k codes The d-parameter specifies the minimum number of 0 s occurring between 1 s while the k-parameter specifies the maximum number of 0 s between l s The n-track d k codes denoted as d k n codes are extensions of d k codes for use in multiple-track systems Instead of imposing each track to individually satisfy both constraints d k n codes satisfy the d-constraint in each track individually while relaxing the k-constraint by allowing it to be satisfied jointly by the multiple tracks Although d k n codes can provide significant capacity increases over d k codes they suffer from the fact that a single faulty track can cause loss of synchronization and hence loss of the data on all tracks Orcutt and Marcellin see IEEE Trans on Inform Theory Sept 1993 introduced n-track d k codes with a redundancy of r denoted as d k n r codes which allow for r faulty tracks by mandating that all subsets of n-r tracks satisfy the joint k-constraint We propose a new method to construct d k n r codes These codes have simple encoding and decoding_schemes gain a large part of the capacity increase possible when using d k n r codes and are considerably more robust to faulty tracks
Structure and reaction based evaluation of synthetic accessibility
De novo design systems provide powerful methods to suggest a set of novel structures with high estimated binding affinity One deficiency of these methods is that some of the suggested structures could be synthesized only with great difficulty We devised a scoring method that rapidly evaluates synthetic accessibility of structures based on structural complexity similarity to available starting materials and assessment of strategic bonds where a structure can be decomposed to obtain simpler fragments These individual components were combined to an overall score of synthetic accessibility by an additive scheme The weights of the scoring function components were calculated by linear regression_analysis based on accessibility scores derived from medicinal chemists The calculated values for synthetic accessibility agree with the values proposed by chemists to an extent that compares well with how chemists agree with each other
Evolution of Protection Technologies in Metro Core optical_networks
The market of metro optical_networking has increased rapidly over the last few years Traditional telecommunication infrastructure has an emphasis on long-haul optical_transmission with ultra broadband capacity relying mostly on large pure dense_wavelength_division_multiplexing Dwdm_systems Today however metro core optical_networks take the major role in provisioning local access services and interconnecting service points of presences POPs with long-haul transmission This represents a pivotal point in business operations of data communication services for service providers and large enterprises In addition the upper layer data services completely leans upon the substrate wavelength communication and hence the survivability and reliability issues in the optical domain are now becoming crucial topics This paper provides a detailed discussion around the development process of protection technologies in metro core optical transport infrastructure
Packet radio and the factory of the future
