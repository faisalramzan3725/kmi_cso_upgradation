A new approach of 3D watermarking based on image segmentation
In this paper, a robust 3D triangular mesh watermarking algorithm based on 3D segmentation is proposed. In this algorithm three classes of watermarking are combined. First, we segment the original image to many different regions. Then we mark every type of region with the corresponding algorithm based on their curvature value. The experiments show that our watermarking is robust against numerous attacks including RST transformations, smoothing, additive random noise, cropping, simplification and remeshing.
Attractor neural networks with activity-dependent synapses: The role of synaptic facilitation
We studied an autoassociative neural network with dynamic synapses which include a facilitating mechanism. We have developed a general mean-field framework to study the relevance of the different parameters defining the dynamics of the synapses and their influence on the collective properties of the network. Depending on these parameters, the network shows different types of behaviour including a retrieval phase, an oscillatory regime, and a non-retrieval phase. In the oscillatory phase, the network activity continously jumps between the stored patterns. Compared with other activity-dependent mechanisms such as synaptic depression, synaptic facilitation enhances the network ability to switch among the stored patterns and, therefore, its adaptation to external stimuli. A detailed analysis of our system reflects an efficient-more rapid and with lesser errors-network access to the stored information with stronger facilitation. We also present a set of Monte Carlo simulations confirming our analytical results.
A characterization of balanced episturmian sequences
It is well-known that Sturmian sequences are the non ultimately periodic sequences that are balanced over a 2-letter alphabet. They are also characterized by their complexity: they have exactly $(n+1)$ distinct factors of length $n$. A natural generalization of Sturmian sequences is the set of infinite episturmian sequences. These sequences are not necessarily balanced over a $k$-letter alphabet, nor are they necessarily aperiodic. In this paper, we characterize balanced episturmian sequences, periodic or not, and prove Fraenkel's conjecture for the special case of episturmian sequences. It appears that balanced episturmian sequences are all ultimately periodic and they can be classified in 3 families.
Exploring the space of a human action
One of the fundamental challenges of recognizing actions is accounting for the variability that arises when arbitrary cameras capture humans performing actions. In this paper, we explicitly identify three important sources of variability: (1) viewpoint, (2) execution rate, and (3) anthropometry of actors, and propose a model of human actions that allows us to investigate all three. Our hypothesis is that the variability associated with the execution of an action can be closely approximated by a linear combination of action bases in joint spatio-temporal space. We demonstrate that such a model bounds the rank of a matrix of image measurements and that this bound can be used to achieve recognition of actions based only on imaged data. A test employing principal angles between subspaces that is robust to statistical fluctuations in measurement data is presented to find the membership of an instance of an action. The algorithm is applied to recognize several actions, and promising results have been obtained.
Generalized upper bounds on the minimum distance of PSK block codes
This paper generalizes previous optimal upper bounds on the minimum Euclidean distance for phase shift keying (PSK) block codes, that are explicit in three parameters: alphabet size, block length a ...
Applying BCMP multi-class queueing networks for the performance evaluation of hierarchical and modular software systems
Queueing networks with multiple classes of customers play a fundamental role for evaluating the performance of both software and hardware architectures. The main strength of product–form models, in particular of BCMP queueing networks, is that they combine a flexible formalism with efficient analysis techniques and solution algorithms. In this paper we provide an algorithm that starting from a high–level description of a system, and from the definition of its components in terms of interacting sub–systems, computes a multiple–class and multiple–chain BCMP queueing network. We believe that the strength of this approach is twofold. First, the modeller deals with simplified models, which are defined in a modular and hierarchical way. Hence, we can carry on sensitivity analysis that may easily include structural changes (and not only on the time parameters). Second, maintaining the product–form property allows one to derive the average system performance indices very efficiently. The paper also discusses the ...
A Push–Pull Class-C CMOS VCO
A CMOS oscillator employing differential transistor pairs working in Class-C in push-pull configuration is presented. The oscillator exhibits the same advantages enjoyed by complementary topologies on oscillators based on a single differential pair, while yielding a substantial power consumption reduction thanks to the Class-C operation. The phase-noise performance and the fundamental conditions required to keep the transistors working in Class-C are analyzed in detail. It is shown that, for an optimal performance, both nMOS and pMOS transistors should not be pushed into the deep triode region by the instantaneous resonator voltage, and a simple circuit solution is proposed to accommodate a large oscillation swing. A 0.18- μm CMOS prototype of the (voltage-controlled) oscillator displays an oscillation frequency from 6.09 to 7.50 GHz. The phase noise at 2-MHz offset is below -120 dBc/Hz with a power dissipation of 2.2 mW, for a state-of-the-art figure-of-merit ranging from 189 to 191 dBc/Hz.
On computability of pattern recognition problems
In statistical setting of the pattern recognition problem the number of examples required to approximate an unknown labelling function is linear in the VC dimension of the target learning class. In this work we consider the question whether such bounds exist if consider only computable pattern recognition methods, assuming that the unknown labelling function is also computable. We find that in this case the number of examples required for a computable method to approximate the labelling function not only is not linear, but grows faster (in the VC dimension of the class) than any computable function. No time or space constraints are put on the predictors or target functions; the only resource we consider is the training examples.#R##N##R##N#The task of pattern recognition is considered in conjunction with another learning problem — data compression. An impossibility result for the task of data compression allows us to estimate the sample complexity for pattern recognition.
Manipulating biological and mechanical micro-objects using LIGA-microfabricated end-effectors
We first discuss some general aspects of micromanipulation and possible different approaches. Then, we present new results in the micromanipulation of mechanical and biological objects. The apparatus we use is a purposely developed workstation comprising macro- and micro-manipulators. The most innovative component of the workstation is a micro-gripper fabricated using LIGA technology and actuated by piezoelectric actuators. We describe the design, fabrication and performance of a few prototypes of LIGA micro-grippers. Results are presented which demonstrate the ability of the system to manipulate effectively both micro-mechanical and biological micro-objects.
An abundance of invariant polynomials satisfying the Riemann hypothesis
In 1999, Iwan Duursma defined the zeta function for a linear code as a generating function of its Hamming weight enumerator. It can also be defined for other homogeneous polynomials not corresponding to existing codes. If the homogeneous polynomial is invariant under the MacWilliams transform, then its zeta function satisfies a functional equation and we can formulate an analogue of the Riemann hypothesis. As far as existing codes are concerned, the Riemann hypothesis is believed to be closely related to the extremal property. In this article, we show there are abundant polynomials invariant by the MacWilliams transform which satisfy the Riemann hypothesis. The proof is carried out by explicit construction of such polynomials. To prove the Riemann hypothesis for a certain class of invariant polynomials, we establish an analogue of the Enestrom-Kakeya theorem.
Entity resolution with iterative blocking
Entity Resolution (ER) is the problem of identifying which records in a database refer to the same real-world entity. An exhaustive ER process involves computing the similarities between pairs of records, which can be very expensive for large datasets. Various blocking techniques can be used to enhance the performance of ER by dividing the records into blocks in multiple ways and only comparing records within the same block. However, most blocking techniques process blocks separately and do not exploit the results of other blocks. In this paper, we propose an  iterative blocking framework  where the ER results of blocks are reflected to subsequently processed blocks. Blocks are now iteratively processed until no block contains any more matching records. Compared to simple blocking, iterative blocking may achieve higher accuracy because reflecting the ER results of blocks to other blocks may generate additional record matches. Iterative blocking may also be more efficient because processing a block now saves the processing time for other blocks. We implement a scalable iterative blocking system and demonstrate that iterative blocking can be more accurate and efficient than blocking for large datasets.
Evaluating the accuracy of Java profilers
Performance analysts profile their programs to find methods that are worth optimizing: the "hot" methods. This paper shows that four commonly-used Java profilers ( xprof , hprof , jprofile, and yourkit ) often disagree on the identity of the hot methods. If two profilers disagree, at least one must be incorrect. Thus, there is a good chance that a profiler will mislead a performance analyst into wasting time optimizing a cold method with little or no performance improvement.   This paper uses causality analysis to evaluate profilers and to gain insight into the source of their incorrectness. It shows that these profilers all violate a fundamental requirement for sampling based profilers: to be correct, a sampling-based profilermust collect samples randomly.   We show that a proof-of-concept profiler, which collects samples randomly, does not suffer from the above problems. Specifically, we show, using a number of case studies, that our profiler correctly identifies methods that are important to optimize; in some cases other profilers report that these methods are cold and thus not worth optimizing.
Robust group-of-picture architecture for video transmission over error-prone channels
In motion-compensated video-coding schemes, such as MPEG, an I frame is normally followed by several P frames and possibly B frames in a group-of-picture (GOP). In error-prone environments, errors happening in the previous frames in a GOP may propagate to all the following frames until the next I frame, which is the beginning of the next GOP. In this paper, we propose a novel GOP structure for robust transmission of MPEG video bitstream. By selecting the optimal position of the I frame in a GOP, robustness can be achieved without reducing any coding efficiency. Experimental results demonstrate the robustness of the proposed GOP structure.
Useful computations need useful numbers
Most of us have taken the exact rational and approximate numbers in our computer algebra systems for granted for a long time, not thinking to ask if they could be significantly better. With exact rational arithmetic and adjustable-precision floating-point arithmetic to precision limited only by the total computer memory or our patience, what more could we want for such numbers? It turns out that there is much more that can be done that permits us to obtain exact results more often, more intelligible results, approximate results guaranteed to have requested error bounds, and recovery of exact results from approximate ones.
Fast Content Aware Image Retargeting
This paper addresses the problem of retargeting, namely adapting large source images for effective viewing at a smaller size with possible applications to PDAs, or dynamic page layouts. Instead of extracting regions of interest for retargeting, the uninteresting parts are removed from the scene in Shai Avidan and Shamir, A. (2007). This is done by computing the RGB variance within non-overlapping 3times3 blocks and removing the block path with minimal variance cost using dynamic programming. It is shown that transformation to CIELAB space is more effective for visual interpretation of image content. The implementations are shown to be much faster than the seam carving approach of Shai Avidan and Shamir, A. (2007). Schemes are also presented for speeding up the seam carving scheme itself.
SCADDAR: an efficient randomized technique to reorganize continuous media blocks
Scalable storage architectures allow for the addition of disks to increase storage capacity and/or bandwidth. In its general form, disk scaling also refers to disk removals when either capacity needs to be conserved or old disk drives are retired. Assuming random placement of blocks on multiple nodes of a continuous media server, our optimization objective is to redistribute a minimum number of media blocks after disk scaling. This objective should be met under two restrictions. First, uniform distribution and hence a balanced load should be ensured after redistribution. Second, the redistributed blocks should be retrieved at the normal mode of operation in one disk access and through low complexity computation. We propose a technique that meets the objective, while we prove that it also satisfies both restrictions. The SCADDAR approach is based on using a series of REMAP functions which can derive the location of a new block using only its original location as a basis.
Importance sampling in Markovian settings
Rare event simulation for stochastic models of complex systems is still a great challenge even for Markovian models. We review results in importance sampling for Markov chains, provide new viewpoints and insights, and we pose some future research directions.
Identification of cold-induced genes in cereal crops and arabidopsis through comparative analysis of multiple EST sets
Freezing tolerance in plants is obtained during a period of low nonfreezing temperatures before the winter sets on, through a biological process known as cold acclimation. Cold is one of the major stress factors that limits the growth, productivity and distribution of plants, and understanding the mechanism of cold tolerance is therefore important for crop improvement. Expressed sequence tags (EST) analysis is a powerful, economical and time-efficient way of assembling information on the transcriptome. To date, several EST sets have been generated from cold-induced cDNA libraries from several different plant species. In this study we utilize the variation in the frequency of ESTs sampled from different cold-stressed plant libraries, in order to identify genes preferentially expressed in cold in comparison to a number of control sets. The species included in the comparative study are oat (Avena sativa), barley (Hordeum vulgare), wheat (Triticum aestivum), rice (Oryza sativa) and Arabidopsis thaliana. However, in order to get comparable gene expression estimates across multiple species and data sets, we choose to compare the expression of tentative ortholog groups (TOGs) instead of single genes, as in the normal procedure. We consider TOGs as preferentially expressed if they are detected as differentially expressed by a test statistic and up-regulated in comparison to all control sets, and/or uniquely expressed during cold stress, i.e., not present in any of the control sets. The result of this analysis revealed a diverse representation of genes in the different species. In addition, the derived TOGs mainly represent genes that are long-term highly or moderately expressed in response to cold and/or other stresses.
Real-time distributed computing
This position paper concerns itself with real-time safety critical distributed systems. It presents a computational model that is appropriate for this type of application and architecture. It then defines a resource allocations scheme based upon fixed priority scheduling. Such a scheme has the advantage (over purely static schedules) of supporting greater levels of flexibility and non-determinism, whilst still providing static guarantees of necessary timing behaviour (i.e. end-to-end deadlines through the systems). Priority based communication protocols are investigated, with possible future techniques reviewed.
On the impact of using volume as an independent variable for the solution of P-T fluid-phase equilibrium with equations of state
a b s t r a c t The constant pressure–temperature (P–T) flash plays an important role in the modelling of fluid-phase behaviour, and its solution is especially challenging for equations of state in which the volume is expressed as an implicit function of the pressure. We explore the relative merits of solving the P–T flash in two ensembles: mole numbers, pressure and temperature, in which each free-energy evaluation requires the use of a numerical solver; and mole numbers, volume and temperature, in which a direct evaluation of the free-energy is possible. We examine the performance of two algorithms, HELD (Helmholtz free energy Lagrangian dual), introduced in Pereira et al. (2012), and GILD (Gibbs free energy Lagrangian dual), introduced here, for the fluid-phase equilibria of 8 mixtures comprising up to 10 components, using two equations of state. While the reliability of both algorithms is comparable, the computational cost of HELD is consistently lower; this difference becomes increasingly pronounced as the number of components is increased. © 2014 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license
Semantic Processing of Natural Language Queries in the OntoNL Framework
The OntoNL Framework provides an architecture and re-usable components for automating as much as possible the building of natural language interfaces to information systems. In addition to the syntactic analysis components, OntoNL has semantic analysis components which exploit domain ontologies to provide better disambiguation of the user input. We present in this paper the algorithms used for semantic processing of the natural language queries, as well as an ontology-driven semantic relatedness measure developed for this purpose. We also present extensive evaluation results with different ontologies using human subjects.
A Comparison of Linear Keyword and Restricted Natural Language Data Base Interfaces for Novice Users
This study compares a linear keyword language interface and a restricted natural language interface for data retrieval by a novice user. The comparison focuses on the effect of different data base interfaces on user performance as measured by query correctness and query writing time in a query writing task across varying query types and training levels. To accomplish this objective, a laboratory experiment was conducted using a split-plot factorial design using two between-subjects factors and one within-subjects factor. The results indicate that the restricted natural language subjects performed significantly better than the linear keyword language subjects in terms of both query correctness and query writing time.
Reactive Virtual Position-Based Routing in Wireless Sensor Networks
Virtual position-based routing protocols have many attractive characteristics for wireless sensor networks. Typically, such protocols use a proactive scheme for updating routing tables. Because sensor networks can have very low data rate, sending periodic beacons to update routing tables can be very expensive. Instead, reactive approaches might be more appropriate in such scenarios. MANET-inspired reactive routing protocols do not scale well because of the effort in the order of O(n) for each routing information update. In this paper, we present Reactive Virtual Cord Protocol (RVCP), a data-centric reactive virtual position based routing protocol for use in sensor networks. Route discovery is directed towards the destination and hence there is no need to flood the entire network to discover a route. Our approach is based on Virtual Cord Protocol (VCP), an efficient, virtual relative position based routing protocol that also provides support for data management as known from typical Distributed Hash Table (DHT) services. To minimize the end-to-end delay and energy consumption, we used adaptive techniques for the development of RVCP.
A note on the complexity of scheduling coupled tasks on a single processor
This paper considers a problem of coupled task scheduling on one processor, where all processing times are equal to 1, the gap has exact length h, precedence constraints are strict and the criterion is to minimise the schedule length. This problem is introduced e.g. in systems controlling radar operations. We show that the general problem is NP-hard.
Novel Weighting-Delay-Based Stability Criteria for Recurrent Neural Networks With Time-Varying Delay
In this paper, a weighting-delay-based method is developed for the study of the stability problem of a class of recurrent neural networks (RNNs) with time-varying delay. Different from previous results, the delay interval [0, d(t)] is divided into some variable subintervals by employing weighting delays. Thus, new delay-dependent stability criteria for RNNs with time-varying delay are derived by applying this weighting-delay method, which are less conservative than previous results. The proposed stability criteria depend on the positions of weighting delays in the interval [0, d(t)], which can be denoted by the weighting-delay parameters. Different weighting-delay parameters lead to different stability margins for a given system. Thus, a solution based on optimization methods is further given to calculate the optimal weighting-delay parameters. Several examples are provided to verify the effectiveness of the proposed criteria.
Cross-Language Information Retrieval
Search for information is no longer exclusively limited within the native language of the user, but is more and more extended to other languages. This gives rise to the problem of cross-language information retrieval (CLIR), whose goal is to find relevant information written in a different language to a query. In addition to the problems of monolingual information retrieval (IR), translation is the key problem in CLIR: one should translate either the query or the documents from a language to another. However, this translation problem is not identical to full-text machine translation (MT): the goal is not to produce a human-readable translation, but a translation suitable for finding relevant documents. Specific translation methods are thus required. The goal of this book is to provide a comprehensive description of the specifi c problems arising in CLIR, the solutions proposed in this area, as well as the remaining problems. The book starts with a general description of the monolingual IR and CLIR problems. Different classes of approaches to translation are then presented: approaches using an MT system, dictionary-based translation and approaches based on parallel and comparable corpora. In addition, the typical retrieval effectiveness using different approaches is compared. It will be shown that translation approaches specifically designed for CLIR can rival and outperform high-quality MT systems. Finally, the book offers a look into the future that draws a strong parallel between query expansion in monolingual IR and query translation in CLIR, suggesting that many approaches developed in monolingual IR can be adapted to CLIR. The book can be used as an introduction to CLIR. Advanced readers can also find more technical details and discussions about the remaining research challenges in the future. It is suitable to new researchers who intend to carry out research on CLIR.
Best case energy analysis of localized euclidean minimum spanning tree based multicasting in ad hoc and sensor networks
I consider the known localized multicast protocol MSTEAM and derive the energy consumed by the multicast tree constructed by this protocol in the best case. Moreover, I show that the length of multicast links connecting into a multicast branch can not be bounded from above. For typical wireless networks where links have a limited communication range, however, I can show that asymptotically the relation between the derived best case energy consumption of MSTEAM and a known lower bound on multicast energy consumption is limited by a factor of 2.
DQAINF: an algorithm for automatic integration of infinite oscillating tails
We describe an automatic quadrature routine which is specifically designed for real functions having a certain type of infinite oscillating tails. The algorithm is designed to integrate a vector function over an infinite interval. A FORTRAN implementation of the algorithm is included.
An Interoperability Framework for Pan-European E-Government Services (PEGS)
Interoperability between public administrations receives nowadays a lot of attention. Also in the European Union interworking is high on the priority list, but the challenges to achieve the European administrative space is enormous. Many research projects are undertaken, especially in the domain of semantic interoperability. Many of these efforts seem to start from a technical solution rather than from an actual business problem. By taking a narrow view on the problem space, they only promise limited support for the many challenges in the domain of interoperability and innovation of e-government services. In this paper we present a business driven approach that looks promising in enabling entire classes of interoperability solutions
Performance optimization of interference-limited multihop networks
The performance of a multihop wireless network is typically affected by the interference caused by transmissions in the same network. In a statistical fading environment, the interference effects become harder to predict. Information sources in a multihop wireless network can improve throughput and delay performance of data streams by implementing interference-aware packet injection mechanisms. Forcing packets to wait at the head of queues and coordinating packet injections among different sources enable effective control of copacket interference. In this paper, throughput and delay performance in interference-limited multihop networks is analyzed. Using nonlinear probabilistic hopping models, waiting times which jointly optimize throughput and delay performances are derived. Optimal coordinated injection strategies are also investigated as functions of the number of information sources and their separations. The resulting analysis demonstrates the interaction of performance constraints and achievable capacity in a wireless multihop network.
Action Reaction Learning: Automatic Visual Analysis and Synthesis of Interactive Behaviour
We propose Action-Reaction Learning as an approach for analyzing and synthesizing human behaviour. This paradigm uncovers causal mappings between past and future events or between an action and its reaction by observing time sequences. We apply this method to analyze human interaction and to subsequently synthesize human behaviour. Using a time series of perceptual measurements, a system automatically discovers correlations between past gestures from one human participant (action) and a subsequent gesture (reaction) from another participant. A probabilistic model is trained from data of the human interaction using a novel estimation technique, Conditional Expectation Maximization (CEM). The estimation uses general bounding and maximization to monotonically find the maximum conditional likelihood solution. The learning system drives a graphical interactive character which probabilistically predicts a likely response to a user's behaviour and performs it interactively. Thus, after analyzing human interaction in a pair of participants, the system is able to replace one of them and interact with a single remaining user.
Circuit Delay Models and Their Exact Computation Using Timed Boolean Functions
We propose a general circuit delay model that unifies all previous delay models, e.g. floating, viability, and transition delays, and models introduced in this paper, e.g. delays by sequences of vectors and minimum delays. Then, we formulate the computation of the exact circuit delays, under both bounded and unbounded gate delay models, as a mixed Boolean linear programming using a new formulation technique, called Timed Boolean Function. Next, we compute the exact delays of combinational circuits for transition delay and delay by sequences of vectors. We show that delays by sequences of vectors and floating (or viability) delays are invariant under both bounded and unbounded gate delay models. Finally, we address the effect of gate delay lower bounds on delays of circuits. We demonstrate the effectiveness of the method by giving exact delay results for all ISCAS benchmark circuits (except C6188).
An anisotropic evolution formulation applied in 2-D unwrapping of discontinuous phase surfaces
In this paper, a new method to reconstruct piecewise continuous phase estimates using inphase and quadrature components acquired from interferometry measurements is derived and discussed. The method, based on the concept of anisotropic evolution formulations, is shown to be far less noise sensitive than similar methods operating on modulo-mapped data (i.e., traditional phase unwrapping methods). The method is able to produce reliable phase estimates from data containing complex sheared structures in combination with high noise content without relying on user-defined weights.
A score function of splitting band for two-band speech model
Two-band speech model which assumes lower band is a quasi-periodic component and upper band is a non-periodic component is widely used due to its natural and simple framework. In this paper, a score function is defined for splitting lower and upper band of two-band speech model and estimation method of band-splitting frequency which is the boundary of the two bands is proposed. The score function is calculated for each harmonic frequency using the normalized autocorrelation function of the time signal corresponding to the each sub-band divided by the given frequency. By using the score function, tracking technique is applied to the band-splitting frequency estimation procedure to reflect the continuity between neighboring frames. Experimental tests confirm that the proposed score function is effective for estimation of the band-splitting frequency and produces better results compared with the previous other methods.
The Reliability Study of the Single Hydraulic Prop Based on Finite Element Analysis
In allusion to the reliability, which exists in the parametric design and optimizing process of the single hydraulic prop, this paper presents the new method in comparison with the traditional calculating and checking method for the reliability. The geometry model of the hydraulic prop is built firstly based on the 3D software, then analyzed and optimized by the finite element software-ANSYS. Results show that the method presented for the reliability is efficient and accurate.
Fuzzy Capacitated Location-allocation Problem with Minimum Risk Criteria
Based on credibility theory, a new class of two-stage minimum risk location-allocation model is first proposed. Then we deal with the approximation of the location and allocation problem after that, a hybrid algorithm, which integrates the approximation approach, neural network and simulated annealing, is designed to solve the proposed location-allocation problem, and a numerical example is provided to test the effectiveness of the hybrid algorithm
Towards End User Service Composition
The popularity of service oriented computing (SOC) brings a large number of distributed, well-encapsulated and reusable services all over Internet, and makes it possible to create value-added services by means of service composition. Current composition styles are too professional to those end users when building their own applications. Actually, the end user would prefer rapidly discovering the best-of-breed services to assemble as well as visually personalizing the presentation to enjoy rich experiences. We propose an end user service composition approach for reducing the composition complexity and difficulty from the end user perspective. In our approach, similar candidate services are aggregated together as a unified resource, whose wide QoS spectrum can be easily manipulated by the end users to satisfy their requirements. Then they can personalize the services and, the composition occurs only at the presentation layer. The main contributions of the approach are: (i) enabling the end users to personalize the composite application with more powerful presentation; (ii) supporting the end users to dynamically customize the service composition in terms of QoS; (iii) alleviating the end users from the time-consuming task of selecting service to compose.
Optimal detection of functional connectivity from high-dimensional EEG synchrony data.
article i nfo Computing phase-locking values between EEG signals is a popular method for quantifying functional connectivity. However, this method involves large-scale, high-resolution datasets, which impose a serious multiple testing problem. Standard multiple testing methods fail to exploit the information from the complex dependence structure that varies across hypotheses in spectral, temporal, and spatial dimensions and result in a severe loss of power. They tend to control the false positives at the cost of hiding true positives. We introduce a new approach, called optimal discovery procedure (ODP) for identifying synchrony that is statistically significant. ODP maximizes the number of true positives for a given number of false positives, and thus offers a theoretical optimum for detecting significant synchrony in a multiple testing situation. We demonstrate the utility of this method with PLV data obtained from a visual search study. We also present simulation analysis to confirm the validity and relevance of using ODP in comparison with the standard FDR method for given configurations of true synchrony. We also compare the effectiveness of ODP with our previously published investigation of hierarchical FDR method (Singh and Phillips, 2010).
starBase v2.0: decoding miRNA-ceRNA, miRNA-ncRNA and protein–RNA interaction networks from large-scale CLIP-Seq data
Although microRNAs (miRNAs), other non-coding RNAs (ncRNAs) (e.g. lncRNAs, pseudogenes and circRNAs) and competing endogenous RNAs (ceRNAs) have been implicated in cell-fate determination and in various human diseases, surprisingly little is known about the regulatory interaction networks among the multiple classes of RNAs. In this study, we developed starBase v2.0 (http://starbase.sysu. edu.cn/) to systematically identify the RNA–RNA and protein–RNA interaction networks from 108 CLIP-Seq (PAR-CLIP, HITS-CLIP, iCLIP, CLASH) data sets generated by 37 independent studies. By analyzing millions of RNA-binding protein binding sites, we identified � 9000 miRNA-circRNA, 16 000 miRNApseudogene and 285 000 protein–RNA regulatory relationships. Moreover, starBase v2.0 has been updated to provide the most comprehensive CLIP-Seq experimentally supported miRNA-mRNA and miRNAlncRNA interaction networks to date. We identified � 10 000 ceRNA pairs from CLIP-supported miRNA target sites. By combining 13 functional genomic annotations, we developed miRFunction and ceRNAFunction web servers to predict the function of miRNAs and other ncRNAs from the miRNAmediated regulatory networks. Finally, we developed interactive web implementations to provide visualization, analysis and downloading of the aforementioned large-scale data sets. This study will greatly expand our understanding of ncRNA functions and their coordinated regulatory networks.
A Low Phase Noise 100MHz Silicon BAW Reference Oscillator
The paper presents a temperature compensated 100MHz reference oscillator based on a capacitive silicon Bulk Acoustic Wave (BAW) resonator interfaced with a CMOS amplifier. The resonator is optimized for high quality factor (92000) and low impedance. The CMOS IC comprises of a trans-impedance amplifier to sustain oscillations and an oven control mechanism for temperature control. A phase noise floor of ?136dBc/Hz was measured for the oscillator and the temperature drift of frequency was measured to be 56ppm over 100°C.
Performance Bounds for MLSD Reception of OFDM Signals in Fast Fading
For OFDM systems in fast fading, it is difficult to obtain a closed form expression for the OFDM symbol error probability. Thus, tight analytical bounds on actual performance are extremely useful for performance prediction and verification. Additionally, the structure of the bound provides insight into system behavior. A new expurgated 2-dimensional union bound is proposed and applied to an OFDM system in fast fading. This bound becomes extremely tight as the rate of fading L increases. Performance comparison of the new bound to a lower bound on a known expurgated union bound demonstrates a gain of up to 1.5 dB at P(e) = 10 2 for channel implicit diversity order of L = 4. A new simulated upper bound that uses a reduced state vector in a modified trellis search algorithm and a simulated lower bound that assumes limited knowledge of the ICI are also presented. Both bounds are derived from a "time-varying" finite state machine model of the received signal. Performance results for these bounds are extremely tight for small to large values of N, where N is the number of OFDM signal tones. Also, the reduction in computational complexity achieved for N = 512 is from 2 512  to 2 6  for both bounds.
Creating GIS-based spatial interaction models for retail centres in Jeddah City
Spatial interaction models are used today in facilities planning research for predicting and for allocating flows of demand between origin and destination areas based on the attractiveness of each facility and based on the distance between facilities and demand areas. These models have been adapted to a wide range of application areas including predicting flows of people to shops, offices, schools, and hospitals. The aim of this paper is to use GIS for producing spatial interaction models for two retail centres Jeddah City, Saudi Arabia. These models are created using ArcGIS software and using the interaction function which is available within the network analysis module. To produce these models, detailed geo-database was created that covers location of retail centres, the capacity of each centre, the size of centres demand at the study area, and road network coverage for Jeddah City. The created models can be used by city planners for identifying areas of the city that are poorly served by existing retail centres. In addition, these models can be used to define the impacts of expanding retail supply and or retail demand at the study area.
Surface models of tube trees
This paper describes a new method for generating surfaces of branching tubular structures with given center-lines and radii. As the centerlines are not straight lines, the cross-sections are not parallel and well-known algorithms for surface tiling from parallel cross-sections cannot be used. Nonparallel cross-sections can be tiled by means of the maximal-disc interpolation method; special methods for branching-structures modeling by means of convolution surfaces produce excellent results, but these methods are more complex than our approach. The proposed method tiles nonparallel circular cross-sections and constructs a topologically-correct surface mesh. The method is not artifact-free, but it is fast and simple. The surface mesh serves as a data representation of a vessel tree suitable for real-time virtual reality operation planning and operation support within a medical application. Proposed method extracts a "classical" polygonal representation, which can be used in common surface-oriented graphic accelerators
Node-to-Set Disjoint-path Routing in Metacube
The metacube interconnection network introduced a few years ago has some very interesting properties: it has a short diameter similar to the hypercube, and its degree is much lower than that of a hypercube of the same size. In this paper, we describe an efficient algorithm for finding disjoint paths between one source node and at most m+k target nodes in a metacube MC(k, m) excluding MC(*,1), MC(2,2), MC(3,2) and MC(3,3). We show that we can find m+k disjoint paths between the source node and the m+k targets of length at most metacube diameter plus (k+4) with time complexity of order of metacube degree times its diameter.
Systematic error in the organization of physical action
Current views of the control of complex, purposeful movements acknowledge that organizational processes must reconcile multiple concerns. The central priority is of course accomplishing the actor’s goal. But in specifying the manner in which this occurs, the action plan must accommodate such factors as the interaction of mechanical forces associated with the motion of a multilinked system (classical mechanics) and, in many cases, intrinsic bias toward preferred movement patterns, characterized by so-called “coordination dynamics.” The most familiar example of the latter is the symmetry constraint, where spatial trajectories and/or temporal landmarks (e.g., reversal points) of concurrentlymoving body segments (limbs, digits, etc.) exhibit mutual attraction. The natural coordination tendencies that emerge through these constraints can facilitate or hinder motor control, depending on the degree of congruency with the desired movement pattern. Motor control theorists have long recognized the role of classical mechanics in theories of movement organization, but an appreciation of the importance of intrinsic interlimb bias has been gained only recently. Although detailed descriptions of temporal coordination dynamics have been provided, systematic attempts to identify additional salient dimensions of interlimb constraint have been lacking. We develop and implement here a novel method for examining this problem by exploiting two robust principles of psychomotor behavior, the symmetry constraint and the Two-Thirds Power Law. Empirical evidence is provided that the relative spatial patterns of concurrently moving limbs are naturally constrained in much the same manner as previously identified temporal constraints and, further, that apparent velocity interference is an indirect, secondary consequence of primary spatial assimilation. The theoretical implications of spatial interference are elaborated with respect to movement organization and motor learning. The need to carefully consider the appropriate dimensions with which to characterize coordination dynamics is also discussed. © 2001 Cognitive Science
Modeling the performance of evolutionary algorithms on the root identification problem: A case study with pbil and chc algorithms
The availability of a model to measure the performance of evolutionary algorithms is very important, especially when these algorithms are applied to solve problems with high computational requirements. That model would compute an index of the quality of the solution reached by the algorithm as a function of run-time. Conversely, if we fix an index of quality for the solution, the model would give the number of iterations to be expected. In this work, we develop a statistical model to describe the performance of PBIL and CHC evolutionary algorithms applied to solve the root identification problem. This problem is basic in constraint-based, geometric parametric modeling, as an instance of general constraint-satisfaction problems. The performance model is empirically validated over a benchmark with very large search spaces.
Towards a Probabilistic Calculus for Mobile Ad Hoc Networks
In this paper we present a probabilistic calculus for formally modeling and reasoning about Mobile Ad Hoc Networks (MANETs) with unreliable connections and mobility of nodes. In our calculus, a MANET node can locally broadcast messages to a group of nodes within its physical transmission range. The group probability is also introduced since two distinct nodes within different groups should receive messages from the same sender with different possibilities. Our calculus naturally captures essential features of MANETs, i.e., local broadcast, mobility and probability. Moreover, we give a formal operational semantics of the calculus in terms of the labeled transition system and define the notion of open bisimulation. Finally, we illustrate our calculus with a toy example.
Slow motion replay of video sequences using fractal zooming
Slow motion replay is a special effect used in the video entertainment field. It consists in a presentation of a video scene at a rate display lower than the original. Already consolidated as a commercial feature of analog video players, today slow motion is likely to be extended to the digital environment. Purpose of this paper is to present a technique combining fractals (I. F. S.) and wavelets to obtain a subjectively pleasant zoom and slow motion of digital video sequences. Active scene detection and post processing techniques are used to reduce computational cost and improve visual quality respectively. This study shows that the proposed technique produces better results than the state of the art techniques based either on data replication or classical interpolation.
Joint Semiblind Frequency Offset and Channel Estimation for Multiuser MIMO–OFDM Uplink
A semiblind method is proposed for simultaneously estimating the carrier frequency offsets (CFOs) and channels of an uplink multiuser multiple-input multiple-output orthogonal frequency-division multiplexing (MIMO-OFDM) system. By incorporating the CFOs into the transmitted symbols and channels, the MIMO-OFDM with CFO is remodeled into an MIMO-OFDM without CFO. The known blind method for channel estimation (Zeng and Ng in 2004) (Y. H. Zeng and T. S. Ng, ldquoA semi-blind channel estimation method for multi-user multi-antenna OFDM systems,rdquo IEEE Trans. Signal Process., vol. 52, no. 5, pp. 1419-1429, May 2004.) is then directly used for the remodeled system to obtain the shaped channels with an ambiguity matrix. A pilot OFDM block for each user is then exploited to resolve the CFOs and the ambiguity matrix. Two dedicated pilot designs, periodical and consecutive pilots, are discussed. Based on each pilot design and the estimated shaped channels, two methods are proposed to estimate the CFOs. As a result, based on the second-order statistics (SOS) of the received signal and one pilot OFDM block, the CFOs and channels are found simultaneously. Finally, a fast equalization method is given to recover the signals corrupted by the CFOs.
The cluster density of a distributed clustering algorithm in ad hoc networks
Given is a wireless multihop network whose nodes are randomly distributed according to a homogeneous Poisson point process of density /spl rho/ (in nodes per unit area). The network employs Basagni's distributed mobility-adaptive clustering (DMAC) algorithm to achieve a self-organizing network structure. We show that the cluster density, i.e., the expected number of cluster- heads per unit area, is /spl rho//sub c/= /spl rho//spl divide/(1+/spl mu//spl divide/2), where /spl mu/ denotes the expected number of neighbors of a node. Consequently, a clusterhead is expected to incorporate half of its neighboring nodes into its cluster. This result also holds in a scenario with mobile nodes and serves as a bound for inhomogeneous spatial node distributions.
Fault-tolerant ring embedding in faulty arrangement graphs
The arrangement graph A/sub n,k/, which is a generalization of the star graph (n-t=1), presents more flexibility than the star graph in adjusting the major design parameters: number of nodes, degree, and diameter. Previously the arrangement graph has proven hamiltonian. In this paper we further show that the arrangement graph remains hamiltonian even if it is faulty. Let |F/sub e/| and |F/sub v/| denote the numbers of edge faults and vertex faults, respectively. We show that A/sub n,k/ is hamiltonian when (1) (k=2 and n-k/spl ges/4, or k/spl ges/3 and n-k/spl ges/4+[k/2]), and |F/sub e/|/spl les/k(n-k-2)-1, or (2) k/spl ges/2, n-k/spl ges/2+[k/2], and |F/sub e/|/spl les/k(n-k-3)-1, or (3) k/spl ges/2, n-k/spl ges/3, and |F/sub 3/|/spl les/k.
The Local Structure of a Bipartite Distance-regular Graph
In this paper, we consider a bipartite distance-regular graph ?= (X, E) with diameter d? 3. We investigate the local structure of? , focusing on those vertices with distance at most 2 from a given vertex x. To do this, we consider a subalgebra R=R(x) ofMat0307a0x.gif X(C), where 0307a1x.gifX denotes the set of vertices in X at distance 2 from x. R is generated by matrices A, 0307a2x.gif J, and 0307a3x.gif D defined as follows. For all y, z? 0307a4x.gif X, the (y,z )-entry of A is 1 if y, z are at distance 2, and 0 otherwise. The (y, z)-entry of 0307a5x.gif J equals 1, and the (y,z )-entry of 0307a6x.gif D equals the number of vertices of X adjacent to each ofx , y, and z. We show that R is commutative and semisimple, with dimension at least 2. We assume thatdimR is one of 2, 3, or 4, and explore the combinatorial implications of this. We are motivated by the fact that if ? has a Q-polynomial structure, thendimR? 4.
Prerequesites for symbiotic brain-machine interfaces
Recent advancements in the neuroscience and engineering of Brain-Machine Interfaces are providing a blueprint for how new co-adaptive designs based on reinforcement learning change the nature of a user's ability to accomplish tasks that were not possible using static methodologies. By designing adaptive controls and artificial intelligence into the neural interface, computers can become active assistants in goal-directed behavior and further enhance human performance. This paper presents a set of minimal prerequisites that enable a cooperative symbiosis and dialogue between biological and artificial systems.
Computation and analysis of natural compliance in fixturing and grasping arrangements
This paper computes and analyzes the natural compliance of fixturing and grasping arrangements. Traditionally, linear-spring contact models have been used to determine the natural compliance of multiple contact arrangements. However, these models are not supported by experiments or elasticity theory. We derive a closed-form formula for the stiffness matrix of multiple contact arrangements that admits a variety of nonlinear contact models, including the well-justified Hertz model. The stiffness matrix formula depends on the geometrical and material properties of the contacting bodies and on the initial loading at the contacts. We use the formula to analyze the relative influence of first- and second-order geometrical effects on the stability of multiple contact arrangements. Second-order effects, i.e., curvature effects, are often practically beneficial and sometimes lead to significant grasp stabilization. However, in some contact arrangements, curvature has a dominant destabilizing influence. Such contact arrangements are deemed stable under an all-rigid body model but, in fact, are unstable when the natural compliance of the contacting bodies is taken into account. We also consider the combined influence of curvature and contact preloading on stability. Contrary to conventional wisdom, under certain curvature conditions, higher preloading can increase rather than decrease grasp stability. Finally, we use the stiffness matrix formula to investigate the impact of different choices of contact model on the assessment of the stability of multiple contact arrangements. While the linear-spring model and the more realistic Hertz model usually lead to the same stability conclusions, in some cases, the two models lead to different stability results.
Adapting Information Theoretic Clustering to Binary Images
We consider the problem of finding points of interest along local curves of binary images. Information theoretic vector quantization is a clustering algorithm that shifts cluster centers towards the modes of principal curves of a data set. Its runtime characteristics, however, do not allow for efficient processing of many data points. In this paper, we show how to solve this problem when dealing with data on a 2D lattice. Borrowing concepts from signal processing, we adapt information theoretic clustering to the quantization of binary images and gain significant speedup.
Construction of robust prognostic predictors by using projective adaptive resonance theory as a gene filtering method
Motivation: For establishing prognostic predictors of various diseases using DNA microarray analysis technology, it is desired to find selectively significant genes for constructing the prognostic model and it is also necessary to eliminate non-specific genes or genes with error before constructing the model.#R##N##R##N#Results: We applied projective adaptive resonance theory (PART) to gene screening for DNA microarray data. Genes selected by PART were subjected to our FNN-SWEEP modeling method for the construction of a cancer class prediction model. The model performance was evaluated through comparison with a conventional screening signal-to-noise (S2N) method or nearest shrunken centroids (NSC) method. The FNN-SWEEP predictor with PART screening could discriminate classes of acute leukemia in blinded data with 97.1% accuracy and classes of lung cancer with 90.0% accuracy, while the predictor with S2N was only 85.3 and 70.0% or the predictor with NSC was 88.2 and 90.0%, respectively. The results have proven that PART was superior for gene screening.#R##N##R##N#Availability: The software is available upon request from the authors.#R##N##R##N#Contact: honda@nubio.nagoya-u.ac.jp
The relationship between minimum entropy control and risk-sensitive control for time-varying systems
The connection between minimum entropy control and risk-sensitive control for linear time-varying systems is investigated. For time-invariant systems, the entropy functional and the linear exponential quadratic Gaussian cost are the same. In this paper, it is shown that this is not true for general time varying systems. It does hold, however, when the system admits a state-space representation.
A Novel Hybrid Parallel-Prefix Adder Architecture With Efficient Timing-Area Characteristic
Two-operand binary addition is the most widely used arithmetic operation in modern datapath designs. To improve the efficiency of this operation, it is desirable to use an adder with good performance and area tradeoff characteristics. This paper presents an efficient carry-lookahead adder architecture based on the parallel-prefix computation graph. In our proposed method, we define the notion of triple-carry-operator, which computes the generate and propagate signals for a merged block which combines three adjacent blocks. We use this in conjunction with the classic approach of the carry-operator to compute the generate and propagate signals for a merged block combining two adjacent blocks. The timing-driven nature of the proposed design reduces the depth of the adder. In addition, we use a ripple-carry type of structure in the nontiming critical portion of the parallel-prefix computation network. These techniques help produce a good timing-area tradeoff characteristic. The experimental results indicate that our proposed adder is significantly faster than the popular Brent-Kung adder with some area overhead. On the adder hand, the proposed adder also shows marginally faster performance than the fast Kogge-Stone adder with significant area savings.
Control of Doubly-Fed Induction Generator System Using PIDNNs
An intelligent control stand-alone doubly-fed induction generator (DFIG) system using proportional-integral-derivative neural network (PIDNN) is proposed in this study. This system can be applied as a stand-alone power supply system or as the emergency power system when the electricity grid fails for all sub-synchronous, synchronous and super-synchronous conditions. The rotor side converter is controlled using the field-oriented control to produce three-phase stator voltages with constant magnitude and frequency at different rotor speeds. Moreover, the stator side converter, which is also controlled using field-oriented control, is primarily implemented to maintain the magnitude of the DC-link voltage. Furthermore, the intelligent PIDNN controller is proposed for both the rotor and stator side converters to improve the transient and steady-state responses of the DFIG system for different operating conditions. Both the network structure and on-line learning algorithm are introduced in detail. Finally, the feasibility of the proposed control scheme is verified through experimentation.
Password-based tripartite key exchange protocol with forward secrecy
A tripartite authenticated key agreement protocol is designed for three entities to communicate securely over an open network particularly with a shared key. Password-authenticated key exchange (PAKE) allows the participants to share a session key using a human memorable password only. In this paper, A password-based authenticated tripartite key exchange protocol(3-PAKE) is presented in the standard model. The security of the protocol is reduced to theDecisional Bilinear Diffie-Hellman (DBDH) problem, and the protocol provides not only the properties of forward secrecy, but also resistance against known key attacks. The proposed protocol is more efficient than the similar protocols in terms of both communication and computation.
Evaluation of high-altitude balloons as a learning technology
The utility of high-altitude balloons as a learning technology to facilitate education in several disciplines is considered in this paper. The role that a high-altitude balloon can play as a learning technology is discussed and its utility in this role is considered. The need for a formal design framework for high-altitude ballooning is also discussed. A framework for the assessment of high-altitude ballooning in supporting an undergraduate-level course is evaluated and an assessment using this framework is conducted. The paper concludes with a discussion of techniques that can be used to broaden access to high-altitude ballooning in education.
Automatic Discovery of Action Taxonomies from Multiple Views
We present a new method for segmenting actions into primitives and classifying them into a hierarchy of action classes. Our scheme learns action classes in an unsupervised manner using examples recorded by multiple cameras. Segmentation and clustering of action classes is based on a recently proposed motion descriptor which can be extracted efficiently from reconstructed volume sequences. Because our representation is independent of viewpoint, it results in segmentation and classification methods which are surprisingly efficient and robust. Our new method can be used as the first step in a semi-supervised action recognition system that will automatically break down training examples of people performing sequences of actions into primitive actions that can be discriminatingly classified and assembled into high-level recognizers.
Dr.VIS: a database of human disease-related viral integration sites
Viral integration plays an important role in the development of malignant diseases. Viruses differ in preferred integration site and flanking sequence. Viral integration sites (VIS) have been found next to oncogenes and common fragile sites. Understanding the typical DNA features near VIS is useful for the identification of potential oncogenes, prediction of malignant disease development and assessing the probability of malignant transformation in gene therapy. Therefore, we have built a database of human disease-related VIS (Dr.VIS, http://www.scbit.org/dbmi/drvis) to collect and maintain human disease-related VIS data, including characteristics of the malignant disease, chromosome region, genomic position and viral–host junction sequence. The current build of Dr.VIS covers about 600 natural VIS of 5 oncogenic viruses representing 11 diseases. Among them, about 200 VIS have viral–host junction sequence.
Research on Internet marketing relationship model
The present models about Internet marketing, to a certain extent, have some limits, which cannot systematically reveals the process and characteristics of Internet marketing. Based on the theories of traditional marketing and Internet marketing, the paper builds the model of Internet marketing relationship that describes consumer's purchase decision process and firm's Internet marketing process, and correlation among consumer, firm, and bank and logistics firm. Through systematically analyzing the two processes, the model reveals intrinsic characteristics and essences of Internet marketing that are all different from traditional marketing. The model provides a researchful platform for the researchers, and a fundamental basis for further researching Internet marketing.
Expanded rectangles: a new VLSI data structure
A data structure derived from corner stitching which allows efficient representation of VLSI layouts is presented. While each entry in the expanded rectangle database is larger than the corresponding corner-stitched entry, generally fewer entries are required to represent the same VLSI layout. The data structure has two important features: first, the VLSI design is represented as a slicing structure in which each slice contains a portion of the solid material; and second, corner stitches are used to provide two-dimensional nearness information. Initial measurements indicate that expanded rectangles is a viable data structure for use in a complete VLSI layout system. >
Predicting opponent actions by observation
In competitive domains, the knowledge about the opponent can give players a clear advantage. This idea lead us in the past to propose an approach to acquire models of opponents, based only on the observation of their input-output behavior. If opponent outputs could be accessed directly, a model can be constructed by feeding a machine learning method with traces of the opponent. However, that is not the case in the Robocup domain. To overcome this problem, in this paper we present a three phases approach to model low-level behavior of individual opponent agents. First, we build a classifier to label opponent actions based on observation. Second, our agent observes an opponent and labels its actions using the previous classifier. From these observations, a model is constructed to predict the opponent actions. Finally, the agent uses the model to anticipate opponent reactions. In this paper, we have presented a proof-of-principle of our approach, termed OMBO (Opponent Modeling Based on Observation), so that a striker agent can anticipate a goalie. Results show that scores are significantly higher using the acquired opponent's model of actions.
Web accessibility compliance of government web sites in Korea
This paper introduces Korean web accessibility activities, such as relational laws, ordinances, policies, guidelines. It also presents analytical result of the investigation on web-contents accessibilities of the 39 Korean government agencies. The result shows that only one agency provides web contents satisfying all the minimum requirements, while 97% of the agencies does not satisfy all the minimum requirements. Unfortunately, 6 agencies do not satisfy any.
Free-Viewpoint Video Sequences: a New Challenge for Objective Quality Metrics
Free-viewpoint television is expected to create a more natural and interactive viewing experience by providing the ability to interactively change the viewpoint to enjoy a 3D scene. To render new virtual viewpoints, free-viewpoint systems rely on view synthesis. However, it is known that most objective metrics fail at predicting perceived quality of synthesized views. Therefore, it is legitimate to question the reliability of commonly used objective metrics to assess the quality of free-viewpoint video (FVV) sequences. In this paper, we analyze the performance of several commonly used objective quality metrics on FVV sequences, which were synthesized from decompressed depth data, using subjective scores as ground truth. Statistical analyses showed that commonly used metrics were not reliable predictors of perceived image quality when different contents and distortions were considered. However, the correlation improved when considering individual conditions, which indicates that the artifacts produced by some view synthesis algorithms might not be correctly handled by current metrics.
Dealing with Hardware in Embedded Software: A General Framework Based on the Devil Language
Writing code that talks to hardware is a crucial part of any embedded project. Both productivity and quality are needed, but some flaws in the traditional development process make these requirements difficult to meet.  We have recently introduced a new approach of dealing with hardware, based on the Devil language. Devil allows to write a high-level, formal definition of the programming interface of a peripheral circuit. A compiler automatically checks the consistency of a Devil specification, from which it generates the low-level, hardware-operating code.  In our original framework, the generated code is dependent of the host architecture (CPU, buses, and bridges). Consequently, any variation in the hardware environment requires a specific tuning of the compiler. Considering the variability of embedded architectures, this is a serious drawback. In addition, this prevents from mixing different buses in the same circuit interface.  In this paper, we remove those limitations by improving our framework in two ways. (i) We propose a better isolation between the Devil compiler and the host architecture. (ii) We introduce Trident, a language extension aimed at mapping one or several buses to each peripheral circuit.
Uniprocessor Scheduling Under Precedence Constraints
In this paper we present a novel approach to the constrained scheduling problem, while addressing a more general class of constraints that arise from the timing requirements on real-time embedded controllers and from the implementation of mixed data-flow/event-driven real-time systems. We provide general necessary and sufficient conditions for scheduling under precedence constraints and derive sufficient conditions for two well-known scheduling policies. We define mathematical problems that provide optimum priority and deadline assignments, while ensuring both precedence constraints and systems schedulability.We show how these problems can be relaxed to corresponding ILP formulations leveraging on available solvers.
Adaptive Multi-Layer Traffic Engineering with Shared Risk Group Protection
In this paper we propose a new traffic engineering scheme to be used jointly with protection in multi-layer, grooming-capable, optical-beared networks. To make the working and protection paths of demands better adapt to changing traffic and network conditions we propose the adaptive multi-layer traffic engineering (AMLTE) scheme that "tailors" i.e., fragments and de-fragments wavelength paths in a fully automatic distributed way.
Endowing Spoken Language Dialogue Systems with Emotional Intelligence
While most dialogue systems restrict themselves to the adjustment of the propositional contents, our work concentrates on the generation of stylistic va- riations in order to improve the user's perception of the interaction. To accomplish this goal, our approach integrates a social theory of politeness with a cognitive theory of emotions. We propose a hierarchical selection process for politeness behaviors in order to enable the refinement of decisions in case additional context information becomes available.
A multiple subset sum formulation for feedback implosion suppression over satellite networks
In this paper, we present a feedback implosion suppression (FIS) algorithm that reduces the volume of feedback information transmitted through the network without relying on any collaboration between users, or on any infrastructure other than the satellite network. Next generation satellite systems that utilize the Ka frequency band are likely to rely on various fade mitigation techniques, in order to guarantee a service quality that is comparable to other broadband technologies. User feedback would be a valuable input for a number of such components, however, collecting periodic feedback from a large number of users would result in the well-known feedback implosion problem. Feedback implosion is identified as a major problem when a large number of users try to transmit their feedback messages through the network, holding up a significant portion of the uplink resources and clogging the shared uplink medium. In this paper, we look at a system where uplink channel access is organized in time-slots. The goal of the FIS algorithm is to reduce the number of uplink time-slots hold up for the purpose of feedback transmission. Our analysis show that the FIS algorithm effectively suppresses the feedback messages of 95% of all active users, but still achieves acceptable performance results when the ratio of available time-slots to number of users is equal to or higher than 5%
Computing subgraph probability of random geometric graphs with applications in quantitative analysis of ad hoc networks
Random geometric graphs (RGG) contain vertices whose points are uniformly distributed in a given plane and an edge between two distinct nodes exists when their distance is less than a given positive value. RGGs are appropriate for modeling ad hoc networks consisting of n mobile devices that are independently and uniformly distributed randomly in an area. To the best of our knowledge, this work presents the first paradigm to compute the subgraph probability of RGGs in a systematical way. In contrast to previous asymptotic bounds or approximation, which always assume that the number of nodes in the network tends to infinity, the closed-form formulas we derived herein are fairly accurate and of practical value. Moreover, computing exact subgraph probability in RGGs is shown to be a useful tool for counting the number of induced subgraphs, which explores fairly accurate quantitative property on topology of ad hoc networks.
Unequal Error Protection for Video Streaming Over Wireless LANs using Content-Aware Packet Retry Limit
In this paper, we propose a content-aware retry limit adaptation scheme for video streaming over IEEE 802.11 wireless LANs (WLANs). Video packets of different importance are unequally protected with different retry limits at the MAC layer. The loss impact of each packet is estimated to guide the selection of its retry limit. More retry numbers are allocated to packets of higher loss impact to achieve unequal error protection. Experimental results show that the proposed adaptation scheme can effectively mitigate the error propagation due to packet loss and assure the on-time arrival of packets for presentation, thereby improving video quality significantly.
Linearization of ancestral multichromosomal genomes.
Background#R##N#Recovering the structure of ancestral genomes can be formalized in terms of properties of binary matrices such as the Consecutive-Ones Property (C1P). The Linearization Problem asks to extract, from a given binary matrix, a maximum weight subset of rows that satisfies such a property. This problem is in general intractable, and in particular if the ancestral genome is expected to contain only linear chromosomes or a unique circular chromosome. In the present work, we consider a relaxation of this problem, which allows ancestral genomes that can contain several chromosomes, each either linear or circular.
OdinTools--Model-Driven Development of Intelligent Mobile Services
Today's computationally able mobile devices are capable of acting as service providers as opposed to their traditional role as consumers. To address the challenges associated with the development of these mobile services, we have developed Odin, a middleware which masks complexity, allowing rapid development of mobile services. Odin, however, does not allow cross-platform development, which is an important concern with today's wide variety of mobile devices. To solve this problem, we have designed Odin Tools - a model-driven toolkit for cross-platform development of mobile services. Leveraging appropriate metamodels, a prototype has been implemented in Eclipse and Marama that allows developers to model mobile services in a platform-independent manner. We are currently working on transformations between levels of the model hierarchy which will allow full Odin-based service implementations to be generated automatically.
A socio political model of the relationship between IT investments and business performance
In recent years many studies have been published on the assessment of payoffs from investments in IT. The research has produced mainly mixed results. Different explanations can be given for these mixed results. One possible explanation might be the dominance of the rational perspective in previous research. The consequence of this overemphasis on rationality is that the social political nature of the IT investment process has largely been neglected in previous research on IT business value. This omission produces an incomplete picture and might contribute to the conflicting empirical results. In this research we studied whether and how the socio political perspective can be used to explain the mixed results. Case study research was used to test whether the attitude towards the value of IT, destructive conflict, and a low level of trust influence the relationship between IT investments and business performance.
A new metrics set for evaluating testing efforts for object-oriented programs
Software metrics proposed and used for procedural paradigm have been found inadequate for object oriented software products, mainly because of the distinguishing features of the object oriented paradigm such as inheritance and polymorphism. Several object oriented software metrics have been described in the literature. These metrics are goal driven in the sense that they are targeted towards specific software qualities. We propose a new set of metrics for object oriented programs; this set is targeted towards estimating the testing efforts for these programs. The definitions of these metrics are based on the concepts of object orientation and hence are independent of the object oriented programming languages. The new metrics set has been critically compared with three other metrics sets published in the literature.
Towards a universal client for grid monitoring systems: design and implementation of the Ovid browser
In this paper, we present the design and implementation of Ovid, a browser for grid-related information. The key goal of Ovid is to support the seamless navigation of users in the grid information space. Key aspects of Ovid are: (i) a set of navigational primitives, which are designed to cope with problems such as network disorientation and information overloading; (ii) a small set of Ovid views, which present the end-user with high-level, visual abstractions of grid information; these abstractions correspond to simple models that capture essential aspects of a grid infrastructure; (iii) support for embedding and implementing hyperlinks that connect related entities represented within different information views; (iv) a plug-in mechanism, which enables the seamless integration with Ovid of third-party software that retrieves and displays data from various grid information sources; and (v) a modular software design, which allows the easy integration of different visualization algorithms that support the graphical representation of large amounts of grid-related information in the context of Ovid's views.
A trainable, single-pass algorithm for column segmentation
Column segmentation logically precedes OCR in the document analysis process. The trainable algorithm XYCUT relies on horizontal and vertical binary profiles to produce an XY-tree representing the column structure of a page of a technical document in a single pass through the bit image. Training against ground truth adjusts a single, resolution independent, parameter using only local information and guided by an edit distance function. The algorithm correctly segments the page image for a (fairly) wide range of parameter values, although small, local and repairable errors may be made, an effect measured by a repair cost function.
Hybrid Wordlength Optimization Methods of Pipelined FFT Processors
Quickly and accurately predicting the performance based on the requirements for IP-based system implementations optimizes the design and reduces the design time and overall cost. This study describes a novel hybrid method for the word-length optimization of pipelined FFT processors that is the arithmetic kernel of OFDM-based systems. This methodology utilizes the rapid computing of statistical analysis and the accurate evaluation of simulation-based analysis to investigate a speedy optimization flow. A statistical error model for varying word-lengths of PE stages of an FFT processor was developed to support this optimization flow. Experimental results designate that the word-length optimization employing the speedy flow reduces the percentage of the total area of the FFT processor that increases with an increasing FFT length. Finally, the proposed hybrid method requires a shorter prediction time than the absolute simulation-based method does and achieves more accurate outcomes than a statistical calculation does.
Generating organic textures with controlled anisotropy and directionality
This article presents a method for generating organic textures by tessellating a region into a set of pseudo-Voronoi polygons using a particle model and then generating the detailed geometry each of the polygons with fractal noise.
Guidelines for reporting an fMRI study
In this editorial, we outline a set of guidelines for the reporting of methods and results in functional magnetic resonance imaging studies and provide a checklist to assist authors in preparing manuscripts that meet these guidelines.
On Load Regulated CSMA
In this paper, we derive throughput of a threshold-based transmission policy, namely load-regulated CSMA, taking into account the propagation delay of the medium and the offered load at different probability of the fading channel. In case of the saturated load regulated CSMA, a trivial relationship between deterministic offered load to the channel at a particular fading channel condition and the maximum possible offered load has been shown. We further extend the load regulation concept into multi-channel domain. Both single and multi-channel load regulated CSMA improves the throughput of the system compared to the existing CSMA system which does not consider channel fading to control the packet transmissions.
Component based design using constraint programming for module placement on FPGAs
Constraint satisfaction modeling is both an efficient, and an elegant approach to model and solve many real world problems. In this paper, we present a constraint solver targeting module placement in static and partial run-time reconfigurable systems. We use the constraint solver to compute feasible placement positions. Our placement model incorporates communication, implementation variants and device configuration granularity. In addition, we model heterogeneous resources such as embedded memory, multipliers and logic. Furthermore, we take into account that logic resources consist of different types including logic only LUTs, arithmetic LUTs with carry chains, and LUTs with distributed memory. Our work targets state of the art field-programmable gate arrays (FPGAs) in both design-time and run-time applications. In order to evaluate our placement model and module placer implementation, we have implemented a repository containing 200 fully functional, placed and routed relocatable modules. The modules are used to implement complete systems. This validates the feasibility of both the model and the module placer. Furthermore, we present simulated results for run-time applications, and compare this to other state of the art research. In run-time applications, the results point to improved resource utilization. This is a result of using a finer tile grid and complex module shapes.
E-BRAINSTORMING: OPTIMIZATION OF COLLABORATIVE LEARNING THANKS TO ONLINE QUESTIONNAIRES.
The purpose of this article is to present a methodology and tools allowing the use of online multiple-choice questionnaires to enhance collaborative work. The first goal is to allow the questionnaires generation and setting with a simple and ergonomic manner, but also to let questioned people making comments and proposing new questions to other contributors. The developed system provides a visualization of a synthesis of the questionnaire results that is also accessible by the mean of external applications through standard Web services. These principles were developed and tested on a sample of users.
Bioinformatics integration framework for metabolic pathway data-mining
A vast amount of bioinformatics information is continuously being introduced to different databases around the world. Handling the various applications used to study this information present a major data management and analysis challenge to researchers. The present work investigates the problem of integrating heterogeneous applications and databases towards providing a more efficient data-mining environment for bioinformatics research. A framework is proposed and GeXpert, an application using the framework towards metabolic pathway determination is introduced. Some sample implementation results are also presented.
Schedulability analysis of fixed priority real-time systems with offsets
For a number of years, work has been performed in collaboration with industry to establish improved techniques for achieving and proving the system timing constraints. The specific requirements encountered during the course of this work for both uniprocessor and distributed systems indicate a need for an efficient mechanism for handling the timing analysis of task sets which feature offsets. Little research has been performed on this subject. The paper describes a new technique tailored to a set of real world problems so that the results are effective and the complexity is manageable.
Change Impact Analysis for Generic Libraries
Since the Standard Template Library (STL), generic libraries in C++ rely on concepts to precisely specify the requirements of generic algorithms (function templates) on their parameters (template arguments). Modifying the definition of a concept even slightly, can have a potentially large impact on the (interfaces of the) entire library. In particular the non-local effects of a change, however, make its impact difficult to determine by hand. In this paper we propose a conceptual change impact analysis (CCIA), which determines the impact of changes of the conceptual specification of a generic library. The analysis is organized in a pipe-and-filter manner, where the first stage finds any kind of impact, the second stage various specific kinds of impact. Both stages describe reachability algorithms, which operate on a conceptual dependence graph. In a case study, we apply CCIA to a new proposal for STL iterator concepts, which is under review by the C++ standardization committee. The analysis shows a number of unexpected incompatibilities and, for certain STL algorithms, a loss of genericity.
A model and case study for efficient shelf usage and assortment analysis
In the rapidly changing environment of Fast Moving Consumer Goods sector where new product launches are frequent, retail channels need to reallocate their shelf spaces intelligently while keeping up their total profit margins, and to simultaneously avoid product pollution. In this paper we propose an optimization model which yields the optimal product mix on the shelf in terms of profitability, and thus helps the retailers to use their shelves more effectively. The model is applied to the shampoo product class at two regional supermarket chains. The results reveal not only a computationally viable model, but also substantial potential increases in the profitability after the reorganization of the product list.
Multiprocessors May Reduce System Dependability under File-Based Race Condition Attacks
Attacks exploiting race conditions have been considered rare and "low risk". However, the increasing popularity of multiprocessors has changed this situation: instead of waiting for the victim process to be suspended to carry out an attack, the attacker can now run on a dedicated processor and actively seek attack opportunities. This change from fortuitous encountering to active exploiting may greatly increase the success probability of race condition attacks. This point is exemplified by studying the TOCTTOU (Time-of- Check-to-Time-of-Use) race condition attacks in this paper. We first propose a probabilistic model for predicting TOCTTOU attack success rate on both uniprocessors and multiprocessors. Then we confirm the applicability of this model by carrying out TOCTTOU attacks against two widely used utility programs: vi and gedit. The success probability of attacking vi increases from low single digit percentage on a uniprocessor to almost 100% on a multiprocessor. Similarly, the success rate of attacking gedit jumps from almost zero to 83%. These case studies suggest that our model captures the sharply increased risks, and hence the decreased dependability of our systems, represented by race condition attacks such as TOCTTOU on the next generation multiprocessors.
Members of Random Closed Sets
The members of Martin-Lof random closed sets under a distribution studied by Barmpalias et al. are exactly the infinite paths through Martin-Lof random Galton-Watson trees with survival parameter $\frac{2}{3}$. To be such a member, a sufficient condition is to have effective Hausdorff dimension strictly greater than $\gamma=\log_2 \frac{3}{2}$, and a necessary condition is to have effective Hausdorff dimension greater than or equal to  ***  .
Information extraction from nanotoxicity related publications
High-quality experimental data are important when developing predictive models for studying nanomaterial environmental impact (NEI). Given that raw data from experimental laboratories and manufacturing workplaces are usually proprietary and small-scaled, extracting information from publications is an attractive alternative for collecting data. We developed an information extraction system that can extract useful information from full-text nanotoxicity related publications. This information extraction system consists of five components: raw data transformation into machine readable format, data preprocessing, ontology-based named entity recognition, rule-based numerical attribute extraction from both tables and unstructured text, and relation extraction among entities and attributes. The information extraction system is applied on a dataset made of 94 publications, and results in an acceptable accuracy. By storing extracted data into a table according to relations among the data, a dataset that can be used to predict nanomaterial environmental impact is obtained. Such a system is unique in current nanomaterial community, and can help nanomaterial scientists and practitioners quickly locate useful information they need without spending lots of time reading articles.
CMOS body-enhanced cascode current mirror
A cascode current mirror with auxiliary body-driven feedback loop is proposed. Main performance parameters are analytically evaluated and compared to those of a conventional high-swing cascode and of a recently-proposed body-driven topology. Simulations are also provided confirming improvements in the achievable output resistance (important for short channel technologies), DC accuracy, and input dynamic range. Linearity, bandwidth, noise and voltage requirements are substantially the same of the conventional high-swing cascode solution.
Performance of the beacon-less routing protocol in realistic scenarios
The beacon-less routing protocol (BLR) is a position-based routing protocol for mobile ad-hoc networks that makes use of location information to reduce routing overhead. Unlike other position-based routing protocols, BLR does not require nodes to periodically broadcast hello messages. This avoids drawbacks such as extensive use of scarce battery-power, interferences with regular data transmission, and outdated position information in case of high mobility. This paper discusses the behavior and performance of BLR in realistic scenarios, in particular with irregular transmission ranges. BLR has been implemented using appropriate simulation models and in an out-door test-bed consisting of GNU/Linux laptops with wireless LAN network interfaces and GPS receivers.
Two-dimensional orthogonal tiling: from theory to practice
In pipelined parallel computations the inner loops are often implemented in a block fashion. In such programs, an important compiler optimization involves the need to statically determine the grain size. This paper presents extensions and experimental validation of the previous results of Andonov and Rajopadhye (1994) on optimal grain size determination.
A two-level ECN marking for fair bandwidth allocation between HSTCP and TCP Reno
Many versions of TCP have been proposed for transmitting data, and among them, TCP Reno is most widely used today. However, the problem, that it is difficult to use the wide bandwidth efficiently with TCP Reno, has been pointed out. HSTCP is one of the several new versions of TCP that are proposed to address this problem, but when its flows compete with TCP Reno flows at the same link, HSTCP gains most of the bandwidth and it is impossible to conduct fair transfer. In order to address this problem, we propose a two-level ECN marking to increase the frequency of congestion controls of HSTCP flows, holding its throughput. We evaluate our proposal through computer simulations, and the results show that our proposal mitigates bandwidth allocation to HSTCP, promoting fair transfer with TCP Reno.
A distributed simulation based monitoring
MSS, a computer-based monitoring system with integrated cooperative objects is proposed. MSS uses an object-based framework to interface with the user to guide a specific system evolution. MSS espouses a blackboard architecture and runs according a cooperating objects model. To achieve monitoring tasks, MSS selects the appropriate technique(s) within a set of a high performance algorithms. From the user viewpoint, MSS has been developed as a control assistant featuring different levels of interactivity, a hierarchical design style and fully embedded algorithmic tools. Virtually, MSS is able to design a monitoring board for any dynamic system.
Leakage power reduction in dual-Vdd and dual-Vth designs through probabilistic analysis of Vth variation
The noise sensitivity of low power circuits is rapidly increasing with the increasing levels of process variability and uncertainty. In this work, we study the problem of leakage power minimization in dual-Vdd and dual-Vth designs in the presence of significant Vth variation. The impact of the uncertainty in Vth on leakage power and timing are studied through probabilistic analytical models. We develop probabilistic models for timing slack and leakage power considering threshold variations with the objective of achieving an optimal selection of Vth. An analysis of the models indicate that, in the presence of variability, the value of the second Vth must be about 30mV higher than the Vth value obtained without considering variability. We show that our proposed method for the selection of Vth yields the lowest leakage power ratio of the dual-Vdd and dual-Vth versus the single-Vdd and single-Vth designs. In addition, the proposed models can be used to determine the ideal values for the second Vdd and Vth values in the context of variability for a variety of process conditions.
Probabilistic Cluster Signature for Modeling Motion Classes
In this paper, a novel 3-D motion trajectory signature is introduced to serve as an effective description to the raw trajectory. More importantly, based on the trajectory signature, a probabilistic model-based cluster signature is further developed for modeling a motion class. The cluster signature is a mixture model-based motion description that is useful for motion class perception, recognition and to benefit a generalized robot task representation. The signature modeling process is supported by integrating the EM and IPRA algorithms. The conducted experiments verified the cluster signature's effectiveness.
Secure cross-domain positioning architecture for autonomic systems
Positioning, as one of the prime components of context, has been a driving factor in the development of ubiquitous computing applications throughout the past two decades. Based on the redundant positioning architecture, this paper discusses the issues of exchanging positioning data between applications and between different administrative domains, with a focus on implementing security and privacy concepts in a self-learning and self-adapting autonomic systems environment
Refinement of medical knowledge bases: a neural network approach
One important issue in designing medical knowledge-based systems is the management of uncertainty. Among the schemes that have been developed for this purpose, probability and CF (certainty factor) are the most widely used. If rules are organized according to a connectionist model, then neural network learning suggests a promising solution to this problem. When most rules are correct, semantically incorrect rules can be recognized if their associated certainty factors are weakened or change signs after training with correct samples. The techniques for rule base refinement are examined under this approach. The concept has been implemented and tested in an actual medical expert system. >
An IT appliance for remote collaborative review of mechanisms of injury to children in motor vehicle crashes
This paper describes the architecture and implementation of a Java-based appliance for collaborative review of crashes involving injured children in order to determine mechanisms of injury. The multidisciplinary expertise needed for such reviews is not available at any one institution, resulting in the need for remote collaboration, while the sensitive nature of the information requires secure transmission and controlled access of data. The intended users of the appliance are researchers, engineers, medical doctors, government regulators, automobile and restraint manufacturers, insurance company representatives, and others who are interested in understanding the types and causes of injuries to children involved in motor vehicle crashes. The ultimate goal is to devise engineering solutions that prevent similar injuries from occurring in the future. The collaboration appliance (called Telecenter) enables the following activities: (1) the distributed asynchronous collection of digital content needed for each crash case review under a scheme that consistently organizes content across multiple cases; (2) the secure, Web-based remote participation of users in case-review meetings that involve viewing of case-specific content, live communication (written or verbal), multimedia access and sharing (slide presentations/ images), and use of Web resources; and (3) archival and post-review access of case reviews for follow-up activities and other functions (e.g., statistics, search, and networking). The Telecenter design supports audio conferencing, remote delivery and viewing of slide presentations, and other collaboration features also available in commercial and public-domain collaboration middleware products. However, it goes beyond existing solutions by also embedding a specific workflow and content organization suited for traffic injury reviews, supporting spatio-temporal role-based access control, distributed management of content and seamless integration of existing services. The current status and experience from using an early prototype of the Telecenter in actual case reviews are discussed, along with planned extensions to its functionality.
Strategic Design of the Purchase System Toward R&D Supply Chain Based on SNA
In order to make the strategy for research and development R&D purchase system better serve the personalization of material requirements in R&D process, the authors propose to develop a strategy set which will satisfy the internal as well as external constraints simultaneously. Social network analysis is used to analyze the vertical and horizontal relationships among the project, department and enterprise layers. Through a case study, the authors display the regulatory relationship of participants under given organization pattern and supply chain configuration. To disclose the restricted equilibrium mechanism of participants involved, changes under different strategies are compared, which can assist enterprises to enforce the decision making and to improve the R&D purchase system ability. The authors outline some of the managerial implications arising from the research findings at the end of this paper.
Exploring early usage patterns of mobile data services
In this paper we study the nature of factors that facilitate mobile data services use, as well as the characteristics of early adopters, to shed light into diffusion patterns and inform predictions for future growth. We advocate that the use of mobile data services can be associated with one's level of satisfaction with his/her life. Based on the findings of a questionnaire-based survey (N=388), we have found that users satisfied with their personal life use information, mobile e-mail, and stock broking services more frequently than dissatisfied ones, while users satisfied with their professional life tend to use financial, information, and mobile e-mail services more heavily. Furthermore, we identify early adopters' profiles in terms of their demographic characteristics (gender, age, education, and income) to inform the design of effective target marketing strategies.
A Fast Fixed Point Iteration Algorithm for Sparse Channel Estimation
Channels with a long but sparse impulse response arise in a variety of wireless communication applications, such as high definition television (HDTV) terrestrial transmission and underwater acoustic communications. By adopting the $\ell_1$-norm as the sparsity metric of the channel response, the channel estimation is formulated as a complex-valued convex optimization problem. A fast fixed point iteration algorithm is developed to solve the resultant complex-valued $\ell_1$-minimization problem. The proposed fast channel estimation algorithm is easy to implement and has a low computational complexity of $O(N\log N)$ per iteration with $N$ the signal length. Simulation results are provided to demonstrate the performance of the proposed fixed point algorithm.
Model-independent recovery of object orientations
A novel algorithm is presented for determining the orientation of road vehicles in traffic scenes using video images. The algorithm requires no specific 3-D vehicle models and only uses local image gradient values. It may easily be implemented in real-time. Experimental results with a variety of vehicles in routine traffic scenes are included to demonstrate the effectiveness of the algorithm.
Linear Scale and Rotation Invariant Matching
Matching visual patterns that appear scaled, rotated, and deformed with respect to each other is a challenging problem. We propose a linear formulation that simultaneously matches feature points and estimates global geometrical transformation in a constrained linear space. The linear scheme enables search space reduction based on the lower convex hull property so that the problem size is largely decoupled from the original hard combinatorial problem. Our method therefore can be used to solve large scale problems that involve a very large number of candidate feature points. Without using prepruning in the search, this method is more robust in dealing with weak features and clutter. We apply the proposed method to action detection and image matching. Our results on a variety of images and videos demonstrate that our method is accurate, efficient, and robust.
A built-in self-testing approach for minimizing hardware overhead
A built-in self-test (BIST) hardware insertion technique is addressed. Applying to register transfer level designs, this technique utilizes not only the circuit structure but also the module functionality in reducing test hardware overhead. Experimental results have shown up to 38% reduction in area overhead over other system level BIST techniques. >
Multi-Stage TR Scheme for PAPR Reduction in OFDM Signals
In the tone reservation (TR) scheme of the orthogonal frequency division multiplexing (OFDM) systems, there exists a trade-off between the peak to average power ratio (PAPR) reduction performance and the peak reduction tone (PRT) set size. In this paper, we propose a multi-stage TR scheme for PAPR reduction, which adaptively selects one of several PRT sets according to the PAPR of OFDM signal while the PRT set is fixed for the conventional TR scheme. It is shown that the PAPR reduction performance of the proposed scheme is better than that of the conventional TR scheme when the tone reservation rate (TRR) is the same.
Random key predistribution schemes for sensor networks
Key establishment in sensor networks is a challenging problem because asymmetric key cryptosystems are unsuitable for use in resource constrained sensor nodes, and also because the nodes could be physically compromised by an adversary. We present three new mechanisms for key establishment using the framework of pre-distributing a random set of keys to each node. First, in the q-composite keys scheme, we trade off the unlikeliness of a large-scale network attack in order to significantly strengthen random key predistribution's strength against smaller-scale attacks. Second, in the multipath-reinforcement scheme, we show how to strengthen the security between any two nodes by leveraging the security of other links. Finally, we present the random-pairwise keys scheme, which perfectly preserves the secrecy of the rest of the network when any node is captured, and also enables node-to-node authentication and quorum-based revocation.
Quantitative assessment of image noise and streak artifact on CT image: comparison of z-axis automatic tube current modulation technique with fixed tube current technique.
Abstract  The purpose of our study is to quantitatively assess the effects of  z -axis automatic tube current modulation technique on image noise and streak artifact, by comparing with fixed tube current technique. Standard deviation of CT-values was employed as a physical index for evaluating image noise, and streak artifact was quantitatively evaluated using our devised Gumbel evaluation method.  z -Axis automatic tube current modulation technique will improve image noise and streak artifact, compared with fixed tube current technique, and will make it possible to significantly reduce radiation doses at lung levels while maintaining the same image quality as fixed tube current technique.
Coordination policy for a two-stage supply chain considering quantity discounts and overlapped delivery with imperfect quality
Unlike the traditional integrated supplier-buyer coordination model, this research incorporates overlapped delivery and imperfect items into the production-distribution model. This model improves the observable fact that the system might experience shortage during the screening duration and also takes quantity discount into account. This approach has not been discussed in previous integrated supplier-buyer coordination models. The expected annual integrated total cost function is derived and properties and theorems are explored to help develop an algorithm. A solution procedure, free from the convexity associated with an algorithm is established to find the optimal solution. A numerical example is given to illustrate the proposed procedure and algorithm. A sensitivity analysis is made to investigate the effects of five important parameters (the inspect rate, the annual demand, the defective rate, the holding cost, and the receiving cost) on the optimal solution. Managerial insights are also discussed.
Monte Carlo modeling for implantable fluorescent analyte sensors
A Monte Carlo simulation of photon propagation through human skin and interaction with a subcutaneous fluorescent sensing layer is presented. The algorithm will facilitate design of an optical probe for an implantable fluorescent sensor, which holds potential for monitoring many parameters of biomedical interest. Results are analyzed with respect to output light intensity as a function of radial distance from source, angle of exit for escaping photons, and sensor fluorescence (SF) relative to tissue autofluorescence (AF). A sensitivity study was performed to elucidate the effects on the output due to changes in optical properties, thickness of tissue layers, thickness of the sensor layer, and both tissue and sensor quantum yields. The optical properties as well as the thickness of the stratum corneum, epidermis, (tissue layers through which photons must pass to reach the sensor) and the papillary dermis (tissue distal to sensor) are highly influential. The spatial emission profile of the SF is broad compared that of the tissue fluorescence and the ratio of sensor to tissue fluorescence increases with distance from the source. The angular distribution of escaping photons is more concentrated around the normal for SF than for tissue AF. The information gained from these simulations will he helpful in designing appropriate optics for collection of the signal of interest.
Towards a data publishing framework for primary biodiversity data: challenges and potentials for the biodiversity informatics community
Background: Currently primary scientific data, especially that dealing with biodiversity, is neither easily discoverable nor accessible. Amongst several impediments, one is a lack of professional recognition of scientific data publishing efforts. A possible solution is establishment of a ‘Data Publishing Framework’ which would encourage and recognise investments and efforts by institutions and individuals towards management, and publishing of primary scientific data potentially on a par with recognitions received for scholarly publications. Discussion: This paper reviews the state-of-the-art of primary biodiversity data publishing, and conceptualises a ‘Data Publishing Framework’ that would help incentivise efforts and investments by institutions and individuals in facilitating free and open access to biodiversity data. It further postulates the institutionalisation of a ‘Data Usage Index (DUI)’, that would attribute due recognition to multiple players in the data collection/creation, management and publishing cycle. Conclusion: We believe that institutionalisation of such a ‘Data Publishing Framework’ that offers socio-cultural, legal, technical, economic and policy environment conducive for data publishing will facilitate expedited discovery and mobilisation of an exponential increase in quantity of ‘fit-for-use’ primary biodiversity data, much of which is currently invisible.
On transistor level gate sizing for increased robustness to transient faults
In this paper we present a detailed analysis on how the critical charge (Q/sub crit/) of a circuit node, usually employed to evaluate the probability of transient fault (TF) occurrence as a consequence of a particle hit, depends on transistors' sizing. We derive an analytical model allowing us to calculate a node's Q/sub crit/ given the size of the node's driving gate and fan-out gate(s), thus avoiding time costly electrical level simulations. We verified that such a model features an accuracy of the 97% with respect to electrical level simulations performed by HSPICE. Our proposed model shows that Q/sub crit/ depends much more on the strength (conductance) of the gate driving the node, than on the node total capacitance. We also evaluated the impact of increasing the conductance of the driving gate on TFs' propagation, hence on soft error susceptibility (SES). We found that such a conductance increase not only improves the TF robustness of the hardened node, but also that of the whole circuit.
Design-inclusive UX research: design as a part of doing user experience research
Since the third wave in human–computer interaction HCI, research on user experience UX has gained momentum within the HCI community. The focus has shifted from systematic usability requirements and measures towards guidance on designing for experiences. This is a big change, since design has traditionally not played a large role in HCI research. Yet, the literature addressing this shift in focus is very limited. We believe that the field of UX research can learn from a field where design and experiential aspects have always been important: design research. In this article, we discuss why design is needed in UX research and how research that includes design as a part of research can support and advance UX design practice. We do this by investigating types of design-inclusive UX research and by learning from real-life cases of UX-related design research. We report the results of an interview study with 41 researchers in three academic research units where design research meets UX research. Based on our interview findings, and building on existing literature, we describe the different roles design can play in research projects. We also report how design research results can inform designing for experience methodologically or by providing new knowledge on UX. The results are presented in a structured palette that can help UX researchers reflect and focus more on design in their research projects, thereby tackling experience design challenges in their own research.
Multi-factory optimization enables kit reconfiguration in semiconductor manufacturing
To enable the huge saving of the kit-breakdown, we developed MaxIt v1.2 to generate an optimal capacity plan at the kit component level for the mid-range build plan in multi-factory environment. We describe the MILP (mixed integer linear programming) model and system architecture of MaxIt v1.2. We also conduct detailed sensitivity analysis on parameter setting and objective prioritizing. With the implementation in the Intel Shanghai and Manila sites, we have significantly improved data integrity and enabled a -US/spl ges/ cost savings.
Eudaemonic computing ('underwearables')
This paper presents a framework for wearable computing, based on the principle that it be unobtrusive, and that it be integrated into ordinary clothing. This design philosophy, called 'eudaemonic computing' (named in honor of the group of physicists who designed the first truly unobtrusive wearable computers with vibrotactile displays) is reduced to practice through the 'underwearable computer' ('underwearable' for short). The 'underwearable' is a computer system that is meant to be worn within or under ordinary clothing. The first 'underwearables' were built in the early 1980s, and have evolved into a form that very much resembles a tank-top. There were three reasons for the tank structure: (1) weight is evenly and comfortably distributed over the body, and bulk is distributed unobtrusively; (2) it provides privacy by situating the apparatus within the corporeal boundary we consider our own (personal) space, and others also so-regard; and (3) proximity to the body affords capability to both sense biological signal quantities (such as respiration and heart signals which are both accessible to a vest-based device), as well as produce output that we can sense, unobtrusively. The vibrotactile output modality (VibraVest) was explored as a means of assisting the visually challenged (to avoid bumping into objects through an ability to 'feel' objects at a distance). The success of VibraVest suggests other possibilities for similar unobtrusive devices that can be worn over an extended period of time, in all facets of day-to-day life.
Information systems for the age of consequences
This paper discusses what kinds of computer information systems might be of broad social value in the context of the increasingly severe ecological and social consequences of economic growth, and how they might be built and maintained. The paper has two parts. The first offers a particular understanding of the ecological and social “limits” to economic growth. The second considers how this understanding can inform computer information systems design and operation and characterizes good “limits-aware” computing research.
Active Selection of Training Examples for Meta-Learning
Meta-learning has been used to relate the performance of algorithms and the features of the problems being tackled. The knowledge in meta-learning is acquired from a set of meta-examples which are generated from the empirical evaluation of the algorithms on problems in the past. In this work, active learning is used to reduce the number of meta-examples needed for meta-learning. The motivation is to select only the most relevant problems for meta-example generation, and consequently to reduce the number of empirical evaluations of the candidate algorithms. Experiments were performed in two different case studies, yielding promising results.
Engineering Semantic Web Information Systems
Web Information Systems (WIS) use the Web paradigm and technologies to retrieve information from sources connected to the Web, and present the information in a web or hypermedia presentation to the user. Hera is a design methodology that supports the design of WIS. It is a model -driven method that distinguishes integration, data gathering, and presentation generation. In this paper we address the Hera methodology and specifically explain the integration model that covers the different aspects of integration, and the adaptation model, that specifies how the generated presentations are adaptable (e.g. device capabilities, user preferences). The Hera software framework provides a set of transformations that allow a WIS to go from integration to presentation generation. These transformations are based on RDF(S), and we show how RDF(S) has proven its value in combining all relevant aspects of WIS design. In this way, RDF(S) being the foundation of the Semantic Web, Hera allows the engineering of Semantic Web Information Systems (SWIS).
Optimal wiresizing for interconnects with multiple sources
The optimal wiresizing problem for nets with multiple sources is studied under the distributed Elmore delay model. We decompose such a net into a source subtree (SST) and a set of loading subtrees (LSTs), and show the optimal wiresizing solution satisfies a number of interesting properties, including: the LST separability, the LST monotone property, the SST local monotone property and the general dominance property. Furthermore, we study the optimal wiresizing problem using a variable grid and reveal the bundled refinement property. These properties lead to efficient algorithms to compute the lower and upper bounds of the optimal solutions. Experiment results on nets from an Intel processor layout show an interconnect delay reduction of up to 35.9\% when compared to the minimum-width solution. In addition, the algorithm based on a variable grid yields a speedup of two orders of magnitude without loss of accuracy, when compared with the fixed grid based methods.
Magneto- and electroencephalographic manifestations of reward anticipation and delivery
article i nfo Article history: Accepted 19 April 2012 Available online 26 April 2012 The monetary incentive delay task was used to characterize reward anticipation and delivery with concur- rently acquired evoked magnetic fields, EEG potentials and EEG/MEG oscillatory responses, obtaining a pre- cise portrayal of their spatiotemporal evolution. In the anticipation phase, differential activity was most prominent over midline electrodes and parieto-occipital sensors. Differences between non-reward- and reward-predicting cues were localized in the cuneus and later in the dorsal PCC, suggesting a modulation by potential reward information during early visual processing, followed by a coarse emotional evaluation of the cues. Oscillatory analysis revealed increased theta power after non-reward cues over fronto-central sites. In the beta range, power decreased with the magnitude of the potential reward and increased with re- action time, probably reflecting the influence of the striatal response to potential reward on the sensorimotor cortex. At reward delivery, negative prediction errors led to a larger mediofrontal negativity. The spatiotem- poral evolution of reward processing was modulated by prediction error: whereas differences were located in PCC and putamen in the prediction error comparison, in the case of expected outcomes they were located in PCC, ACC and parahippocampal gyrus. In the oscillatory realm, theta power was largest following rewards and, in the case of non-rewards, was largest when these were unexpected. Higher beta activity following re- wards was also observed in both modalities, but MEG additionally showed a significant power decrease for this condition over parieto-occipital sensors. Our results show how visual, limbic and striatal structures are involved in the different stages of reward anticipation and delivery, and how theta and beta oscillations have a prominent role in the processing of these stimuli.
Ranking Weblogs by Analyzing Reading and Commenting Activities
In this paper, we analyze people’s reading and commenting behaviors in blogspace and proposed an algorithm for blog ranking. Upon two selected communities, AI and Medical, we show how comments, reading records, active browsing and multi time browsing can help to construct the weblog graph and reflect a blog’s popularity. Based on these analysis, we propose cRank, a graph based algorithm, to rank blog among community members. Finally, we divide our dataset temporally and present how the proposed algorithm can make prediction on blogs’ rankings. The experiment shows that cRank has a better performance upon several baseline systems.
Robust lip region segmentation for lip images with complex background
Robust and accurate lip region segmentation is of vital importance for lip image analysis. However, most of the current techniques break down in the presence of mustaches and beards. With mustaches and beards, the background region becomes complex and inhomogeneous. We propose in this paper a novel multi-class, shape-guided FCM (MS-FCM) clustering algorithm to solve this problem. For this new approach, one cluster is set for the object, i.e. the lip region, and a combination of multiple clusters for the background which generally includes the skin region, lip shadow or beards. The proper number of background clusters is derived automatically which maximizes a cluster validity index. A spatial penalty term considering the spatial location information is introduced and incorporated into the objective function such that pixels having similar color but located in different regions can be differentiated. This facilitates the separation of lip and background pixels that otherwise are inseparable due to the similarity in color. Experimental results show that the proposed algorithm provides accurate lip-background partition even for the images with complex background features like mustaches and beards.
First formant difference for /i/ and /u/: A cross-linguistic study and an explanation
The value of the first formant of high back and high front vowels (/u/ and /i/) has been determined for near minimal pairs in a 30-language sample. It is found that for 29 out of 30 languages the average of the first formant is higher for high back vowels than for high front vowels, and that for 26 out of 28 languages the majority of minimal pairs has a high back vowel with a higher first formant than that of the high front vowel. A trend towards smaller differences was found in women, but this is not significant in the present data set.#R##N##R##N#Two factors may explain this observation. Firstly, the human vocal tract can only vary the position of gradual (and not abrupt) transitions of cross-sectional area. Secondly, there is a narrow tube just above the glottis (the epilarynx tube). Both factors cause the first formant of high back vowels to be raised, but neither is sufficiently important to explain the observed differences on its own.
An e-Learning Library on the Web
The main topic addressed in this paper is how to help learners select some instructive hypermedia-based learning resources according to their learning contexts from the Web. Our approach is to provide a digital library for web-based learning called e-Learning Library, which includes learning resource repository, local indexing, and adaptive navigation support. This aims to promote their learning with diverse learning resources involving a certain topic.
Stability of a class of linear switching systems with applications to two consensus problems
In this paper, we first establish a stability result for a class of linear switching systems involving Kronecker product. The problem is intriguing in that the system matrix does not have to be Hurwitz in any time instant. We have established the main result by a combination of the Lyapunov stability analysis and a generalized Barbalat's Lemma applicable to piecewise continuous linear systems. As applications of this stability result, we study both the leaderless consensus problem and the leader-following consensus problem for general marginally stable linear multi-agent systems under switching network topology. In contrast with many existing results, our result only assume that the dynamic graph is uniformly connected.
Factoring nonnegative matrices with linear programs
This paper describes a new approach, based on linear programming, for computing nonnegative matrix factorizations (NMFs). The key idea is a data-driven model for the factorization where the most salient features in the data are used to express the remaining features. More precisely, given a data matrix X, the algorithm identifies a matrix C that satisfies X ≈ CX and some linear constraints. The constraints are chosen to ensure that the matrix C selects features; these features can then be used to find a low-rank NMF of X. A theoretical analysis demonstrates that this approach has guarantees similar to those of the recent NMF algorithm of Arora et al. (2012). In contrast with this earlier work, the proposed method extends to more general noise models and leads to efficient, scalable algorithms. Experiments with synthetic and real datasets provide evidence that the new approach is also superior in practice. An optimized C++ implementation can factor a multigigabyte matrix in a matter of minutes.
Three-dimensional subband coding of video
We describe and show the results of video coding based on a three-dimensional (3-D) spatio-temporal subband decomposition. The results include a 1-Mbps coder based on a new adaptive differential pulse code modulation scheme (ADPCM) and adaptive bit allocation. This rate is useful for video storage on CD-ROM. Coding results are also shown for a 384-kbps rate that are based on ADPCM for the lowest frequency band and a new form of vector quantization (geometric vector quantization (GVQ)) for the data in the higher frequency bands. GVQ takes advantage of the inherent structure and sparseness of the data in the higher bands. Results are also shown for a 128-kbps coder that is based on an unbalanced tree-structured vector quantizer (UTSVQ) for the lowest frequency band and GVQ for the higher frequency bands. The results are competitive with traditional video coding techniques and provide the motivation for investigating the 3-D subband framework for different coding schemes and various applications. >
Translation of UML state machines to Modelica: Handling semantic issues
ModelicaML is a UML profile that enables modeling and simulation of systems and their dynamic behavior. ModelicaML combines the power of the OMG UML standardized graphical notation for systems and software modeling, and the simulation power of Modelica. This addresses the increasing need for precise and integrated modeling of products containing both software and hardware. This article discusses the usage of executable UML state machines for system modeling, i.e. usage of the same formalism for describing the state-based dynamic behavior of physical system components and software. Moreover, it points out that the usage of Modelica as an action language enables an integrated simulation of continuous-time and reactive/event-based system dynamics. The main purpose of this article is however to highlight issues that are identified regarding the UML specification which are experienced with typical executable implementations of UML state machines. The issues identified are resolved and rationales for the taken design decisions are provided.
Compressed sensing with linear correlation between signal and measurement noise
Existing convex relaxation-based approaches to reconstruction in compressed sensing assume that noise in the measurements is independent of the signal of interest. We consider the case of noise being linearly correlated with the signal and introduce a simple technique for improving compressed sensing reconstruction from such measurements. The technique is based on a linear model of the correlation of additive noise with the signal. The modification of the reconstruction algorithm based on this model is very simple and has negligible additional computational cost compared to standard reconstruction algorithms, but is not known in existing literature. The proposed technique reduces reconstruction error considerably in the case of linearly correlated measurements and noise. Numerical experiments confirm the efficacy of the technique. The technique is demonstrated with application to low-rate quantization of compressed measurements, which is known to introduce correlated noise, and improvements in reconstruction error compared to ordinary Basis Pursuit De-Noising of up to approximately 7dB are observed for 1bit/sample quantization. Furthermore, the proposed method is compared to Binary Iterative Hard Thresholding which it is demonstrated to outperform in terms of reconstruction error for sparse signals with a number of non-zero coefficients greater than approximately 1/10th of the number of compressed measurements.
Architecture optimization of a 3-DOF translational parallel mechanism for machining applications, the orthoglide
This paper addresses the architecture optimization of a three-degree-of-freedom translational parallel mechanism designed for machining applications. The design optimization is conducted on the basis of a prescribed Cartesian workspace with prescribed kinetostatic performances. The resulting machine, the Orthoglide, features three fixed parallel linear joints which are mounted orthogonally, and a mobile platform which moves in the Cartesian x-y-z space with fixed orientation. The interesting features of the Orthoglide are a regular Cartesian workspace shape, uniform performances in all directions, and good compactness. A small-scale prototype of the Orthoglide under development is presented at the end of this paper.
Prototype learning with margin-based conditional log-likelihood loss
The classification performance of nearest prototype classifiers largely relies on the prototype learning algorithms, such as the learning vector quantization (LVQ) and the minimum classification error (MCE). This paper proposes a new prototype learning algorithm based on the minimization of a conditional log-likelihood loss (CLL), called log-likelihood of margin (LOGM). A regularization term is added to avoid over-fitting in training. The CLL loss in LOGM is a convex function of margin, and so, gives better convergence than the MCE algorithm. Our empirical study on a large suite of benchmark datasets demonstrates that the proposed algorithm yields higher accuracies than the MCE, the generalized LVQ (GLVQ), and the soft nearest prototype classifier (SNPC).
Smart sensor architecture customized for image processing applications
A system level design methodology is applied to the embedded system design for a typical sensor network application: face detection for security purpose. The tradeoff analysis is performed for hardware and software implementations of the tasks in this application. The best system design is achieved with limited hardware resources.
A problem-driven collaborative approach to eliciting requirements of internetwares
In the software development, most stakeholders cannot clearly and objectively express their needs for the envisioned software systems. In this paper, we propose a problem-driven collaborative requirements elicitation approach, with the purpose of helping identify and extract the requirements of the Internetwares (a complex and new software paradigm). The basic idea of our approach is that the requirements of the software systems should be stated by stakeholders in an objective way (i.e. problem-identifying-solving way). That is, first identify the problems existed in the as-is problem domain, and then find the solutions to the problems. The solutions to the problems are the requirements of the envisioned software systems. To this end, we propose the structure of problems and a collaborative process for achieving the solutions.
A diversity-based method for infrequent purchase decision support in e-commerce
In this paper we propose a method for supporting consumer buying decisions in e-commerce. We are advocating the diversity-driven approach to generating alternatives for infrequently purchased products (i.e., computers, vehicles, etc.). Our method is based upon the well-known ''divergence/convergence'' principle of problem solving. The paper discusses the method based on fuzzy weighted-sum model and cluster analysis, the architecture and the operation of the decision support system for generating product alternatives. The preliminary experiments with the prototype for notebook selection provide some support in favor of our approach over the catalog-based systems.
Heuristic scheduling of jobs on parallel batch machines with incompatible job families and unequal ready times
This research is motivated by a scheduling problem found in the diffusion and oxidation areas of semiconductor wafer fabrication, where the machines can be modeled as parallel batch processors. We attempt to minimize total weighted tardiness on parallel batch machines with incompatible job families and unequal ready times of the jobs. Given that the problem is NP-hard, we propose two different decomposition approaches. The first approach forms fixed batches, then assigns these batches to the machines using a genetic algorithm (GA), and finally sequences the batches on individual machines. The second approach first assigns jobs to machines using a GA, then forms batches on each machine for the jobs assigned to it, and finally sequences these batches. Dispatching and scheduling rules are used for the batching phase and the sequencing phase of the two approaches. In addition, as part of the second decomposition approach, we develop variations of a time window heuristic based on a decision theory approach for forming and sequencing the batches on a single machine.
BrainKnowledge: A Human Brain Function Mapping Knowledge-Base System
Associating fMRI image datasets with the available literature is crucial for the analysis and interpretation of fMRI data. Here, we present a human brain function mapping knowledge-base system (BrainKnowledge) that associates fMRI data analysis and literature search functions. BrainKnowledge not only contains indexed literature, but also provides the ability to compare experimental data with those derived from the literature. BrainKnowledge provides three major functions: (1) to search for brain activation models by selecting a particular brain function; (2) to query functions by brain structure; (3) to compare the fMRI data with data extracted from the literature. All these functions are based on our literature extraction and mining module developed earlier (Hsiao, Chen, Chen. Journal of Biomedical Informatics 42, 912–922, 2009), which automatically downloads and extracts information from a vast amount of fMRI literature and generates co-occurrence models and brain association patterns to illustrate the relevance of brain structures and functions. BrainKnowledge currently provides three co-occurrence models: (1) a structure-to-function co-occurrence model; (2) a function-to-structure co-occurrence model; and (3) a brain structure co-occurrence model. Each model has been generated from over 15,000 extracted Medline abstracts. In this study, we illustrate the capabilities of BrainKnowledge and provide an application example with the studies of affect. BrainKnowledge, which combines fMRI experimental results with Medline abstracts, may be of great assistance to scientists not only by freeing up resources and valuable time, but also by providing a powerful tool that collects and organizes over ten thousand abstracts into readily usable and relevant sources of information for researchers.
Radio resource allocation for cellular networks based on OFDMA with QoS guarantees
In this paper we address the problem of radio resource allocation for QoS support in the downlink of a cellular OFDMA system. The major impairments considered are cochannel interference (CCI) and frequency selective fading. The allocation problem involves assignment of base stations and subcarriers, bit loading, and power control, for multiple users. We propose a three-stage, low-complexity, heuristic algorithm to distribute radio resources among multiple users according to their individual QoS requirements, while at the same time maintaining the QoS of already established links in all the cochannel cells. The allocation objective is to minimize the total transmit power, which adds to reducing CCI. Simulation results show a superior performance of the proposed method when compared to classical radio resource management techniques. Our scheme allows us to achieve almost 6 times higher capacity (sum data rate) than the method based on FDMA with power control, at a blocking probability of 0.02.
Spectral texturing for real-time applications
In this sketch we present a new method for rendering large-scale, high-resolution, non-repetitive textures in real-time using multi layer texturing. The basic idea of spectral texturing is to construct the nal texture by multiple texture layers where each layer provides a certain range of the spectrum of the texture’s spatial frequencies. Alpha channels are used to introduce statistical dependencies between the frequency bands. This approach extends a method called detail texturing, which does not use alpha channels to model higher statistical properties of the resulting texture. Other approaches to generate textures with specied statistical properties, like [DeBonet 1997], are not suitable for real-time use and would require storage of the generated texture. Spectral texturing is very easy to implement, runs on all contemporary 3d graphics cards, and is especially suitable for naturalistic textures in real-time applications which are viewed from a large range of distances.
Streams on wires: a query compiler for FPGAs
Taking advantage of many-core, heterogeneous hardware for data processing tasks is a difficult problem. In this paper, we consider the use of FPGAs for data stream processing as coprocessors in many-core architectures. We present Glacier, a component library and compositional compiler that transforms continuous queries into logic circuits by composing library components on an operator-level basis. In the paper we consider selection, aggregation, grouping, as well as windowing operators, and discuss their design as modular elements.#R##N##R##N#We also show how significant performance improvements can be achieved by inserting the FPGA into the system's data path (e.g., between the network interface and the host CPU). Our experiments show that queries on the FPGA can process streams at more than one million tuples per second and that they can do this directly from the network, removing much of the overhead of transferring the data to a conventional CPU.
Exploring component-based approaches in forest landscape modeling
Forest management issues are increasingly required to be addressed in a spatial context, which has led to the development of spatially explicit forest landscape models. The numerous processes, complex spatial interactions, and diverse applications in spatial modeling make the development of forest landscape models difficult for any single research group. New developments in componentbased modeling approaches provide a viable solution. Component-based modeling breaks a monolithic model into small, interchangeable, and binary components. They have these advantages compared to the traditional modeling work: 1) developing a component is a much smaller task than developing a whole model, 2) a component can be developed using most programming languages, since the interface format is binary, and 3) new components can replace the existing ones under the same model framework; this reduces the duplication and allows the modeling community to focus resources on the common products, and to compare results. In this paper, we explore the design of a spatially explicit forest landscape model in a component-based modeling framework, based on our work on object-oriented forest landscape modeling. We examine the representation of the major components and the interactions between them. Our goal is to facilitate the use of the component-based modeling approach at the early stage of spatially explicit landscape modeling.  2002 Elsevier Science Ltd. All rights reserved.
Extensive feature detection of N-terminal protein sorting signals
Motivation: The prediction of localization sites of various proteins is an important and challenging problem in the field of molecular biology. TargetP, by Emanuelsson et al. (J. Mol. Biol., 300, 1005‐1016, 2000) is a neural network based system which is currently the best predictor in the literature for N-terminal sorting signals. One drawback of neural networks, however, is that it is generally difficult to understand and interpret how and why they make such predictions. In this paper, we aim to generate simple and interpretable rules as predictors, and still achieve a practical prediction accuracy. We adopt an approach which consists of an extensive search for simple rules and various attributes which is partially guided by human intuition. Results: We have succeeded in finding rules whose prediction accuracies come close to that of TargetP, while still retaining a very simple and interpretable form. We also discuss and interpret the discovered rules. Availability: An (experimental) web service using rules obtained by our method is provided at http:
Optimum color masking matrix determination for digital color platemaking, using virtual color samples
Abstract#R##N##R##N#For high-fidelity color reproduction with digital color platemaking systems, it is the most important to determine the color masking matrix which converts the red, green and blue intensities of the monitor to cyan, magenta, yellow and black inks halftone dot area rates. With regard to this determination process, the author previously developed a color difference least square method by which the optimum matrix is determined, using a simulation with the Neugebauer equation and virtual color samples. First, this paper evaluates the method with actual images. Next, the method is extended for adaptive masking matrix optimization. By adapting the matrix to each color image using black ink, it was shown that the average color differences of reproduced images could be reduced to below six when black ink is used as much as possible, i.e., in the case of achromatic printing. This masking matrix adaption is a new concept which was impossible by conventional color scanner processing, but became possible by using virtual color samples.
Downlink Optimization with Interference Pricing and Statistical CSI
In this paper, we propose a downlink transmission strategy based on intercell interference pricing and a distributed algorithm that enables each base station (BS) to design locally its own beamforming vectors without relying on downlink channel state information of links from other BSs to the users. This algorithm is the solution to an optimization problem that minimizes a linear combination of data transmission power and the resulting weighted intercell interference with pricing factors at each BS and maintains the required signal-to-interference-plus-noise ratios (SINR) at user terminals. We provide a convergence analysis for the proposed distributed algorithm and derive conditions for its existence. We characterize the impact of the pricing factors in expanding the operational range of SINR targets at user terminals in a power-efficient manner. Simulation results confirm that the proposed algorithm converges to a network-wide equilibrium point by balancing and stabilizing the intercell interference levels and assigning power optimal beamforming vectors to the BSs. The results also show the effectiveness of the proposed algorithm in closely following the performance limits of its centralized coordinated beamforming counterpart.
Secure authentication watermarking for localization against the Holliman---Memon attack
Authentication watermarking schemes using block-wise watermarks for tamper localization are vulnerable to the Holliman---Memon attack. In this paper, we propose a novel method based on the Wong's localization scheme (Proceedings of the IS&T PIC, Portland) to resist this attack. A unique image index scheme is used for computing the authentication signature that is embedded in the least significant bit-plane of the block. The informed detector estimates the correct image index by using the side information about the watermarked image. The image index estimation from the fake image can definitely be an alternative to keeping a directory of image indices. So it is not necessary to manage the database of image indices for the verification purpose. The authenticity measure is defined to quantify the attack severity by taking the connectivity among possible authentic blocks into consideration. There are more blocks verified as authentic when this measure is high for a fake image constructed using this attack. As such, the blocks for a fake image can be chosen from a reduced number of database images. The blocks from any such image are to be connected with each other to maximize the authenticity measure. Thus, the attacker's task to generate a fake image of reasonable perceptual quality becomes increasingly difficult. With the proposed method there is no loss or ambiguity in localization after the Holliman---Memon attack and content tampering in an image. The localization accuracy in the proposed method is demonstrated by the simulation results and is equal to the chosen block size, similar to the Wong's scheme.
Directions of external knowledge search: investigating their different impact on firm performance in high-technology industries
Purpose – The aim of the paper is to identify the different directions of external knowledge search and to investigate their individual effect on performance at the firm level. Design/methodology/approach – The empirical study is based on survey data gathered from two distinct informants of 248 large- and medium-sized high-tech manufacturing Spanish firms. In dealing with concerns on simultaneity and reverse causality, perceived time-lags among dependent and independent variables were introduced. Quantitative methods based on questionnaire answers were used. Findings – Findings reveal six distinct external search patterns and indicate that, while market sources such as customers and competitors are positively associated with performance, knowledge acquired from general information sources, other firms beyond the core business and patents and databases have no significant effect. Moreover, knowledge obtained from science and technology organizations and from suppliers displays an inversed U-shaped effect o...
An Empirical Evaluation of the Student-Net Delay Tolerant Network
Radio equipped mobile devices have enjoyed tremendous growth in the past few years. We observe that in the near future it might be possible to build a network that routes delay-tolerant packets by harnessing user mobility and the pervasive availability of wireless devices. Such a delay-tolerant network could be used to supplement wireless infrastructure or provide service where none is available. Since mobile devices in a delay-tolerant network forward packets to nearby users, the devices can use short-range radio, which potentially reduces device power consumption and radio contention. The design of a user mobility based delay-tolerant network raises two key challenges: determining the connectivity of such a network, and determining the latency characteristics and replication requirements of routing algorithms in such a network. To determine realistic contact patterns, we collected user mobility data by conducting two user studies. We outfitted groups of students with instrumented wireless-enabled PDAs that logged pairwise contacts between study participants over a period of several weeks. Experiments conducted on these traces show that it is possible to form a delay-tolerant network based on human mobility. The network has good connectivity, so that routes exist between almost all study participants via some multi-hop path. Moreover, it is possible to effectively route packets with modest replication.
Local Strategy Improvement for Parity Game Solving
Theproblemofsolvinga paritygameis atthecore ofmanyproblemsin modelchecking,satisfiability checking and program synthesis. Some of the best algorithms for solving parity game are strategy improvement algorithms. These are global in nature since they require the entire parity game to be present at the beginning. This is a distinct disadvantagebecause in many applications one only needs to know which winning region a particular node belongs to, an daw itnessing winning strategy may cover only a fractional part of the entire game graph. We present a local strategy improvement algorithm which explores the game graph on-the-fly whilst performing the improvement steps. We also compare it empirically with existing global strategy improvementalgorithms and the currently only other local algorithm for solving parity games. It turnsout that local strategy improvementcan outperformthese othersby severalordersof magnitude.
White matter integrity, fiber count, and other fallacies: The do's and don'ts of diffusion MRI
Diffusion-weighted MRI (DW-MRI) has been increasingly used in imaging neuroscience over the last decade. An early form of this technique, diffusion tensor imaging (DTI) was rapidly implemented by major MRI scanner companies as a scanner selling point. Due to the ease of use of such implementations, and the plausibility of some of their results, DTI was leapt on by imaging neuroscientists who saw it as a powerful and unique new tool for exploring the structural connectivity of human brain. However, DTI is a rather approximate technique, and its results have frequently been given implausible interpretations that have escaped proper critique and have appeared misleadingly in journals of high reputation. In order to encourage the use of improved DW-MRI methods, which have a better chance of characterizing the actual fiber structure of white matter, and to warn against the misuse and misinterpretation of DTI, we review the physics of DW-MRI, indicate currently preferred methodology, and explain the limits of interpretation of its results. We conclude with a list of ‘Do's and Don'ts’ which define good practice in this expanding area of imaging neuroscience.
The International Technology Alliance in Network and Information Sciences
In May 2006, the US Army Research Laboratory and UK Ministry of Defense created the international technology alliance. The consortium of 26 partners including the ARL and MoD offers an open research environment in which leading US and UK companies and universities can collaborate (see table 1). It will also fuse the best aspects of the US Army's Collaborative Technology Alliances and UK MoD's Defense Technology Centers on an international scale. The ITA aims to develop flexible, distributed, and secure decision-making procedures to improve networked coalition operations. Network science is a young discipline we have limited information models and network theories to describe the behavior and scaling of large, complex mobile ad hoc networks.1 moreover, you can't understand a coalition network's performance without understanding its cognitive and sociocultural aspects and physical characteristics. A key ITA goal is to perform basic research in network-centric coalition decision making across four technical areas: network theory, security across a system of systems, sensor information processing and delivery, and distributed coalition planning and decision making, 2. we focus on the last area because this is where intelligent systems will play the biggest role.
Standoff Detection Using Millimeter and Submillimeter Wave Spectroscopy
The millimeter (MM) wave and sub-MM wave (30-600 GHz) frequency band contains fundamental rotational and vibrational resonances of many molecular gases composed of carbon, nitrogen, oxygen, and sulphur. The high specificity of rotational spectra to organic molecules affords MM wave spectroscopy having potential use in remotely sensing atmospheric pollutants and the detection of airborne chemicals is important for arms control treaty verification, intelligence collection, and environmental monitoring. This paper considers the sensitivity requirements of radiofrequency receiver systems for measuring MM wave absorption/emission signatures. The significance of receiver sensitivity and material optical depth to sensing is highlighted. A background to the technology needed for sensing at MM and sub-MM wavelengths then provides the basis for a review of MM wave spectroscopy and its role on profiling the concentrations of trace polar molecules and ionized radicals in the high altitude atmosphere. The application of the MM wave spectroscopic technique in ambient conditions is then reviewed and the issues associated with developing the technique for standoff remote sensing is discussed.
Let's talk about rings
Defining the ring topology of a molecule belongs to the central and elementary problems of cheminformatics. Questions like 'How many rings does a molecule contain?' or 'In how many rings is a specific atom involved?' are based on such a definition. Obviously, the ring topology must be unique, i.e. it does not depend on atom order, chemical meaningful, and of reasonable size, at most polynomial in the number of atoms. For a long period, the smallest set of smallest rings (SSSR) was used in cheminformatics applications ignoring a very critical flaw, namely that it is not unique even for simple structures. Other definitions like the set of relevant cycles heal this flaw; however they are sometimes chemically not meaningful and can become exponential in size. Among all attempts made, none fulfils all three criteria at the same time [1].#R##N##R##N#Recently, we developed a ring definition named 'unique ring family' (URF) and a corresponding algorithm for calculating it in polynomial time [2]. URFs match the common chemical sense of a molecule's ring topology. The definition results in a unique ring description consisting of a number of ring prototypes which is at most quadratic in the number of atoms. In this talk, we will present the algorithm, benchmarks as well as several examples demonstrating the usefulness of URFs.
Instability of submicron anisotropic liquid cylinders and jets in magnetic field
The capillary instability of a magnetically anisotropic liquid cylinder and jets, such as Nematic liquid crystals (LC), in magnetic fields, is considered using an energy approach. The boundary problem is solved in the linear approximation of the anisotropy /spl chi//sub a/ of the magnetic susceptibility /spl chi/. The effect of the anisotropy, in the region 1 |/spl chi/|>| /spl chi//sub a/| /spl chi//sup 2/, can be strong enough to counteract and even reverse the tendency of the field to enhance stabilization by increasing the cut-off wave number k/sub s/, beyond the conventional one set by Rayleigh. It is shown that the elastic effect, which is typical of LC, is significant on the scale of nano-jets, where it prevails over the magnetic effect. The jet instability is determined by surface tension, elasticity, and magnetic permeability and anisotropy. The relative influence of the elasticity and permeability on the jet stability depends on its radius. This is particularly true on the nano-scale.
Information retrieval methods for automatic speech recognition
In this paper, we use information retrieval (IR) techniques to improve a speech recognition (ASR) system. The potential benefits include improved speed, accuracy, and scalability. Where conventional HMM-based speech recognition systems decode words directly, our IR-based system first decodes subword units. These are then mapped to a target word by the IR system. In this decoupled system, the IR serves as a lightweight, data-driven pronunciation model. Our proposed method is evaluated in the Windows Live Search for Mobile (WLS4M) task, and our best system has 12% fewer errors than a comparable HMM classifier. We show that even using an inexpensive IR weighting scheme (TF-IDF) yields a 3% relative error rate reduction while maintaining all of the advantages of the IR approach.
A decision making model using soft set and rough set on fuzzy approximation spaces
In modern era of computing, there is a need of development in data analysis and decision making. Most of our tools are crisp, deterministic and precise in character. But general real life situations contains uncertainties. To handle such uncertainties many theories are developed such as fuzzy set, rough set, rough set on fuzzy approximation spaces etc. But all these theories have their own limitations. To overcome the limitations, the concept of soft set is introduced. But, soft set also fails if the attributes in the information system are almost identical rather exactly identical. In this paper, we propose a decision making model that consists of two processes such as preprocess and postprocess to mine decisions. In preprocess we use rough set on fuzzy approximation spaces to get the almost equivalence classes whereas in postprocess we use soft set techniques to obtain decisions. The proposed model is tested over an institutional dataset and the results show practical viability of the proposed research.
The influence of parental and peer attachment on Internet usage motives and addiction
The impact of parental and peer attachment on four Internet usage motives and Internet addiction was compared using path modelling of survey data from 1,577 adolescent Malaysian school students. The model accounted for 31 percent of Internet addiction score variance. Lesser parental attachment was associated with greater Internet addiction risk. Psychological escape motives were more strongly related to Internet addiction than other motives, and had the largest mediating effect upon the parental attachment–addiction relationship. Peer attachment was unrelated to addiction risk, its main influence on Internet usage motives being encouragement of use for social interaction. It is concluded that dysfunctional parental attachment has a greater influence than peer attachment upon the likelihood of adolescents becoming addicted to Internet–related activities. It is also concluded that the need to relieve dysphoria resulting from poor adolescent–parent relationships may be a major reason for Internet addiction, and that parents’ fostering of strong bonds with their children should reduce addiction risk.
An integer programming based approach for verification and diagnosis of workflows
Workflow analysis is indispensable to capture modeling errors in workflow designs. While several workflow analysis approaches have been defined previously, these approaches do not give precise feedback, thus making it hard for a designer to pinpoint the exact cause of modeling errors. In this paper we introduce a novel approach for analyzing and diagnosing workflows based on integer programming (IP). Each workflow model is translated into a set of IP constraints. Faulty control flow connectors can be easily detected using the approach by relaxing the corresponding constraints. We have implemented this diagnosis approach in a tool called DiagFlow which reads and diagnoses XPDL models using an existing open source IP solver as a backend. We show that the diagnosis approach is correct and illustrate it with realistic examples. Moreover, the approach is flexible and can be extended to handle a variety of new constraints, as well as to support new workflow patterns. Results of testing on large process models show that DiagFlow outperforms a state of the art tool like Woflan in terms of the solution time.
A hybrid wireless network enhanced with multihopping for emergency communications
This paper proposes a hybrid wireless network scheme enhanced with ad hoc networking for disaster damage assessment and emergency communications. The network aims to maintain the connection between a base station (BS) and nodes by way of multihopping. In the event that a direct link between BS and a node is disconnected, the node switches modes from cellular to ad hoc in order to access BS via neighboring nodes. A routing protocol proposed in this paper is capable of building a route using unicast-based route discovery process without route request flooding. A proposed MAC protocol satisfies the requirement of maintaining accessibility and a short delay even in emergency circumstances. We discuss an analytical model based on a Markov process. Experimental results are shown regarding reachability, throughput and delay.
A wavelet-like filter based on neuron action potentials for analysis of human scalp electroencephalographs
This paper describes the development and testing of a wavelet-like filter, named the SNAP, created from a neural activity simulation and used, in place of a wavelet, in a wavelet transform for improving EEG wavelet analysis, intended for brain-computer interfaces. The hypothesis is that an optimal wavelet can be approximated by deriving it from underlying components of the EEG. The SNAP was compared to standard wavelets by measuring Support Vector Machine-based EEG classification accuracy when using different wavelets/filters for EEG analysis. When classifying P300 evoked potentials, the error, as a function of the wavelet/filter used, ranged from 6.92% to 11.99%, almost twofold. Classification using the SNAP was more accurate than that with any of the six standard wavelets tested. Similarly, when differentiating between preparation for left- or right-hand movements, classification using the SNAP was more accurate (10.03% error) than for four out of five of the standard wavelets (9.54% to 12.00% error) and internationally competitive (7% error) on the 2001 NIPS competition test set. Phenomena shown only in maps of discriminatory EEG activity may explain why the SNAP appears to have promise for improving EEG wavelet analysis. It represents the initial exploration of a potential family of EEG-specific wavelets.
Distributed Multi Class SVM for Large Data Sets
Data mining algorithms are originally designed by assuming the data is available at one centralized site. These algorithms also assume that the whole data is fit into main memory while running the algorithm. But in today's scenario the data has to be handled is distributed even geographically. Bringing the data into a centralized site is a bottleneck in terms of the bandwidth when compared with the size of the data. In this paper for multiclass SVM we propose an algorithm which builds a global SVM model by merging the local SVMs using a distributed approach(DSVM). And the global SVM will be communicated to each site and made it available for further classification. The experimental analysis has shown promising results with better accuracy when compared with both the centralized and ensemble method. The time complexity is also reduced drastically because of the parallel construction of local SVMs. The experiments are conducted by considering the data sets of size 100s to hundred of 100s which also addresses the issue of scalability.
Across boundaries of influence and accountability: the multiple scales of public sector information systems
The use of ICTs in the public sector has long been touted for its potential to transform the institutions that govern and provide social services. The focus, however, has largely been on systems that are used within particular scales of the public sector, such as at the scale of state or national government, the scale of regional or municipal entity, or at the scale of local service providers. The work presented here takes aim at examining ICT use that crosses these scales of influence and accountability. We report on a year long ethnographic investigation conducted at a variety of social service outlets to understand how a shared information system crosses the boundaries of these very distinct organizations. We put forward that such systems are central to the work done in the public sector and represent a class of collaborative work that has gone understudied.
Symmetric Exponential Integrators with an Application to the Cubic Schrödinger Equation
In this article, we derive and study symmetric exponential integrators. Numerical experiments are performed for the cubic Schrodinger equation and comparisons with classical exponential integrators and other geometric methods are also given. Some of the proposed methods preserve the L 2-norm and/or the energy of the system.
Limits of homology detection by pairwise sequence comparison
Motivation: Noise in database searches resulting from random sequence similarities increases as the databases expand rapidly. The noise problems are not a technical shortcoming of the database search programs, but a logical consequence of the idea of homology searches. The effect can be observed in simulation experiments. Results: We have investigated noise levels in pairwise alignment based database searches. The noise levels of 38 releases of the SwissProt database, display perfect logarithmic growth with the total length of the databases. Clustering of real biological sequences reduces noise levels, but the effect is marginal.
Business Process Development in Semantically-Enriched Environment
Middleware support for business process Management BPM has met some of the challenges with respect to encoding, performance and maintenance of workflows. A remaining challenge is complexity: business processes are becoming widely distributed, interoperating across a range of inter- and intra-organizational behaviours, vocabularies and semantics. It is important that this semantic complexity is checked and analyzed for optimality and trustworthiness prior to deployment. Petri nets are a formal method that successfully provides behavioural analysis. A shortcoming of Petri nets is that the data exchanged between business activities abstract too far away from the importance of data in actual business processes. This paper addresses this abstraction gap via additional semantic enrichment, through a two stage, model-driven approach.
Distributed dynamic scheduling for end-to-end rate guarantees in wireless ad hoc networks
We present a framework for the provision of deterministic end-to-end bandwidth guarantees in wireless ad hoc networks. Guided by a set of local feasibility conditions, multi-hop sessions are dynamically offered allocations, further translated to link demands. Using a distributed Time Division Multiple Access (TDMA) protocol nodes adapt to the demand changes on their adjacent links by local, conflict-free slot reassignments. As soon as the demand changes stabilize, the nodes must incrementally converge to a TDMA schedule that realizes the global link (and session) demand allocation.We first derive sufficient local feasibility conditions for certain topology classes and show that trees can be maximally utilized.We then introduce a converging distributed link scheduling algorithm that exploits the logical tree structure that arises in several ad hoc network applications.Decoupling bandwidth allocation to multi-hop sessions from link scheduling allows support of various end-to-end Quality of Service (QoS) objectives. We focus on the max-min fairness (MMF) objective and design an end-to-end asynchronous distributed algorithm for the computation of the session MMF rates. Once the end-to-end algorithm converges, the link scheduling algorithm converges to a TDMA schedule that realizes these rates.We demonstrate the applicability of this framework through an implementation over an existing wireless technology. This implementation is free of restrictive assumptions of previous TDMA approaches: it does not require any a-priori knowledge on the number of nodes in the network nor even network-wide slot synchronization.
Evaluating Various Branch-Prediction Schemes for Biomedical-Implant Processors
This paper evaluates various branch-prediction schemes under different cache configurations in terms of performance, power, energy and area on suitably selected biomedical workloads. The benchmark suite used consists of compression, encryption and data-integrity algorithms as well as real implant applications, all executed on realistic biomedical input datasets. Results are used to drive the (micro)architectural design of a novel microprocessor targeting microelectronic implants. Our profiling study has revealed that, under strict or relaxed area constraints and regardless of cache size, the ALWAYS TAKEN and ALWAYS NOT-TAKEN static prediction schemes are, in almost all cases, the most suitable choices for the envisioned implant processor. It is further shown that bimodal predictors with small Branch-Target-Buffer (BTB) tables are suboptimal yet also attractive solutions when processor I/D-cache sizes are up to 1024KB/512KB, respectively.
A New User Authentication Protocol for Mobile Terminals in Wireless Network
For constructing a ubiquitous network, the highspeed wireless LAN (WLAN) attracts attention as an infrastructure for global access. However, some issues are impeding further adoption of the technology, in particular, security problems including user authentication, message compromising, password theft, connection hijacking, etc. In this paper, we discuss a fast authentication method of mobile ubiquitous terminals in WLAN. To achieve an efficient access control between Access Points (APs) and mobile terminals and sharing of a session key between terminals, we propose a new user secure authentication method and a session key distribution protocol based on service ticket issuing system.
On the Basis of the Generated Foundation of Blended E-learning - Transcendence and Integration
Learning theory develops in the constant process of transcendence and integration. Not only from knowledge to people, but also its theory approaches are all beyond each other and integration. Therefore, the emergence of blended e_learning is inevitable, which is based on the theoretical study?s transcendence and integration, and as a learning concepts and theories, blended e_learning is also bound to each other than with the integration. Such mutual transcendence and integration of learning theory is just an important basis for the starting point.
An Improved Signcryption Scheme and Its Variation
Signcryption is a new cryptographic primitive which simultaneously provides both confidentiality and authenticity. This paper proposes an improved signcryption scheme and a variant scheme providing message recovery. The first scheme is revised from an authenticated encryption scheme which has been found to have a security-flaw. Our scheme solves the security-flaw and provides an additional property called the public verifiability of the signature. The second scheme is a message recovery type. It surpasses most of the current signcryption schemes on the size of the signcrypted ciphertext. That is, in our second scheme, we require only two parameters, (r, s), with r epsi Z p  and s epsi Z  q  while most signcryption schemes require three parameters (c, r, s) with the additional parameter c epsi Z p . This second scheme is modified from an authenticated encryption scheme with message recovery and surpasses the based authenticated encryption scheme on the property of non-repudiation of the origin
A new network architecture with intelligent node (IN) to enhance IEEE 802.14 HFC networks
In the hybrid fiber/coax (HFC) architecture, over several hundreds subscribers in CATV (community antenna TV) network may cause serious collisions. In this paper, we propose a new network architecture which using an intelligent node (IN) to stand for a group of subscribers to request the demand resources. The IN has the ability to reduce the collision probability as well as the collision resolving period. The simulation results show that the proposed architecture in terms of throughput, buffer delay, and fairness outperforms the standard architecture.
Dynamic load balancing schemes for computing accessible surface area of protein molecules
This paper presents an experimental study of dynamic load balancing methods for a parallelized solution to a well-known problem in computational molecular biology: computing the accessible surface areas (ASA) of proteins. The main contribution is a better understanding of how certain techniques for load estimation and redistribution must be combined carefully for effectiveness and how these combinations need to change during the course of a computation. In particular, the Shrake-Rupley ASA algorithm is implemented and three aspects of dynamic load balancing are studied: how to estimate load imbalance (the estimation problem); when to invoke load redistribution (the invocation problem); and how to load balance (the mapping problem). The results in this paper show that a dynamically-selected mix of algorithms in each category that adapts to changing structure within the protein works better than a static periodic application of a static mix of algorithms.
Level crossing rate and average fade duration of MRC and EGC diversity in Ricean fading
The average level crossing rate and average fade duration of the output signal of a maximal ratio combiner (MRC) and equal gain combiner (EGC), operating on independent Ricean fading input branch signals, are derived. Exact, closed-form results are obtained for MRC diversity, while precise expressions for EGC diversity are presented with an infinite series method. The results are valid for an arbitrary number of independent, identically distributed diversity branches, isotropic scattering, and a specular component perpendicular to the line of motion of the mobile.
Design and implementation of WIRE Diameter
This paper presents the design and implementation of WIRE Diameter. The WIRE Diameter is an open source implementation of Diameter Based Protocol and Diameter EAP application developed by the Wireless Internet Research & Engineering (WIRE) Laboratory. Research has shown that traditional RADIUS protocol may suffer performance degradation and data loss in a large system. Diameter, thus, was proposed to address the deficiencies in RADIUS. Both 3GPP and 3GPP2 have adopted Diameter as their AAA protocol. The WIRE Diameter could be used to authenticate and authorize 802.1x supplicant. It provides various authentication schemes, including EAP-MD5, EAP-TLS, EAP-TTLS, and PEAP. The WIRE Diameter is developed to be independent of OS as much as possible. Currently it supports Linux, FreeBSD and various versions of MS Windows. It is believed that the WIRE Diameter is the first open source implementation of Diameter EAP Application in the world. The source code can be downloaded freely. The WIRE Diameter should be useful for the research community. This paper demonstrates the design and implementation of the WIRE Diameter.
Stability and performance of intersecting aircraft flows under decentralized conflict avoidance rules
This paper considers the problem of two intersecting aircraft flows under decentralized conflict resolution rules. Considering aircraft flowing through a fixed control volume, new air traffic control models and scenarios are defined that enable the study of long-term aircraft flow stability. For a class of two intersecting aircraft flows, this paper considers conflict scenarios involving arbitrary encounter angles. It is shown that aircraft flow stability, defined both in terms of safety and performance, is preserved under the decentralized conflict resolution algorithm considered. It is shown that the lateral deviations experienced by aircraft in each flow are bounded.
Informational acquisition and cognitive models
Abstract#R##N##R##N#Life forms must organize information into cognitive models reflecting the outside environment, and in a complex and changing environment a life form must constantly select and organize this mass of information to avoid slipping into a chaotic cognitive state. The task of developing and maintaining adaptive cognitive models can be understood through two processes, crucial to regulating the interconnections between environmental elements. The inclusion and exclusion of information follows a process designated by P and the process by which cognitive models change is designated by K. Higher order concepts are created by reducing the interconnections between elements to a minimal number to avoid cognitive chaos. © 2004 Wiley Periodicals, Inc. Complexity 9:31–37, 2004
Contextual motion field-based distance for video analysis
In this work, we propose a general method for computing distance between video frames or sequences. Unlike conventional appearance-based methods, we first extract motion fields from original videos. To avoid the huge memory requirement demanded by the previous approaches, we utilize the “bag of motion vectors” model, and select Gaussian mixture model as compact representation. Thus, estimating distance between two frames is equivalent to calculating the distance between their corresponding Gaussian mixture models, which is solved via earth mover distance (EMD) in this paper. On the basis of the inter-frame distance, we further develop the distance measures for both full video sequences.#R##N##R##N#Our main contribution is four-fold. Firstly, we operate on a tangent vector field of spatio-temporal 2D surface manifold generated by video motions, rather than the intensity gradient space. Here we argue that the former space is more fundamental. Secondly, the correlations between frames are explicitly exploited using a generative model named dynamic conditional random fields (DCRF). Under this framework, motion fields are estimated by Markov volumetric regression, which is more robust and may avoid the rank deficiency problem. Thirdly, our definition for video distance is in accord with human intuition and makes a better tradeoff between frame dissimilarity and chronological ordering. Lastly, our definition for frame distance allows for partial distance.
Minimum effort inverse kinematics for redundant manipulators
This paper investigates the use of an infinity norm in formulating the optimization measures for computing the inverse kinematics of redundant arms. The infinity norm of a vector is its maximum absolute value component and hence its minimization implies the determination of a minimum effort solution as opposed to the minimum-energy criterion associated with the Euclidean norm. In applications where individual magnitudes of the vector components are of concern, this norm represents the physical requirements more closely than does the Euclidean norm. We first study the minimization of the infinity-norm of the joint velocity vector itself, and discuss its physical interpretation. Next, a new method of optimizing a subtask criterion, defined using the infinity-norm, to perform additional tasks such as obstacle avoidance or joint limit avoidance is introduced. Simulations illustrating these methods and comparing the results with the Euclidean norm solutions are presented.
A Traceability Technique for Specifications
Traceability in software involves discovering links between different artifacts, and is useful for a myriad of tasks in the software life cycle. We compare several different Information Retrieval techniques for this task, across two datasets involving real-world software with the accompanying specifications and documentation. The techniques compared include dimensionality reduction methods, probabilistic and information theoretic approaches, and the standard vector space model.
A generic tension-closure analysis method for fully-constrained cable-driven parallel manipulators
Cable-driven parallel manipulators (CDPMs) are a special class of parallel manipulators that are driven by cables instead of rigid links. Due to the unilateral property of the cables, all the driving cables in a fully-constrained CDPM must always maintain positive tension. As a result, tension analysis is the most essential issue for these CDPMs. By drawing upon the mathematical theory from convex analysis, a sufficient and necessary tension-closure condition is proposed in this paper. The key point of this tension-closure condition is to construct a critical vector that must be positively expressed by the tension vectors associated with the driving cables. It has been verified that such a tension-closure condition is general enough to cater for CDPMs with different numbers of cables and DOFs. Using the tension-closure condition, a computationally efficient algorithm is developed for the tension-closure pose analysis of CDPMs, in which only a limited set of deterministic linear equation systems need to be resolved. This algorithm has been employed for the tension-closure workspace analysis of CDPMs and verified by a number of computational examples. The computational time required by the proposed algorithm is always shorter as compared to other existing algorithms.
Parallel Support Vector Machines: The Cascade SVM
We describe an algorithm for support vector machines (SVM) that can be parallelized efficiently and scales to very large problems with hundreds of thousands of training vectors. Instead of analyzing the whole training set in one optimization step, the data are split into subsets and optimized separately with multiple SVMs. The partial results are combined and filtered again in a 'Cascade' of SVMs, until the global optimum is reached. The Cascade SVM can be spread over multiple processors with minimal communication overhead and requires far less memory, since the kernel matrices are much smaller than for a regular SVM. Convergence to the global optimum is guaranteed with multiple passes through the Cascade, but already a single pass provides good generalization. A single pass is 5x - 10x faster than a regular SVM for problems of 100,000 vectors when implemented on a single processor. Parallel implementations on a cluster of 16 processors were tested with over 1 million vectors (2-class problems), converging in a day or two, while a regular SVM never converged in over a week.
Estimation of heart-surface potentials using regularized multipole sources
Direct inference of heart-surface potentials from body-surface potentials has been the goal of most recent work on electrocardiographic inverse solutions. We developed and tested indirect methods for inferring heart-surface potentials based on estimation of regularized multipole sources. Regularization was done using Tikhonov, constrained-least-squares, and multipole-truncation techniques. These multipole-equivalent methods (MEMs) were compared to the conventional mixed boundary-value method (BVM) in a realistic torso model with up to 20% noise added to body-surface potentials and /spl plusmn/1 cm error in heart position and size. Optimal regularization was used for all inverse solutions. The relative error of inferred heart-surface potentials of the MEM was significantly less (p<0.05) than that of the BVM using zeroth-order Tikhonov regularization in 10 of the 12 cases tested. These improvements occurred with a fourth-degree (24 coefficients) or smaller multipole moment. From these multipole coefficients, heart-surface potentials can be found at an unlimited number of heart-surface locations. Our indirect methods for estimating heart-surface potentials based on multipole inference appear to offer significant improvement over the conventional direct approach.
Towards runtime testing in automotive embedded systems
Runtime testing is a common way to detect faults during normal system operation. To achieve a specific diagnostic coverage runtime testing is also used in safety critical, automotive embedded systems. In this paper we propose a test architecture to consolidate the hardware resource consumption and timing needs of runtime tests and of application and system tasks in a hard real-time embedded system as applied to the automotive domain. Special emphasis is put to timing requirements of embedded systems with respect to hard real-time and concurrent hardware resource accesses of runtime tests and tasks running on the target system.
An Environment for (re)configuration and Execution Managenment of Flexible Radio Platforms
This paper presents the Flexible Radio Kernel (FRK), a configuration and execution management environment for hybrid hardware/software flexible radio platform. The aim of FRK is to manage platform reconfiguration for multi-mode, multi-standard operation, with different levels of abstraction. A high level framework is described, to manage multiple MAC layers, and to enable MAC cooperation algorithms for cognitive radio. A low-level environment is also available to manage platform reconfiguration for radio operations. Radio can be implemented using hardware or software elements. Configuration state is hidden to the high-level layers, offering pseudo concurrency (time sharing) properties. This study presents a global view of FRK, with details on some specific parts of the environment. A practical study with algorithmic description is presented.
A simple approach to evaluate the ergodic capacity and outage probability of correlated Rayleigh diversity channels with unequal signal-to-noise ratios
In this article, we propose a novel method to derive exact closed-form ergodic capacity and outage probability expressions for correlated Rayleigh fading channels with receive diversity. Unlike the existing works, the proposed method employ a simple approach for the capacity and outage analysis for receiver diversity channels operating at different signal-to-noise ratios depicted in the diagonal elements of matrix Ω. With x being the channel gain vector, random variable of the form Y(a)=a + x∗Ω x is considered. Novelty of the work resides in the fact that the distribution of Y(a) is accurately determined by employing Fourier representation of unit step function followed by complex integration in a straight forward way. The ergodic channel capacity is thus calculated by using the first-order moment, #N#                  #N#                    #N#                  #N#                  #N#                    #N#                      E#N#                      [#N#                      #N#                        #N#                          log#N#                        #N#                        #N#                          2#N#                        #N#                      #N#                      (#N#                      Y#N#                      (#N#                      1#N#                      )#N#                      )#N#                      ]#N#                    #N#                  #N#                , while the outage probability for a certain threshold γ0is evaluated using #N#                  #N#                    #N#                  #N#                  #N#                    #N#                      #N#                        #N#                          ∫#N#                        #N#                        #N#                          0#N#                        #N#                        #N#                          #N#                            #N#                              γ#N#                            #N#                            #N#                              0#N#                            #N#                          #N#                        #N#                      #N#                      #N#                        #N#                          f#N#                        #N#                        #N#                          Y#N#                          (#N#                          0#N#                          )#N#                        #N#                      #N#                      (#N#                      y#N#                      )#N#                      dy#N#                    #N#                  #N#                . Extensive experiments have been conducted demonstrating the accuracy of the proposed approach.
On the feasibility of synthesizing CAD software from specifications: generating maze router tools in ELF
The application of program synthesis techniques to the generation of technology-sensitive VLSI physical design tools is described. The architecture and implementation of a particular software generator (called ELF) targeted at the generation of maze routing software is described. ELF strives to meet the demands of the target technology by automatically generating maze router implementations to match the application requirements. ELF has three key features. First, a very high level language, lacking data structure implementation specifications, is used to describe algorithm design styles. Second, application-specific expertise about routing and application independent code synthesis techniques are used to guide search among alternative design styles for algorithms and data structures. Third, code generation is used to transform the resulting abstract descriptions of selected algorithms and data structures into final, executable code. Code generation is an incremental, stepwise refinement process. Experimental results are presented covering several correct. fully functional routers synthesized by ELF from varying high-level specifications. Results from synthetic and industrial benchmarks are examined to illustrate ELF's capabilities. >
Canonical Sequence Directed Tactics Analyzer for Computer Go Games
We present an approach used in CSDTA (canonical sequence directed tactics analyzer) that uses canonical sequences (Joseki) in hoping to improve computer Go programs. We collect 1278 canonical sequences and their deviations in our system. Instead of trivially matching the current game to the collected sequences, we define a notion of similarity to extract the most suitable move from the candidate sequences for the next move. The simplicity of our method and its positive outcome make our approach a promising tool to be integrated into a complete computer Go program for a foreseeable improvement
An improved approximation to the estimation of the critical F values in best subset regression.
Variable selection methods are routinely applied in regression modeling to identify a small number of descriptors which "best" explain the variation in the response variable. Most statistical packages that perform regression have some form of stepping algorithm that can be used in this identification process. Unfortunately, when a subset of p variables measured on a sample of n objects are selected from a set of k(>p) to maximize the squared sample multiple regression coefficient, the significance of the resulting regression is upwardly biased. The extent of this bias is investigated by using Monte Carlo simulation and is presented as an inflation factor which when multiplied by the usual tabulated F ratio gives an estimate of the true 5% critical value. The results show that selection bias can be very high even for moderate-size data sets. Selecting three variables from 50 generated at random with 20 observations will almost certainly provide a significant result if the usual tabulated F values are used. An interpolation formula is provided for the calculation of the inflation factor for different combinations of (n, p, k). Four real data sets are examined to illustrate the effect of correlated descriptor variables on the degree of inflation.
MultiK-MHKS: A Novel Multiple Kernel Learning Algorithm
In this paper, we develop a new effective multiple kernel learning algorithm. First, we map the input data into m different feature spaces by m empirical kernels, where each generated feature space is taken as one view of the input space. Then, through borrowing the motivating argument from Canonical Correlation Analysis (CCA) that can maximally correlate the m views in the transformed coordinates, we introduce a special term called Inter-Function Similarity Loss R IFSI . into the existing regularization framework so as to guarantee the agreement of multiview outputs. In implementation, we select the Modification of Ho-Kashyap algorithm with Squared approximation of the misclassification errors (MHKS) as the incorporated paradigm and the experimental results on benchmark data sets demonstrate the feasibility and effectiveness of the proposed algorithm named MultiK-MHKS.
On the rank of random sparse matrices
We investigate the rank of random (symmetric) sparse matrices. Our main finding is that with high probability, any dependency that occurs in such a matrix is formed by a set of few rows that contains an overwhelming number of zeros. This allows us to obtain an exact estimate for the co-rank.
Maximum Utility Peer Selection for P2P Streaming in Wireless Ad Hoc Networks
In the recent years, the peer-to-peer (P2P) overlay network has been a promising architecture for multimedia streaming services besides its common use for efficient file sharing. By simply increasing the number of peers, the P2P overlay network can meet the high bit rate requirements of multimedia applications. Optimal peer selection for newly joining peers is one of the important problems, especially in wireless networks which have limited resources and capacity, since the peer selection process has a direct impact on the throughput of the underlay network and the co-existing unicast traffic. In this paper we tackle the problem of peer selection for streaming applications over wireless ad hoc networks. We devise a novel peer selection algorithm which maximizes the throughput of the underlay network, and at the same time makes P2P streaming friendly towards the co-existing data traffic. The proposed receiver based rate allocation and peer selection (RPS) algorithm is derived using the network utility maximization (NUM) framework. The algorithm solves the peer selection and rate allocation problem distributedly while optimally adapting the medium access control (MAC) layer parameters and is easily extensible to large P2P networks. Simulation results show that by using the proper price exchange mechanism, the peer receivers can effectively maximize the throughput of the underlay network by intelligently selecting its source peers.
Time‐dependent groundwater modeling using spreadsheet
Abstract#R##N##R##N#Time-dependent groundwater modeling using spreadsheet simulation (TGMSS) model is developed as solution technique. It is a practical method that uses spreadsheets instead of the conventional solution methods. All of the aquifer parameters can easily be described in TGMSS model. The results of TGMSS are validated with MODFLOW. Results showed that TGMSS and MODFLOW results were in good agreement in terms of resulting values of hydraulic heads. © 2005 Wiley Periodicals, Inc. Comput Appl Eng Educ 13: 192–199, 2005; Published online in Wiley InterScience (www.interscience.wiley.com); DOI 10.1002/cae.20048
A Simulation Tool to Study High-Frequency Chest Compression Energy Transfer Mechanisms and Waveforms for Pulmonary Disease Applications
High-frequency chest compression (HFCC) can be used as a therapeutic intervention to assist in the transport and clearance of mucus and enhance water secretion for cystic fibrosis patients. An HFCC pump-vest and half chest-lung simulation, with 23 lung generations, has been developed using inertance, compliance, viscous friction relationships, and Newton's second law. The simulation has proven to be useful in studying the effects of parameter variations and nonlinear effects on HFCC system performance and pulmonary system response. The simulation also reveals HFCC waveform structure and intensity changes in various segments of the pulmonary system. The HFCC system simulation results agree with measurements, indicating that the HFCC energy transport mechanism involves a mechanically induced pulsation or vibration waveform with average velocities in the lung that are dependent upon small air displacements over large areas associated with the vest-chest interface. In combination with information from lung physiology, autopsies and a variety of other lung modeling efforts, the results of the simulation can reveal a number of therapeutic implications.
Distributed-information neural control: the case of dynamic routing in traffic networks
Large-scale traffic networks can be modeled as graphs in which a set of nodes are connected through a set of links that cannot be loaded above their traffic capacities. Traffic flows may vary over time. Then the nodes may be requested to modify the traffic flows to be sent to their neighboring nodes. In this case, a dynamic routing problem arises. The decision makers are realistically assumed 1) to generate their routing decisions on the basis of local information and possibly of some data received from other nodes, typically, the neighboring ones and 2) to cooperate on the accomplishment of a common goal, that is, the minimization of the total traffic cost. Therefore, they can be regarded as the cooperating members of informationally distributed organizations, which, in control engineering and economics, are called team organizations. Team optimal control problems cannot be solved analytically unless special assumptions on the team model are verified. In general, this is not the case with traffic networks. An approximate resolutive method is then proposed, in which each decision maker is assigned a fixed-structure routing function where some parameters have to be optimized. Among the various possible fixed-structure functions, feedforward neural networks have been chosen for their powerful approximation capabilities. The routing functions can also be computed (or adapted) locally at each node. Concerning traffic networks, we focus attention on store-and-forward packet switching networks, which exhibit the essential peculiarities and difficulties of other traffic networks. Simulations performed on complex communication networks point out the effectiveness of the proposed method.
Matching of PDB chain sequences to information in public databases as a prerequisite for 3D functional site visualization
The 3D structures of biomacromolecules stored in the Protein Data Bank [1] were correlated with different external, biological information from public databases. We have matched the feature table of SWISS-PROT [2] entries as well InterPro [3] domains and function sites with the corresponding 3D-structures. OMIM [4] (Online Mendelian Inheritance in Man) records, containing information of genetic disorders, were extracted and linked to the structures. The exhaustive all-against-all 3D structure comparison of protein structures stored in DALI [5] was condensed into single files for each PDB entry. Results are stored in XML format facilitating its incorporation into related software. The resulting annotation of the protein structures allows functional sites to be identified upon visualization. Availability: http://leger.gbf.de/PDBXML/
$L^2$ Optimization in Discrete FIR Estimation: Exploiting State-Space Structure
This paper studies the $L^2$ (mean-square) optimal design of discrete-time FIR estimators. A solution procedure, which reduces the problem to a static matrix optimization problem admitting a closed-form solution, is proposed. In the latter solution, a special state-space structure of the associated matrices is exploited to obtain efficient formulae with the computational complexity proportional to the length of the impulse response of the estimator. Unlike previously available least-square FIR results, our treatment does not impose unnecessarily restrictive assumptions on the process dynamics and can handle interpolation constraints on the unit circle, which facilitates the inclusion of steady-state performance requirements.
Application of plane waves for accurate measurement of microwave scattering from geophysical surfaces
The authors utilized the concept of a compact antenna range to obtain plane-wave illumination to accurately measure scattering properties of simulated sea ice. They also made simultaneous measurements using conventional antennas. Measured scattering coefficients obtained with the plane-wave system at 10 GHz decreased by about 35 dB when the incidence angle increased from 0/spl deg/ to 10/spl deg/. Scattering coefficients derived from data collected with the radar system at 13.5 GHz using conventional far-field antennas decreased by about 20 dB over the same angular region. This demonstrates that the far-field properties of a widebeam antenna are inadequate for measuring the angular scattering response of smooth surfaces. They believe that application of the compact antenna range concept for scattering measurements has a wide range of applications and is the solution to the long-standing problem of how to directly measure scattering consisting of coherent and incoherent components. >
A Parallelized Surface Extraction Algorithm for Large Binary Image Data Sets Based on an Adaptive 3-D Delaunay Subdivision Strategy
In this paper, we describe a novel 3D subdivision strategy to extract the surface of binary image data. This iterative approach generates a series of surface meshes that capture different levels of detail of the underlying structure. At the highest level of detail, the resulting surface mesh generated by our approach uses only about 10 percent of the triangles in comparison to the Marching Cube (MC) algorithm, even in settings where almost no image noise is present. Our approach also eliminates the so-called "staircase effect," which voxel-based algorithms like the MC are likely to show, particularly if nonuniformly sampled images are processed. Finally, we show how the presented algorithm can be parallelized by subdividing 3D image space into rectilinear blocks of subimages. As the algorithm scales very well with an increasing number of processors in a multithreaded setting, this approach is suited to process large image data sets of several gigabytes. Although the presented work is still computationally more expensive than simple voxel-based algorithms, it produces fewer surface triangles while capturing the same level of detail, is more robust toward image noise, and eliminates the above-mentioned "staircase" effect in anisotropic settings. These properties make it particularly useful for biomedical applications, where these conditions are often encountered.
AutoVision - flexible processor architecture for video-assisted driving
Summary form only given. Future automotive security systems will benefit from visual scene analysis based on a fusion of video, infrared, and radar images. Today we have already functions like lane departure warning and automatic cruise control (ACC) for pretty well defined driving environments, such as highways and primary roads. Recent research activities concentrate on more complex environments, such as city traffic with a wide variety of traffic participants moving in an unpredictable manner, e.g. bikes, pedestrians, children, and even animals, and under changing weather and lighting conditions. The ITRS semiconductor roadmap for microelectronics forecasts a continued doubling of transistor capacity per chip every 2 to 2.5 years enabling billion transistor ASIC designs in the near future. Multi processor system on chip (MPSoC) solutions with 8, 16 or even more standard RISC CPU cores, mega-bytes of fast (ns access latencies) on-chip SRAM memories, giga-byte per second interconnect buses or NoC (network on chip) meshes, high-speed serial I/Os and, last but not least, million gate equivalent dedicated hardware accelerator functions in eFPGA (embedded field programmable gate array) logic are becoming reality on a single silicon substrate. Examples of current research projects shall illustrate our perception on how this tremendous increase in functionality and computational performance per chip area may impact automotive control unit (ACU) architectures for driver assistance applications. The AutoVision processor is a dynamically reconfigurable MPSoC prototype where video-specific pixel processing engines are on-the-fly loaded or exchanged without interrupting regular system operations. For the time being, pixel processing engines cover functions such as object edge detection or luminance segmentation, and are implemented as dedicated hardware accelerators to ensure real-time frame processing capabilities of the AutoVision processor. Dynamic replacement of processing engines ensures an automatic and area efficient adaptation to various driving conditions. Segmented objects are, in a subsequent step, characterized by means of standard MPEG-7 descriptors and entered as search criteria into traffic scene analysis databases. Goal is to obtain a clean distinction between passenger cars, trucks, and big rectangular traffic signs, and to identify pedestrians or bikers in complex traffic situations. The AutoVision processor project is supported by the German Research Foundation (DFG) in the special emphasis research programme "reconfigurable computing".
TALP at GikiCLEF 2009
This paper describes our experiments in Geographical Information Retrieval with the Wikipedia collection in the context of our participation in the GikiCLEF 2009 Multilingual task in English and Spanish. Our system, called gikiTALP, follows a very simple approach that uses standard Information Retrieval with the Sphinx full-text search engine and some Natural Language Processing techniques without Geographical Knowdledge.
Transmit Beamforming for Frequency-Selective Channels
In this paper, we propose beamforming schemes for frequency-selective channels with decision-feedback equalization (DFE) at the receiver. We consider both finite impulse response (FIR) and infinite impulse response (IIR) beamforming filters (BFFs). In case of IIR beamforming, we are able to derive closed-form expressions for the optimum BFFs. In addition, we provide an efficient numerical method for recursive calculation of the optimum FIR BFFs. Simulation and numerical results for typical GSM/EDGE channels confirm the significant performance gains achievable with beamforming compared to single-antenna transmission and optimized delay diversity.
All possible second-order four-impedance two-stage Colpitts oscillators
The authors report all the possible four-impedance settings that yield a valid second-order two-stage Colpitts oscillator. These settings are obtained following an exhaustive search conducted on two possible structures of the oscillator modelled through two-port network transmission parameters. Only valid second-order cases with a maximum of three reactive elements are reported. Experimental and Spice verification of a selected example using both MOS and BJT transistors is given.
Mining Frequent Sequential Patterns under a Similarity Constraint
Many practical applications are related to frequent sequential pattern mining, ranging from Web Usage Mining to Bioinformatics. To ensure an appropriate extraction cost for useful mining tasks, a key issue is to push the user-defined constraints deep inside the mining algorithms. In this paper, we study the search for frequent sequential patterns that are also similar to an user-defined reference pattern. While the effective processing of the frequency constraints is well-understood, our contribution concerns the identification of a relaxation of the similarity constraint into a convertible anti-monotone constraint. Both constraints are then used to prune the search space during a levelwise search. Preliminary experimental validations have confirmed the algorithm efficiency.
Dispatching Petroleum Products
Petroleum products are distributed worldwide from refineries and lube plants to retail outlets and industrial customers. Proper dispatching of shipments of such products, packaged and in bulk, may result in significant transportation and inventory cost savings. This work examines the variety of operational environments which exist in dispatching petroleum products, and the operations research tools used by oil companies to dispatch such products. In addition, it identifies gaps where additional research is needed.
CLASSIFICATION, AVERAGING AND RECONSTRUCTION OF MACROMOLECULES IN ELECTRON TOMOGRAPHY
Electron tomography provides opportunities to determine three-dimensional cellular architecture at resolutions high enough to identify individual macromolecules such as proteins. Image analysis of such data poses a challenging problem due to the extremely low signal-to-noise ratios that makes individual volumes simply too noisy to allow reliable structural interpretation. This requires using averaging techniques to boost the signal-to-noise ratios, a common practice in electron microscopy single particle analysis where they have proven to be very powerful in elucidating high resolution structure. Although there are significant similarities in the way data is processed, several new problems arise in the tomography case that have to be properly dealt with. Such problems involve dealing with the missing wedge characteristic of limited angle tomography, the need for robust and efficient 3D alignment routines, and design of methods that account for diverse conformations through the use of classification. We present a framework for reconstruction via alignment, classification and averaging of volumes obtained from limited angle electron tomography, providing a powerful tool for high resolution structure determination and description of conformational variability in a biological context
Performance analysis of the conventional complex LMS and augmented complex LMS algorithms
Recently, the augmented complex LMS (ACLMS) algorithm has been proposed for modeling complex-valued signal relationships in which a widely-linear model can be more appropriate [1]. It is not clear, however, how the behavior of ACLMS differs from that of the conventional complex LMS (CCLMS) algorithm. In this paper, we leverage a recently-developed analysis for the complex LMS algorithm [2] to illuminate the performance relationships between the ACLMS and CCLMS algorithms. Our analysis shows that the ACLMS algorithm can potentially achieve a lower steady-state mean-squared error as compared to that of CCLMS, but the convergence speed of ACLMS is slowed in the presence of highly non-circular complex-valued input signals. An adaptive beamforming example indicates the utility of the results.
Extending wireless sensor network lifetime through order-based genetic algorithm
Extending the lifetime is a key issue in wireless sensor networks. An effective way to extend the lifetime is to partition the sensors into several covers and activate the covers one by one. Thus, the more the covers, the longer the lifetime. To find the maximum number of covers has been modeled as the Set K-Cover problem. In this paper we propose using order-based genetic algorithm to solve the Set K-Cover problem for extending the lifetime of wireless sensor networks. The proposed algorithm needs neither an upper bound nor any assumption about the maximum number of covers. Experimental results show that the order-based genetic algorithm can achieve near-optimal solutions efficiently.
Robust neural predictor for noisy chaotic time series prediction
A robust neural predictor is designed for noisy chaotic time series prediction in this paper. The main idea is based on the consideration of the bounded uncertainty in predictor input, and it is a typical Errors-in-Variables problem. The robust design is based on the linear-in-parameters ESN (Echo State Network) model. By minimizing the worst-case residual induced by the bounded perturbations in the echo state variables, the robust predictor is obtained in coping with the uncertainty in the noisy time series. In the experiment, the classical Mackey-Glass 84-step benchmark prediction task is investigated. The prediction performance is studied for the nominal and robust design of ESN predictors.
Minimization strategies for maximally parallel multiset rewriting systems
Maximally parallel multiset rewriting systems (MPMRS) give a convenient way to express relations between unstructured objects. The functioning of various computational devices may be expressed in terms of MPMRS (e.g., register machines and many variants of P systems). In particular, this means that MPMRS are Turing universal; however, a direct translation leads to quite a large number of rules. Like for other classes of computationally complete devices, there is a challenge to find a universal system having the smallest number of rules. In this article we present different rule minimization strategies for MPMRS based on encodings and structural transformations. We apply these strategies to the translation of a small universal register machine (Korec (1996) [9]) and we show that there exists a universal MPMRS with 23 rules. Since MPMRS are identical to a restricted variant of P systems with antiport rules, the results we obtained improve previously known results on the number of rules for those systems.
The Introduction of the OSCAR Database API (ODA)
The OSCAR [14] cluster installation toolkit was created by the Open Cluster Group (OCG) for one particular type of High Performance Computing (HPC) cluster. OSCAR is currently one of the widely used cluster installation toolkits; it boasts hundreds of thousands of downloads and active mailing lists. OSCAR has expanded its area with several sub-projects targeting other types of HPC clusters. Each of these projects share a core set of OSCAR code, including the OSCAR Database and its access API, "ODA" (OSCAR Database API). The ODA abstraction layer, consisting of a database schema and corresponding API, hides a commodity back-end database (e.g., MySQL [15]). Because OSCAR and its sub-projects are targeted at new, innovative environments (including non-HPC environments), there are significant issues with managing various configurations of each project. For example, as we previously showed [8], previous versions of ODA were unable to represent the complex, ever-growing set of data required to accurately describe the clusters that it manages. Further, its API was extremely complex, requiring a steep learning curve for OSCAR developers. Therefore, we have designed and implemented a new database schema to deal with these issues. This new version of ODA has not only resolved the above problems but also, as proposed in our previous paper, enabled storage and retrieval of various configuration information, and encouraged data re-use between the main OSCAR project and its derivative projects. In addition, the new version of ODA has sped up the OSCAR installation process. This document presents a simpler, highly flexible design and implementation of ODA slated to be included in OSCAR v5.0. It also suggests a blueprint for maintaining the database modules of ODA in a systematic, organized way.
Learning for Sustainability Transition through Bounded Socio-technical Experiments in Personal Mobility
Abstract A bounded socio-technical experiment (BSTE) attempts to introduce a new technology, service, or a social arrangement on a small scale. Many such experiments in personal mobility are ongoing worldwide. They are carried out by coalitions of diverse actors, and are driven by long term and large scale visions of advancing society’s sustainability agenda. This paper focuses on the processes of higher-order learning that occur through BSTEs. Based on the conceptual frameworks from theories of organizational learning, policy-oriented learning, and diffusion of innovation, we identify two types of learning: the first type occurs among the participants in the experiment and their immediate professional networks; the second type occurs in the society at large. Both types play a key role in the societal transition towards sustainable mobility systems. Two case studies, in which the Design for Sustainability Group at Technical University of Delft has participated, provide empirical data for the analysis. One...
Attitude Control of a Quadruped Trot While Turning
During a complete running stride, which involves significant periods of flight during which no legs are contacting the ground, a quadruped cannot employ static stability techniques. Instead, the corrective forces necessary to maintain dynamic stability must be applied during the short stance intervals inherent to high-speed running. Because of this complexity and the large coupled forces required to run, much of the research on the control of quadruped running has focused on planar systems which are not required to simultaneously control attitude in all three dimensions. The 3D trot controller presented here overcomes these and other complexities to control a trot up to 3.75 m/s, approximately 3 body lengths per second, and turning rates up to 20 deg/s. The biomimetic method of banking into a high-speed turn is also investigated here. Along with the details of the attitude control algorithm, a set of control principles for high-speed legged motion is presented. These principles, such as the need to counteract the disturbance of swing leg return and the usefulness of force redistribution during stance, are not dependent on a particular scale or actuation scheme and can be applied to a wider range of legged systems.
Robust fuzzy and recurrent neural network motion control among dynamic obstacles for robot manipulators
An integration of a fuzzy controller and modified Elman neural networks (NN) approximation-based computed-torque controller is proposed for motion control of autonomous manipulators in dynamic and partially known environments containing moving obstacles. The navigation technique of robot control using artificial potential fields is based on the fuzzy controller. The NN controller can deal with unmodeled bounded disturbances and or unstructured unmodeled dynamics of the robot arm. The NN weights are tuned online, with no off-line learning phase required. The stability of the closed-loop system is guaranteed by the Lyapunov theory. The purpose of the controller, which is designed as a neuro-fuzzy controller, is to generate the commands for the servo-systems of the robot so it may choose its way to its goal autonomously, while reacting in real-time to unexpected events. The proposed scheme has been successfully tested. The controller also demonstrates remarkable performance in adaptation to changes in manipulator dynamics. Sensor-based motion control is an essential feature for dealing with model uncertainties and unexpected obstacles in real-time world systems.
Non‐binary protograph low‐density parity‐check codes for space communications
SUMMARY#R##N##R##N#Protograph-based non-binary low-density parity-check (LDPC) codes with ultra-sparse parity-check matrices are compared with binary LDPC and turbo codes (TCs) from space communication standards. It is shown that larger coding gains are achieved, outperforming the binary competitors by more than 0.3 dB on the additive white Gaussian noise channel (AWGN). In the short block length regime, the designed codes gain more than 1 dB with respect to the binary protograph LDPC codes recently proposed for the next generation up-link standard of the Consultative Committee for Space Data Systems. Copyright © 2012 John Wiley & Sons, Ltd.
Coding theoretic approach to image segmentation
This paper introduces multi-scale tree-based approaches to image segmentation, using Rissanen's coding theoretic minimum description length (MDL) principle to penalize overly complex segmentations. Images are modelled as Gaussian random fields of independent pixels, with piecewise constant mean and variance. This model captures variations in both intensity (mean value) and texture (variance). Segmentation thus amounts to detecting changes in the mean and/or variance. One algorithm is based on an adaptive (greedy) rectangular recursive partitioning scheme. The second algorithm is an optimally pruned "wedgelet" decorated dyadic partitioning. We compare the two schemes with an alternative constant variance dyadic CART (classification and regression tree) scheme which accounts only for variations in mean, and demonstrate their performance on SAR images.
An empirical Bayes approach to inferring large-scale gene association networks
Motivation: Genetic networks are often described statistically using graphical models (e.g. Bayesian networks). However, inferring the network structure offers a serious challenge in microarray analysis where the sample size is small compared to the number of considered genes. This renders many standard algorithms for graphical models inapplicable, and inferring genetic networks an 'ill-posed' inverse problem.#R##N##R##N#Methods: We introduce a novel framework for small-sample inference of graphical models from gene expression data. Specifically, we focus on the so-called graphical Gaussian models (GGMs) that are now frequently used to describe gene association networks and to detect conditionally dependent genes. Our new approach is based on (1) improved (regularized) small-sample point estimates of partial correlation, (2) an exact test of edge inclusion with adaptive estimation of the degree of freedom and (3) a heuristic network search based on false discovery rate multiple testing. Steps (2) and (3) correspond to an empirical Bayes estimate of the network topology.#R##N##R##N#Results: Using computer simulations, we investigate the sensitivity (power) and specificity (true negative rate) of the proposed framework to estimate GGMs from microarray data. This shows that it is possible to recover the true network topology with high accuracy even for small-sample datasets. Subsequently, we analyze gene expression data from a breast cancer tumor study and illustrate our approach by inferring a corresponding large-scale gene association network for 3883 genes.#R##N##R##N#Availability: The authors have implemented the approach in the R package 'GeneTS' that is freely available from http://www.stat.uni-muenchen.de/~strimmer/genets/, from the R archive (CRAN) and from the Bioconductor website.#R##N##R##N#Contact: korbinian.strimmer@lmu.de
Analytical models for crosstalk excitation and propagation in VLSI circuits
The authors develop a general methodology to analyze crosstalk effects that are likely to cause errors in deep submicron high-speed circuits. They focus on crosstalk due to capacitive coupling between a pair of lines. Closed form equations are derived that quantify the severity of these effects and describe qualitatively the dependence of these effects on the values of circuit parameters, the rise/fall times of the input transitions, and the skew between the transitions. For noise propagation, they present a new way for predicting the output waveform produced by an inverter due to a nonsquare wave pulse at its input. To expedite the computation of the response of a logic gate to an input pulse, the authors have developed a novel way of modeling such gates by an equivalent inverter. The results of their analysis provide conditions that must be satisfied by a sequence of vectors used for validation of designs as well as post-manufacturing testing of devices in the presence of significant crosstalk. They present data to demonstrate accuracy of their results, including example runs of a test generator that uses these results.
Land use Dynamic Monitoring using Multi-Temporal SPOT Data in Beijing City from 1986 to 2004
Remote sensing dynamic monitoring of land use can detect the change information of land use and update the current land use map, which is important for rational utilization and scientific management to land resources. This paper discussed the technological procedure of land use dynamic monitoring, including the process of remote sensed images, the information classification and extraction of remote sensed imagery, and analysis of land use changes. Based on SPOT imagery data in three periods, the paper took Beijing city as an example, extracted the land use information during 1986-2004, and the land use changes were required in the period. The object-oriented method was used to extract information, and contrastive method after classification was used to confirm change zones.
VoiceXML and the W3C speech interface framework
VoiceXML is a markup language for creating voice-user interfaces. It uses speech and telephone touchtone recognition for input and prerecorded audio and text-to-speech synthesis (TTS) for output. It's based on the World Wide Web Consortium's (W3C's) Extensible Markup Language (XML) and leverages the Web paradigm for application development and deployment. By having a common language, application developers, platform vendors, and tool providers all can benefit from code portability and reuse. The paper discusses VoiceXML and the W3C speech interface framework.
Flexible ASIC: shared masking for multiple media processors
ASIC provides more than an order of magnitude advantage in terms of density, speed, and power requirement per gate. However, economic (cost of masks) and technological (deep micron manufacturability) trends favor FPGA as an implementation platform. In order to combine the advantages of both platforms and alleviate their disadvantages, recently a number of approaches, such as structured ASIC/regular fabrics, have been proposed. Our goal is to introduce an approach that has the same objective, but is orthogonal to those already proposed. The idea is to implement several ASIC designs in such a way that they share the datapath, memory structure, and several bottom layers of interconnect, while each design has only a few unique metal layers. We identified and addressed two main problems in our quest to develop a CAD flow for realization of such designs. They are: (i) the creation of the datapath, and (ii) the identification of common and unique interconnects for each design. Both problems are solved optimally using ILP formulations. We assembled a design flow platform using two new programs and the Trimaran and Shade tools. We quantitatively analyzed the advantages and disadvantages of the approach using the Mediabench benchmark suite.
Buy-at-Bulk Network Design with Protection
We consider approximation algorithms for buy-at-bulk network design, with the additional constraint that demand pairs be protected against edge or node failures in the network. In practice, the most popular model used in high speed telecommunication networks for protection against failures, is the so-called 1+1 model. In this model, two edge or node-disjoint paths are provisioned for each demand pair. We obtain the first non-trivial approximation algorithms for buy-at-bulk network design in the 1+1 model for both edge and node-disjoint protection requirements. Our results are for the single-cable cost model, which is prevalent in optical networks. More specifically, we present a constant-factor approximation for the single-sink case, and an O(log 3  n) approximation for the multi-commodity case. These results are of interest for practical applications and also suggest several new challenging theoretical problems.
Software-defined infrastructure and the Future Central Office
This paper discusses the role of virtualization and software-defined infrastructure (SDI) in the design of future application platforms, and in particular the Future Central Office (CO). A multi-tier computing cloud is presented in which resources in the Smart Edge of the network play a crucial role in the delivery of low-latency and data-intensive applications. Resources in the Smart Edge are virtualized and managed using cloud computing principles, but these resources are more diverse than in conventional data centers, including programmable hardware, GPUs, etc. We propose an architecture for future application platforms, and we describe the SAVI Testbed (TB) design for the Smart Edge. The design features a novel Software-Defined Infrastructure manager that operates on top of OpenStack and OpenFlow. We conclude with a discussion of the implications of the Smart Edge design on the Future CO.
Fisher Kernels for Handwritten Word-spotting
The Fisher kernel is a generic framework which combines the benefits of generative and discriminative approaches to pattern classification. In this contribution, we propose to apply this framework to handwritten word-spotting. Given a word image and a keyword generative model, the idea is to generate a vector which describes how the parameters of the keyword model should be modified to best fit the word image.This vector can then be used as the input of a discriminative classifier. We compare the performance of the proposed approach with that of a generative baseline on a challenging real-world dataset of customer letters. When the kernel used by the classifier is linear, the performance improvement is marginal but the proposed system is approximately 15 times faster than the baseline. If we use a non-linear kernel devised for this task, we obtain a 15\% relative reduction of the error but the detector is approximately 15 times slower.
Nonintrusive Load Monitoring and Diagnostics in Power Systems
This paper describes a transient event classification scheme, system identification techniques, and implementation for use in nonintrusive load monitoring. Together, these techniques form a system that can determine the operating schedule and find parameters of physical models of loads that are connected to an AC or DC power distribution system. The monitoring system requires only off-the-shelf hardware and recognizes individual transients by disaggregating the signal from a minimal number of sensors that are installed at a central location in the distribution system. Implementation details and field tests for AC and DC systems are presented.
False alarm reduction by improved filler model and post-processing in speech keyword spotting
This paper proposes four methods for improving the performance of keyword spotting (KWS) systems. Keyword models are usually created by concatenating the phoneme HMMs and garbage models consist of all phonemes HMMs. We present the results of investigations involving the use of skips in states of keyword HMMs and we focus on improving the hit ratio; then for false alarm reduction in KWS we model the words that are similar to keywords and we create HMMs for highly frequent words. These models help to improve the performance of the filler model. Two post-processing steps based on phoneme and word probabilities are used on the results of KWS to reduce the false alarms. We evaluate the performance of the improved keyword spotting in FarsDat corpus and compare the approaches. The presented techniques depict better performances than the popular KWS systems.
Automated Pathfinding tool chain for 3D-stacked integrated circuits: Practical case study
New technologies for manufacturing 3D Stacked ICs offer numerous opportunities for the design of complex and effcient embedded systems. But these technologies also introduce many design options at system/chip design level, hard to grasp during the complete design cycle. Because of the sequential nature of current design practices, designers are often forced to introduce design margins to meet required specications, resulting in sub-optimal designs. In this paper we introduce new design methodology and practical tool chain, called PathFinding Flow, that can help designers to easily trade-off between different system level design choices, physical design and/or technology options and understand their impact on typical design parameters such as cost, performance and power. Proposed methodology and the tool chain will be demonstrated on a practical case study, involving fairly complex Multi-Processor System-on-Chip using Network-on-Chip for communication medium. With this example we will show how High-Level Synthesis can be used to quickly move from high-level to RTL models, necessary for accurate physical prototyping for both computation and communication. We will also show how the possibility of design iteration, through the mechanism of feedback based on physical information from physical prototyping, can improve design performance. Finally, we will show how we can move in no time from traditional 2D to 3D design and how we can measure benets of such design choice.
Managing traceability information in manufacture
In this paper, an approach to design information systems for traceability is proposed. The paper applies gozinto graph modelling for traceability of the goods flow. A gozinto graph represents a graphical listing of raw materials, parts, intermediates and subassemblies, which a process transforms into an end product, through a sequence of operations. Next, the graphical listing has been translated into a reference data model that is the basis for designing an information system for tracking and tracing. Materials that are modelled this way represent production and/or purchase lots or batches. The composition of a certain end product is then represented through modelling all its constituent materials along with their intermediate relations. By registering all relations between sub-ordinate and super-ordinate material lots, a method of tracking the composition of the end product is obtained. When the entire sequence of operations required for manufacturing an end product adheres to this registering of relations, a multilevel bill of lots can be compiled. That bill of lots then, provides the necessary information to determine the composition of a material item out of component items. These composition data can be used to recall any items having consumed a certain component of specific interest (e.g., deficient), but also to certify product quality or to pro-actively adjust production processes to optimise the product quality in relation to its production characteristics (e.g., scarcity, costs or time).
PhD forum: Calibrating and using the global network of outdoor webcams
The vast imaging resources available via the Internet are underutilized. We propose to lay the foundation for the use of cameras attached to the Internet, also known as webcams, as free and flexible sensors. Developing an understanding of the relationship between signals in the world and the image variations they cause is critical to this effort. We use this understanding to develop methods to calibrate webcams, to estimate scene properties, and to report the weather.
Acquiring a Holistic Picture: The 4Screens Web-Based Simulator Helping Students to Unify Behaviours of Electronic Systems
Engineering knowledge is complex. Therefore, discipline concepts cannot usually be introduced to students all at once. Normally, individual models and behaviours are taught in several courses, by different teaching staff and in separate years of study. This often results in inadequate comprehension of the disciplinepsilas ldquobig picturerdquo - that is, the interrelationship of various concepts, models and behaviours. Students studying electronic circuit design, for example, often overlook important aspects of linear circuit performance. Their knowledge of the effects of steady-state and transient responses requires improved unification. Simple computer-based simulators can significantly help students to establish well-developed holistic views of systems in many engineering disciplines. The 4Screens Web-based simulator has helped hundreds of students in becoming more proficient with electronics systems. It has been developed by a group of undergraduates to make the Four screens model easy to utilise. A student using the simulator can simultaneously display four important characteristics of an electronic system on a computer screen: its algebraic transfer function H(s), systempsilas pole-zero plot, its Bode plots of magnitude and phase, as well as a graph of its time response to a unit step. Questionnaire responses and test performances over the last seven years confirm the effectiveness of the Four screens and the 4Screens Web-based simulator in improving student learning.
Benefits derived from restructuring the practical component of an introductory course in electronic communication systems
In an effort to address students' complaints regarding the tedious and frustrating nature of the practical component of the department's introductory communication course we embarked on a restructuring exercise. The restructuring of this component of the course shows promise based on preliminary results. The students are obtaining higher grades and gave the practical component a more favorable rating. Additionally the restructuring exercise allows for the addition of several more experiments enabling us to cover a broader portion of the course in the practical component. This paper looks at the changes that were made in the restructuring exercise and present preliminary results on the improvement in student's grade which can be attributed to this exercise. Over 16% of students are now obtaining grades over 80% compare to 0% before the restructuring exercise. Results from students' evaluations of the practical component of the course before and after the restructuring exercise is also presented, which shows an effective 24% improvement in the student rating.
Minimizing data and synchronization costs in one-way communication
Minimizing communication and synchronization costs is crucial to the realization of the performance potential of parallel computers. This paper presents a general technique which uses a global data-flow framework to optimize communication and synchronization in the context of the one-way communication model. In contrast to the conventional send/receive message-passing communication model, one-way communication is a new paradigm that decouples message transmission and synchronization. In parallel machines with appropriate low-level support, this may open up new opportunities not only to further optimize communication, but also to reduce the synchronization overhead. We present optimization techniques using our framework for eliminating redundant data communication and synchronization operations. Our approach works with the most general data alignments and distributions in languages like High Performance Fortran (HPF) and uses a combination of the traditional data-flow analysis and polyhedral algebra. Empirical results for several scientific benchmarks on a Cray T3E multiprocessor machine demonstrate that our approach is successful in reducing the number of data (communication) and synchronization messages, thereby reducing the overall execution times.
Pattern Mining in Visual Concept Streams
Pattern mining algorithms are often much easier applied than quantitatively assessed. In this paper we address the pattern evaluation problem by looking at both the capability of models and the difficulty of target concepts. We use four different data mining models: frequent itemset mining, k-means clustering, hidden Markov model, and hierarchical hidden Markov model to mine 39 concept streams from the a 137-video broadcast news collection from TRECVID-2005. We hypothesize that the discovered patterns can reveal semantics beyond the input space, and thus evaluate the patterns against a much larger concept space containing 192 concepts defined by LSCOM. Results show that HHMM has the best average prediction among all models, however different models seem to excel in different concepts depending on the concept prior and the ontological relationship. Results also show that the majority of the target concepts are better predicted with temporal or combination hypotheses, and there are novel concepts found that are not part of the original lexicon. This paper presents the first effort on temporal pattern mining in the large concept space. There are many promising directions to use concept mining to help construct better concept detectors or to guide the design of multimedia ontology.
Adaptive Goals for Self-Adaptive Service Compositions
Service compositions need to continuously self- adapt to cope with unexpected failures. In this context adaptation becomes a fundamental requirement that must be elicited along with the other functional and non functional requirements. Beside modelling, effective adaptation also demands means to trigger it at runtime as soon as the actual behavior of the composition deviates from stated requirements. This paper extends traditional goal models with adaptive goals to support continuous adaptation. Goals become live, runtime entities whose satisfaction level is dynamically updated. Furthermore, boundary infringement triggers adaptation capabilities. The paper also provides a methodology to trace goals onto the underlying composition, assess goals satisfaction at runtime, and activate adaptation consequently. All the key elements are demonstrated on the definition of the process to control an advanced washing machine.
Integrating Replenishment Decisions with Advance Demand Information
There is a growing consensus that a portfolio of customers with different demand lead times can lead to higher, more regular revenues and better capacity utilization. Customers with positive demand lead times place orders in advance of their needs, resulting inadvance demand information. This gives rise to the problem of finding effective inventory control policies under advance demand information. We show that state-dependent ( s, S) and base-stock policies are optimal for stochastic inventory systems with and without fixed costs. The state of the system reflects our knowledge of advance demand information. We also determine conditions under which advance demand information has no operational value. A numerical study allows us to obtain additional insights and to evaluate strategies to induce advance demand information.
A new construction for n-track (d, k) codes with redundancy
Digital magnetic and optical storage systems employing NRZI recording use (d, k) codes. The d-parameter specifies the minimum number of 0's occurring between 1's while the k-parameter specifies the maximum number of 0's between l's. The n-track (d,k) codes (denoted as (d,k;n) codes) are extensions of (d, k) codes for use in multiple-track systems. Instead of imposing each track to individually satisfy both constraints, (d,k;n) codes satisfy the d-constraint in each track individually while relaxing the k-constraint by allowing it to be satisfied jointly by the multiple tracks. Although (d,k;n) codes can provide significant capacity increases over (d, k) codes, they suffer from the fact that a single faulty track can cause loss of synchronization and hence, loss of the data on all tracks. Orcutt and Marcellin (see IEEE Trans. on Inform. Theory, Sept., 1993) introduced n-track (d,k) codes with a redundancy of r (denoted as (d,k;n,r) codes) which allow for r faulty tracks by mandating that all subsets of n-r tracks satisfy the joint k-constraint. We propose a new method to construct (d,k; n, r) codes. These codes have simple encoding and decoding schemes, gain a large part of the capacity increase possible when using (d,k; n,r) codes, and are considerably more robust to faulty tracks. >
Structure and reaction based evaluation of synthetic accessibility
De novo design systems provide powerful methods to suggest a set of novel structures with high estimated binding affinity. One deficiency of these methods is that some of the suggested structures could be synthesized only with great difficulty. We devised a scoring method that rapidly evaluates synthetic accessibility of structures based on structural complexity, similarity to available starting materials and assessment of strategic bonds where a structure can be decomposed to obtain simpler fragments. These individual components were combined to an overall score of synthetic accessibility by an additive scheme. The weights of the scoring function components were calculated by linear regression analysis based on accessibility scores derived from medicinal chemists. The calculated values for synthetic accessibility agree with the values proposed by chemists to an extent that compares well with how chemists agree with each other.
Evolution of Protection Technologies in Metro Core Optical Networks
The market of metro optical networking has increased rapidly over the last few years. Traditional telecommunication infrastructure has an emphasis on long-haul optical transmission with ultra broadband capacity, relying mostly on large pure Dense Wavelength Division Multiplexing (DWDM) systems. Today, however, metro core optical networks take the major role in provisioning local access services and interconnecting service points of presences (POPs) with long-haul transmission. This represents a pivotal point in business operations of data communication services for service providers and large enterprises. In addition, the upper layer data services completely leans upon the substrate wavelength communication, and hence the survivability and reliability issues in the optical domain are now becoming crucial topics. This paper provides a detailed discussion around the development process of protection technologies in metro core optical transport infrastructure.
Packet radio and the factory of the future
Progress is reported in the design and analysis of spread-spectrum packet radio networks for the factory of the future. This progress includes the accurate analysis of the probability of packet success in a direct-sequence spread-spectrum communication channel and analytical results on the transient behavior of packet radio networks. The first use of these networks will be to provide communications between a transport system controller and a fleet of autonomous guided vehicles. >
Manufacturing decision making with FACTOR
The FACTOR system is designed to support the effective management of the capacity of manufacturing organization. This philosophy is best described as total capacity management (TCM). The TCM fundamental principle suggests that through a thorough understanding of a system's capacity and the ability to control that capacity, a manufacturing system can profitably and predictably deliver quality products to its customers. This tutorial covers the basic concepts of FACTOR. FACTOR has been applied to engineering, design, scheduling and planning problems within many manufacturing organizations. Topics covered include: the FACTOR modeling constructs, integration with existing production data, the use of FACTOR for schedule creation and adjustment, FACTOR/AIM, and new enhancements to the products.
Learning with Technology: Using Discussion Forums to Augment a Traditional-Style Class
There is considerable evidence that using technology as an instructional tool improves student learning and educational outcomes (Hanna & de Nooy, 2003). In developing countries, pre-university education focuses on memorization, although meting the mission of AUST requires students to manage technology and to think more independently. This study examines the impact of incorporating a discussion forum on the achievement of university students enrolled in a Distance Education course, Educational Technology Department at Ajman University of Science and Technology (AUST), United Arab Emirates. The study was conducted with 34 students divided into two sections, one a treatment group and one a control group. Both sections were exposed to the same teaching techniques covering the same course material on Distance Education. Four weeks after the course had commenced they were given the same teacher constructed test. However, after the first test, the treated group was exposed to the use of a World Wide Web (WWW) interactive discussion forum. At the end of the semester-long treatment period, a final test was given to both groups, and student scores were analyzed for any statistically significant difference. Questionnaires and interviews were also conducted to see if students had enjoyed the experience. The results of the study indicated that students in both groups showed learning improvement over the course of one semester, but discussion forums had an obvious impact on student achievement and attitude in distance learning/ educational technology course.
CCHMM_PROF: a HMM-based Coiled-Coil Predictor with Evolutionary Information
Motivation: The widespread coiled-coil structural motif in proteins is known to mediate a variety of biological interactions. Recognizing a coiled-coil containing sequence and locating its coiled-coil domains are key steps towards the determination of the protein structure and function. Different tools are available for predicting coiled-coil domains in protein sequences, including those based on positionspecific score matrices and machine learning methods. Results: In this article, we introduce a hidden Markov model (CCHMM_PROF) that exploits the information contained in multiple sequence alignments (profiles) to predict coiled-coil regions. The new method discriminates coiled-coil sequences with an accuracy of 97% and achieves a true positive rate of 79% with only 1% of false positives. Furthermore, when predicting the location of coiled-coil segments in protein sequences, the method reaches an accuracy of 80% at the residue level and a best per-segment and perprotein efficiency of 81% and 80%, respectively. The results indicate that CCHMM_PROF outperforms all the existing tools and can be adopted for large-scale genome annotation. Availability: The dataset is available at http://www.biocomp.unibo .it/∼lisa/coiled-coils. The predictor is freely available at http://gpcr .biocomp.unibo.it/cgi/predictors/cchmmprof/pred_cchmmprof.cgi. Contact: piero@biocomp.unibo.it
Performance of local area network protocols for hard real-time applications
Simulation experiments show that the token ring protocol gave a lower average message delay at low transfer rates, but the token bus protocol gave a better overall performance for applications where only average delay is of interest. On the other hand, in hard real-time systems, the criterion of importance is not the average message delay, but the maximum message delay and the ability to meet deadlines. Slotted ring in this case is a much better protocol than the others because of its low maximum message delay and more predictable message delay. Because of this, and because the average performance of the slotted ring remains good as the size or the transfer rate of the network increases, the slotted ring protocol is preferred over the token ring and token bus protocols for hard real-time systems. >
A simple algorithm for decomposing convex structuring elements
A finite subset of Z/sup 2/ is called a structuring element. The paper presents a new and simple algorithm for decomposing a convex structuring element as a sequence of Minkowski additions of a minimum number of subsets of the elementary square (i.e., the 3/spl times/3 square centered at the origin). Besides its simplicity, the advantage of this algorithm over some known algorithms is that it generates a sequence of non necessarily convex subsets, which means subsets with smaller cardinality and consequently faster implementation of the corresponding dilations and erosions. The algorithm is based on algebraic and geometrical properties of Minkowski additions. Theoretical analysis of correctness and computational time complexity are also presented.
A BIST pattern generator design for near-perfect fault coverage
A new design methodology for a pattern generator is proposed, formulated in the context of on-chip BIST. The design methodology is circuit-specific and uses synthesis techniques to design BIST generators. The pattern generator consists of two components: a pseudorandom pattern generator (like an LFSR or, preferably, a GLFSR) and a combinational logic to map the outputs of the pseudorandom pattern generator. This combinational logic is synthesized to produce a given set of target patterns by mapping the outputs of the pseudorandom pattern generator. It is shown that, for a particular CUT, an area-efficient combinational logic block can be designed/synthesized to achieve 100 (or almost 100) percent single stuck-at fault coverage using a small number of test the This method is significantly different from weighted pattern generation and can guarantee testing of all hard-to-detect faults without expensive test point insertion. Experimental results on common benchmark netlists demonstrate that the fault coverage of the proposed pattern generator is significantly higher compared to conventional pattern generation techniques. The design technique for the logic mapper is unique and can be used effectively to improve existing pattern generators for combinational logic and scan-based BIST structures.
MODEL OF CARDIAC TISSUE AS A CONDUCTIVE SYSTEM WITH INTERACTING PACEMAKERS AND REFRACTORY TIME
The model of the cardiac tissue as a conductive system with two interacting pacemakers and a refractory time is proposed. In the parametric space of the model the phase locking areas are investigated in detail. The obtained results make possible to predict the behavior of excitable systems with two pacemakers, depending on the type and intensity of their interaction and the initial phase. Comparison of the described phenomena with intrinsic pathologies of cardiac rhythms is given.
The Systolic Pixel: A Visible Surface Algorithm for VLSI.
Abstract#R##N##R##N#The Systolic Pixel or Spixel is a novel architecture for an intelligent pixel-based graphics database for geometric-solid models. An algorithm is described which performs visible surface calculations for any complexity of coloured 3-dimensional (3-D) surface and which structures geometric-solid model data in a natural way. The algorithm/architecture of the spixel features a simple set of priority rules acting upon data in nearest neighbour locations and a simple set of movement rules of data to nearest neighbour locations. The spixel is constructed out of identical functional units. These features are attractive for an implementation of the algorithm in Very Large Scale Integration (VLSI).
Generating Case Markers in Machine Translation
We study the use of rich syntax-based statistical models for generating grammatical case for the purpose of machine translation from a language which does not indicate case explicitly (English) to a language with a rich system of surface case markers (Japanese). We propose an extension of n-best re-ranking as a method of integrating such models into a statistical MT system and show that this method substantially outperforms standard n-best re-ranking. Our best performing model achieves a statistically significant improvement over the baseline MT system according to the BLEU metric. Human evaluation also confirms the results.
Soft-limiter receivers for coded DS/DPSK systems
The performance of a soft-limiter metric and a quantized soft-limiter metric is evaluated for coded DS/DPSK (direct sequence/differential phase shift keying) in the presence of worst case pulse jamming and background noise. The metrics are easy to implement and do not require jammer state information. Instead they rely on the use of receiver thresholds, which must be adjusted according to the code rate and the received bit-energy-to-background-noise ratio. The performance of the metrics is evaluated by using the cutoff rate criterion and a number of specific convolutional and block codes. It is shown that the metrics can offer a significant soft-decision decoding gain and can perform to within 0.5-1.5 dB of the maximum-likelihood soft-decision metric with perfect jammer state information. >
Technology Dependence In Function Point Analysis: A Case Study And Critical Review
Because Function Point Analysis (FPA) has now been in use for a decade, and in spite of its increasing popularity has met with some recent criticisms, it is time to review how appropriate it still is for today's technologies. A critical review of the FPA approach examines in particular the pioneering and continuing work of Albrecht and more recent work by Symons. Technological dependencies in FPA-type metrics are identified and a general model for deriving a new FPA-type metric for a new software technology is given. A model for the calibration of FPA-type metrics for new technologies in terms of a reference technology is also presented. Such calibration is essential for comparative productivity studies. The role of module estimation in exposing parts of the 'anatomy' of the FPA approach is investigated. The derivation and calibration models are applied to a significant case study in which a new FPA-type metric suited to a particular software development technology is derived, calibrated and compared with other published versions of FPA metrics.
Pruning recurrent neural networks for improved generalization performance
The experimental results in this paper demonstrate that a simple pruning/retraining method effectively improves the generalization performance of recurrent neural networks trained to recognize regular languages. The technique also permits the extraction of symbolic knowledge in the form of deterministic finite-state automata (DFA) which are more consistent with the rules to be learned. Weight decay has also been shown to improve a network's generalization performance. Simulations with two small DFA (/spl les/10 states) and a large finite-memory machine (64 states) demonstrate that the performance improvement due to pruning/retraining is generally superior to the improvement due to training with weight decay. In addition, there is no need to guess a 'good' decay rate. >
Adaptive design of digital filters
In this paper, we present a novel technique for the design of FIR and IIR digital filters. The design approach begins with the specification of a discrete set of arbitrary magnitude and phase characteristics which describe a desired filter response. These frequency domain characteristics are used to create an ideal "pseudo-filter" whose impulse response is unknown and possibly non-causal, but whose input/output characteristics can be determined for a finite sum of sinusoids. Time-domain techniques common to adaptive system identification are then used to identify a realizable FIR or IIR digital filter which best matches the pseudo-filter. The advantages of this method include the ability to specify response at arbitrarily-spaced frequencies, to use arbitrary cost weighting, and to apply (possibly non-linear) constraints to the range of the filter coefficients.
Experimental Examination in simulated interactive situation between people and mobile robot with preliminary-announcement and indication function of upcoming operation
This paper presents the result of the experimental examination by "passing each other" and "positional prediction" in simulated interactive situation between people and mobile robot. We have developed four prototype robots based on four proposed methods for preliminarily announcing and indicating to people the speed and direction of upcoming movement of mobile robot moving on two-dimensional plane. We observed significant difference between when there was a preliminary-announcement and indication (PAI) function and when there was not even in each experiment. Therefore the effect of preliminary-announcement and indication of upcoming operation was declared. In addition the feature and effective usage of each type of preliminary-announcement and indication method were clarified. That is, the method of announcing state of operation just after the present is effective when a person has to judge to which direction he should get on immediately due to the feature that simple information can be quickly transmitted. The method of indicating operations from the present to some future time continuously is effective when a person wants to avoid contact or collision surely and correctly owing to the feature that complicated information can be accurately transmitted. We would like to verify the result in various conditions such as the case that traffic lines are obliquely crossed.
Opportunistic Beamforming over Rayleigh Channels with Partial Side Information
In recent years, diversity techniques have evolved into highly attractive technology for wireless communications in different forms. For instance, the channel fluctuations of the users in a network are exploited as multiuser diversity by scheduling the user with the best signal-to-noise ratio (SNR). When fading is slow, beamforming at a multiple antenna transmitter is used to induce artificial channel fluctuations to ensure multiuser diversity in the network. Such a beamforming scheme is called opportunistic beamforming since the transmitter uses random beamforming to artificially induce opportunism in the network [1]. Opportunism requires a large number of users in the system in order to reach the performance of the true beamforming that uses perfect channel state information (CSI). In this paper we investigate the benefit of having partial CSI at an opportunistic transmitter. In the investigation, we focus on the maximum normalized SNR scheduling where user's feedback consists of SNR relative to its channel gain. We show that opportunism can be beneficially used to increase the average throughput of the system. Simulations support the analytical average throughput results obtained as the amount of CSI and the number of users vary.
An ILP formulation for system level throughput and power optimization in multiprocessor SoC architectures
System-level low power scheduling techniques are required for optimizing the performance and power of embedded applications that are mapped to multiprocessor System-on-Chip (SoC) architectures. In this paper, we present an integer linear programming (ILP) formulation that combines loop transformations (pipelining and unrolling) and system-level low power optimization techniques (dynamic voltage scaling (DVS) and power management (DPM)) to minimize the power consumption, while satisfying the period and deadline constraints of the application. We also present three modifications that relax one or more constraints in the optimal formulation in order to obtain smaller run times. We present experimental analysis by applying the formulations on an MPEG decoder algorithm. All results are compared against two existing techniques. Our formulations result in large system-level power reductions (max: 48.2%, min: 15.92%, avg: 31.9%). The modified ILP formulations result in exponential decrease in runtimes, and a corresponding linear degradation in the result quality.
Enhancement of semantics in CBIR
Although much research has been done in the area of content based image retrieval (CBIR), little progress has been made to fully implement an engine solely based on the search of image content. This paper examines one of the basic problems in pattern recognition which highlights the difficulty in the area of content understanding in CBIR, i.e. the inability of current systems to fully incorporate low level features of image, such as intensity, colour, texture, shape and spatial constraints characteristics, with the high level features such as semantic content. To further the development of content based image processing, semantic algorithms should be combined with low level features and be used to process the image objects.
MASISH: a database for gene expression in maize seeds
Grass seeds are complex organs composed by multiple tissues and cell types which develop coordinately to produce a viable embryo. The identification of genes involved in seed development is of great interest, but systematic spatial analyses of gene expression on maize seeds at the cell level have not yet been performed. MASISH is an online database holding information for gene expression spatial patterns in maize seeds based on in situ hybridization experiments. The web-based query interface allows the execution of gene queries and provides hybridization images, published references and information of the analyzed genes. Availability: http://masish.uab.cat/ Contact: cvsgmp@cid.csic.es The maize kernel is classified botanically as a caryopsis. In consequence it is a fruit composed by one seed and the remnants of the seed coats and nucellus and is permanently enclosed in the pericarp. The endosperm occupies most of the seed and is basically a storage organ that accumulates starch and proteins. The aleurone layer is part of the endosperm and consists in a continuous layer of large cubical cells which accumulate protein and lipid granules and surrounds most of the endosperm. In the area of the pedicel, which connects the seed to the mother plant, the cells adopt a special morphology, typical of transfer cells, and form the basal transfer cell layer. The embryo consists of an embryonic axis and a single cotyledon, which is called the scutellum. The embryo axis is formed by the plumule, covered by the coleoptile and the radicle, covered by coleorhiza. All these organs are almost completely surrounded by the scutellum, an organ whose major function is to accumulate nutrient reserves, mainly lipids and proteins. A single layer of cells directly in contact with the endosperm, which is called the scutellar epithelium, is important in the digestion and transport of the nutrients from the endosperm to the embryo axis during germination. Both endosperm and embryo derive from the fusion of gametes, but while the embryo is derived from the fertilized egg, triploid endosperm is derived from fertilized polar nuclei. Surrounding the endosperm and embryo lays the pericarp, a protective organ derived from maternal tissues (more information at http://masish.uab.cat/masish/images/maizeseedanatomy.pdf). Full genome sequencing allows the identification of the complete catalog of genes in a species. However, the roles of a high proportion of these genes remain unknown. The description of
Unified Blind Method for Multi-Image Super-Resolution and Single/Multi-Image Blur Deconvolution
This paper presents, for the first time, a unified blind method for multi-image super-resolution (MISR or SR), single-image blur deconvolution (SIBD), and multi-image blur deconvolution (MIBD) of low-resolution (LR) images degraded by linear space-invariant (LSI) blur, aliasing, and additive white Gaussian noise (AWGN). The proposed approach is based on alternating minimization (AM) of a new cost function with respect to the unknown high-resolution (HR) image and blurs. The regularization term for the HR image is based upon the Huber-Markov random field (HMRF) model, which is a type of variational integral that exploits the piecewise smooth nature of the HR image. The blur estimation process is supported by an edge-emphasizing smoothing operation, which improves the quality of blur estimates by enhancing strong soft edges toward step edges, while filtering out weak structures. The parameters are updated gradually so that the number of salient edges used for blur estimation increases at each iteration. For better performance, the blur estimation is done in the filter domain rather than the pixel domain, i.e., using the gradients of the LR and HR images. The regularization term for the blur is Gaussian (L2 norm), which allows for fast noniterative optimization in the frequency domain. We accelerate the processing time of SR reconstruction by separating the upsampling and registration processes from the optimization procedure. Simulation results on both synthetic and real-life images (from a novel computational imager) confirm the robustness and effectiveness of the proposed method.
Synthesis of multi-qudit hybrid and d-valued quantum logic circuits by decomposition
Recent research in generalizing quantum computation from 2-valued qudits to d-valued qudits has shown practical advantages for scaling up a quantum computer. A further generalization leads to quantum computing with hybrid qudits where two or more qudits have different finite dimensions. Advantages of hybrid and d-valued gates (circuits) and their physical realizations have been studied in detail by Muthukrishnan and Stroud [Multi-valued logic gates for quantum computation, Phys. Rev. A 62 (2000) 052309. [10]], Daboul et al. [Quantum gates on hybrid qudits, J. Phys. A Math. Gen. 36 (2003) 2525-2536. [5]], and Bartlett et al. [Quantum encodings in spin systems and harmonic oscillators, Phys. Rev. A 65 (2002) 052316. [17]]. In both cases, a quantum computation is performed when a unitary evolution operator, acting as a quantum logic gate, transforms the state of qudits in a quantum system. Unitary operators can be represented by square unitary matrices. If the system consists of a single qudit, then Tilma et al. [Generalized Euler angle parameterization for SU(N), J. Phys. A Math. Gen. 35 (2002) 10467-10501. [15]] have shown that the unitary evolution matrix (gate) can be synthesized in terms of its Euler angle parametrization. However, if the quantum system consists of multiple qudits, then a gate may be synthesized by matrix decomposition techniques such as QR factorization and the cosine-sine decomposition (CSD). In this article, we present a CSD based synthesis method for n qudit hybrid quantum gates, and as a consequence, derive a CSD based synthesis method for n qudit gates where all the qudits have the same dimension.
Point location in zones of k -flats in arrangements
Abstract   Let  A ( H ) be the arrangement of a set H of n hyperplanes in  d -space. A  k -flat is a  k -dimensional affine subspace of  d -space. The  zone  of a  k -flat f with respect to H is the set of all faces in  A ( H ) that intersect f. this paper we study some problems on zones of  k -flats. Our most important result is a data structure for point location in the zone of a  k -flat. This structure uses   O  (n      ⌊  d  2  ⌋+e    +n     k+e   )   preprocessing time and space and has a query time of O(log 2   n ). We also show how to test efficiently whether two flats are visible from each other with respect to a set of hyperplanes. Then point location in m faces in arrangements is studied. Our data structure for this problem has size   O  (n      ⌊  d  2  ⌋+e    m      ⌈  d  2  ⌉  d    )   and the query time is O(log 2   n ).
Dynamic optimal battery array management in high energy density fuel cell/battery hybrid power source
The goal of this paper is to address the problem of dynamic optimal battery array management to extend the life of battery array used in the hybrid power source. Unlike previous efforts which mainly solve the problem using the off-line optimization, our design addresses the problem using the idea of “feedback”. The proposed approach can handle the possible dynamic uncertainty of the hybrid power source introduced by battery failure or insert of a new battery. The detailed battery model and the converter model are derived to facilitate development of dynamic optimal management algorithm. The other goal of this paper is to call possible attention of the control community in potential contribution for newest smart grid technology.
Expectation-maximization approach to Boolean factor analysis
Methods for hidden structure of high-dimensional binary data discovery are one of the most important challenges facing machine learning community researchers. There are many approaches in literature that try to solve this hitherto rather ill-defined task. In the present study, we propose a most general generative model of binary data for Boolean factor analysis and introduce new Expectation-Maximization Boolean Factor Analysis algorithm which maximizes likelihood of Boolean Factor Analysis solution. Using the so-called bars problem benchmark, we compare efficiencies of Expectation-Maximization Boolean Factor Analysis algorithm with Dendritic Inhibition neural network. Then we discuss advantages and disadvantages of both approaches as regards results quality and methods efficiency.
Bridgeless SEPIC PFC Rectifier With Reduced Components and Conduction Losses
In this paper, a new bridgeless single-ended primary inductance converter power-factor-correction rectifier is introduced. The proposed circuit provides lower conduction losses with reduced components simultaneously. In conventional PFC converters (continuous-conduction-mode boost converter), a voltage loop and a current loop are required for PFC. In the proposed converter, the control circuit is simplified, and no current loop is required while the converter operates in discontinuous conduction mode. Theoretical analysis and simulation results are provided to explain circuit operation. A prototype of the proposed converter is realized, and the results are presented. The measured efficiency shows 1% improvement in comparison to conventional SEPIC rectifier.
Spurious-Free Dynamic Range of a Uniform Quantizer
Quantization plays an important role in many systems where analog-to-digital conversion and/or digital-to-analog conversion take place. If the quantization error is correlated with the input signal, then the spectrum of the quantization error will contain spurious peaks. Although analytical formulas describing this effect exist, numerical evaluation can take much effort. This brief provides approximations for the spurious-free dynamic range (SFDR) of a uniform quantizer with a single sinusoidal input, with and without additive Gaussian noise. It is shown that the SFDR increases by approximately 8 dB/bit, in case there is no noise. Generalizing this result to multitone inputs results in an additional 2 dB/bit per additional tone. Additive Gaussian noise decorrelates the sinusoid(s) and the quantization error, which results in a dramatic increase in SFDR.
Binding time in distributed shared memory architectures
The paper revisits three distributed shared memory (DSM) architectures to clarify them with their binding times for new addresses at the local memory: page fault time, node miss time, and cache miss time. The DSM architectures which have different binding times arrange data in different ways with different overheads at an event of reference. Since a large number of cache misses can occur in a large (relative to the cache size) working set, binding at the page fault time alone cannot efficiently utilize locality of reference at the local memory. In a small working set, most of the addresses bound to the local memory at a node miss time are not effective due to the low cache miss rate. The paper shows that binding at the cache miss time can improve system performance.
Reasoning about Human Intention Change for Individualized Runtime Software Service Evolution
While software evolution has been studied extensively in software engineering, few of these efforts have involved a systematic exploration of human epistemological attitudes, such as human desire and intention, as the driving force of software service evolution. Our work proposes a theoretical framework to monitor and reason about human intention and its changes, which in turn can be used to determine how software and services should evolve to be individualized and better serve each user. Extending the Situ framework, we explore the service satisfiability problem through sub-world coverage following Kripke semantics, which enjoys wide application in AI and other fields related to human epistemic reasoning.
A comparative assessment of Web accessibility and technical standards conformance in four EU states
The Internet is playing a progressively more important part in our day–to–day life, through its power of making information universally available. People with disabilities have particular opportunities to benefit. Using the Internet in conjunction with dedicated assistive technologies, tasks that were very difficult if not impossible to achieve for people with various types of disability can now be made fully accessible — at least, in principle. However, in practice, many online resources and services are still poorly accessible to those with disability due to unsatisfactory Web content design.#N##N#Design of accessible Web content is codified in standards and guidelines of the World Wide Web Consortium (W3C). Conformance with W3C’s Web Content Accessibility Guidelines 1.0 (WCAG) (and/or similar, derivative guidelines) is now the subject of considerable activity, both legal and technical, in many different jurisdictions.#N##N#This paper presents results of a comparative survey of Web accessibility guidelines and HTML standards conformance for samples of Web sites drawn from Ireland, the United Kingdom, France and Germany. It also gives some recommendations on how to improve the accessibility level of Web content.#N##N#A particular conclusion of the study is that the general level of Web accessibility guidelines and HTML standards conformance in all of the samples studied is very poor; and that the pattern of failure is strikingly consistent in the four samples. Although considerable efforts are being made to promote Web accessibility for users with disabilities, this is certainly not yet manifesting itself in improving Web accessibility and HTML validity.
Evolution of a four wheeled active suspension rover with minimal actuation for rough terrain mobility
In this paper we deduce the evolution of a four wheeled active suspension rover from a five wheeled passive suspension rover. The aim of this paper is to design a suspension mechanism which utilizes the advantages of both passive suspension and active suspension rover. Both the design considered here are simpler than the existing suspension mechanisms in the sense that the number of links as wells as the number of joints have been significantly reduced without compromising the climbing capability of the rover. We first analyze the kinematics of the five wheeled rover and its motion pattern while climbing an obstacle and try to deduce the same motion pattern and capability in the four wheeled rover. Both the suspension mechanism consists of two planar closed kinematic chains on each side of the rover. We also deduce the control strategy for the active suspension rover wherein only two actuators are used to control the internal configuration of the rover. To the best of author's knowledge this is the minimum number of actuators required to control the internal configuration of a active suspension while operating on a fully 3D rough terrain. Extensive uneven terrain simulations are performed for both 5-wheeled and 4-wheeled rover and a comparative analysis has been done on maximum coefficient of friction and torque requirements.
Data fusion algorithms for network anomaly detection: classification and evaluation
In this paper, the problem of discovering anomalies in a large-scale network based on the data fusion of heterogeneous monitors is considered. We present a classification of anomaly detection algorithms based on data fusion, and motivated by this classification, the operational principles and characteristics of two different representative approaches, one based on the Demster-Shafer theory of evidence and one based on principal component analysis, are described. The detection effectiveness of these strategies are evaluated and compared under different attack scenarios, based on both real data and simulations. Our study and corresponding numerical results revealed that in principle the conditions under which they operate efficiently are complementary, and therefore could be used effectively in an integrated way to detect a wider range of attacks..
A Method for Real-Time Identification of Malformed BGP Messages
The BGP routing system is one of the key component of today's Internet infrastructure responsible for carrying data traffic across different Autonomous Systems (ASes). Recently, malformed BGP messages have become a threat to the operational community as they repeatedly cause BGP session resets until identified. However, the identification of the message itself is often difficult in large ISP networks. In this paper, we propose a novel method for real-time identification of these messages by using passively collects BGP messages. Our method focuses on the frequency of observed attributes and values of prefixes advertised by each AS. Based on our heuristics that common attributes are observed at similar time scale, we periodically measure the usage frequency of attributes from BGP messages observed in real-time and mark attributes and values used by minority of the AS as suspicious. We verify the efficiency of our method using BGP data obtained from operational networks.
Link Characteristics Measuring in 2.4 GHz Body Area Sensor Networks
With the increasing demands on the remote healthcare and the rich human-machine interacting, body area sensor network (BASN) has been attracting more and more attention. In practice, understanding the link performance and its dynamics in the emerging BASN applications is very important to design reliable, real-time, and energy-efficient protocols. In this paper we study the link characteristics of body area sensor network (BASN) through extensive experiments with very realistic configurations. We evaluate the packet reception ratio, RSSI, LQI, and movement intensity of body under indoor and outdoor environments, all of which can provide direct insights to practical account.
Implicit spatial inference with sparse local features
This paper introduces a novel way to leverage the implicit geometry of sparse local features (e.g. SIFT operator) for the purposes of object detection and segmentation. A two-class Bayesian scheme is used as a framework, and the likelihood is derived from the real-valued classification of machine learning algorithm Gentle AdaBoost, whose output is transformed to a probabilistic distribution using either of two models investigated; Log-Sigmoid or Bi-Gaussian. The main contribution is a novel scheme for the injection of prior contextual spatial information. This occurs on a uniquely designed Markov Random Field defined by Delaunay Tri- angulation of the feature points. Our experiments show that this framework is useful for object detection and segmentation, and we achieve good, mostly invariant results in these tasks.
A geometrical framework for the determination of ambiguous directions in subspace methods
In signal subspace parameter estimation techniques, like MUSIC, degradations may occur due to parasite peaks in the spectrum, which may be connected to high sidelobes in the beam pattern or to ambiguities themselves. This paper studies the presence of ambiguities in an array of given planar geometry. We propose a general framework for the analysis and thus we obtain a generalisation of results published by Lo and Marple (1992) and by Proukakis and Manikas (see Proc. ICASSP'94, vol.4, p.549-52, 1994) for rank one and two ambiguities. For rank k/spl ges/3 ambiguities the study is restricted to linear arrays, for which we derive original and synthetic results. We present a geometrical construction that is able to determine all the ambiguous directions which can appear for a given linear array. The method allows determination of any rank ambiguities and for each ambiguous direction set, the rank of ambiguity is obtained. The search is exhaustive. Application of the method requires no assumption for the linear array and is easy to implement. An example is detailed for a non-uniform linear array.
Nonrigid Intraoperative Cortical Surface Tracking Using Game Theory
During neurosurgery, nonrigid brain deformation prevents preoperatively acquired images from accurately depicting the intraoperative brain. Stereo vision systems can be used to track cortical surface deformation and update preoperative brain images in conjunction with a biomechanical model. However, these stereo systems are often plagued with calibration error, which can corrupt the deformation estimation. In order to decouple the effects of camera calibration and surface deformation, a framework is needed which can solve for disparate and often competing variables. Game theory, which was developed specifically to handle decision making in this type of competitive environment, has been applied to various fields from economics to biology. In this paper, we apply game theory to cortical surface tracking and use it to infer information about the physical processes of brain deformation and image acquisition.
A quality control mechanism for networked virtual reality system with video capability
Introduction of motion video including live video into networked virtual reality systems makes virtual spaces more attractive. To handle live video in networked virtual reality systems based on VRML, the scalability of networked virtual reality systems becomes very important on the Internet where the performance of the network and the end systems varies dynamically. We propose a new quality control mechanism suitable for networked virtual reality systems with live video capability. Our approach is to introduce the notion of the importance of presence (IoP) which represents the importance of objects in virtual spaces. According to IoP, the degree of the deterioration of object presentation will be determined in case of the starvation of system resources.
50th Anniversary Article: The Evolution of Research on Information Systems: A Fiftieth-Year Survey of the Literature in Management Science
The development of the information systems (IS) literature inManagement Science during the past 50 years reflects the inception, growth, and maturation of several different research streams. The five research streams we identify incorporate different definitions of the managerial problems that relate to IS, the alternate theoretical perspectives and different methodological paradigms to study them, and the levels of the organization at which their primary results impact managerial practice. Thedecision support and design science research stream studies the application of computers in decision support, control, and managerial decision making. Thevalue of information research stream reflects relationships established based on economic analysis of information as a commodity in the management of the firm. Thehuman-computer systems design research stream emphasizes the cognitive basis for effective systems design. TheIS organization and strategy research stream focuses the level of analysis on the locus of value of the IS investment instead of on the perceptions of a system or its user. Theeconomics of information systems and technology research stream emphasizes the application of theoretical perspectives and methods from analytical and empirical economics to managerial problems involving IS and information technologies (IT). Based on a discussion of these streams, we evaluate the IS literature's core contributions to theoretical and managerial knowledge, and make some predictions about the road that lies ahead for IS researchers.
Transient Signal Detection with Neural Networks: The Search for the Desired Signal
Matched filtering has been one of the most powerful techniques employed for transient detection. Here we will show that a dynamic neural network outperforms the conventional approach. When the artificial neural network (ANN) is trained with supervised learning schemes there is a need to supply the desired signal for all time, although we are only interested in detecting the transient. In this paper we also show the effects on the detection agreement of different strategies to construct the desired signal. The extension of the Bayes decision rule (0/1 desired signal), optimal in static classification, performs worse than desired signals constructed by random noise or prediction during the background.
Recursive construction and evolution of collaborative business processes
Virtual Enterprises (VEs) bring together expertise and processes of different companies to react to a market opportunity.#N#Here we propose a novel approach to support the collaborative construction and evolution of such VEs and their business processes,#N#comprising a model of the VE, and a set of model construction rules and operators. Our approach is based on the principles#N#of iterative elaboration, devolved decision-making and situatedness, and achieves flexibility by treating the processes of#N#work, coordination and selection in a uniform manner. We argue that certain assumptions behind existing approaches make them#N#unsuitable to the business practices we observed in the target business ecosystem. We then show how the proposed approach#N#can underpin software support for informal business practices of collaborative process construction by manufacturing SMEs.
Workspace of a six-revolute decoupled robot manipulator
In this paper we study the working space of a six-revolute decoupled robot manipulator. A simple and direct method is presented to obtain the boundaries of the total and primary workspace. The technique is based on finding the limit configurations of the general geometry positioning mechanism of the decoupled manipulator. In order to do this we derive a fourth-order displacement equation in the first joint variable. It is shown that the method only requires the simultaneous solution of two second-order nonlinear equations.
The rise and fall of an executive information system: a case study
The progress of an executive information system project within a manufacturing organization over a period of 9 years is described. The case study illustrates the importance of the interaction between the business environment, the organizational environment and the perceptions and interpretations of events and facts by stakeholders on the success or failure of an information system. It shows the importance of context in the development and implementation of an executive information system and the dynamic nature of the influence of social, economic and technical factors. The reasons for the initial success and the subsequent failure of the EIS within the company are explored from a contingency perspective.
Adapting the eBlock Platform for Middle School STEM Projects: Initial Platform Usability Testing
The benefits of project-based learning environments are well documented; however, setting up and maintaining these environments can be challenging due to the high cost and expertise associated with these platforms. To alleviate some of these roadblocks, the existing eBlock platform which is composed of fixed function building blocks targeted to enable nonexperts users to easily build a variety of interactive electronic systems is expanded to incorporate newly defined integer-based building blocks to enable a wider range of project possibilities for middle school STEM projects. We discuss various interface possibilities, including initial usability experiments, and summarize our overall experiences and observations in working with local middles school students utilizing the eBlock platform.
Cooperative Multi-Antenna Relaying in Heterogeneous Networks
In this paper, we investigate the performance of heterogeneous networks with multi-antenna cooperative relays. Specifically, threshold-based maximum ratio combining (MRC) and selection combining (SC) schemes are adopted for decoding at the relays and the end-to-end (E2E) error rate performance is analyzed by assuming a Nakagami channel model. Numerical results show that the deployment of multi-antennas can reduce the number of required relay nodes, and thus significantly reduce the system cost. On the other hand, the selection of optimal decoding threshold depends on the number of relay nodes, number of antennas, as well as the average SNR value at the receiver. It is also demonstrated that when the BER requirement is not high, the SC relaying scheme is sufficient to provide satisfactory performance, such that the complexity of relays can be effectively reduced.
A visual representation for knowledge structures
Knowledge-based systems often represent their knowledge as a network of interrelated units. Such networks are commonly presented to the user as a diagram of nodes connected by lines. These diagrams have provided a powerful visual metaphor for knowledge representation. However, their complexity can easily become unmanageable as the knowledge base (KB) grows.  This paper describes an alternate visual representation for navigating knowledge structures, based on a virtual museum metaphor. This representation uses nested boxes rather than linked nodes to represent relations. The intricate structure of the knowledge base is conveyed by a combination of position, size, color, and font cues, MUE (Museum Unit Editor) was implemented using this representation to provide a graphic front end for the Cyc knowledge base.
Variety Is the Spice of (Virtual) Life
Before an environment can be populated with characters, a set of models must first be acquired and prepared. Sometimes it may be possible for artists to create each virtual character individually - for example, if only a small number of individuals are needed, or there are many artists available to create a larger population of characters. However, for most applications that need large and heterogeneous groups or crowds, more automatic methods of generating large numbers of humans, animals or other characters are needed. Fortunately, depending on the context, it is not the case that all types of variety are equally important. Sometimes quite simple methods for creating variations, which do not over-burden the computing resources available, can be as effective as, and perceptually equivalent to, far more resource-intensive approaches. In this paper, we present some recent research and development efforts that aim to create and evaluate variety for characters, in their bodies, faces, movements, behaviours and sounds.
Finite horizon linear quadratic regulation for linear discrete time-varying systems with single/multiple input delay(s)
This paper studies the linear quadratic regulation problem for linear discrete time-varying systems with one or multiple delays in control input. This type of input-delay system can be used to model delayed actuation, where the system depends on the input after various (could be more than one) time delays. We provide explicit forms of the finite horizon closed-loop optimal control laws. Numerical examples are also provided to show the performance of our derived control laws.
VirusMeter: Preventing Your Cellphone from Spies
Due to the rapid advancement of mobile communication technology, mobile devices nowadays can support a variety of data services that are not traditionally available. With the growing popularity of mobile devices in the last few years, attacks targeting them are also surging. Existing mobile malware detection techniques, which are often borrowed from solutions to Internet malware detection, do not perform as effectively due to the limited computing resources on mobile devices.#R##N##R##N#In this paper, we propose VirusMeter, a novel and general malware detection method, to detect anomalous behaviors on mobile devices. The rationale underlying VirusMeter is the fact that mobile devices are usually battery powered and any malicious activity would inevitably consume some battery power. By monitoring power consumption on a mobile device, VirusMeter catches misbehaviors that lead to abnormal power consumption. For this purpose, VirusMeter relies on a concise user-centric power model that characterizes power consumption of common user behaviors. In a real-time mode, VirusMeter can perform fast malware detection with trivial runtime overhead. When the battery is charging (referred to as a battery-charging mode), VirusMeter applies more sophisticated machine learning techniques to further improve the detection accuracy. To demonstrate its feasibility and effectiveness, we have implemented a VirusMeter prototype on Nokia 5500 Sport and used it to evaluate some real cellphone malware, including FlexiSPY and Cabir. Our experimental results show that VirusMeter can effectively detect these malware activities with less than 1.5% additional power consumption in real time.
Author Profiling for Vietnamese Blogs
This paper presents the first work in the task of author profiling for Vietnamese blogs. This task is important in threat identification and marketing intelligence. We have developed a Vietnamese Blog Profiling framework to automatically predict age, gender, geographic origin and occupation of weblogs’ authors purely based on language use. The experiments on the blogs corpus we collected show very promising results with accuracy of around 80% across all traits.
Integrating heterogeneous personal devices with public display-based information services
Based on a requirements analysis for public location information displays in on-campus settings, we describe the implementation of a system called "SynchroBoard". Especially, we elaborate on mechanisms to integrate different personal devices in this framework.
Performance analysis of wireless fair queuing algorithms with compensation mechanism
Scheduling packet transmission over wireless links requires quantification of the QoS performance such as delay and packet loss in terms of known system parameters. One of the key issues is how to account for effects of the compensation mechanism on the system's QoS performance. In this paper, we develop a model, namely the two-stage tandem queuing (TSTQ), to characterize the behaviors of packet flows in the system, which applies the wireless fair scheduling with the compensation mechanism. Using queueing analysis, we derive performance parameters: average delay and packet loss rate, in closed-form expressions. These expressions are functions of the source, wireless channel and compensation mechanism parameters. Moreover, the trade-off relationship between delay and packet loss rate is revealed, which is controlled by the parameter of lagging bound. Numerical and simulation results are used to verify the validity of the modeling and analysis work.
Channel Assignment for Wireless Networks Modelled as d-Dimensional Square Grids
In this paper, we study the problem of channel assignment for wireless networks modelled as d-dimensional grids. In particular, for d-dimensional square grids, we present optimal assignments that achieve a channel separation of 2 for adjacent stations where the reuse distance is 3 or 4. We also introduce the notion of a colouring schema for d- dimensional square grids, and present an algorithm that assigns colours to the vertices of the grid satisfying the schema constraints.
Efficient Implementation of the Overlap Operator on Multi-GPUs
Lattice QCD calculations were one of the first applications to show the potential of GPUs in the area of high performance computing. Our interest is to find ways to effectively use GPUs for lattice calculations using the overlap operator. The large memory footprint of these codes requires the use of multiple GPUs in parallel. In this paper we show the methods we used to implement this operator efficiently. We run our codes both on a GPU cluster and a CPU cluster with similar interconnects. We find that to match performance the CPU cluster requires 20-30 times more CPU cores than GPUs.
Flat vs. symbiotic evolutionary subspace clusterings
Subspace clustering coevolves the attribute space supporting clusters at the same time as parameterizing the cluster location and combination. Typically, a 'flat' representation is pursued in which individuals describe both the property of individual clusters as well as the combination of clusters used to define the overall solution; hereafter F-ESC. Conversely, a symbiotic approach was recently proposed in which candidate clusters and the combination of clusters are coevolved from independent populations; hereafter S-ESC. In this work a common framework is pursued in order for flat and symbiotic evolutionary subspace clustering to be compared directly. We show that F-ESC might match S-ESC results for data sets with high proportions of cluster support, however, the gap between the two algorithm increases as cluster support decreases.
Context-Dependent Force-Feedback Steering Wheel to Enhance Drivers' On-Road Performances
In this paper the topic of the augmented cognition applied to the driving task, and specifically to the steering maneuver, is discussed. We analyze how the presence of haptic feedback on the steering wheel could help drivers to perform a visually-guided task by providing relevant information like vehicle speed and trajectory. Starting from these considerations, a Context-Dependant Steering Wheel force feedback (CDSW) had been developed, able to provide to the driver the most suitable feeling of the vehicle dynamics according to the driven context. With a driving simulator the CSWD software had been tested twice and then compared with a traditional steering wheel.
Direct segmentation of smooth, multiple point regions
The purpose of reverse engineering is to convert a large point cloud into an accurate, fair and consistent CAD model. For a class of conventional engineering objects we have the 'a priori' assumption that the object is bounded exclusively by simple, analytic surfaces. In this case it is possible to generate the model with a minimal amount of user interaction. The key issue is segmentation, i.e., to separate the point cloud into smaller regions, where each can be approximated by a single surface. While this is relatively simple, where the regions are bounded by sharp edges, problems arise when smoothly connected regions need to be separated. The direct segmentation method described in this paper is based on a special sequence of tests, by means of which a large point cloud can be robustly splitted into smaller subregions until no further subdivision is possible. Surfaces of linear extrusion and revolution are also detected. The structure of the smooth, multiple regions is the basis of constrained surface fitting in the final model building phase.
Research on Dynamic Reputation Management Model Based on PageRank
For the purpose of developing a usable trust relationship between the resource providers (hosts) and the resource consumers (users) in an open computing environment and providing a unified management of the reputation degree of the resource provides and users, a dynamic reputation management model based on Google PageRank (DRMPR) is proposed. The DRMPR system can achieve self-study from a large amount of data and feedback, and with the system obtaining a plenty of resources, the judgment is more accurate. At the end of the paper, an experimental project has been built to demonstrate that the DRMPR can provide a unified management of the reputation degree of the resource provides and users accurately.
Parameterization of the MISO IFC rate region: the case of partial channel state information
We study the achievable rate region of the multiple-input single-output (MISO) interference channel (IFC), under the assumption that all receivers treat the interference as additive Gaussian noise. We assume the case of two users, and that the channel state information (CSI) is only partially known at the transmitters. Our main result is a characterization of Pareto-optimal transmit strategies, for channel matrices that satisfy a certain technical condition. Numerical examples are provided to illustrate the theoretical results.
Introducing a rasch-type anthropomorphism scale
In human-robot interaction research, much attention is given to the extent to which people perceive humanlike attributes in robots. Generally, the concept anthropomorphism is used to describe this process. Anthropomorphism is defined in different ways, with much focus on either typical human attributes or uniquely human attributes. This difference has caused different measurement tools to be developed. We argue that anthropomorphism can best be described as a continuum ranging from low to high human likeness, and should be measured accordingly. We found that anthropomorphic characteristics can be invariantly ordered according to the ease with which these can be ascribed to robots.
Knowledge formation and dialogue using the KRAKEN toolset
The KRAKEN toolset is a comprehensive interface for knowledge acquisition that operates in conjunction with the Cyc knowledge base. The KRAKEN system is designed to allow subject-matter experts to make meaningful additions to an existing knowledge base, without the benefit of training in the areas of artificial intelligence, ontology development, or logical representation. Users interact with KRAKEN via a natural-language interface, which translates back and forth between English and the KB's logical representation language. A variety of specialized tools are available to guide users through the process of creating new concepts, stating facts about those concepts, and querying the knowledge base. KRAKEN has undergone two independent performance evaluations. In this paper we describe the general structure and several of the features of KRAKEN, focussing on key aspects of its functionality in light of the specific knowledge-formation and acquisition challenges they are intended to address.
APTEEN: A Hybrid Protocol for Efficient Routing and Comprehensive Information Retrieval in Wireless Sensor Networks
Wireless sensor networks with thousands of tiny sensor nodes, are expected to find wide applicability and increasing deployment in coming years, as they enable reliable monitoring and analysis of the environment. In this paper, we propose a hybrid routing protocol (APTEEN) which allows for comprehensive information retrieval. The nodes in such a network not only react to time-critical situations, but also give an overall picture of the network at periodic intervals in a very energy efficient manner. Such a network enables the user to request past, present and future data from the network in the form of historical, one-time and persistent queries respectively. We evaluated the performance of these protocols and observe that these protocols are observed to outperform existing protocols in terms of energy consumption and longevity of the network.
Motion parameter estimation of multiple ground moving targets in multi-static passive radar systems
Multi-static passive radar (MPR) systems typically use narrowband signals and operate under weak signal conditions, making them difficult to reliably estimate motion parameters of ground moving targets. On the other hand, the availability of multiple spatially separated illuminators of opportunity provides a means to achieve multi-static diversity and overall signal enhancement. In this paper, we consider the problem of estimating motion parameters, including velocity and acceleration, of multiple closely located ground moving targets in a typical MPR platform with focus on weak signal conditions, where traditional time-frequency analysis-based methods become unreliable or infeasible. The underlying problem is reformulated as a sparse signal reconstruction problem in a discretized parameter search space. While the different bistatic links have distinct Doppler signatures, they share the same set of motion parameters of the ground moving targets. Therefore, such motion parameters act as a common sparse support to enable the exploitation of group sparsity-based methods for robust motion parameter estimation. This provides a means of combining signal energy from all available illuminators of opportunity and, thereby, obtaining a reliable estimation even when each individual signal is weak. Because the maximum likelihood (ML) estimation of motion parameters involves a multi-dimensional search and its performance is sensitive to target position errors, we also propose a technique that decouples the target motion parameters, yielding a two-step process that sequentially estimates the acceleration and velocity vectors with a reduced dimensionality of the parameter search space. We compare the performance of the sequential method against the ML estimation with the consideration of imperfect knowledge of the initial target positions. The Cramer-Rao bound (CRB) of the underlying parameter estimation problem is derived for a general multiple-target scenario in an MPR system. Simulation results are provided to compare the performance of the sparse signal reconstruction-based methods against the traditional time-frequency-based methods as well as the CRB.
Topology Preserving Marching Cubes-like Algorithms on the Face-Centered Cubic Grid
The well-known marching cubes algorithm is modified to apply to the face-centered cubic (fee) grid. Thus, the local configurations that are considered when extracting the local surface patches are not cubic anymore. This paper presents three different partitionings of the fee grid to be used for the local configurations. The three candidates are evaluated theoretically and experimentally and compared with the original marching cubes algorithm. It is proved that the reconstructed surface is topologically equivalent to the surface of the original object when the surface of the original object that is digitized is smooth and a sufficiently dense fee grid is used.
An on-line signature verification system based on fusion of local and global information
An on-line signature verification system exploiting both local and global information through decision-level fusion is presented. Global information is extracted with a feature-based representation and recognized by using Parzen Windows Classifiers. Local information is extracted as time functions of various dynamic properties and recognized by using Hidden Markov Models. Experimental results are given on the large MCYT signature database (330 signers, 16500 signatures) for random and skilled forgeries. Feature selection experiments based on feature ranking are carried out. It is shown experimentally that the machine expert based on local information outperforms the system based on global analysis when enough training data is available. Conversely, it is found that global analysis is more appropriate in the case of small training set size. The two proposed systems are also shown to give complementary recognition information which is successfully exploited using decision-level score fusion.
Tumor-Immune Interaction, Surgical Treatment, and Cancer Recurrence in a Mathematical Model of Melanoma
Malignant melanoma is a cancer of the skin arising in the melanocytes. We present a mathematical model of melanoma invasion into healthy tissue with an immune response. We use this model as a framework with which to investigate primary tumor invasion and treatment by surgical excision. We observe that the presence of immune cells can destroy tumors, hold them to minimal expansion, or, through the production of angiogenic factors, induce tumorigenic expansion. We also find that the tumor–immune system dynamic is critically important in determining the likelihood and extent of tumor regrowth following resection. We find that small metastatic lesions distal to the primary tumor mass can be held to a minimal size via the immune interaction with the larger primary tumor. Numerical experiments further suggest that metastatic disease is optimally suppressed by immune activation when the primary tumor is moderately, rather than minimally, metastatic. Furthermore, satellite lesions can become aggressively tumorigenic upon removal of the primary tumor and its associated immune tissue. This can lead to recurrence where total cancer mass increases more quickly than in primary tumor invasion, representing a clinically more dangerous disease state. These results are in line with clinical case studies involving resection of a primary melanoma followed by recurrence in local metastases.
Metric rectification for perspective images of planes
We describe the geometry constraints and algorithmic implementation for metric rectification of planes. The rectification allows metric properties, such as angles and length ratios, to be measured on the world plane from a perspective image. The novel contributions are: first, that in a stratified context the various forms of providing metric information, which include a known angle, two equal though unknown angles, and a known length ratio; can all be represented as circular constraints on the parameters of an affine transformation of the plane-this provides a simple and uniform framework for integrating constraints; second, direct rectification from right angles in the plane; third, it is shown that metric rectification enables calibration of the internal camera parameters; fourth, vanishing points are estimated using a Maximum Likelihood estimator; fifth, an algorithm for automatic rectification. Examples are given for a number of images, and applications demonstrated for texture map acquisition and metric measurements.
Improved Pairwise Key Establishment for Wireless Sensor Networks
Due to the resource constraints, pre-distributing secret keys into sensor nodes before they are deployed is an applicable approach to achieve information security in wireless sensor networks. Several key pre-distribution schemes have been proposed in literature to establish pairwise keys between sensor nodes; they are either too complicated, or insecure for some common attacks. To address these weaknesses, we propose an improved pairwise key establishment mechanism for wireless sensor networks in this paper. Compared with existing approaches, our scheme has better network resilience against node capture attack. Analysis and simulation results show that our scheme performs better than earlier proposed schemes in terms of network connectivity, key storage overhead, maximum supported network size, computational and communication overheads
Applications of Plotkin-terms: partitions and morphisms for closed terms
This theoretical pearl is about the closed term model of pure untyped lambda-terms modulo β-convertibility. A consequence of one of the results is that for arbitrary distinct combinators (closed lambda terms) M, M′, N, N′ there is a combinator H such thatdisplay formula hereThe general result, which comes from Statman (1998), is that uniformly r.e. partitions of the combinators, such that each ‘block’ is closed under β-conversion, are of the form {H−1{M}}M∈ΛΦ. This is proved by making use of the idea behind the so-called Plotkin-terms, originally devised to exhibit some global but non-uniform applicative behaviour. For expository reasons we present the proof below. The following consequences are derived: a characterization of morphisms and a counter-example to the perpendicular lines lemma for β-conversion.
Cost/Revenue Optimisation of Multi-Service Cellular Planning for City Centre E-UMTS
An overview of all-IP enhanced Universal Mobile Telecommunication System, E-UMTS, service needs in the business city centre, BCC, scenario is first presented. Then, E-UMTS traffic generation and activity models are described and characterised. System level simulations are carried out and the enhanced performance is demonstrated based in a single quality parameter, which simultaneously accounts for call blocking and handover failure probabilities. End-to-end delays do not present a limitation. By considering a grade of service of 1% for the quality parameter, and different hypothesis for costs and prices, an optimum coverage distance is obtained around ~200-425 m, which maximises the supported throughput per km 2 . However, results for the profit in percentage indicates that coverage distances in the range 395-425 m should be used in BCC.
Implementing fault-tolerance in real-time systems by automatic program transformations
We present a formal approach to implement and certify fault-tolerance in real-time embedded systems. The fault-intolerant initial system consists of a set of independent periodic tasks scheduled onto a set of fail-silent processors. We transform the tasks such that, assuming the availability of an additional spare processor, the system tolerates one failure at a time (transient or permanent). Failure detection is implemented using heartbeating, and failure masking using checkpointing and roll-back. These techniques are described and implemented by automatic program transformations on the tasks' programs. The proposed formal approach to fault-tolerance by program transformation highlights the benefits of separation of concerns and allows us to establish correctness properties.
Using literature-based discovery to identify disease candidate genes
Summary  We present BITOLA, an interactive literature-based biomedical discovery support system. The goal of this system is to discover new, potentially meaningful relations between a given starting concept of interest and other concepts, by mining the bibliographic database MEDLINE ® . To make the system more suitable for disease candidate gene discovery and to decrease the number of candidate relations, we integrate background knowledge about the chromosomal location of the starting disease as well as the chromosomal location of the candidate genes from resources such as LocusLink and Human Genome Organization (HUGO). BITOLA can also be used as an alternative way of searching the MEDLINE database. The system is available at http://www.mf.uni-lj.si/bitola/.
Modeling nondisclosure in terms of the subject-instruction stream
A formal definition is given of nondisclosure for a computing system and the author describes a functional decomposition of the system into two kinds of activities, namely, the selection and execution of subject instructions. Security requirements for each of the two resulting subsystems are given, and it is proved that, if each subsystem satisfies its security requirements, then the entire system satisfies the given nondisclosure property. Finally, in order to show how security can be enforced by the system, an access-control model is given for subject-instruction processing that guarantees satisfaction of the given security requirements for subject-instruction processing. >
Galois theory and a new homotopy double groupoid of a map of spaces
The authors have used generalised Galois Theory to construct a homotopy double groupoid of a surjective Þbration of Kan simplicial sets. Here we apply this to construct a new homotopy double groupoid of a map of spaces, which includes constructions by others of a 2-groupoid, cat 1 -group or crossed module. An advantage of our construction is that the double groupoid can give an algebraic model of a foliated bundle. 1
Lane-Change Decision Aid System Based on Motion-Driven Vehicle Tracking
Overtaking and lane changing are very dangerous driving maneuvers due to possible driver distraction and blind spots. We propose an aid system based on image processing to help the driver in these situations. The main purpose of an overtaking monitoring system is to segment the rear view and track the overtaking vehicle. We address this task with an optic-flow-driven scheme, focusing on the visual field in the side mirror by placing a camera on top of it. When driving a car, the ego-motion optic-flow pattern is very regular, i.e., all the static objects (such as trees, buildings on the roadside, or landmarks) move backwards. An overtaking vehicle, on the other hand, generates an optic-flow pattern in the opposite direction, i.e., moving forward toward the vehicle. This well-structured motion scenario facilitates the segmentation of regular motion patterns that correspond to the overtaking vehicle. Our approach is based on two main processing stages: First, the computation of optical flow in real time uses a customized digital signal processor (DSP) particularly designed for this task and, second, the tracking stage itself, based on motion pattern analysis, which we address using a standard processor. We present a validation benchmark scheme to evaluate the viability and robustness of the system using a set of overtaking vehicle sequences to determine a reliable vehicle-detection distance.
Inquire: predicate-based use and reuse
There are four fundamental aspects of use and reuse in building systems from components: conceptualization, retrieval, selection and correct use. The most important barrier to use and reuse is that of conceptualization. The Inscape environment is a specification-based software development environment integrated by the constructive use of formal interface specifications. The purpose of the formal interface specifications and the semantic interconnections is to make explicit the invisible semantic dependencies that result in conventionally-built systems. The important ingredient provided by Inquire in conceptualization, retrieval, selection and use is the set of predicates that describe the semantics of the elements in the interface. These predicates define the abstractions that are germane to the module interface and describe the properties of data objects and the assumptions and results of operations in a module. Use and reuse of components is based on a component's ability to provide needed semantics at a particular point in a system. It is the purpose of Inquire, the browser and predicate-based search mechanism, to aid both the environment and the user in the search for the components that will provide the desired predicates that are required to build and evolve an implementation correctly. >
Least squares detection of multiple changes in fractional ARIMA processes
We address the problem of estimating changes in fractional integrated ARMA (FARIMA) processes. These changes may be in the long range dependence (LRD) parameter or the ARMA parameters. The signal is divided into "elementary" segments: the objective is then to estimate the segments in which the changes occur. This estimation is achieved by minimizing a penalized least-squares criterion based on the parameter estimates computed in each segment. The optimization problem is then solved using a dynamic programming algorithm. Simulation results on synthetic data (computer network traffic) are reported.
Shape from point features
We present a nonparametric and efficient method for shape localization that improves on the traditional sub-window search in capturing the fine geometry of an object from a small number of feature points. Our method implies that the discrete set of features capture more appearance and shape information than is commonly exploited. We use the a-complex by Edelsbrunner et al. to build a filtration of simplicial complexes from a user-provided set of features. The optimal value of a is determined automatically by a search for the densest complex connected component, resulting in a parameter-free algorithm. Given K features, localization occurs in O(K logK) time. For VGA-resolution images, computation takes typically less than 10 milliseconds. We use our method for interactive object cut, with promising results.
Formation control for cooperative containment of a diffusing substance
We present a decentralized controller to keep a group of agents at equal spacing while moving around the perimeter of a loop defined by a constant distance from a convex polygon, motivated by a cooperative containment problem. Traveling at constant speed, the agents achieve and maintain their formation by using small steering adjustments to equalize the distance between themselves and their respective leading and following neighbors. Since the formation moves around a common loop, an agent can move forward or back in formation by respectively steering slighting inside or outside the reference loop. These adjustments are controlled with the use of variable radius parameters for each agent that are shown to converge to the desired reference loop as equal spacing is achieved. We show that the proposed controller renders the desired formation locally asymptotically stable and provide simulations to demonstrate the performance of the controller for an example scenario in which the formation must recover from the loss of an agent.
A Consensus Support System Model for Group Decision-Making Problems With Multigranular Linguistic Preference Relations
The group decision-making framework with linguistic preference relations is studied. In this context, we assume that there exist several experts who may have different background and knowledge to solve a particular problem and, therefore, different linguistic term sets (multigranular linguistic information) could be used to express their opinions. The aim of this paper is to present a model of consensus support system to assist the experts in all phases of the consensus reaching process of group decision-making problems with multigranular linguistic preference relations. This consensus support system model is based on i) a multigranular linguistic methodology, ii) two consensus criteria, consensus degrees and proximity measures, and iii) a guidance advice system. The multigranular linguistic methodology permits the unification of the different linguistic domains to facilitate the calculus of consensus degrees and proximity measures on the basis of experts' opinions. The consensus degrees assess the agreement amongst all the experts' opinions, while the proximity measures are used to find out how far the individual opinions are from the group opinion. The guidance advice system integrated in the consensus support system model acts as a feedback mechanism, and it is based on a set of advice rules to help the experts change their opinions and to find out which direction that change should follow in order to obtain the highest degree of consensus possible. There are two main advantages provided by this model of consensus support system. Firstly, its ability to cope with group decision-making problems with multigranular linguistic preference relations, and, secondly, the figure of the moderator, traditionally presents in the consensus reaching process, is replaced by the guidance advice system, and in such a way, the whole group decision-making process is automated
An Aspect-Oriented Approach to Resource Composition in Petri net-based Software Architectural Models
Petri net has been widely used for modeling software systems due to its mathematical soundness and support of various tools. In many cases, performance-related analyses of Petri net for a software system need to consider the resource limitations caused by a specific platform. In terms of aspect-oriented approach, the interference of various resources scattered across a software system can be regarded as a specific concern that is only necessary during its performance-related analysis process. To capture the interactions of resources within the Petri net-based software architectural model, we propose an aspect-oriented resource composition model and the XML-based representation language called the resource extension markup language (ReML) for its description. In our approach, one or more resource composition models can be developed and described in ReML separately from the development of base software architectural models. Through the weaving process, a resource composition model can be applied to extend several Petri net-based software architectural models. The resource weaver generates an augmented Petri net used for analyzing performance characteristics of the extended software architectural model with resource interactions. Our aspect-oriented approach facilitates selecting an optimal or superior resource composition model for a software architectural model, and vice versa, which is illustrated using two exemplary server models.
Building sub-knowledge bases using concept lattices
A theory of concept (Galois) lattices was first introduced by Wille. An extension of his work to simple structures called concept sublattices has also been published. This paper shows that concept sublattices can be applied to (i) determining subsumption of specifications and (ii) decomposing specifications in terms of others. I show that the latter application of the theory may provide us with new conceptualizations of a specification.
Coordination strategies for networked control systems: A power system application
In this paper we present a distributed supervisory strategy for load/frequency control problems in networked multi-area power systems. Coordination between the control center and the areas is accomplished via data networks subject to communication latency which is modelled by time-varying time-delay. The aim here is at finding strategies able of reconfiguring, whenever necessary in response to unexpected load changes and/or faults, the nominal set-points on frequency and generated power of each area so that viable evolutions arise for the overall networked system and a new suitable equilibrium is reached.
Cross-talk attack monitoring and localization in all-optical networks
The effects of an attack connection can propagate quickly to different parts of an all-optical transparent network. Such attacks affect the normal traffic and can either cause service degradation or outright service denial. Quick detection and localization of an attack source can avoid losing large amounts of data in an all-optical network. Attack monitors can collect the information from connections and nodes for diagnostic purpose. However, to detect attack sources, it is not necessary to put monitors at all nodes. Since those connections affected by the attack connection would provide valuable information for diagnosis, we show that by placing a relatively small number of monitors on a selected set of nodes in a network is sufficient to achieve the required level of performance. However, the actual monitor placement, routing, and attack diagnosis are challenging problems that need research attention. In this paper, we first develop our models of crosstalk attack and monitor node. With these models, we prove the necessary and sufficient condition for one-crosstalk-attack diagnosable networks. Next, we develop a scalable diagnosis method which can localize the attack connection efficiently with sparse monitor nodes in the network.
Adaptive algorithms for balanced multidimensional clustering
The G-K-D tree (generalized K-D tree) method aims at reducing the average number of data page accesses per query, but it ignores the cost of index search. The authors propose two adaptive algorithms that take into consideration both data page access cost and index page access cost. It attempts to find a minimum total cost. Experimental results indicate that the proposed algorithms are superior to the G-K-D tree method. >
An optimized stereo vision implementation for embedded systems: application to RGB and Infra-Red images
The aim of this paper is to demonstrate the applicability and the effectiveness of a computationally demanding stereo-matching algorithm in different low-cost and low-complexity embedded devices, by focusing on the analysis of timing and image quality performances. Various optimizations have been implemented to allow its deployment on specific hardware architectures while decreasing memory and processing time requirements: (1) reduction of color channel information and resolution for input images; (2) low-level software optimizations such as parallel computation, replacement of function calls or loop unrolling; (3) reduction of redundant data structures and internal data representation. The feasibility of a stereo vision system on a low-cost platform is evaluated by using standard datasets and images taken from infra-red cameras. Analysis of the resulting disparity map accuracy with respect to a full-size dataset is performed as well as the testing of suboptimal solutions.
Parametric Mixing for Centralized VOIP Conferencing using ITU-T Recommendation G.722.2
VoIP conferencing with a centralized speech mixing bridge introduces additional end-to-end latency into packetized voice communication. This paper investigates how full tandem speech decoding, time-domain mixing, speech encoding cycle can be circumvented by instead extracting the coded speech parameters and performing the speech packet mixing without time-domain reconstruction. By mixing through coded speech parameters, we show that nearly an 85% decrease in computational complexity can be achieved over full tandem mixing of two speakers for G.722.2, thus significantly reducing the packet latency at the centralized speech mixing bridge. For the G.722.2 parametric mixer presented, linear prediction coefficients (LPCs), pitch lags, fixed codebooks, and gains, are extracted (without full speech reconstruction) from the encoded bit stream, mixed, and then re-encoded instead of the full tandem approach where each speech frame must be fully reconstructed. We investigate the mixing in two scenarios: i) mix two 12.65 kbps G.722.2 speech streams at a mixed rate of 12.65 kbps, and ii) mix two 12.65 kbps G.722.2 speech streams at a mixed rate of 18.25 kbps. PAMS is used to evaluate the speech quality of the parametric mixer, resulting in an average distortion of 0.37 MOS (compared to tandem mixing) as shown by simulations using typical conversation models.
Self-adaptive, on-line reclustering of complex object data
A likely trend in the development of future CAD, CASE and office information systems will be the use of object-oriented database systems to manage their internal data stores. The entities that these applications will retrieve, such as electronic parts and their connections or customer service records, are typically large complex objects composed of many interconnected heterogeneous objects, not thousands of tuples. These applications may exhibit widely shifting usage patterns due to their interactive mode of operation. Such a class of applications would demand clustering methods that are appropriate for clustering large complex objects and that can adapt on-line to the shifting usage patterns. While most object-oriented clustering methods allow grouping of heterogeneous objects, they  are usually static and can only be changed off-line. We present one possible architecture for performing complex object reclustering in an on-line manner that is adaptive to changing usage patterns. Our architecture involves the decomposition of a clustering method into concurrently operating components that each handle one of the fundamental tasks involved in reclustering, namely statistics collection, cluster analysis, and reorganization. We present the results of an experiment performed to evaluate its behavior. These results show that the average miss rate for object accesses can be effectively reduced using a combination of rules that we have developed for deciding when cluster analyses and reorganizations should be performed.
Reim & ReImInfer: checking and inference of reference immutability and method purity
Reference immutability ensures that a reference is not used to modify the referenced object, and enables the safe sharing of object structures. A pure method does not cause side-effects on the objects that existed in the pre-state of the method execution. Checking and inference of reference immutability and method purity enables a variety of program analyses and optimizations. We present ReIm, a type system for reference immutability, and ReImInfer, a corresponding type inference analysis. The type system is concise and context-sensitive. The type inference analysis is precise and scalable, and requires no manual annotations. In addition, we present a novel application of the reference immutability type system: method purity inference.   To support our theoretical results, we implemented the type system and the type inference analysis for Java. We include a type checker to verify the correctness of the inference result. Empirical results on Java applications and libraries of up to 348kLOC show that our approach achieves both scalability and precision.
MODEL-FREE CONTROL AND INTELLIGENT PID CONTROLLERS: TOWARDS A POSSIBLE TRIVIALIZATION OF NONLINEAR CONTROL?
We are introducing a model-free control and a control with a restricted model for finite-dimensional complex systems. This control design may be viewed as a contribution to ``intelligent'' PID controllers, the tuning of which becomes quite straightforward, even with highly nonlinear and/or time-varying systems. Our main tool is a newly developed numerical differentiation. Differential algebra provides the theoretical framework. Our approach is validated by several numerical experiments.
Modified Differential Evolution for Constrained Optimization
In this paper, we present a Differential-Evolution based approach to solve constrained optimization problems. The aim of the approach is to increase the probability of each parent to generate a better offspring. This is done by allowing each solution to generate more than one offspring but using a different mutation operator which combines information of the best solution in the population and also information of the current parent to find new search directions. Three selection criteria based on feasibility are used to deal with the constraints of the problem and also a diversity mechanism is added to maintain infeasible solutions located in promising areas of the search space. The approach is tested in a set of test problems proposed for the special session on Constrained Real Parameter Optimization. The results obtained are discussed and some conclusions are established.
Performance Evaluation of Piggyback Requests in IEEE 802.16
WiMAX (Worldwide Interoperability for Microwave Access) is a wireless access technology that aims to provide last mile wireless broadband access for fixed and mobile users as an alternative to the wired DSL and cable access. It is specified in the IEEE 802.16 standard. The standard defines several possible bandwidth request methods that can be implemented in an actual deployment of a WiMAX network. In this paper, we will study the performance of two different bandwidth request mechanisms, namely piggyback and broadcast requests and will show in which situations piggybacking performs better than the contention based broadcast bandwidth requests.
Use of coded infrared light as artificial landmarks for mobile robot localization
This paper presents mobile robot localization using coded infrared light as artificial landmarks. Different from RFID, identification using infrared light has highly deterministic characteristics. IRID(infrared identification) is implemented with IR LEDs and photo transistors. By putting several infrared LEDs on the ceiling, the floor is divided into several sectors and each sector is set to have a unique identification. The coded infrared light tells which sector the robot is in, but the size of the uncertainty is still too large if the sector size is large, which usually occur. Dead-reckoning provides the estimated robot configuration but the error is getting accumulated as the robot travels. This paper presents an algorithm which fuses both the encoder and the IRID information so that the size of the uncertainty becomes smaller. It also introduces a framework which can be used with other types of the artificial landmarks. The characteristics of the developed IRID and the proposed algorithm are verified from the experiments.
The growth of international collaboration in East European scholarly communities: a bibliometric analysis of journal articles published between 1989 and 2009
In the last two decades international collaboration in the Eastern European academic communities has strongly intensified. Scientists from developed countries within the European Union play a key role in stimulating the international collaboration of academics in this region. In addition, many of the research projects that engage East-European scholars are only possible in the framework of the large European programmes. The present study focuses on the role of EU and other developed nations as a partner of these countries and the analysis of the performance of collaborative research as reflected by the citation impact of internationally co-authored publications.
INEXACT KLEINMAN-NEWTON METHOD FOR RICCATI EQUATIONS
In this paper we consider the numerical solution of the algebraic Riccati equation using Newton's method. We propose an inexact variant which allows one control the number of the inner iterates used in an iterative solver for each Newton step. Conditions are given under which the monotonicity and global convergence result of Kleinman also hold for the inexact Newton iterates. Numerical results illustrate the efficiency of this method.
Dimensional synthesis of a 3-DOF parallel manipulator
Kinematics analysis and dimensional synthesis are two important problems of a parallel manipulator. Dimensional synthesis is optimization of the kinematic parameters according to desired workspace or other design requirements. In this paper, the dimensional synthesis of a 3-DOF parallel manipulator, which mimics DELTA robots, is studied considering maximum inscribed workspace and reciprocal of the condition number on the workspace section based on the concept. The kinematic model of the 3-DOF parallel manipulator is given at first. The golden section search is used to search the workspace of the manipulator and mesh the boundary. Then the algorithm for calculating the inscribed workspace and dimensional synthesis considering the inscribed workspace are presented. Thirdly, the distribution of dexterity index on the workspace section and dimensional synthesis considering the reciprocal of condition number is studied. The two dimensional synthesis results are compared at the end of the paper. While the synthesis methodology of the manipulator is studied. It is helpful to improve design efficiency of the 3-DOF parallel manipulator. The quickly design for the manipulator can be performed through the proposed methods of dimensional synthesis.
Kinematics analysis for obstacle-climbing performance of a rescue robot
A tracked robot is designed for destroyed mine search and rescue. The mechanical system is introduced from reconfigurable structure, suspension system and anti- explosive and waterproof. The sensors include CCD camera, CO, CH4, temperature and air speed are equipped on the robot. Two pairs of swing arms are equipped on the robot. Their motions help robot climb up obstacle. Because the center of gravity (CG) plays an important role in the process of climbing up an obstacle, the CG kinematics model is built. Using this model, the CG change situation is obtained, and the maximum height of the obstacle which can be climbed up is obtained, and the stability angle margin is obtained too. The relationship between the robot pitch angle and the height of the obstacle is obtained. Using this relationship, the geometry parameter of the uncertain environment can be known. These analysis help to design and control the robot.
Dynamic software updates: a VM-centric approach
Software evolves to fix bugs and add features. Stopping and restarting programs to apply changes is inconvenient and often costly. Dynamic software updating (DSU) addresses this problem by updating programs while they execute, but existing DSU systems for managed languages do not support many updates that occur in practice and are inefficient. This paper presents the design and implementation of J volve , a DSU-enhanced Java VM. Updated programs may add, delete, and replace fields and methods anywhere within the class hierarchy. Jvolve implements these updates by adding to and coordinating VM classloading, just-in-time compilation, scheduling, return barriers, on-stack replacement, and garbage collection. J volve , is  safe : its use of bytecode verification and VM thread synchronization ensures that an update will always produce type-correct executions. Jvolve is  flexible : it can support 20 of 22 updates to three open-source programs--Jetty web server, JavaEmailServer, and CrossFTP server--based on actual releases occurring over 1 to 2 years. Jvolve is  efficient : performance experiments show that incurs  no overhead  during steady-state execution. These results demonstrate that this work is a significant step towards practical support for dynamic updates in virtual machines for managed languages.
Creating Rule Ensembles from Automatically-Evolved Rule Induction Algorithms
Ensembles are a set of classification models that, when combined, produce better predictions than when used by themselves. This chapter proposes a new evolutionary algorithm-based method for creating an ensemble of rule sets consisting of two stages. First, an evolutionary algorithm (more precisely, a genetic programming algorithm) is used to automatically create complete rule induction algorithms. Secondly, the automatically-evolved rule induction algo- rithms are used to produce rule sets that are then combined into an ensemble. Concerning this second stage, we investigate the effectiveness of two different approaches for combining the votes of all rule sets in the ensemble and two dif- ferent approaches for selecting which subset of evolved rule induction algorithms (out of all evolved algorithms) should be used to produce the rule sets that will be combined into an ensemble.
XML and Meta Data Based EDI for Small Enterprises
Today in many cases electronic data interchange (EDI) is limited to large scale industry connected to their own value added networks. Small-scale enterprises are not yet integrated in the communication flow, because actual EDI solutions are to complex, to inflexible or to expensive.#R##N##R##N#The approach presented in this paper separates knowledge about data structures and data formats from the process of generation of destination files. This knowledge is transformed into a meta data structure represented by XML document type definitions (DTD) which itself are stored within a database system. If any changes of the data interchange specification are necessary, it is sufficient to update the corresponding meta data information within the XML DTDs. The implementation of the data interchange processor remains unchanged. This type of adaptation does not require a software specialist and therefore it meets an important requirement of small-scale enterprises. Data transmission is done using the advantages of XML and Internet technology.
An FPGA-Based Embedded System for Fingerprint Matching Using Phase-Only Correlation Algorithm
Biometric identification systems are defined as systems exploiting automated methods of personal recognition based on physiological or behavioural characteristics. Among these, fingerprints are very reliable biometric identifiers. Trying to fasten the image processing step makes the recognition process more efficient, especially concerning embedded systems for real-time authentication. In this paper we propose an FPGA-based architecture that efficiently implements the high computationally demanding core of a matching algorithm based on phase-only spatial correlation. Moreover, we show how it is possible to use COTS components to embed an entire AFIS on chip and so reducing cost, space and energy used.
Classification of breast masses in mammograms using neural networks with shape, edge sharpness, and texture features
We propose an approach using artificial neural networks to classify masses in mammograms as malignant or benign. Single- layer and multilayer perceptron networks are used in a study on perceptron topologies and training procedures for pattern classifica- tion of breast masses. The contours of a set of 111 regions on mam- mograms related to breast masses and tumors are manually delin- eated and represented by polygonal models for shape analysis. Ribbons of pixels are extracted around the boundaries of a subset of 57 masses by dilating and eroding the contours. Three shape fac- tors, three measures of edge sharpness, and 14 texture features based on gray-level co-occurrence matrices of the pixels in the rib- bons are computed. Several combinations of the features are used with perceptrons of varying topology and training procedures for the classification of benign masses and malignant tumors. The results are compared in terms of the area Az under the receiver operating characteristics curve. Values of Az up to 0.99 are obtained with the shape factors and texture features. However, only feature sets that included at least one shape factor provide consistently high perfor- mance with respect to variations in network topology and training.
The analysis of the performance of multi-beamforming in memory nonlinear power amplifier
With the increasingly diverse and complex requirements of radar systems and communication systems, the application of multifunction-phased array radar has become a trend, and the digital multi-beamforming technology plays a crucial role in it. In practice, power amplifier (PA) is an essential component in radar systems and communication systems. Unfortunately, it is always nonlinear to provide a high output power. With the purpose of a high output power and efficiency, it is necessary to study the influence of PA nonlinear characteristics on the digital multi-beamforming. In this paper, a form of the multi-beamforming signal and a nonlinear model with memory for PA are given. The output signal via the PA model has been analyzed subsequently. As the result of analysis, it can be found that the output signal is divided into the original signal and the interferential signal. The power ratio of original signal to interference signal can reflect the influence of PA nonlinear characteristics on the digital multi-beamforming. Finally, according to the ratio, the results of computer simulation show that the memory effect plays a key role for the small power signal, while the nonlinearity plays an important role for the large power signal.
Relaxed multiple routing configurations for IP fast reroute
Multi-topology routing is an increasingly popular IP network management concept that allows transport of different traffic types over disjoint network paths. The concept is of particular interest for implementation of IP fast reroute (IP FRR). First, it can support guaranteed, instantaneous recovery from any link or node failure. Second, different failures result in routing over different network topologies, which augments the parameter space for load distribution optimizations. Multiple routing configurations (MRC) is the state-of-the-art IP FRR scheme based on multi-topology routing today. In this paper we present a new, enhanced IP FRR scheme which we call ldquorelaxed MRCrdquo (rMRC). rMRC simplifies the topology construction and increases the routing flexibility in each topology. According to our experimental evaluation, rMRC has several benefits compared to MRC. The number of backup topologies required to provide protection against the same set of failures is reduced, hence reducing state in routers. In addition, the backup paths are shorter, and the link utilization is significantly better.
Similarity measures between intuitionistic fuzzy (vague) sets: A comparative analysis
Existing similarity measures between intuitionistic fuzzy sets/vague sets are analyzed, compared and summarized by their counter-intuitive examples in pattern recognition. The positive aspects of each similarity measure are demonstrated, along with counter cases and discussion of the conditions under which each may not work as desired. The research presented here could benefit selection and applications of similarity measures for intuitionistic fuzzy sets and vague sets in practice.
Distributed approximation of capacitated dominating sets
We study local, distributed algorithms for the capacitated minimum dominating set (CapMDS) problem, which arises in various distributed network applications. Given a network graph  G  = ( V,E ), and a capacity  cap(v)  ∈  N  for each node  v  ∈  V  , the CapMDS problem asks for a subset  S  ⊆  V  of minimal cardinality, such that every network node not in  S  is covered by at least one neighbor in  S , and every node  v  ∈  S  covers at most  cap(v)  of its neighbors. We prove that in general graphs and even with uniform capacities, the problem is inherently  non-local , i.e., every distributed algorithm achieving a non-trivial approximation ratio must have a time complexity that essentially grows linearly with the network diameter. On the other hand, if for some parameter e > 0, capacities can be violated by a factor of 1 + e, CapMDS becomes much more local. Particularly, based on a novel distributed randomized rounding technique, we present a distributed bi-criteria algorithm that achieves an O(log Δ)-approximation in time O(log 3  n  + log( n )/e), where  n  and Δ denote the number of nodes and the maximal degree in  G , respectively. Finally, we prove that in geometric network graphs typically arising in wireless settings, the uniform problem can be approximated within a constant factor in logarithmic time, whereas the non-uniform problem remains entirely non-local.
Using wordnet hypernyms and dependency features for phrasal-level event recognition and type classification
The goal of this research is to devise a method for recognizing and classifying TimeML events in a more effective way. TimeML is the most recent annotation scheme for processing the event and temporal expressions in natural language processing fields. In this paper, we argue and demonstrate that unit feature dependency information and deep-level WordNet hypernyms are useful for event recognition and type classification. The proposed method utilizes various features including lexical semantic and dependency-based combined features. The experimental results show that our proposed method outperforms a state-of-the-art approach, mainly due to the new strategies. Especially, the performance of noun and adjective events, which have been largely ignored and yet significant, is significantly improved.
Three-dimensional focusing with multipass SAR data
Deals with the use of multipass synthetic aperture radar (SAR) data in order to achieve three-dimensional tomography reconstruction in presence of volumetric scattering. Starting from azimuth- and range-focused SAR data relative to the same area, neglecting any mutual interaction between the targets, and assuming the propagation in homogeneous media, we investigate the possibility to focus the data also in the elevation direction. The problem is formulated in the framework of linear inverse problem and the solution makes use of the singular value decomposition of the relevant operator. This allows us to properly take into account nonuniform orbit separation and to exploit a priori knowledge regarding the size of the volume interested by the scattering mechanism, thus leading to superresolution in the elevation direction. Results obtained on simulated data demonstrate the feasibility of the proposed processing technique.
Software component independence
Independence is a fundamental requirement for calculating system reliability from component reliabilities, whether in hardware or software systems. Markov analysis is often used in such calculation; however, procedures as conventionally used do not qualify as nodes in a Markov system. We outline the requirements for several classes of component independence and use the CPS (continuation passing style) transformation to convert conventional procedures into fragments that are appropriate to Markov analysis.
Interpolated Allpass Fractional-Delay Filters Using Root Displacement
Fractional-delay filter is the general name given to filters modelling non-integer delays. Such filters have a flat phase delay for a wide frequency band, with the value of the phase delay approximating the fractional delay. A maximally-flat delay IIR fractional-delay filter can be obtained by the Thiran approximation. A simple and efficient method for obtaining filters modelling intermediate fractional delays from two Thiran fractional-delay filters is proposed. The proposed method allows continuously modifying the fractional delay. Computational complexity of the proposed method is discussed. A practical application of the method in model-based sound synthesis is given as an example.
MMsINC: a large-scale chemoinformatics database
MMsINC (http://mms.dsfarm.unipd.it/MMsINC/search) is a database of non-redundant, richly annotated and biomedically relevant chemical structures. A primary goal of MMsINC is to guarantee the highest quality and the uniqueness of each entry. MMsINC then adds value to these entries by including the analysis of crucial chemical properties, such as ionization and tautomerization processes, and the in silico prediction of 24 important molecular properties in the biochemical profile of each structure. MMsINC is consequently a natural input for different chemoinformatics and virtual screening applications. In addition, MMsINC supports various types of queries, including substructure queries and the novel ‘molecular scissoring’ query. MMsINC is interfaced with other primary data collectors, such as PubChem, Protein Data Bank (PDB), the Food and Drug Administration database of approved drugs and ZINC.
IBM Solves a Mixed-Integer Program to Optimize Its Semiconductor Supply Chain
IBM Systems and Technology Group uses operations research models and methods extensively for solving large-scale supply chain optimization (SCO) problems for planning its extended enterprise semiconductor supply chain. The large-scale nature of these problems necessitates the use of computationally efficient solution methods. However, the complexity of the models makes developing robust solution methods a challenge. We developed a mixed-integer programming (MIP) model and supporting heuristics for optimizing IBM's semiconductor supply chain. We designed three heuristics, driven by practical applications, for capturing the discrete aspects of the MIP. We leverage the model structure to overcome computational hurdles resulting from the large-scale problem. IBM uses the model and method daily for operational and strategic planning decisions and has saved substantial costs.
On passivity based control of stochastic port-Hamiltonian systems
This paper introduces Stochastic Port-Hamiltonian Systems (SPHS's), whose dynamics are described by Ito stochastic differential equations. SPHS's are extension of the deterministic port-Hamiltonian systems which are used to express various passive systems. First, we show a necessary and sufficient condition to preserve the stochastic port-Hamiltonian structure of the system under a class of coordinate transformations. Second, we derive a condition for the system to be stochastic passive. Third, we equip Stochastic Generalized Canonical Transformations (SGCT's), which are pairs of coordinate and feedback transformations preserving the stochastic port-Hamiltonian structure. Finally, we propose a stochastic stabilization framework based on stochastic passivity and SGCT's.
A survey of single and multi-hop link schedulers for mmWave wireless systems
Wireless communication at 60GHz, aka mmWave, provides extremely high data rates, i.e., several Gb/s. Moreover, devices have a much shorter transmission range as compared to those operating in the 2.4 and 5GHz bands. Indeed, links can be treated as pseudo-wires with minimal interference leakage. As a result, future 60GHz systems will have very high spatial reuse. This, however, is at the expense of high propagation loss, which can be overcome using directional or smart antennas. Another promising solution is to employ relays to boost the signal of weak links. In particular, if relays are properly selected, they are able to offer higher data rates than direct links, and also help circumvent obstacles. To this end, we review state-of-the-art schedulers that take advantage of the high spatial re-use afforded by 60GHz wireless systems to activate multiple links within a channel time allocation. Moreover, we survey works that use passive and active relays to overcome obstacles and to facilitate novel applications. We also survey those that maximize both spatial reuse and throughput of both direct and indirect (relay) links simultaneously.
A data-driven approach for building macroeconomic decision support system
More and more economic and financial data have been collected by the governmental departments in China since China started its “socialist market economy” in late 1980s. These government departments, in particular the departments in charge of economic development, pay a great attention to the economic information in their decision-making. There is an urgent demand for efficient decision support systems in macroeconomic decision-making. In this paper we present a data-driven approach for building macroeconomic decision support system. We first give a comprehensive discussion about the basic elements and their data-processing methods for Macroeconomic Decision Support Systems according to China's situation. These elements include: leading indicator system about business cycle, state identification of economic movement, forecasting of economic trend, promotion of successful cases, choice of regulation instruments, evaluation method of macroeconomic policies. Based on the discussion, we put forward to a general structure of macroeconomic decision support system. Premier implementation shows that the structure can not only satisfy the governmental departments' demand fairly, but also reflect the future trend.
TCP Performance in IEEE 802.11-Based Ad Hoc Networks with Multiple Wireless Lossy Links
We propose a packet-level model to investigate the impact of channel error on the transmission control protocol (TCP) performance over IEEE-802.11-based multihop wireless networks. A Markov renewal approach is used to analyze the behavior of TCP Reno and TCP Impatient NewReno. Compared to previous work, our main contributions are listed as follows: 1) modeling multiple lossy links, 2) investigating the interactions among TCP, Internet Protocol (IP), and media access control (MAC) protocol layers, specifically the impact of 802.11 MAC protocol and dynamic source routing (DSR) protocol on TCP throughput performance, 3) considering the spatial reuse property of the wireless channel, the model takes into account the different proportions between the interference range and transmission range, and 4) adopting more accurate and realistic analysis to the fast recovery process and showing the dependency of throughput and the risk of experiencing successive fast retransmits and timeouts on the packet error probability. The analytical results are validated against simulation results by using GloMoSim. The results show that the impact of the channel error is reduced significantly due to the packet retransmissions on a per-hop basis and a small bandwidth delay product of ad hoc networks. The TCP throughput always deteriorates less than ~ 10 percent, with a packet error rate ranging from 0 to 0.1. Our model also provides a theoretical basis for designing an optimum long retry limit for IEEE 802.11 in ad hoc networks.
Toward Secure Multikeyword Top-k Retrieval over Encrypted Cloud Data
Cloud computing has emerging as a promising pattern for data outsourcing and high-quality data services. However, concerns of sensitive information on cloud potentially causes privacy problems. Data encryption protects data security to some extent, but at the cost of compromised efficiency. Searchable symmetric encryption (SSE) allows retrieval of encrypted data over cloud. In this paper, we focus on addressing data privacy issues using SSE. For the first time, we formulate the privacy issue from the aspect of similarity relevance and scheme robustness. We observe that server-side ranking based on order-preserving encryption (OPE) inevitably leaks data privacy. To eliminate the leakage, we propose a two-round searchable encryption (TRSE) scheme that supports top-k multikeyword retrieval. In TRSE, we employ a vector space model and homomorphic encryption. The vector space model helps to provide sufficient search accuracy, and the homomorphic encryption enables users to involve in the ranking while the majority of computing work is done on the server side by operations only on ciphertext. As a result, information leakage can be eliminated and data security is ensured. Thorough security and performance analysis show that the proposed scheme guarantees high security and practical efficiency.
Improved 16-QAM constellation labeling for BI-STCM-ID with the Alamouti scheme
We design constellation labeling maps for bit-interleaved space-time coded modulation with iterative decoding (BI-STCM-ID) over Rayleigh block-fading channels using the Alamouti scheme and N/sub r/ receive antennas. To achieve the largest asymptotic coding gain from the constellation labeling, we propose a new design criterion that maximizes the (-2N/sub r/)-th power mean of the squared Euclidean distances associated with all "error-free feedback" events in the constellation. Based on this power mean criterion, we show that the labeling optimization problem falls into the category of quadratic assignment problems. We propose two novel 16-QAM labeling maps that are particularly designed for N/sub r/=1 and N/sub r/=2, respectively. Numerical results show that both labeling maps achieve about 1 dB coding gain over the conventional 16-QAM modified set partitioning labeling.
Perfect Output Feedback in the Two-User Decentralized Interference Channel
In this paper, the    $\eta $   -Nash equilibrium (   $\eta $   -NE) region of the two-user Gaussian interference channel (IC) with perfect output feedback is approximated to within 1 bit/s/Hz and    $\eta $    arbitrarily close to 1 bit/s/Hz. The relevance of the    $\eta $   -NE region is that it provides the set of rate pairs that are achievable and stable in the IC when both transmitter–receiver pairs autonomously tune their own transmit–receive configurations seeking an    $\eta $   -optimal individual transmission rate. Therefore, any rate tuple outside the    $\eta $   -NE region is not stable as there always exists one link able to increase by at least    $\eta $    bits/s/Hz its own transmission rate by updating its own transmit–receive configuration. The main insights that arise from this paper are as follows. First, the    $\eta $   -NE region achieved with feedback is larger than or equal to the    $\eta $   -NE region without feedback. More importantly, for each rate pair achievable at an    $\eta $   -NE without feedback, there exists at least one rate pair achievable at an    $\eta $   -NE with feedback that is weakly Pareto superior. Second, there always exists an    $\eta $   -NE transmit–receive configuration that achieves a rate pair that is at most 1 bit/s/Hz per user away from the outer bound of the capacity region.
A Design and Control Environment for Internet-Based Telerobotics
This paper describes an environment for the design, simulation, and control of Internet-based force-reflecting telerobotic systems. We define these systems as using a segment of the computer network to connect the master to the slave. Computer networks introduce a time delay that is best described by a time-varying random process. Thus, known techniques for controlling time-delay telerobots are not directly applicable, and an environment for iterative designing and testing is necessary. The underlying software architecture sup ports tools for modeling the delay of the computer network, design ing a stable controller, simulating the performance of a telerobotic system, and testing the control algorithms using a force-reflecting input device. Furthermore, this setup provides data about including the Internet into more general telerobotic control architectures. To demonstrate the features of this environment, the complete proce dure for the design of a telerobotic controller is discussed. First, the delay pa...
No-flow underfill flip chip assembly--an experimental and modeling analysis
In the flip-chip assembly process, no-flow underfill materials have a particular advantage over traditional underfill: the application and curing of the former can be undertaken before and during the reflow process. This advantage can be exploited to increase the flip-chip manufacturing throughput. However, adopting a no-flow underfill process may introduce reliability issues such as underfill entrapment, delamination at interfaces between underfill and other materials, and lower solder joint fatigue life. This paper presents an analysis on the assembly and the reliability of flip-chips with no-flow underfill. The methodology adopted in the work is a combination of experimental and computer-modeling methods. Two types of no-flow underfill materials have been used for the flip chips. The samples have been inspected with X-ray and scanning acoustic microscope inspection systems to find voids and other defects. Eleven samples for each type of underfill material have been subjected to thermal shock test and the number of cycles to failure for these flip chips have been found. In the computer modeling part of the work, a comprehensive parametric study has provided details on the relationship between the material properties and reliability, and on how underfill entrapment may affect the thermal–mechanical fatigue life of flip chips with no-flow underfill.
Reflections on designing networked exertion games
Research in human-computer interaction has begun to acknowledge the benefits of physicality in the way people interact with computers. However, the role of physicality is often understood in terms of the characteristics of physical smart objects and their digital augmentation. We are stressing that the physicality lies within the interaction, not the object, and use a subset of bodily actions, exertion interactions, as an example to demonstrate our point. Emerging game designs have shown that supporting such exertion interactions can enable beneficial experiences between geographically distant participants. Based on several designs from our own work as well as others in this area we articulate reflections for the design of systems that support and facilitate bodily aspects of physicality in networked environments. We believe our work can serve as guidance for designers who are interested in creating future systems that support networked exertion interactions.
Stack-free process-oriented simulation
The process interaction world view is widely used in the general simulation community for its expressive power, and is supported by most modern simulation languages. In parallel discrete event simulation, however, its use remains comparatively rare due to the perceived inefficiency (and difficulty) of parallel implementations.We present a new implementation strategy for parallel process-oriented simulation languages. This innovative, semantics-based approach directly addresses two common concerns of such languages. By concentrating on the intrinsic threads of control, we avoid the proliferation of simulation objects (and their associated costs) that might result from a naive translation. More fundamentally, the primary costs associated with process-oriented languages -- those of context switching between stacks and, in an optimistic setting, of saving the state of these stacks -- are entirely eliminated since our explicit use of continuations avoids the need for stacks in the first place. We similarly obtain cheap and natural thread preemption.
Network congestion control with Markovian multipath routing
In this paper we consider an integrated model for TCP/IP protocols with multipath routing. The model combines a Network Utility Maximization for rate control based on end-to-end queuing delays, with a Markovian Traffic Equilibrium for routing based on total expected delays. We prove the existence of a unique equilibrium state which is characterized as the solution of an unconstrained strictly convex program. A distributed algorithm for solving this optimization problem is proposed, with a brief discussion of how it can be implemented by adapting the current Internet protocols.
Motion compensated lossy-to-lossless compression of 4-D medical images using integer wavelet transforms
This paper proposes a method for progressive lossy-to-lossless compression of four-dimensional (4-D) medical images (sequences of volumetric images over time) by using a combination of three-dimensional (3-D) integer wavelet transform (IWT) and 3-D motion compensation. A 3-D extension of the set-partitioning in hierarchical trees (SPIHT) algorithm is employed for coding the wavelet coefficients. To effectively exploit the redundancy between consecutive 3-D images, the concepts of key and residual frames from video coding is used. A fast 3-D cube matching algorithm is employed to do motion estimation. The key and the residual volumes are then coded using 3-D IWT and the modified 3-D SPIHT. The experimental results presented in this paper show that our proposed compression scheme achieves better lossy and lossless compression performance on 4-D medical images when compared with JPEG-2000 and volumetric compression based on 3-D SPIHT.
A multiuser receiver for code division multiple access communications over multipath channels
A multiuser communication system is considered where K users share a channel with multipath propagation by using code division for multiple access. Data modulation is carried out by binary phase shift keying and direct sequence spread spectrum signaling. The micro-cellular communication media is modeled as a frequency selective fading channel with multipath propagation. The multipath diversity of the received signals from the K users is exploited by a bank of K RAKE correlators. Algorithms based on the maximum likelihood rule have been developed for estimating the complex channel coefficients as well as for detection of the desired data packets from the sufficient statistics provided by the RAKE correlators. The performance of the resulting multiuser detector is evaluated analytically and via Monte Carlo simulations. The results indicate that the estimator of the channel coefficients has a variance close to the Cramer-Rao lower bound, and that the proposed multiuser detector is capable of eliminating the near-far effect as well as processing the signals propagated through multiple paths. >
Spatial Synchronization Using Watermark Key Structure
Recently, we proposed a method for constructing a template for efficient temporal synchronization in video watermarking. 1 Our temporal synchronization method uses a state machine key generator for producing the watermark embedded in successive frames of video. A feature extractor allows the watermark key schedule to be content dependent, increasing the difficulty of copy and ownership attacks. It was shown that efficient synchronization can be achieved by adding temporal redundancy into the key schedule. In this paper, we explore and extend the concepts of our temporal synchronization method to spatial synchronization. The key generator is used to construct the embedded watermark of non-overlapping blocks of the video, creating a tiled structure. 2–4 The autocorrelation of the tiled watermark contains local maxima or peaks with a grid-like structure, where the distance between the peaks indicates the scale of the watermark and the orientation of the peaks indicate the watermark rotation. Experimental results are obtained using digital image watermarks. Scaling and rotation attacks are investigated.
SDG model-based analysis of fault propagation in control systems
In the area of fault analysis, SDG (Signed Directed Graph) models can be used to describe the system states and the fault propagation paths which are the composition of qualitative deviation from the normal state. In control systems, besides the natural relations caused by the physical properties, the forced control actions determine the dynamic properties of the systems, which cause the particularity of SDG model-based analysis. In this paper, the SDG description and the analysis methods of fault propagation in control systems are presented, and the typical cases like PID control, feedforward control, split-range control, cascade control etc are illustrated. A graphical analysis method is proposed to substitute the algebraic methods based on equations. These results can be expanded to various control systems and even be applied to large-scale industrial systems by the combination and connection of several basic elements.
The distribution of citations from nation to nation on a field by field basis — A computer calculation of the parameters
Following the methodology established byPrice, this paper analyzes the empirical evidence of citation matrices. Using the data cleaned and tabulated by Computer Horizons, Inc. from the Science Citation Index data banks, it is shown that the non-diagonal elements of the square citation matrices can be accounted for very satisfactorily by assigning each nation a characteristic output and input coefficient in each field measured; the ratio of these coefficients provides a measure of quality. Deviations from this simple model give measures of particular linkage strengths between nations showing some evidence of preferences and avoidances that exist for reason of language, social structure, etc. It is also shown that the diagonal data can be accounted for by the measurable phenomenon that each nation seems to publish partly for the international knowledge system and party for its own domestic purposes. Thus, three parameters and a cluster map can parsimoniously describe the citation data within the limits of random error.
An approach to object-oriented discrete-event simulation of manufacturing systems
It is shown how the object-oriented approach can be applied to discrete-event simulation and, in particular, to discrete-event simulation of manufacturing systems. A hierarchical structure of object classes is proposed, consisting of three class libraries: base classes, simulation support object classes, and manufacturing systems simulation object classes. The definition of each class and how the class objects interact with one another are discussed. An example of a discrete-event simulation model developed using the object classes is presented. The example illustrates the basic nature, merits, and drawbacks of this approach. >
Free subcarrier optimization for peak-to-average power ratio minimization in OFDM systems
Peak-to-average power ratio (PAR) reduction techniques are often employed to increase the power efficiency of orthogonal frequency division multiplexing (OFDM) systems. A recently proposed PAR optimization method demonstrates how the PAR can be minimized when free subcarriers and a certain distortion allowance on the error vector magnitude (EVM) are available. In this paper, we derive the lower bound on the capacity for such a system and investigate the capacity- maximizing number of free subcarriers that should be used.
A spatial mapping algorithm for heterogeneous coarse-grained reconfigurable architectures
In this work, we investigate the problem of automatically mapping applications onto a coarse-grained reconfigurable architecture and propose an efficient algorithm to solve the problem. We formalize the mapping problem and show that it is NP-complete. To solve the problem within a reasonable amount of time, we divide it into three subproblems: covering, partitioning and layout. Our empirical results demonstrate that our technique produces nearly as good performance as hand-optimized outputs for many kernels.
Market equilibrium via a primal--dual algorithm for a convex program
We give the first polynomial time algorithm for exactly computing an equilibrium for the linear utilities case of the market model defined by Fisher. Our algorithm uses the primal--dual paradigm in the enhanced setting of KKT conditions and convex programs. We pinpoint the added difficulty raised by this setting and the manner in which our algorithm circumvents it.
Enhancement of an Optical Fiber Sensor: Source Separation Based on Brillouin Spectrum
Distributed optical fiber sensors have gained an increasingly prominent role in structural-health monitoring. These are composed of an optical fiber cable in which a light impulse is launched by an opto-electronic device. The scattered light is of interest in the spectral domain: the spontaneous Brillouin spectrum is centered on the Brillouin frequency, which is related to the local strain and temperature changes in the optical fiber. When coupled with an industrial Brillouin optical time-domain analyzer (B-OTDA), an optical fiber cable can provide distributed measurements of strain and/or temperature, with a spatial resolution over kilometers of 40 cm. This paper focuses on the functioning of a B-OTDA device, where we address the problem of the improvement of spatial resolution. We model a Brillouin spectrum measured within an integration base of 1 m as the superposition of the elementary spectra contained in the base. Then, the spectral distortion phenomenon can be mathematically explained: if the strain is not constant within the integration base, the Brillouin spectrum is composed of several elementary spectra that are centered on different local Brillouin frequencies. We propose a source separation methodology approach to decompose a measured Brillouin spectrum into its spectral components. The local Brillouin frequencies and amplitudes are related to a portion of the integration base where the strain is constant. A layout algorithm allows the estimation of a strain profile with new spatial resolution chosen by the user. Numerical tests enable the finding of the optimal parameters, which provides a reduction to 1 cm of the 40-cm spatial resolution of the B-OTDA device. These parameters are highlighted during a comparison with a reference strain profile acquired by a 5-cm-resolution Rayleigh scatter analyzer under controlled conditions. In comparison with the B-OTDA strain profile, our estimated strain profile has better accuracy, with centimeter spatial resolution.
Intra dynamic scenario relations and a dynamic decision making algorithm
In this paper we present the composition of a general dynamic scenario, the relations of its components and a dynamic making algorithm called ECA (Evaluation and Correction Algorithm). The paper proposes three relations, namely, stable relation, controllable relation and uncontrollable relation. Numerical results are given by some simple dynamic scenarios.
PGMRA: a web server for (phenotype × genotype) many-to-many relation analysis in GWAS
It has been proposed that single nucleotide polymorphisms (SNPs) discovered by genome-wide association studies (GWAS) account for only a small fraction of the genetic variation of complex traits in human population. The remaining unexplained variance or missing heritability is thought to be due to marginal effects of many loci with small effects and has eluded attempts to identify its sources. Combination of different studies appears to resolve in part this problem. However, neither individual GWAS nor meta-analytic combinations thereof are helpful for disclosing which genetic variants contribute to explain a particular phenotype. Here, we propose that most of the missing heritability is latent in the GWAS data, which conceals intermediate phenotypes. To uncover such latent information, we propose the PGMRA server that introduces phenomics—the full set of phenotype features of an individual—to identify SNP-set structures in a broader sense, i.e. causally cohesive genotype‐ phenotype relations. These relations are agnostically identified (without considering disease status of the subjects) and organized in an interpretable fashion. Then, by incorporating a posteriori the subject status within each relation, we can establish the risk surface of a disease in an unbiased mode. This approach complements—instead of replaces— current analysis methods. The server is publically available at http://phop.ugr.es/fenogeno.
Parallel Lanczos bidiagonalization for total least squares filter in robot navigation
In the robot navigation problem, noisy sensor data must be filtered to obtain the best estimate of the robot position. The discrete Kalman filter, which usually is used for prediction and detection of signals in communication and control problems has become a commonly used method to reduce the effect of uncertainty from the sensor data. However, due to the special domain of robot navigation, the Kalman approach is very limited. The use of total least squares filter has been proposed (Boley and Sutherland, 1993) which is capable of converging with many fewer readings and achieving greater accuracy than the classical Kalman filter. The main disadvantage of those approaches is that they can not deal with the case where the noise subspace is of dimension higher than one. Here a parallel Krylov subspace method on parallel distributed memory computers which uses the Lanczos bidiagonalization process with updating techniques is proposed which is more computationally attractive to solve the total least squares problems. The parallel algorithm is derived such that all inner products of a single iteration step are independent. Therefore, the cost of global communication which represents the bottleneck of the parallel performance on parallel distributed memory computers can be significantly reduced. This filter is very promising for very large data information and from our very preliminary experiments we can obtain more precise accuracy and better speedup.
Bit and power loading for the multiband impulse radio UWB architecture
In this paper we present two different bit and power loading algorithms for the non-coherent multiband impulse UWB architecture. The first one is a very simple threshold based bit loading algorithm and an extension of the detect and avoid (DAA) algorithm presented in [1]. The second one is a more powerful algorithm, enabling also power loading. These algorithms allow a more efficient and flexible spectrum use, higher data rates and use less transmission power. The bit error rate performance is significantly improved. Both algorithms support inherent powerful DAA.
Intelligent decision system and its application in business innovation self assessment
In this paper, it is described how a multiple criteria decision analysis software tool, the Intelligent Decision System (IDS), can be used to help business self-assessment. Following a brief outline of a model for assessing business innovation capability and the IDS software, the process of using IDS to implement different types of assessment questions is discussed. It is demonstrated that IDS is a flexible tool capable of handling different types of data in self-assessment, including uncertain and incomplete data, and providing a wide range of information including scores, performance diversity, strength and weakness profile and graphics.
Cold delay defect screening
Delay defects can escape detection during the normal production test flow; particularly if they do not affect any of the long paths included in the test flow. Some delay defects can have their delay increased, making them easier to detect, by carrying out the test with a very low supply voltage (VLV testing). However, VLV testing is not effective for delay defects caused by high resistance interconnects. This paper presents a screening technique for such defects. This technique, cold testing, relies on carrying out the test at low temperature. One particular type of defect, silicide open, is analyzed and experimental data are presented to demonstrate the effectiveness of cold testing.
Knowledge representation for conceptual simulation modeling
Simulation is a powerful tool that helps decision makers in business and industry to solve difficult and complex problems, reduce cost, improve quality and productivity, and shorten time-to-market. However the technology is still underutilized in many applications due to several reasons. In this study we address these issues using a knowledge engineering approach, i.e. develop efficient and robust models and formats to capture, represent and organize the knowledge for developing conceptual simulation models that can be generalized and interfaced with different applications and implementation tools. The research fits into a larger project effort that aims to create a sustained research program on knowledge-based simulation.
An application-level implementation of causal timestamps and causal ordering
Maintenance of causality information in distributed systems has previously been implemented in the communications infrastructure with the focus on providing reliability and availability for distributed services. While this approach has a number of advantages, moving causality information up into the view and control of the application programmer is useful, and in some cases, preferable. In an experiment at the University of Queensland, libraries to support application-level maintenance of causality information have been implemented. The libraries allow the collection and use of causality information under programmer control, supplying a basis for making causal dependency information available for application management and troubleshooting. The libraries are also unique in supporting existing distributed systems based on the remote procedure call paradigm. This paper describes the underlying theory of causality, and the design and implementation of the libraries. An event reporting service example is used to motivate the approach, and a number of previously unresolved practical problems are addressed in the design process.
A high-quality multirate real-time CELP coder
The design and implementation of a real-time CELP coder for mobile communication applications are discussed. To realize a single-chip implementation, several tradeoffs were made without compromising speech quality. In addition, techniques that make the coder more robust under a variety of channel conditions are discussed. The real-time coder can be operated at different bit rates (8, 6.8, 4.6 kb/s) by simply changing the frame update rates. The speech quality was evaluated through a formal listening test, and it was found that this coder compares favorably with other (standardized) coders operating at similar or higher rates. >
Weighted least squares training of support vector classifiers leading to compact and adaptive schemes
An iterative block training method for support vector classifiers (SVCs) based on weighted least squares (WLS) optimization is presented. The algorithm, which minimizes structural risk in the primal space, is applicable to both linear and nonlinear machines. In some nonlinear cases, it is necessary to previously find a projection of data onto an intermediate-dimensional space by means of either principal component analysis or clustering techniques. The proposed approach yields very compact machines, the complexity reduction with respect to the SVC solution is especially notable in problems with highly overlapped classes. Furthermore, the formulation in terms of WLS minimization makes the development of adaptive SVCs straightforward, opening up new fields of application for this type of model, mainly online processing of large amounts of (static/stationary) data, as well as online update in nonstationary scenarios (adaptive solutions). The performance of this new type of algorithm is analyzed by means of several simulations.
A distributed parallel approach for BGP routing table partitioning in next generation routers
The rapid growth of routing tables represents a major challenge facing the scalability of BGP and indeed the whole Internet infrastructure. In this paper, we introduce a novel distributed algorithmic scheme for partitioning the BGP routing table on multiple controller cards, where we exploit parallelism to enhance both the lookup speed and the scalability of the RIB (Routing Information Base). The proposed scheme increases the lookup performance by letting unrelated tasks, such as the Best Match Prefix (BMP) lookup and the BGP decision process to be executed in parallel at different controller cards. Simulations show that our proposal outperforms classical central lookup mechanisms with a reasonably acceptable cost, while it increases considerably the space scalability of the BGP routing table.
Fast committee machines for regression and classification
In many data mining applications we are given a set of training examples and asked to construct a regression machine or a classifier that has low prediction error or low error rate on new examples, respectively. An important issue is speed especially when there are large amounts of data. We show how both classification and prediction error can be reduced by using boosting techniques to implement committee machines. In our implementation of committees using either classification trees or regression trees, we show how we can trade off speed against either error rate or prediction error.
Maximum-likelihood detection of nonlinearly distorted multicarrier symbols by iterative decoding
This paper proposes a new method for decoding multicarrier symbols with severe nonlinear distortion. The first part evaluates mutual information expressions for practical nonlinear models and shows the performance bounds for commonly used receiver structures. Then, we derive the maximum-likelihood (ML) sequence estimator, which unfortunately has an exponential complexity due to the nonlinear distortion. This extremely large complexity can be reduced with a simple algorithm that iteratively estimates the nonlinear distortion, thereby reducing the exponential ML to the standard ML without nonlinear distortion. The proposed method can be used to reduce the peak-to-average power ratio of multicarrier signals by clipping the transmit sequence. It can also be used to correct any nonlinear distortion present in transmitter/receiver amplifiers that are operating close to saturation.
Pricing Longevity Bonds Based on Stochastic Mortality Forecasting by Panel Data Procedures
In order to hedge the longevity risk, longevity bonds are designed, whose payoff structure depends on the changes in mortality. To forecast the mortality more precisely, we use a time-dynamic stochastic model by utilizing a panel data approach to forecast the mortality rates and get a survival index. Empirical study is conducted with the data in China. Then we apply these forecasting mortality rates to evaluate one kind of longevity bond. It turns out that it is reliable for the social security systems and the life insurance industry.
Partitions and Edge Colourings of Multigraphs
Erdős and Lovasz conjectured in 1968 that for every graph $G$ with $\chi(G)>\omega(G)$ and any two integers $s,t\geq 2$ with $s+t=\chi(G)+1$, there is a partition $(S,T)$ of the vertex set $V(G)$ such that $\chi(G[S])\geq s$ and $\chi(G[T])\geq t$. Except for a few cases, this conjecture is still unsolved. In this note we prove the conjecture for line graphs of multigraphs.
Robust Adaptive Fuzzy Sliding Mode Control for a Class of Uncertain Nonlinear Systems with Unknown Dead-Zone
In this paper, a robust adaptive fuzzy sliding mode control scheme is presented for a class of uncertain nonlinear systems preceded by an unknown dead-zone. Dead-zone characteristics are quite commonly encountered in actuators, such as hydraulic and pneumatic valves, electric servomotors, and electronic circuits, etc. Therefore, by using a description of a dead-zone and exploring the properties of this dead-zone model intuitively and mathematically, a robust adaptive fuzzy sliding control method is presented without constructing the dead-zone inverse. The unknown nonlinear functions of the plant are approximated by the fuzzy logic system according to some adaptive laws. Based on Lyapunov stability theorem and the theory of variable structure control, the proposed robust adaptive fuzzy sliding mode control scheme can guarantee the robust stability of the whole closed-loop system with an unknown dead-zone in the actuator and obtain good tracking performance as well. Finally, an example and simulation results are provided to illustrate the effectiveness of the proposed method.
Tracer kinetic modeling by morales-smith hypothesis in hepatic perfusion CT
Most of the existing tracer kinetic models for dynamic contrast-enhanced CT or MRI do not fully describe the principles of intra- and transcapillary transport of tracers. One point is to disregard the concentration profiles between the inlets and outlets of capillaries, which may cause a biased estimation of tissue parameters by a systematic error. The Morales-Smith hypothesis enables one to resolve this ambiguity by assuming that the difference between arterial and venous concentrations is proportional to the difference between the arterial and capillary concentrations. If the backflow of administered tracer into the plasma compartment is negligible compared to its outflow into the interstitial compartment during the initial enhancement phase after tracer administration, the capillary concentration can be considered to fall exponentially along the capillary from the arterial concentration to the venous concentration by the Renkin-Crone model, i.e., unidirectional extraction fraction, which can be incorporated in the concept of the Morales-Smith hypothesis. In this study, we reformed the mass-balance equations and mathematical solutions of several representative and well-known tracer kinetic models so that the Morales-Smith hypothesis could be incorporated into their compartment tracer kinetics, considering a tissue-specific factor independent of time as proposed by Brix et al. [5]. The tissue-specific factor was applied to a liver tumor case study in perfusion CT to illustrate the potential effectiveness of the Morales-Smith hypothesis. The proposed scheme was shown to be potentially useful for more consistent and reliable estimation of physiologic tissue parameters.
Algorithm for decomposing an analytic signal into AM and positive FM components
An analytic signal permits unambiguous characterization of the phase and envelope of a real signal. But the analytic signal's phase-derivative, i.e. the instantaneous frequency (IF) is typically a wild function and can take on values ranging from negative infinity to positive infinity. Fortunately, any analytic signal can be decomposed into a minimum phase (MinP) signal component and an all-phase (AllP) signal component. While the MinP signal's log-envelope and its phase form a Hilbert transform pair, the AllP signal has a positive definite instantaneous frequency (PIF) unlike that of the original analytic signal. We propose an elegant computational algorithm that separates the MinP and AllP components of the analytic signal. The envelope of the MinP component corresponds to the AM and the PIF of the AllP component corresponds to the positive FM.
Processing nested complex sequence pattern queries over event streams
Complex event processing (CEP) has become increasingly important for tracking and monitoring applications ranging from health care, supply chain management to surveillance. These monitoring applications submit complex event queries to track sequences of events that match a given pattern. As these systems mature the need for increasingly complex nested sequence queries arises, while the state-of-the-art CEP systems mostly focus on the execution of flat sequence queries only. In this paper, we now introduce an iterative execution strategy for nested CEP queries composed of sequence, negation, AND and OR operators. Lastly we have introduced the promising direction of applying selective caching of intermediate results to optimize the execution. Our experimental study using real-world stock trades evaluates the performance of our proposed iterative execution strategy for different query types.
Learning Top-Down Grouping of Compositional Hierarchies for Recognition
The complexity of real world image categorization and scene analysis requires compositional strategies for object representation. This contribution establishes a compositional hierarchy by first performing a perceptual bottom-up grouping of edge pixels to generate salient contour curves. A subsequent recursive top-down grouping yields a hierarchy of compositions. All entities in the compositional hierarchy are incorporated in a Bayesian network that couples them together by means of a shape model. The probabilistic model underlying top-down grouping as well as the shape model is learned automatically from a set of training images for the given categories. As a consequence, compositionality simplifies the learning of complex category models by building them from simple, frequently used compositions. The architecture is evaluated on the highly challenging Caltech 101 database1 which exhibits large intra-category variations. The proposed compositional approach shows competitive retrieval rates in the range of 53 .0 ± 0 .49%.
On the energy-efficiency of speculative hardware
Microprocessor trends are moving towards wider architectures and more aggressive speculation. With the increasing transistor budgets, energy consumption has become a critical design constraint. To address this problem, several researchers have proposed and evaluated energy-efficient variants of speculation mechanisms. However, such hardware is typically evaluated in isolation and its impact on the energy consumption of the rest of the processor, for example, due to wrong-path executions, is ignored. Moreover, the available metrics that would provide a thorough evaluation of an architectural optimization employ somewhat complicated formulas with hard-to-measure parametersIn this paper, we introduce a simple method to accurately compare the energy-efficiency of speculative architectures. Our metric is based on runtime analysis of the entire processor chip and thus captures the energy consumption due to the positive as well as the negative activities that arise from the speculation activities. We demonstrate the usefulness of our metric on the example of value speculation, where we found some proposed value predictors, including low-power designs, not to be energy-efficient
A new clutter rejection algorithm for Doppler ultrasound
Several strategies, known as clutter or wall Doppler filtering, were proposed to remove the strong echoes produced by stationary or slow moving tissue structures from the Doppler blood flow signal. In this study, the matching pursuit (MP) method is proposed to remove clutter components. The MP method decomposes the Doppler signal into wavelet atoms that are selected in a decreasing energy order. Thus, the high-energy clutter components are extracted first. In the present study, the pulsatile Doppler signal s(n) was simulated by a sum of random-phase sinusoids. Two types of high-amplitude clutter signals were then superimposed on s(n): time-varying low-frequency components, covering systole and early diastole, and short transient clutter signals, distributed within the whole cardiac cycle. The Doppler signals were modeled with the MP method and the most dominant atoms were subtracted from the time-domain signal s(n) until the signal-to-clutter (S/C) ratio reached a maximum. For the low-frequency clutter signal, the improvement in S/C ratio was 19.0 /spl plusmn/ 0.6 dB, and 72.0 /spl plusmn/ 4.5 atoms were required to reach this performance. For the transient clutter signal, ten atoms were required and the maximum improvement in S/C ratio was 5.5 /spl plusmn/ 0.5 dB. The performance of the MP method was also tested on real data recorded over the common carotid artery of a normal subject. Removing 15 atoms significantly improved the appearance of the Doppler sonogram contaminated with low-frequency clutter. Many more atoms (over 200) were required to remove transient clutter components. These results suggest the possibility of using this signal processing approach to implement clutter rejection filters on ultrasound commercial instruments.
Causal time series analysis of functional magnetic resonance imaging data
This review focuses on dynamic causal analysis of functional magnetic resonance (fMRI) data to infer brain connectivity from a time series analysis and dynamical systems perspective. Causal influence is expressed in the Wiener-Akaike-Granger-Schweder (WAGS) tradition and dynamical systems are treated in a state space modeling framework. The nature of the fMRI signal is reviewed with emphasis on the involved neuronal, physiological and physical processes and their modeling as dynamical systems. In this context, two streams of development in modeling causal brain connectivity using fMRI are discussed: time series approaches to causality in a discrete time tradition and dynamic systems and control theory approaches in a continuous time tradition. This review closes with discussion of ongoing work and future perspectives on the integration of the two approaches.
Medium access control in ad hoc networks with MIMO links: optimization considerations and algorithms
we present a medium access control (MAC) protocol for ad hoc networks with multiple input multiple output (MIMO) links. MIMO links provide extremely high spectral efficiencies in multipath channels by simultaneously transmitting multiple independent data streams in the same channel. MAC protocols have been proposed in related work for ad hoc networks with other classes of smart antennas such as switched beam antennas. However, as we substantiate in the paper, the unique characteristics of MIMO links coupled with several key optimization considerations, necessitate an entirely new MAC protocol. We identify several advantages of MIMO links, and discuss key optimization considerations that can help in realizing an effective MAC protocol for such an environment. We present a centralized algorithm called stream-controlled medium access (SCMA) that has the key optimization considerations incorporated in its design. Finally, we present a distributed SCMA protocol that approximates the centralized algorithm and compare its performance against that of baseline protocols that are CSMA/CA variants.
Design and Implementation of Physical Layer Private Key Setting for Wireless Networks
Due to the enormous spreading of applied wireless networks, security is actually one of the most important issues for telecommunications. One of the main issue in the field of securing wireless information exchanging is the initial common knowledge between source and destination. A shared secret is normally mandatory in order to decide the encryption (algorithm or code or key) of the information stream. It is usual to exchange this common a priori knowledge by using a "secure" channel. Now a days a secure wireless channel is not possible. In fact normally the common a priori knowledge is already established (but this is not secure) or by using a non-radio channel (that implies a waste of time and resource). This contribution deals with the proposal of a new modulation technique ensuring secure communication in a full wireless environment. The information is modulated, at physical layer, by the thermal noise experienced by the link between two terminals. A loop scheme is designed for unique recovering of mutual information. The probability of error/detection is analytically derived for the legal users and for the third unwanted listener. The proposed scheme has also been implemented in a Xilinx Virtex II FPGA.#R##N##R##N#All the results show that the performance of the proposed scheme yields the advantage of intrinsic security, i.e., the mutual information cannot be physically demodulated (passive attack) or denied (active attack) by a third terminal, leading us to conclude that the proposed technique is really useful for private key distribution in every wireless network.
Circular Acoustic Vector-Sensor Array for Mode Beamforming
Undersea warfare relies heavily on acoustic means to detect a submerged vessel. The frequency of the acoustic signal radiated by the vessel is typically very low, thus requires a large array aperture to achieve acceptable angular resolution. In this paper, we present a novel approach for low-frequency direction-of-arrival (DOA) estimation using miniature circular vector-sensor array mounted on the perimeter of a cylinder. Under this approach, we conduct beamforming using decomposition in the acoustic mode domain rather than frequency domain, to avoid the long wavelength constraints. We first introduce a multi-layer acoustic gradient scattering model to provide a guideline and performance predication tool for the mode beamformer design and algorithm. We optimize the array gain and frequency response with this model. We further develop the adaptive DOA estimation algorithm based on this model. We formulate the Capon spectra of the mode beamformer which is independent of the frequency band after the mode decomposition. Numerical simulations are conducted to quantify the performance and evaluate the theoretical results developed in this study.
On the expressive power of KLAIM-based calculi
We study the expressive power of variants of KLAIM, an experimental language with programming primitives for network-aware programming that combines the process algebra approach with the coordination-oriented one. KLAIM has proved to be suitable for programming a wide range of distributed applications with agents and code mobility, and has been implemented on the top of a runtime system written in Java. In this paper, the expressivity of its constructs is tested by distilling from it a few, more and more foundational, languages and by studying the encoding of each of them into a simpler one. The expressive power of the considered calculi is finally tested by comparing one of them with asynchronous π-calculus.
Fast Principal Component Analysis using Eigenspace Merging
In this paper, we propose a fast algorithm for principal component analysis (PCA) dealing with large high-dimensional data sets. A large data set is firstly divided into several small data sets. Then, the traditional PCA method is applied on each small data set and several eigenspace models are obtained, where each eigenspace model is computed from a small data set. At last, these eigenspace models are merged into one eigenspace model which contains the PCA result of the original data set. Experiments on the FERET data set show that this algorithm is much faster than the traditional PCA method, while the principal components and the reconstruction errors are almost the same as that given by the traditional method.
Towards the Automated Generation of Hard Disk Models through Physical Geometry Discovery
As the High Performance Computing industry moves towards the exascale era of computing, parallel scientific and engineering applications are becoming increasingly complex. The use of simulation allows us to predict how an application's performance will change with the adoption of new hardware or software, helping to inform procurement decisions. In this paper, we present a disk simulator designed to predict the performance of read and write operations to a single hard disk drive (HDD). Our simulator uses a geometry discovery benchmark (Diskovery) in order to estimate the data layout of the HDD, as well as the time spent moving the read/write head. We validate our simulator against two different HDDs, using a benchmark designed to simulate common disk read and write patterns, demonstrating accuracy to within 5% of the observed I/O time for sequential operations, and to within 10% of the observed time for seek-heavy workloads.
A Novel Approach for Binarization of Overlay Text
In this paper, we presents a new binarization approach to extract text pixels from complex background in video frames. The binarization computation is a crucial step for, video text recognition, which can greatly increase the recognition, accuracy of an OCR software. The proposed approach consists, of four phases. First, the text polarity is determined, i.e. light text with dark background or dark text with light background., Then the pixels in the given image are clustered into K clusters, using the K-means algorithm in the RGB color space and the, text cluster is selected based on the text polarity. Further, the, MRF Model is exploited to get the binarization result. Finally, the, result is further refined by the Log-Gabor filter. The Experimental, results on a large dataset show that the significant gains have been, obtained according to the segmentation performance on the pixel, level as well as the OCR accuracy.
An efficient architecture for JPEG2000 coprocessor
JPEG2000 is a new international standard for still image compression. It provides various functions in one single coding stream and the better compression quality than the traditional JPEG, especially in the high compression ratio. However, the heavy computation and large internal memory requirement still restrict the consumer electronics applications. In this paper, we propose a QCB (quad code block)-based DWT method to achieve the higher parallelism than the traditional DWT approach of JPEG2000 coding process. Based on the QCB-based DWT engine, three code blocks can be completely generated after every fixed time slice recursively. Thus, the DWT and EBCOT processors can process simultaneously and the high computational EBCOT then has the higher parallelism of the JPEG2000 encoding system. By changing the output timing of the DWT process and parallelizing with EBCOT, the internal tile memory size can be reduced by a factor of 4. The memory access cycles between the internal tile memory and the code block memory also decrease with the smooth encoding flow.
Predicting Billboard Success Using Data-Mining in P2P Networks
Peer to Peer networks are the leading cause for music piracy but also used for music sampling prior to purchase. In this paper we investigate the relations between music file sharing and sales (both physical and digital)using large Peer-to-Peer query database information. We compare file sharing information on songs to their popularity on the Billboard Hot 100 and the Billboard Digital Songs charts, and show that popularity trends of songs on the Billboard have very strong correlation (0.88-0.89) to their popularity on a Peer-to-Peer network. We then show how this correlation can be utilized by common data mining algorithms to predict a song's success in the Billboard in advance, using Peer-to-Peer information.
Effects of Popular Exemplars in Television News
Common people that are apparently randomly selected by journalists to illustrate a news story (popular exemplars) have a substantial effect on what the audience think about the issue. This effect may be partly due to the mere fact that popular exemplars attract attention and act as attention commanders just like many other speaking sources in the news. Yet, popular exemplars’ effects extend well beyond that of other talking sources. Due to their similarity, trustworthiness, and the vividness of their account, popular exemplars have significantly more impact than experts that are being interviewed or, in particular, than politicians that are quoted in the news. We show this drawing on an internet-based experiment that uses fake television news items as stimuli and that systematically compares the effect of these talking sources in the news. We also find that taking into account preexisting attitudes changes the findings substantially. The effects are more robust and yield a more nuanced picture of what typ...
On second-order statistics of log-periodogram with correlated components
We derive an explicit expression for the covariance of the log-periodogram power spectral density estimator for a zero mean Gaussian process. We do not make the assumption that the spectral components of the process are uncorrelated. Applications to spectral estimation and to cepstral modeling in automatic speech recognition are discussed.
An additive exponential noise channel with a transmission deadline
We derive the maximum mutual information for an additive exponential noise (AEN) channel with a peak input constraint. We find that the optimizing input density is mixed (with singularities) similar to previous results for AEN channels with a mean input constraint. Likewise, the maximum mutual information takes a similar form, though obviously the maximum for the peak constraint is smaller than for the corresponding mean-constrained channel. This model is inspired by multiple biological phenomena and processes which can be abstracted as follows: inscribed matter is sent by an emitter, moves through a medium, and arrives eventually at its destination receptor. The inscribed matter can convey information in a variety of ways such as the number of signaling quanta - molecules, macromolecular complexes, organelles, cells and tissues - that are emitted as well as the detailed pattern of their release. However, rather than focus on a general class of emitter-receptor systems or a particular exemplar of biomedical importance, our ultimate goal is to provide bounds on the potential efficacy of timed-release signaling for any system which emits identical signaling quanta. That is, we seek to apply one of the most potent aspects of information theory to biological signaling - mechanism blindness - in the hopes of gaining insights applicable to diverse systems that span a wide range of spatiotemporal scales.
Exhaustive search for small fully absorbing sets and the corresponding low error-floor decoder
This work provides an exhaustive search algorithm for finding small fully absorbing sets (FASs) of arbitrary low-density parity-check (LDPC) codes. In particular, given any LDPC code, the problem of finding all FASs of size less than t is formulated as an integer programming problem, for which a new branch-&-bound algorithm is devised. New node selection and the tree-trimming mechanisms are designed to further enhance the efficiency of the algorithm. The proposed algorithm is capable of finding all FASs of size ≤ 11 with no larger than 2 induced odd-degree check nodes for LDPC codes of length ≤ 1000. The resulting exhaustive list of small FASs is then used to devise a new post-processing decoder. Numerical results show that by taking advantage of the exhaustive list of small FASs, the proposed decoder significantly lowers the error floor for codes of practical lengths and outperforms the state-of-the-art low-error-floor decoders.
A multi-objective artificial immune algorithm for parameter optimization in support vector machine
Support vector machine (SVM) is a classification method based on the structured risk minimization principle. Penalize, C; and kernel, @s parameters of SVM must be carefully selected in establishing an efficient SVM model. These parameters are selected by trial and error or man's experience. Artificial immune system (AIS) can be defined as a soft computing method inspired by theoretical immune system in order to solve science and engineering problems. A multi-objective artificial immune algorithm has been used to optimize the kernel and penalize parameters of SVM in this paper. In training stage of SVM, multiple solutions are found by using multi-objective artificial immune algorithm and then these parameters are evaluated in test stage. The proposed algorithm is applied to fault diagnosis of induction motors and anomaly detection problems and successful results are obtained.
A novel hidden station detection mechanism in IEEE 802.11 WLAN
The popular IEEE 802.11 wireless local area network (WLAN) is based on a carrier sense multiple access with collision avoidance (CSMA/CA), where a station listens to the medium before transmission in order to avoid collision. If there exist stations which can not hear each other, i.e., hidden stations, the potential collision probability increases, thus dramatically degrading the network throughput. The RTS/CTS (request-to-send/clear-to-send) frame exchange is a solution for the hidden station problem, but the RTS/CTS exchange itself consumes the network resources by transmitting the control frames. In order to maximize the network throughput, we need to use the RTS/CTS exchange adaptively only when hidden stations exist in the network. In this letter, a simple but very effective hidden station detection mechanism is proposed. Once a station detects the hidden stations via the proposed detection mechanism, it can trigger the usage of the RTS/CTS exchange. The simulation results demonstrate that the proposed mechanism can provide the maximum system throughput performance
Low complexity algorithm for robust video frame rate up-conversion (FRUC) technique
Two challenging situations for video frame rate up-conversion (FRUC) are first identified and analyzed; namely, when the input video has abrupt illumination change and/or a low frame rate. Then, a low-complexity processing technique and robust FRUC algorithm are proposed to address these two issues. The proposed algorithm utilizes a translational motion vector model of the first- and the second-order and detects the continuity of these motion vectors. Additionally, in order to improve perceptual quality of interpolated frame, spatial smoothness criterion is employed. The superior performance of the proposed algorithm has been tested extensively and representative examples are given in this work.
Timestamping schemes for MPEG-2 systems layer and their effect on receiver clock recovery
We propose and analyze several strategies for performing timestamping of an MPEG-2 Transport Stream transmitted over a packet-switched network using the PCR-unaware encapsulation scheme, and analyze their effect on the quality of the recovered clock at the MPEG-2 Systems decoder. When the timestamping scheme is based on a timer with a fixed period, the PCR values in the packet stream may switch polarity deterministically, at a frequency determined by the timer period and the transport rate of the MPEG signal. This, in turn, can degrade the duality of the recovered clock at the receiver beyond acceptable limits. We consider three timestamping schemes for solving this problem: (1) selecting a deterministic timer period to avoid the phase difference in PCR values altogether, (2) fine-tuning the deterministic timer period to maximize the frequency of PCR polarity changes, and (3) selecting the timer period randomly to eliminate the deterministic PCR polarity changes. For the case of deterministic timer period, we derive the frequency of the PCR polarity changes as a function of the timer period and the transport rate, and use it to find ranges of the timer period for acceptable quality of the recovered clock. We also analyze a random timestamping procedure based on a random telegraph process and obtain lower bounds on the rate of PCR polarity changes such that the recovered clock does not violate the PAL/NTSC clock specifications. The analytical results are verified by simulations with both synthetic and actual MPEG-2 Transport Streams sent to a simulation model of an MPEG-2 Systems decoder.
Pyramid transform and scale-space analysis in image analysis
The pyramid transform compresses images while preserving global features such as edges and segments. The pyramid transform is efficiently used in optical flow computation starting from planar images captured by pinhole camera systems, since the propagation of features from coarse sampling to fine sampling allows the computation of both large displacements in low-resolution images sampled by a coarse grid and small displacements in high-resolution images sampled by a fine grid.#R##N##R##N#The image pyramid transform involves the resizing of an image by downsampling after convolution with the Gaussian kernel. Since the convolution with the Gaussian kernel for smoothing is derived as the solution of a linear diffusion equation, the pyramid transform is performed by applying a downsampling operation to the solution of the linear diffusion equation.
Adaptive modelling of digital hearing aids using a subband affine projection algorithm
Adaptive modeling of digital hearing aids is useful in characterizing the hearing aid behavior in response to “real world” stimuli such as speech and music. Most modern hearing aids employ amplitude compression in different frequency bands for effective mapping of the wide dynamic range audio signals into the reduced dynamic range of the hearing impaired listeners. Due to the presence of independent compression channels, the conventional fullband adaptive model might not adequately characterize the performance of a multichannel compression hearing aid (MCHA). In this paper, we propose a subband adaptive modeling approach to characterize the electroacoustic performance of a MCHA. The proposed structure employs uniform, oversampled DFT filterbanks for analysis and synthesis, and the affine projection algorithm for adaptive modeling in each subband. Experiments with simulated MCHAs showed that the subband structure outperforms the fullband structure under a variety of operating conditions.
A Cost Effective Centralized Adaptive Routing for Networks-on-Chip
As the number of applications and programmable units in CMPs and MPSoCs increases, the Network-on-Chip (NoC) encounters unpredictable, heterogeneous and time dependent traffic loads. This motivates the introduction of adaptive routing mechanisms that balance the NoC's loads and achieve higher throughput compared with traditional oblivious routing schemes. An effective adaptive routing scheme should be based on a global view of the network state. However, most current adaptive routing schemes, following off-chip networks, are based on distributed reactions to local congestion. In this paper we leverage the unique on-chip capabilities and introduce a novel paradigm of NoC centralized adaptive routing. Our scheme continuously monitors the global traffic load in the network and modifies the routing of packets to improve load balancing accordingly. We present a specific design for the case of mesh topology, where XY or YX routes are adaptively selected for each source-destination pair. We show that while our implementation is lightweight and scalable in hardware costs, it outperforms oblivious and distributed adaptive routing schemes in terms of load balancing and average packet delay.
Robust directional features for wordspotting in degraded Syriac manuscripts
This paper presents a contribution to Word Spotting applied for digitized Syriac manuscripts. The Syriac language was wrongfully accused of being a dead language and has been set aside by the domain of handwriting recognition. Yet it is a very fascinating handwriting that combines the word structure and calligraphy of the Arabic handwriting with the particularity of being intentionally written tilted by an angle of approximately 45deg. For the spotting process, we developed a method that should find all occurrences of a certain query word image, based on a selective sliding window technique, from which we extract directional features and afterwards perform a matching using Euclidean distance correspondence between features. The proposed method does not require any prior information, and does not depend of a word to character segmentation algorithm which would be extremely complex to realize due to the tilted nature of the handwriting.
Statistical semantics for enhancing document clustering
Document clustering algorithms usually use vector space model (VSM) as their underlying model for document representation. VSM assumes that terms are independent and accordingly ignores any semantic relations between them. This results in mapping documents to a space where the proximity between document vectors does not reflect their true semantic similarity. This paper proposes new models for document representation that capture semantic similarity between documents based on measures of correlations between their terms. The paper uses the proposed models to enhance the effectiveness of different algorithms for document clustering. The proposed representation models define a corpus-specific semantic similarity by estimating measures of term–term correlations from the documents to be clustered. The corpus of documents accordingly defines a context in which semantic similarity is calculated. Experiments have been conducted on thirteen benchmark data sets to empirically evaluate the effectiveness of the proposed models and compare them to VSM and other well-known models for capturing semantic similarity.
Transcript mapping for historic handwritten document images
There is a large number of scanned historical documents that need to be indexed for archival and retrieval purposes. A visual word spotting scheme that would serve these purposes is a challenging task even when the transcription of the document image is available. We propose a framework for mapping each word in the transcript to the associated word image in the document. Coarse word mapping based on document constraints is used for lexicon reduction. Then, word mappings are refined using word recognition results by a dynamic programming algorithm that finds the best match while satisfying the constraints.
Less Is More: Mixed-Initiative Model-Predictive Control With Human Inputs
This paper presents a new method for injecting human inputs into mixed-initiative interactions between humans and robots. The method is based on a model-predictive control (MPC) formulation, which inevitably involves predicting the system (robot dynamics as well as human input) into the future. These predictions are complicated by the fact that the human is interacting with the robot, causing the prediction method itself to have an effect on future human inputs. We investigate and develop different prediction schemes, including fixed and variable horizon MPCs and human input estimators of different orders. Through a search-and-rescue-inspired human operator study, we arrive at the conclusion that the simplest prediction methods outperform the more complex ones, i.e., in this particular case, less is indeed more.
Enhanced Color-Theory-Based Dynamic Localization in Mobile Wireless Sensor Networks
There are few localization schemes targeted at mobile wireless sensor networks. This paper proposed an enhanced color-theory-based dynamic localization (E-CDL) which is based on the CDL algorithm (Shee, 2005). However, the location accuracy of this algorithm depends on the accuracy of the average hop distance derivation. Therefore, the authors present two novel schemes to estimate the average hop distance. The authors analyzed the behavior of sensor nodes communication, and computed the expected value of the average hop distance, which is 7r/9 where r is the radio range. In addition, since CDL is based on the DV-hop scheme, the derived shortest path length is usually larger than the corresponding Euclidean distance. With this observation, the derived shortest path length can be adjusted by the ratio of the Euclidean distance and the shortest path distance to further enhance the location accuracy. Finally, in mobile wireless sensor networks, sensor nodes may become isolated. By employing mobile anchor nodes, the isolation problem can be relieved and hence the location accuracy can be improved. Simulation results have shown that the location accuracy of E-CDL is 50%-55% better than that of CDL, and 75%-80% better than that of MCL (Monte Carlo localization) (Lingxuan and David, 2004), In addition, the authors have implemented and verified our algorithm on the MICAz Mote developer's kit.
Biometric binary string generation with detection rate optimized bit allocation
Extracting binary strings from real-valued templates has been a fundamental issue in many biometric template protection systems. In this paper, we present an optimal bit allocation method (OBA). By means of it, a binary string at a pre-defined length with maximized overall detection rate is generated. Experiments with the binary strings and a Hamming distance classifier on FRGC and FERET databases show promising performance in terms of FAR and FRR.
Dynamic identification of a 6 dof robot without joint position data
Off-line robot dynamic identification methods are mostly based on the use of the inverse dynamic model, which is linear with respect to the dynamic parameters. This model is calculated with torque and position sampled data while the robot is tracking reference trajectories that excite the system dynamics. This allows using linear least-squares techniques to estimate the parameters. This method requires the joint force/torque and position measurements and the estimate of the joint velocity and acceleration, through the bandpass filtering of the joint position at high sampling rates. A new method called DIDIM (Direct and Inverse Dynamic Identification Models) has been proposed and validated on a 2 degree-of-freedom robot [1]. DIDIM method requires only the joint force/torque measurement. It is based on a closed-loop simulation of the robot using the direct dynamic model, the same structure of the control law, and the same reference trajectory for both the actual and the simulated robot. The optimal parameters minimize the 2-norm of the error between the actual force/torque and the simulated force/torque. A validation experiment on a 6 dof Staubli TX40 robot shows that DIDIM method is very efficient on industrial robots.
Bit-Stream Switching in Multiple Bit-Rate Video Streaming using Wyner-Ziv Coding
It has been commonly recognized that multiple bit-rate (MBR) encoding provides a concise method for video streaming over bandwidth-fluctuant networks. The key problem of the MBR technique lies in how to seamlessly switch one bit-stream to another one. To tackle this problem, we propose a bit-stream switching framework based on the Wyner-Ziv coding. Within the propose framework, the multiple bit-streams can be individually encoded without data exchange, which also supports the random switching at any desired frame without affecting the original coding efficiency of the regular bit-stream. In particular, two different implementation schemes under the same framework are presented. Different from the traditional switching schemes, the proposed method can use the same switching frame for the switching from any other bit-stream to the current one, which means less storage and less encoding efforts. Simulation results and comparison between the proposed method and the traditional switching method in H.264 are also presented.
An architecture for linking medical decision-support applications to clinical databases and its evaluation
We describe and evaluate a framework, the Medical Database Adaptor (MEIDA), for linking knowledge-based medical decision-support systems (MDSSs) to multiple clinical databases, using standard medical schemata and vocabularies. Our solution involves a set of tools for embedding standard terms and units within knowledge bases (KBs) of MDSSs; a set of methods and tools for mapping the local database (DB) schema and the terms and units relevant to the KB of the MDSS into standardized schema, terms and units, using three heuristics (choice of a vocabulary, choice of a key term, and choice of a measurement unit); and a set of tools which, at runtime, automatically map standard term queries originating from the KB, to queries formulated using the local DB's schema, terms and units. The methodology was successfully evaluated by mapping three KBs to three DBs. Using a unit-domain matching heuristic reduced the number of term-mapping candidates by a mean of 71% even after other heuristics were used. Runtime access of 10,000 records required one second. We conclude that mapping MDSSs to different local clinical DBs, using the three-phase methodology and several term-mapping heuristics, is both feasible and efficient.
Animation Metaphors for Object-Oriented Concepts
Program visualization and animation has traditionally been done at the level of the programming language and its implementation in a computer. However, novices do not know these concepts and visualizations that build upon programming language implementation may easily fail in helping novices to learn programming concepts. Metaphor, on the contrary, involves the presentation of a new idea in terms of a more familiar one and can facilitate active learning. This paper applies a metaphor approach to object-oriented programming by presenting new metaphors for such concepts as class, object, object instantiation, method invocation, parameter passing, object reference, and garbage collection. The use of these metaphors in introductory programming education is also discussed.
Performance of recovery time improvement algorithms for software RAIDs
A software RAID is a RAID implemented purely in software running on a host computer. One problem with software RAIDs is that they do not have access to special hardware such as NVRAM. Thus, software RAIDs may need to check every parity group of an array for consistency following a host crash or power failure. This process of checking parity groups is called recovery, and results in long delays when the software RAID is restarted. The authors review two algorithms to reduce this recovery time for software RAIDs: the PGS bitmap algorithm and the list algorithm. They compare the performance of these two algorithms using trace-driven simulations. Their results show that the PGS bitmap algorithm can reduce recovery time by a factor of 12 with a response time penalty of less than 1%, or by a factor of 50 with a response time penalty of less than 2%, and a memory requirement of around 9 Kbytes. The list algorithm can reduce recovery time by a factor of 50 but cannot achieve a response time penalty of less than 16%.
Independent gate SRAM based on asymmetric gate to source/drain overlap-underlap device FinFET
The read-write ability of SRAM cells is one of the major concern in nanometer regime. This paper analyzes the stability and performance of asymmetric FinFET based different schematic of 6T SRAM cells. The proposed structure exploits asymmetrical behavior of current to improve read-write stability of SRAM. By exploiting the asymmetricity in proposed structure, contradiction between read and write noise margin (RNM and WNM) is relaxed. The overall improvements in static, read and write noise margins for proposed asymmetric FinFET based independent gate SRAM (IGSRAM) are 28%, 71%, and 31% respectively.
Distributed, scalable, and static parallel arc consistency algorithms on private memory machines
Several arc consistency algorithms for sequential and parallel processing computers are reviewed. Three distributed parallel arc consistency algorithms-DSPAC-1, DSPAC-2, and DSPAC-3-are introduced and compared with existing algorithms. Through actual machine experimentation the time required for the DSPAC algorithms was measured and compared with that for existing sequential algorithms. Results indicate that the parallel arc consistency algorithms are very effective and that scalability can be efficiently maintained. >
Provisioning On-Chip Networks under Buffered RC Interconnect Delay Variations
A network-on-chip (NoC) replaces on-chip communication implemented by point-to-point interconnects in a multi-core environment by a set of shared interconnects connected through programmable crosspoints. Since an NoC may provide a number of paths between a given source and destination, manufacturing or runtime faults on one interconnect does not necessarily render the chip useless. It is partly because of this fault tolerance that NoCs have emerged as a viable alternative for implementing communication between functional units of a chip in the nanometer regime, where high defect rates are prevalent. In this paper, the authors quantify the fault tolerance offered by an NoC against process variations. Specifically, the authors develop an analytical model for the probability of failure in buffered global NoC links due to interconnect dishing, and effective channel length variation. Using the developed probability model, the authors study the impact of link failure on the number of cycles required to establish communications in NoC applications
Accurate Distributed Range-Based Positioning Algorithm for Wireless Sensor Networks
Localization of sensor nodes is a fundamental and important problem in wireless sensor networks. In this correspondence, a recursive distributed positioning algorithm is devised with the use of range measurements. Computer simulations are included to contrast the performance of the proposed approach with the conventional semi-definite relaxation positioning method as well as Crameacuter-Rao lower bound.
Prospective multi-centre Voxel Based Morphometry study employing scanner specific segmentations : procedure development using CaliBrain structural MRI data
Background#R##N#Structural Magnetic Resonance Imaging (sMRI) of the brain is employed in the assessment of a wide range of neuropsychiatric disorders. In order to improve statistical power in such studies it is desirable to pool scanning resources from multiple centres. The CaliBrain project was designed to provide for an assessment of scanner differences at three centres in Scotland, and to assess the practicality of pooling scans from multiple-centres.
Improving the scalability of cloud-based resilient database servers
Many rely now on public cloud infrastructure-as-a-service for database servers, mainly, by pushing the limits of existing pooling and replication software to operate large shared-nothing virtual server clusters. Yet, it is unclear whether this is still the best architectural choice, namely, when cloud infrastructure provides seamless virtual shared storage and bills clients on actual disk usage.   This paper addresses this challenge with Resilient Asynchronous Commit (RAsC), an improvement to awell-known shared-nothing design based on the assumption that a much larger number of servers is required for scale than for resilience. Then we compare this proposal to other database server architectures using an analytical model focused on peak throughput and conclude that it provides the best performance/cost trade-off while at the same time addressing a wide range of fault scenarios.
A neural network-based image processing system for detection of vandal acts in unmanned railway environments
Lately, the interest in advanced video-based surveillance applications has been increasing. This is especially true in the field of urban railway transport where video-based surveillance can be exploited to face many relevant security aspects (e.g. vandalism, overcrowding, abandoned object detection etc.). This paper aims at investigating an open problem in the implementation of video-based surveillance systems for transport applications, i.e., the implementation of reliable image understanding modules in order to recognize dangerous situations with reduced false alarm and misdetection rates. We considered the use of a neural network-based classifier for detecting vandal behavior in metro stations. The achieved results show that the classifier achieves very good performance even in the presence of high scene complexity.
Recursive binary dilation and erosion using digital line structuring elements in arbitrary orientations
Performing morphological operations such as dilation and erosion of binary images, using very long line structuring elements is computationally expensive when performed brute-force following definitions. We present two-pass algorithms that run at constant time for obtaining binary dilations and erosions with all possible length line structuring elements, simultaneously. The algorithms run at constant time for any orientation of the line structuring element. Another contribution of this paper is the use of the concept of orientation error between a continuous line and its discrete counterpart. The orientation error is used in determining the minimum length of the basic digital line structuring element used in obtaining what we call dilation and erosion transforms. The transforms are then thresholded by the length of the desired structuring element to obtain the dilation and erosion results. The algorithms require only one maximum operation for erosion transform and only one minimum operation for dilation transform, and one thresholding step and one translation step per result pixel. We tested the algorithms on Sun Sparc Station 10, on a set of 240/spl times/250 salt and pepper noise images with probability of a pixel being a 1-pixel set to 0.25, for orientations of the normals of the structuring elements in the range [/spl pi//2,3/spl pi//2] and lengths, in pixels, in the range [5,145]. We achieved a speed up of about 50 (and for special orientations /spl theta/ /spl isin/ {(/spl pi//2), (3/spl pi//4), /spl pi/, (5/spl pi//4), (3/spl pi//2)} a speed up of about 100) when the structuring elements had lengths of 145 pixels, over the brute-force methods in these experiments. We compared the results of our dilation algorithm with those of the algorithm discussed by Soille et al. (see IEEE Trans. Pattern Anal. Machine Intell., vol.18, p.562-67, 1996) and showed that for binary dilation (and erosion since it is just the dilation of the background with the reflected structuring element) our algorithm performed better and achieved a speed up of about four when dilation or erosion transform alone is obtained.
Using assignment examples to infer weights for ELECTRE TRI method: Some experimental results
Given a finite set of alternatives A, the sorting (or assignment) problem consists in the assignment of each alternative to one of the pre-defined categories. In this paper, we are interested in multiple criteria sorting problems and, more precisely, in the existing method ELECTRE TRI. This method requires the elicitation of preferential parameters (weights, thresholds, category limits,…) in order to construct a preference model which the decision maker (DM) accepts as a working hypothesis in the decision aid study. A direct elicitation of these parameters requiring a high cognitive effort from the DM (V. Mosseau, R. Slowinski, Journal of Global Optimization 12 (2) (1998) 174), proposed an interactive aggregation–disaggregation approach that infers ELECTRE TRI parameters indirectly from holistic information, i.e., assignment examples. In this approach, the determination of ELECTRE TRI parameters that best restore the assignment examples is formulated through a nonlinear optimization program.#R##N##R##N#In this paper, we consider the subproblem of the determination of the weights only (the thresholds and category limits being fixed). This subproblem leads to solve a linear program (rather than nonlinear in the global inference model). Numerical experiments were conducted so as to check the behaviour of this disaggregation tool. Results showed that this tool is able to infer weights that restores in a stable way the assignment examples and that it is able to identify “inconsistencies” in the assignment examples.
A 250 mV 8 kb 40 nm Ultra-Low Power 9T Supply Feedback SRAM (SF-SRAM)
Low voltage operation of digital circuits continues to be an attractive option for aggressive power reduction. As standard SRAM bitcells are limited to operation in the strong-inversion regimes due to process variations and local mismatch, the development of specially designed SRAMs for low voltage operation has become popular in recent years. In this paper, we present a novel 9T bitcell, implementing a Supply Feedback concept to internally weaken the pull-up current during write cycles and thus enable low-voltage write operations. As opposed to the majority of existing solutions, this is achieved without the need for additional peripheral circuits and techniques. The proposed bitcell is fully functional under global and local variations at voltages from 250 mV to 1.1 V. In addition, the proposed cell presents a low-leakage state reducing power up to 60%, as compared to an identically supplied 8T bitcell. An 8 kbit SF-SRAM array was implemented and fabricated in a low-power 40 nm process, showing full functionality and ultra-low power.
Map-Aided Evidential Grids for Driving Scene Understanding
Evidential grids have recently been shown to have interesting properties for mobile object perception. Possessing only partial information is a frequent situation when driving in complex urban areas, and by making use of the Dempster-Shafer framework, evidential grids are able to handle partial information efficiently. This article deals with a lidar perception scheme that is enhanced by geo-referenced maps used as an additional source of information in a multi-grid fusion framework. The paper looks at the key stages of such a data fusion process and presents an adaptation of the conjunctive combination rule for refining the analysis of conflicting information. This method relies on temporal accumulation to distinguish between stationary and moving objects, and applies contextual discounting for modeling information obsolescence. As a result, the method is able to better characterize the state of the occupied cells by differentiating moving objects, parked cars, urban infrastructure and buildings. Another advantage of this approach is its ability to separate the drivable from the non-drivable free space. Experiments carried out in real traffic conditions with a specially equipped car illustrate the performance of this approach.
Interferential Packet Detection Scheme for a Solution to Overlapping BSS Issues in IEEE 802.11 WLANs
In this manuscript, an interferential packet detection scheme in IEEE 802.11 WLANs is proposed. If another Basic Service Set (BSS) is overlapping domestic BSS, some Stations (STAs) may suffer from interference from a hidden terminal in overlapping BSS (OBSS). One of the best ways to avoid this undesirable situation is channel switching after detecting OBSS. However there are some difficulties to recognize existence of OBSS. One difficulty is that STAs in domestic BSS can't receive frames from OBSS correctly and can't check BSSID of frames if traffic in domestic BSS is heavy and if transmission rates of frames from OBSS are higher. Another difficulty is that if the cell radius of domestic BSS is smaller than that of OBSS, some STAs in OBSS may transmit frames asynchronously and interfere with transmissions in domestic BSS. If interference causes frame error, domestic STA can?t distinguish interference from degradation of channel condition. This paper proposes a method whereby STAs can detect interferential packet even while STAs receive frames from domestic BSS. If STAs detect interference, channel switching is performed dynamically. The probability of detecting interferential packets is evaluated by computer simulation, and the results confirm the effectiveness of the proposed method.
Optimal error estimates for finite element discretization of elliptic optimal control problems with finitely many pointwise state constraints
In this paper we consider a model elliptic optimal control problem with finitely many state constraints in two and three dimensions. Such problems are challenging due to low regularity of the adjoint variable. For the discretization of the problem we consider continuous linear elements on quasi-uniform and graded meshes separately. Our main result establishes optimal a priori error estimates for the state, adjoint, and the Lagrange multiplier on the two types of meshes. In particular, in three dimensions the optimal second order convergence rate for all three variables is possible only on properly refined meshes. Numerical examples at the end of the paper support our theoretical results.
Taxonomy of trust: Categorizing P2P reputation systems
The field of peer-to-peer reputation systems has exploded in the last few years. Our goal is to organize existing ideas and work to facilitate system design. We present a taxonomy of reputation system components, their properties, and discuss how user behavior and technical constraints can conflict. In our discussion, we describe research that exemplifies compromises made to deliver a useable, implementable system.
Performance of a 60-GHz DCM-OFDM and BPSK-Impulse Ultra-Wideband System with Radio-Over-Fiber and Wireless Transmission Employing a Directly-Modulated VCSEL
The performance of radio-over-fiber optical transmission employing vertical-cavity surface-emitting lasers (VCSELs), and further wireless transmission, of the two major ultra-wideband (UWB) implementations is reported when operating in the 60-GHz radio band. Performance is evaluated at 1.44 Gbit/s bitrate. The two UWB implementations considered employ dual-carrier modulation orthogonal frequency-division multiplexing (DCM-OFDM) and binary phase-shift keying impulse radio (BPSK-IR) modulation respectively. Optical transmission distances up to 40 km in standard single-mode fiber and up to 500 m in bend-insensitive single-mode fiber with wireless transmission up to 5 m in both cases is demonstrated with no penalty. A simulation analysis has also been performed in order to investigate the operational limits. The analysis results are in excellent agreement with the experimental work and indicate good tolerance to chromatic dispersion due to the chirp characteristics of electro-optical conversion when a directly-modulated VCSEL is employed. The performance comparison indicates that BPSK-IR UWB exhibits better tolerance to optical transmission impairments requiring lower received optical power than its DCM-OFDM UWB counterpart when operating in the 60-GHz band.
DPLL bit synchronizer with rapid acquisition using adaptive Kalman filtering techniques
A second-order DPLL with time-varying loop gains is applied to the symbol synchronization of burst mode data signals. An algorithm to control the DPLL loop gains is derived from adaptive Kalman filtering theory. Simulation results for the variable gain DPLL compared to a fixed gain DPLL demonstrate the improved acquisition performance. >
A 65 nm CMOS Quad-Band SAW-Less Receiver SoC for GSM/GPRS/EDGE
A quad-band 2.5G receiver is designed to replace the front-end SAW filters with on-chip bandpass filters and to integrate the LNA matching components, as well as the RF baluns. The receiver achieves a typical sensitivity of -110 dBm or better, while saving a considerable amount of BOM. Utilizing an arrangement of four baseband capacitors and MOS switches driven by 4-phase 25% duty-cycle clocks, high-Q BPF's are realized to attenuate the 0 dBm out-of-band blocker. The 65 nm CMOS SAW-less receiver integrated as a part of a 2.5G SoC, draws 55 mA from the battery, and measures an out-of-band 1 dB-compression of greater than +2 dBm. Measured as a stand-alone, as well as the baseband running in call mode in the platform level, the receiver passes the 3GPP specifications with margin.
Automated evaluation of HER-2/neu immunohistochemical expression in breast cancer using digital microscopy
HER-2/neu (HER2) has been shown to be a valuable biomarker for breast cancer. However, inter-observer variability has been reported in the evaluation of HER2 with immunohistochemistry. It has been suggested that automated computer-based evaluation can provide a consistent and objective measure of HER2 expression. In this manuscript, we present an automated method for the quantitative assessment of HER2 using digital microscopy. The method employs imaging algorithms on whole slide images of tissue specimens for the extraction of two features describing HER2 membrane staining, namely membrane staining completeness and membrane staining intensity. A classifier was trained to merge the extracted features into an overall slide assessment score. Preliminary results showed good agreement with the provided truth. The developed automated method has the potential to be used as a computer aid for the immunohistochemical evaluation of HER2 expression with the objective of increasing observer reproducibility.
Adding expressiveness to musical messages
A system to add expressiveness to musical messages has been developed, starting from the results of acoustic and perceptual analyses. The system allows to obtain different performances, by modifying the acoustic parameters of a given neutral performance. The modification of the input performance is performed by a model that uses the hierarchical segmentation of the musical organization. For every hierarchical level, opportune curves are applied to the principal acoustic parameters. Level's self-similarity is the main criteria to construct the curves. The modular structure of the system defines an open architecture, where the rendering steps can be realized both with synthesis and post-processing techniques. Different synthesis techniques, like FM, physical models or wavetable have been explored.
Observations on using empirical studies on developing a knowledge-based software engineering tool
There exist a wide variety of techniques for performing empirical studies which researchers in human-computer interaction have adapted from fields of cognitive psychology, sociology and anthropology. An analysis of several of these techniques is presented through an approach that balances empirical study with tool development. The analysis is based on, and illustrated with, a several-year experience of consulting in a scientific software environment and in building an evaluating a prototype knowledge-based tool to capture aspects of that experience. Guidelines for applying specific techniques and cautions about potential pitfalls are discussed. Many additional examples of using the techniques are cited from the literature. >
Fire Detection by Microwave Radiometric Sensors: Modeling a Scenario in the Presence of Obstacles
This paper deals with the problem of fire detection in the presence of obstacles that are nontransparent to visible or infrared wavelengths. Exploiting the obstacle penetration capability of microwaves, a solution based on passive microwave radiometry has been proposed. To investigate such a solution, a theoretical model of the scene sensed by a microwave radiometer is developed, accounting for the presence of both fire spot and wall-like obstacles. By reversing the model's equations, it is possible to directly relate the obstacle emissivity, reflectivity, and transmissivity to the antenna noise temperatures measured in several conditions. These temperatures have been sensed with a portable low-cost instrument. The selected 12.65-GHz operation frequency features good wall penetration capability to be balanced with a reasonable antenna size. In order to verify the aforementioned model, several fire experiments have been carried out, resulting in an overall good agreement between measurements and developed theory. In particular, a 2-cm-thick plasterboard wall, typically used for indoor building construction, shows a transmissivity equal to 0.86 and can easily be penetrated by a microwave radiometer in the X-band.
Efficient evaluation of queries with mining predicates
Modern relational database systems are beginning to support ad-hoc queries on data mining models. In this paper, we explore novel techniques for optimizing queries that apply mining models to relational data. For such queries, we use the internal structure of the mining model to automatically derive traditional database predicates. We present algorithms for deriving such predicates for some popular discrete mining models: decision trees, naive Bayes, and clustering. Our experiments on a Microsoft SQL Server 2000 demonstrate that these derived predicates can significantly reduce the cost of evaluating such queries.
Stochastic processes via the pathway model
After collecting data from observations or experiments, the next step is to analyze the data to build an appropriate mathematical or stochastic model to describe the data so that further studies can be done with the help of the model. In this article, the input-output type mechanism is considered first, where reaction, diffusion, reaction-diffusion, and production-destruction type physical situations can fit in. Then techniques are described to produce thicker or thinner tails (power law behavior) in stochastic models. Then the pathway idea is described where one can switch to different functional forms of the probability density function through a parameter called the pathway parameter. The paper is a continuation of related solar neutrino research published previously in this journal.
Predictive Metamorphic Control
Model Predictive Control (MPC) has become widely accepted in industry. The reason for its success are manifold including easy implementation, ability to handle constraints, capacity to deal with nonlinearities, etc. However, the method does have drawbacks including tuning difficulties. In this paper, we propose an embellishment to the basic MPC strategy by incorporating a tuning parameter such that one can move continuously from an existing controller to a new MPC strategy. The continuous change of this tuning parameter leads to a continuously varying stabilizing control law. Since the proposed strategy allows one to slowly move from an existing control law to a new and better one, we term the strategy Predictive Metamorphic Control. For the case of an infinite horizon problem without constraints and for the general case with state and input constraints, stability results are established. The merits of the proposed method are illustrated by examples.
Efficient network flow based min-cut balanced partitioning
We consider the problem of bipartitioning a circuit into two balanced components that minimizes the number of crossing nets. Previously, the Kernighan and Lin type (K&L) heuristics, the simulated annealing approach, and the spectral method were given to solve the problem. However, network flow techniques were overlooked as a viable approach to min-cut balanced bipartition to due its high complexity. In this paper we propose a balanced bipartition heuristic based on repeated max-flow min-cut techniques, and give an efficient implementation that has the same asymptotic time complexity as that of one max-flow computation. We implemented our heuristic algorithm in a package called FBB. The experimental results demonstrate that FBB outperforms the K&L heuristics and the spectral method in terms of the number of crossing nets, and the efficient implementation makes it possible to partition large, circuit instances with reasonable runtime. For example, the average elapsed time for bipartitioning a circuit S35932 of almost 20K gates is less than 20 minutes.
Reconstruction of polynomial systems from noisy time-series measurements using genetic programming
The problem of functional reconstruction of a polynomial system from its noisy time-series measurement is addressed in this paper. The reconstruction requires the determination of the embedding dimension and the unknown polynomial structure. The authors propose the use of genetic programming (GP) to find the exact functional form and embedding dimension of an unknown polynomial system from its time-series measurement. Using functional operators of addition, multiplication and time delay, they use GP to reconstruct the exact polynomial system and its embedding dimension. The proposed GP approach uses an improved least-squares (ILS) method to determine the parameters of a polynomial system. The ILS method is based on the orthogonal Euclidean distance to obtain an accurate parameter estimate when the series is corrupted by measurement noise. Simulations show that the proposed ILS-GP method can successfully reconstruct a polynomial system from its noisy time-series measurements.
Grid Service for Environmental Data Retrieval and Disasters Detection Based on Satellite Image Analysis
Considering that one of today's biggest global concerns is related to the climate change and its imminent undesired effects, we present the approach of creating and offering a public Web service to provide real-time access to environmental data and information. One of service's direct usages is natural disasters detection, but it could be further used for developing complex statistics and prediction generators, or for other environment related applications. The data is extracted from a satellite imagery repository implemented on a Grid infrastructure. For testing the capabilities of the service for different type of users, a visualization and interaction Web application has been developed. The service is integrated in the MedioGRID system.
An Evaluation Framework for Energy Aware Buildings using Statistical Model Checking
Cyber-physical systems are to be found in numerous applications throughout society. The principal barrier to develop trustworthy cyber-physical systems is the lack of expressive modelling and specification formalisms supported by efficient tools and methodologies. To overcome this barrier, we extend in this paper the modelling formalism of the tool UPPAAL-SMC to stochastic hybrid automata, thus providing the expressive power required for modelling complex cyber-physical systems. The application of Statistical Model Checking provides a highly scalable technique for analyzing performance properties of this formalisms.
ML-Flex: a flexible toolbox for performing classification analyses in parallel
Motivated by a need to classify high-dimensional, heterogeneous data from the bioinformatics domain, we developed ML-Flex, a machine-learning toolbox that enables users to perform two-class and multi-class classification analyses in a systematic yet flexible manner. ML-Flex was written in Java but is capable of interfacing with third-party packages written in other programming languages. It can handle multiple input-data formats and supports a variety of customizations. MLFlex provides implementations of various validation strategies, which can be executed in parallel across multiple computing cores, processors, and nodes. Additionally, ML-Flex supports aggregating evidence across multiple algorithms and data sets via ensemble learning. This open-source software package is freely available from http://mlflex.sourceforge.net.
Pacemaker interference and low-frequency electric induction in humans by external fields and electrodes
The possibility of interference by low-frequency external electric fields with cardiac pacemakers is a matter of practical concern. For pragmatic reasons, experimental investigations into such interference have used contact electrode current sources. However, the applicability to the external electric field problem remains unclear. The recent development of anatomically based electromagnetic models of the human body, together with progress in computational electromagnetics, enable the use of numerical modeling to quantify the relationship between external field and contact electrode excitation. This paper presents a comparison between the computed fields induced in a 3.6-mm-resolution conductivity model of the human body by an external electric field and by several electrode source configurations involving the feet and either the head or shoulders. The application to cardiac pacemaker interference is also indicated.
A Structurally Stable Globally Adaptive Internal Model Regulator for MIMO Linear Systems
The problem of compensating an uncertain disturbance and/or tracking some reference signals for a general linear MIMO system is studied in this work using the robust regulation theory frame. The disturbances are assumed to be composed by a known number of distinct sinusoidal signals with unknown phases, amplitude and frequencies. Under suitable assumptions, an exponentially convergent estimator of the unknown disturbance parameters is proposed and introduced into the classical robust regulator design to obtain an adaptive controller. This controller guarantees that the closed-loop robust regulation is attained in some neighborhood of the nominal values of the parameters of system. A simulated example shows the validity of the proposed approach.
Developing discriminate model and comparative analysis of differentially expressed genes and pathways for bloodstream samples of diabetes mellitus type 2
Background#R##N#Diabetes mellitus of type 2 (T2D), also known as noninsulin-dependent diabetes mellitus (NIDDM) or adult-onset diabetes, is a common disease. It is estimated that more than 300 million people worldwide suffer from T2D. In this study, we investigated the T2D, pre-diabetic and healthy human (no diabetes) bloodstream samples using genomic, genealogical, and phonemic information. We identified differentially expressed genes and pathways. The study has provided deeper insights into the development of T2D, and provided useful information for further effective prevention and treatment of the disease.
An autonomous sewer robots navigation based on stereo camera information
In this paper, we propose a method for autonomous sewer robots to navigate through a sewer pipe system based on stereo camera information. In this method, local features such as manholes and pipe joints are extracting as a feature pixels in the region of interest (ROI) of left image. Then, an accurate and fast stereo matching measure named linear computation is implemented in this ROI image to compute the distance between the robots and local features. Finally, the distance data can be used for navigation map in sewer pipe system. The experimental results show that our method can provide sufficient information for autonomous sewer robots navigation
Introducing a geographic information system as computer tool to apply the problem-based learning process in public buildings indoor routing
Abstract#R##N##R##N#A geographic information system (GIS) is presented in this work with the aim of helping the application of problem-based learning process to show the students how to adopt the appropriate decisions for the adaptation of architectural barriers to ensure the universal accessibility in public buildings. The GIS developed here consists of three layers based on vector maps corresponding to buildings, potential routes and architectural barriers. Hyperlinks in the last layer allow access to some relevant information about each barrier, such as type, description and adaptation cost. Several tests have been carried out to show the capability of the implemented GIS to locate indoor barriers, determine suitable indoor routes by considering criteria such as paths lengths or the total cost of barrier elimination, and update the information corresponding to each architectural barrier. In addition, the application of the proposed GIS has also been explored for indoor route guidance with promising results. This investigation has been carried out with the aim of being combined with the problem-based learning process. © 2010 Wiley Periodicals, Inc. Comput Appl Eng Educ 21: 573–580, 2013
A Practical Location-aided Energy-aware Routing Method for UWB-based Sensor Networks
In this paper, a practical location-aided routing method for sensor networks based on ultra-wideband (UWB) technique is proposed and evaluated. This method makes use of the positioning function of UWB and takes into account the energy consumption in the network. By modeling the property of energy consumption, we find that energy and quality-of-service (QOS) issues are greatly influenced by the route selected. Accordingly, a new routing algorithm is derived to search for energy-efficient routes that can support adequate QOS requirements. The simulation results have proved the advantages of this routing scheme.
Waveband switching networks with limited wavelength conversion
We study reconfigurable multi-granular optical cross-connects (MG-OXCs) in waveband switching networks with limited wavelength conversion and propose a heuristic algorithm to minimize the number of used wavelength converters while reducing the blocking probability.
An identity-based identification scheme based on discrete logarithms modulo a composite number
We first describe a modification of Schnorr's identification scheme, in which the modulus is composite (instead of prime). This modification has some similarity with Brickell-McCurky's one, presented at the same conference. Then, by establishing a new set-up, we derive the first identity-based identification scheme based on discrete logarithms. More precisely, it is based on discrete logarithm modulo a composite number, a problem known to be harder than factorization problem. This scheme has interesting and somewhat paradoxical features. In particular, any user can choose his own secret, and, provided the parameters have convenient sizes, even the trusted center is unable to retrieve it from the public key (contrary to any identity-based scheme known until now).
Control of Artificial Pneumatic Muscle for Robot Application
Pneumatic muscle has many advantages such as elasticity, high power and structural similarity to a living thing's muscle. There has been many researches to control robot actuated by pneumatic muscles, but conventional theories are hard to apply on real robot plants because of their assumptions and disregards of pneumatic muscle's physical aspects like size of pneumatic muscle and its controller. Here, the new method for saving space which is occupied by many controllers to operate robot actuated by pneumatic muscles is proposed. Actually there is easy way to control pneumatic muscle using the commercial proportional pressure regulator, but its size is not suitable to be embedded on stand alone robot. So, new method using the pressure switches of compact size and encoders is suggested. This new method is tested on a robot link with ball joints, actuated by four pneumatic muscles.
The role of EDM in information management within SMEs
Electronic document management (EDM) is a new form of information management. EDM is described to have certain business values in organizations, but no research has been found about EDM and Small and Medium sized Enterprises (SME). In this paper we present an ongoing investigation in two SMEs guided by the following research questions: "How are electronic documents used in the SMEs? "and "What are the business needs of the SMEs, and how do they correspond with stated EDM business values?". The study was carried out as two qualitative case studies in two SMEs in the north of Sweden. The results show that the business need for an SME corresponds with the business values of EDM. Yet is management of electronic document too dependent on individuals' competence, very complex when many systems are involved, and the context where the document is created is not preserved. There is also an emergent need for an organization-specific classification scheme to enable information sharing between systems.
Forensic acquisition and analysis of magnetic tapes
Recovering evidential data from magnetic tapes in a forensically sound manner is a difficult task. There are many different tape technologies in existence today and an even greater number of archive formats used. This paper discusses the issues and challenges involved in the forensic acquisition and analysis of magnetic tapes. It identifies areas of slack space on tapes and discusses the challenges of low level acquisition of an entire length of tape. It suggests a basic methodology for determining the contents of a tape, acquiring tape files, and preparing them for forensic analysis.
BIFURCATION, CHAOS AND THEIR CONTROL IN A TIME-DELAY DIGITAL TANLOCK LOOP
This paper reports the detailed parameter space study of the nonlinear dynamical behaviors and their control in a time-delay digital tanlock loop (TDTL). At first, we explore the nonlinear dynamics of the TDTL in parameter space and show that beyond a certain value of loop gain parameter the system manifests bifurcation and chaos. Next, we consider two variants of the delayed feedback control (DFC) technique, namely, the time-delayed feedback control (TDFC) technique, and its modified version, the extended time-delayed feedback control (ETDFC) technique. Stability analyses are carried out to find out the stable phase-locked zone of the system for both the controlled cases. We employ two-parameter bifurcation diagrams and the Lyapunov exponent spectrum to explore the dynamics of the system in the global parameter space. We establish that the control techniques can extend the stable phase-locked region of operation by controlling the occurrence of bifurcation and chaos. We also derive an estimate of the optimum parameter values for which the controlled system has the fastest convergence time even for a larger acquisition range. The present study provides a necessary detailed parameter space study that will enable one to design an improved TDTL system.
Contribution to the Determination of In Vivo Mechanical Characteristics of Human Skin by Indentation Test
This paper proposes a triphasic model of intact skin in vivo based on a general phenomenological thermohydromechanical and physicochemical (THMPC) approach of heterogeneous media. The skin is seen here as a deforming stratified medium composed of four layers and made out of different fluid-saturated materials which contain also an ionic component. All the layers are treated as linear, isotropic materials described by their own behaviour law. The numerical simulations of in vivo indentation test performed on human skin are given. The numerical results correlate reasonably well with the typical observations of indented human skin. The discussion shows the versatility of this approach to obtain a better understanding on the mechanical behaviour of human skin layers separately.
Transmit power adaptation for multiuser OFDM systems
In this paper, we develop a transmit power adaptation method that maximizes the total data rate of multiuser orthogonal frequency division multiplexing (OFDM) systems in a downlink transmission. We generally formulate the data rate maximization problem by allowing that a subcarrier could be shared by multiple users. The transmit power adaptation scheme is derived by solving the maximization problem via two steps: subcarrier assignment for users and power allocation for subcarriers. We have found that the data rate of a multiuser OFDM system is maximized when each subcarrier is assigned to only one user with the best channel gain for that subcarrier and the transmit power is distributed over the subcarriers by the water-filling policy. In order to reduce the computational complexity in calculating water-filling level in the proposed transmit power adaptation method, we also propose a simple method where users with the best channel gain for each subcarrier are selected and then the transmit power is equally distributed among the subcarriers. Results show that the total data rate for the proposed transmit power adaptation methods significantly increases with the number of users owing to the multiuser diversity effects and is greater than that for the conventional frequency-division multiple access (FDMA)-like transmit power adaptation schemes. Furthermore, we have found that the total data rate of the multiuser OFDM system with the proposed transmit power adaptation methods becomes even higher than the capacity of the AWGN channel when the number of users is large enough.
Fine grain associative feature reasoning in collaborative engineering
This paper explores the vast domain of systematic collaborative engineering with reference to product lifecycle management approach from the angle of feature-level collaboration among partners. A new method of fine grain feature association modelling and reasoning is proposed. The original contribution is on the explicit modelling and reasoning of collaborative feature relations within a dynamic context. A case study has been carried out to illustrate the interweaving feature relations in collaborative oil rig space management and the effective application of such relations modelled in design solution optimisation.
Multiple ant tracking with global foreground maximization and variable target proposal distribution
Motion and behavior analysis of social insects such as ants requires tracking many ants over time. This process is highly labor-intensive and tedious. Automatic tracking is challenging as ants often interact with one another, resulting in frequent occlusions that cause drifts in tracking. In addition, tracking many objects is computationally expensive. In this paper, we present a robust and efficient method for tracking multiple ants. We first prevent drifts by maximizing the coverage of foreground pixels at at global scale. Secondly, we improve speed by reducing markov chain length through dynamically changing the target proposal distribution for perturbed ant selection. Using a real dataset with ground truth, we demonstrate that our algorithm was able to improve the accuracy by 15% (resulting in 98% tracking accuracy) and the speed by 76%.
A Framework for Discrete Modeling of Juxtacrine Signaling Systems
Juxtacrine signaling is intercellular communication, in which the receptor of the signal (typically a protein) as well as the ligand (also typically a protein, responsible for the activation of the receptor) are anchored in the plasma membranes, so that in this type of signaling the activation of the receptor depends on direct contact between the membranes of the cells involved. Juxtacrine signaling is present in many important cellular events of several organisms, especially in the development process. We propose a generic formal model (a modeling framework) for juxtacrine signaling systems that is a class of dynamic discrete systems. It possesses desirable characteristics in a good modeling framework, such as: a) structural similarity with biological models, b) capacity of operating in different scales of time and c) capacity of explicitly treating both the events and molecular elements that occur in the membrane, and those that occur in the intracellular environment and are involved in the juxtacrine signaling process. We implemented this framework and used to develop a new discrete model for the neurogenic network and its participation in neuroblast segregation
DIY interface for enhanced service customization of remote IoT devices: a CoAP based prototype
DIY vision for the design of a smart and customizable world in the form of IoT demands the involvement of general public in its development process. General public lacks the technical depths for programming state-of-the-art prototyping and development kits. Latest IoT kits, for example, Intel Edison, are revolutionizing the DIY paradigm for IoT and more than ever a DIY intuitive programming interface is required to enable masses to interact with and customize the behavior of remote IoT devices on the Internet. This paper presents the novel implementation of such a system enabling general public to customize the behavior of remote IoT devices through a visual interface. The interface enables the visualization of the resources exposed by a remote CoAP device in the form of graphical virtual objects. The VOs are used to create service design through simple operations like drag-and-drop and properties settings. The design is maintained as an XML document, thus being easily distributable and recognizable. CoAP proxy acts as an operation client for the remote device and also provides communication link between the designer and the device. The paper presents the architecture, detailed design, and prototype implementation of the system using state-of-the-art technologies.
The Application and Research of Ontology Construction Technology
In the field of search, the application of ontology is an important research topic. Introduction of ontology technology in the retrieval system with massive data can make the searching results more comprehensive. However, now days the ontology is constructed by domain experts, and there are a lot of shortcomings, such as complex process, long time for the project, and difficulty to update. Thereby, in this paper, a method of semiautomaticly building ontology is proposed, after synthetically analyzing a variety of methods and techniques about it. The building process which is based on user interests, mines not only the concepts but also the potential relationships between concepts from the texts by the method of concepts clustering. On the basis of such research, an unique patent information retrieval system based on ontology has been completed.
Intelligent systems in accounting, finance and management: ISI journal and proceeding citations, and research issues from most-cited papers
This paper analyses the citations from Intelligent Systems in Accounting, Finance and Management that have occurred in ISI's Web of Knowledge in February 2010. I found roughly 1000 citations to the journal under 10 different journal name abbreviations, with roughly 25p of the citations occurring during 2008–2009, associated with 27 of the more frequently cited papers. Using that citation data, the H-index and the 40 (42 with ties) most-cited papers are presented. I found that ISI's new proceedings data appear to have a different citation pattern than ISI's journal citation data, resulting in citations to more sources, but fewer citations per source. I also examine the research methodologies and applications of the most-cited papers in an attempt to determine what areas have been cited most and where there are potential gaps in the research. Copyright © 2010 John Wiley & Sons, Ltd.
Anomalous Network Packet Detection Using Data Stream Mining
In recent years, significant research has been devoted to the development of Intrusion Detection Systems (IDS) able to detect anomalous computer network traffic indicative of malicious activity. While signature-based IDS have proven effective in discovering known attacks, anomaly-based IDS hold the even greater promise of being able to automatically detect previously undocumented threats. Traditional IDS are generally trained in batch mode, and therefore cannot adapt to evolving network data streams in real time. To resolve this limitation, data stream mining techniques can be utilized to create a new type of IDS able to dynamically model a stream of network traffic. In this paper, we present two methods for anomalous network packet detection based on the data stream mining paradigm. The first of these is an adapted version of the DenStream algorithm for stream clustering specifically tailored to evaluate network traffic. In this algorithm, individual packets are treated as points and are flagged as normal or abnormal based on their belonging to either normal or outlier clusters. The second algorithm utilizes a histogram to create a model of the evolving network traffic to which incoming traffic can be compared using Pearson correlation. Both of these algorithms were tested using the first week of data from the DARPA ’99 dataset with Generic HTTP, Shell-code and Polymorphic attacks inserted. We were able to achieve reasonably high detection rates with moderately low false positive percentages for different types of attacks, though detection rates varied between the two algorithms. Overall, the histogram-based detection algorithm achieved slightly superior results, but required more parameters than the clustering-based algorithm. As a result of its fewer parameter requirements, the clustering approach can be more easily generalized to different types of network traffic streams.
Machine learning in automated text categorization
The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last 10 years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely, document representation, classifier construction, and classifier evaluation.
Intelligent energy management agent for a parallel hybrid vehicle-part II: torque distribution, charge sustenance strategies, and performance results
This paper represents the second part of a two-part paper on development of an intelligent energy management agent (IEMA) for parallel hybrid vehicles. In this part, energy management strategies for the torque distribution and charge sustenance tasks are established and implemented. Driving situation awareness-based fuzzy rule bases are developed to make intelligent decisions on the power split function. A charge sustenance strategy is developed in parallel to maintain adequate reserves of energy in the storage device for supporting an extended range of driving. Simulation study is conducted for the proposed IEMA and performance results are analyzed to evaluate its viability as a possible solution to and an extendable framework for energy management for parallel hybrid electric vehicles.
Evolution of Adaptive Synapses: Robots with Fast Adaptive Behavior in New Environments
This paper is concerned with adaptation capabilities of evolved neural controllers. We propose to evolve mechanisms for parameter self-organization instead of evolving the parameters themselves. The method consists of encoding a set of local adaptation rules that synapses follow while the robot freely moves in the environment. In the experiments presented here, the performance of the robot is measured in environments that are different in significant ways from those used during evolution. The results show that evolutionary adaptive controllers solve the task much faster and better than evolutionary standard fixed-weight controllers, that the method scales up well to large architectures, and that evolutionary adaptive controllers can adapt to environmental changes that involve new sensory characteristics (including transfer from simulation to reality and across different robotic platforms) and new spatial relationships.
Set-valued cooperative games with fuzzy payoffs. The fuzzy assignment game
In this paper we study cooperative games with fuzzy payoffs. The main advantage of the approach presented is the incorporation into the analysis of the problem of ambiguity inherent in many real-world collective decision situations. We propose extensions of core concepts which maintain the fuzzy nature of allocations, and lead to a more satisfactory study of the problem within the fuzzy context. Finally, we illustrate the extended core concepts and the approach to obtain the corresponding allocations through the analysis of assignment games with uncertain profits.
Symmetric feedback capacity of the Gaussian interference channel to within one bit
We characterize the symmetric capacity of the two-user Gaussian interference channel with feedback to within 1 bit/s/Hz. The result makes use of a deterministic model to provide insights into the Gaussian channel. We derive a new outer bound to show that a proposed scheme can achieve the symmetric capacity to within one bit for all channel parameters. One consequence of the result is that feedback provides unbounded gain, i.e., the gain becomes arbitrarily large for certain channel parameters. It is a surprising result because feedback has been so far known to provide no gain in memoryless point-to-point channels and only power gain (bounded gain) in the multiple access channels. The gain comes from using feedback to fully exploit the side information provided by the broadcast nature of the wireless medium.
Parallel processing of spatial joins using R-trees
We show that spatial joins are very suitable to be processed on a parallel hardware platform. The parallel system is equipped with a so called shared virtual memory which is well suited for the design and implementation of parallel spatial join algorithms. We start with an algorithm that consists of three phases: task creation, task assignment and parallel task execution. In order to reduce CPU and I/O cost, the three phases are processed in a fashion that preserves spatial locality. Dynamic load balancing is achieved by splitting tasks into smaller ones and reassigning some of the smaller tasks to idle processors. In an experimental performance comparison, we identify the advantages and disadvantages of several variants of our algorithm. The most efficient one shows an almost optimal speed up under the assumption that the number of disks is sufficiently large.
A Fault-Tolerant Strategy for Improving the Reliability of Service Composition
Service composition is an important means for integrating the individual Web services for creating new value added systems that satisfy complex demands. Since, Web services exist in the heterogeneous environments on the Internet, study on how to guarantee the reliability of service composition in a distributed, dynamic and complex environment becomes more and more important. This paper proposes a service composition net(SCN) and fault-tolerant strategy to improve the reliability of service composition. The strategy consists of static strategy, dynamic strategy and exception handling mechanism, which can be used to dynamically adjust component service for achieving good reliability as well as good overall performance. SCN is adopted to model different components of service composition. The fault detection and fault recovery mechanisms are also considered. Based on the constructed model, theories of Petri nets help prove the consistency of processing states and the effectiveness of the strategy. A case study of Export Service illustrates the feasibility of proposed method.
Cybersim: geographic, temporal, and organizational dynamics of malware propagation
Cyber-infractions into a nation's strategic security envelope pose a constant and daunting challenge. We present the modular CyberSim tool which has been developed in response to the need to realistically simulate at a national level, software vulnerabilities and resulting malware propagation in online social networks. CyberSim suite (a) can generate realistic scale-free networks from a database of geo-coordinated computers to closely model social networks arising from personal and business email contacts and online communities; (b) maintains for each host a list of installed software, along with the latest published vulnerabilities; (c) allows to designate initial nodes where malware gets introduced; (d) simulates using distributed discrete event-driven technology, the spread of malware exploiting a specific vulnerability, with packet delay and user online behavior models; (e) provides a graphical visualization of spread of infection, its severity, businesses affected etc to the analyst. We present sample simulations on a national level network with millions of computers.
Preface to the special issue: Commutativity of algebraic diagrams
The problem of the commutativity of algebraic (categorical) diagrams has attracted the attention of researchers for a long time. For example, the related notion of coherence was discussed in Mac Lane's homology book Mac Lane (1963), see also his AMS presidential address Mac Lane (1976). Researchers in category theory view this problem from a specific angle, and for them it is not just a question of convenient notation, though it is worth mentioning the important role that notation plays in the development of science (take, for example, the progress made after the introduction of symbolic notation in logics or matrix notation in algebra). In 1976, Peter Freyd published the paper 'Properties Invariant within Equivalence Types of Categories' (Freyd 1976), where the central role is played by the notion of a 'diagrammatic property'. We may also recall the process of 'diagram chasing', and its applications in topology and algebra. But before we can use diagrams (and the principal property of a diagram is its commutativity), it is vital for us to be able to check whether a diagram is commutative.
Adaptive Estimation for Spectral-Temporal Characterization of Energetic Transient Events
We describe a new approach for performing pseudo-imaging of point energy sources from spectral-temporal sensor data. Pseudo-imaging, which involves the automatic localization, spectrum estimation, and identification of energetic sources, can be difficult for dim sources and/or noisy images, or in data containing multiple sources which are closely spaced such that their signatures overlap. The new approach is specifically designed for these difficult cases. It is developed within the framework of modeling field theory (MFT), a biologically-inspired neural network system that has demonstrated practical value in many diverse areas. MFT performs an efficient optimization over the space of all model parameters and mappings between image pixels and sources, or clutter. The optimized set of parameters is then used for detection, localization and identification of the multiple sources in the data. The paper includes results computed from experimental spectrometer data.
Syntax-based alignment of multiple translations: extracting paraphrases and generating new sentences
We describe a syntax-based algorithm that automatically builds Finite State Automata (word lattices) from semantically equivalent translation sets. These FSAs are good representations of paraphrases. They can be used to extract lexical and syntactic paraphrase pairs and to generate new, unseen sentences that express the same meaning as the sentences in the input sets. Our FSAs can also predict the correctness of alternative semantic renderings, which may be used to evaluate the quality of translations.
Exception diagnosis in agent-based grid computing
Diagnosing exceptions in multi-agent systems (MAS) is a complex task due to the distributed nature of the data and control in such systems. This complexity is exacerbated in open environments where independently developed autonomous agents interact with each other in order to achieve their goals. Inevitably, exceptions would occur in such MAS and these exceptions can arise at one of three levels, namely environmental, knowledge and social levels. In this paper we propose a novel exception diagnosis system that is able to analyse and detect exceptions effectively. The proposed architecture consists of specialised exception diagnosis agents called sentinel agents. The sentinel agents are equipped with knowledge of observable abnormal situations, their underlying causes, and resolution strategies associated with these causes. The sentinel agent applies a heuristic classification approach to collect related data from affected agents in order to uncover the underlying causes of the observed symptoms. We illustrate and evaluate our proposed architecture using an agent-based grid computing case study.
A hybrid global optimisation algorithm based on locally filled functions and cluster analysis
In this paper we will extend the definition of a filled function and propose a new definition of a locally filled function. The difference between the locally filled function and the classical filled function is illustrated by an example. The existence of a locally filled function is also studied in theory. Based on the locally filled function and cluster analysis technique we will present a hybrid global optimisation algorithm. The algorithm integrates the deterministic and stochastic searching techniques and has a very powerful globally searching ability. Numerical performance of the new hybrid algorithm is demonstrated by two examples about the Shubert I and Sine-Square I functions.
Multipath Aware TCP (MATCP)
On the Internet many different paths exist between each source and destination. When single path routing is used these paths can be under utilized, not used fairly or not used at all. One way to overcome this is to allow multipath routing. But when multiple paths are used TCP congestion control can be negatively affected and cause poor goodput performance due to the reordering of packets. We proposeMATCP (Multipath Aware TCP) which makes modifications to TCP that allows it to monitor and select which path it takes through the network for each flow. MATCP is compared to single path routing and is validated using extensive simulation. MATCP is found to greatly improve fairness between flows while providing equal or better utilization of links than single best path networks.
A model for mapping between printed and digital document instances
The first steps towards bridging the paper-digital divide have been achieved with the development of a range of technologies that allow printed documents to be linked to digital content and services. However, the static nature of paper and limited structural information encoded in classical paginated formats make it difficult to map between parts of a printed instance of a document and logical elements of a digital instance of the same document, especially taking document revisions into account. We present a solution to this problem based on a model that combines metadata of the digital and printed instances to enable a seamless mapping between digital documents and their physical counterparts on paper. We also describe how the model was used to develop iDoc, a framework that supports the authoring and publishing of interactive paper documents.
Bias analysis of source localization using the maximum likelihood estimator
The nonlinear nature of the source localization problem creates bias to a location estimate. The bias could play a significant role in limiting the performance of localization and tracking when multiple measurements at different instants are available. This paper performs bias analysis of the source location estimate obtained by the maximum likelihood estimator, where the positioning measurements can be TOA, TDOA, or AOA. The effect of bias to the mean-square localization error is examined and the amounts of bias introduced by the three types of measurements are contrasted.
Schedulability analysis for automated implementations of real-time object-oriented models
The increasing complexity of real time software has led to a recent trend in the use of high level modeling languages for development of real time software. One representative example is the modeling language ROOM (real time object oriented modeling), which provides features such as object orientation, state machine description of behaviors, formal semantics for executability of models, and possibility of automated code generation. However these modeling languages largely ignore the timeliness aspect of real time systems, and fail to provide any guidance for a designer to a priori predict and analyze temporal behavior. We consider schedulability analysis for automated implementations of ROOM models, based on the ObjecTime toolset. This work builds on results presented by M. Saksena (1997), where we developed some guidelines for the design and implementation of real time object oriented models. Using the guidelines, we have modified the run time system library provided by the ObjecTime toolset to make it amenable to schedulability analysis. Based on the modified toolset, we show how a ROOM model can be analyzed for schedulability, taking into account the implementation overheads and structure. The analysis is validated experimentally, first using simple periodic models, and then using a large case study of a train tilting system.
Multimodal people detection and tracking in crowded scenes
This paper presents a novel people detection and tracking method based on a multi-modal sensor fusion approach that utilizes 2D laser range and camera data. The data points in the laser scans are clustered using a novel graph-based method and an SVM based version of the cascaded AdaBoost classifier is trained with a set of geometrical features of these clusters. In the detection phase, the classified laser data is projected into the camera image to define a region of interest for the vision-based people detector. This detector is a fast version of the Implicit Shape Model (ISM) that learns an appearance codebook of local SIFT descriptors from a set of hand-labeled images of pedestrians and uses them in a voting scheme to vote for centers of detected people. The extension consists in a fast and detailed analysis of the spatial distribution of voters per detected person. Each detected person is tracked using a greedy data association method and multiple Extended Kalman Filters that use different motion models. This way, the filter can cope with a variety of different motion patterns. The tracker is asynchronously updated by the detections from the laser and the camera data. Experiments conducted in real-world outdoor scenarios with crowds of pedestrians demonstrate the usefulness of our approach.
Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model
We aim to produce predictive models that are not only accurate, but are also interpretable to human experts. Our models are decision lists, which consist of a series of if ...then...statements (e.g., if high blood pressure, then stroke) that discretize a high-dimensional, multivariate feature space into a series of simple, readily interpretable decision statements. We introduce a generative model called Bayesian Rule Lists that yields a posterior distribution over possible decision lists. It employs a novel prior structure to encourage sparsity. Our experiments show that Bayesian Rule Lists has predictive accuracy on par with the current top algorithms for prediction in machine learning. Our method is motivated by recent developments in personalized medicine, and can be used to produce highly accurate and interpretable medical scoring systems. We demonstrate this by producing an alternative to the CHADS2 score, actively used in clinical practice for estimating the risk of stroke in patients that have atrial fibrillation. Our model is as interpretable as CHADS2, but more accurate.
Stability vs. Effectiveness: Improved Sentence-Level Combination of Machine Translation Based on Weighted MBR
We describe an improved strategy to combine the outputs of machine translation on sentence-level balancing the stability and the effectiveness of the combination. The new method alternates the classical MBR-based sentence-level combination with weighted Minimum Bayes Risk (wMBR). During the calculation of the risk, we weight the hypotheses with the performance of the MT system, which is measured by the automatic evaluation metrics on the development data. In experiments, the wMBR-based method stably achieve better results than other sentence-level methods and get the best position in CWMT08 evaluation track outperforming the other word-level and sentence-level combination systems.
Joint source and channel coding using trellis coded CPM: soft decoding
Joint source and channel (JSC) coding using combined trellis coded quantization (TCQ) and continuous phase modulation (CPM) is studied. The channel is assumed to be the additive white Gaussian noise (AWGN) channel. Optimal soft decoding for JSC coding using jointly designed TCQ/CPM is studied in this paper. The soft decoder is based on the a posteriori probability (APP) algorithm for trellis coded CPM. It is shown that the systems with soft decoding outperform the systems with hard decoding especially when the systems operate at low to medium signal-to-noise ratio (SNR). Furthermore, a TCQ design algorithm for the noisy channel is developed. It has been demonstrated that the combined TCQ/CPM systems are both power and bandwidth efficient compared with the combined TCQ/TCM/8PSK systems. The novelty of this work is the use of a soft decoder and the APP algorithm for combined TCQ/CPM systems.
Google PhD Forum at PerCom 2009
Sixteen PhD students presented their research at the PhD Forum at PerCom 2009. The PhD forum offered the students an opportunity for interaction with and feedback from senior researchers in pervasive computing.
Adaptive estimation of human posture using a component-based model
To detect a human body and recognize its posture, a component-based approach is less susceptible to changes in posture and lighting conditions. This paper proposes a component-based human-body model that comprises ten components and their flexible links. Each component contains geometrical information, appearance information, and information on the links with other components. The proposed method in this paper uses hierarchical links between components of human body, so that it allows to make coarse-to-fine searches and makes human-body matching more time-efficient. To adaptively estimate the posture in change of posture and illumination, we update the component online every time a new human body is incoming.
Impact of fading wireless channel on the performance of game theoretic power control algorithms for CDMA wireless data
Our goal in this paper is to study the performance of the game-theoretic power control algorithms for wireless data introduced by Saraydar et al [1] in two realistic channels: (a1) fast flat fading channel and (a2) Slow flat fading channel. The fading coefficients under both (a1) and (a2) are studied under an appropriate small scale channel model that is used in the CDMA cellular systems, namely Nakagami channel model. To do so, we derive a closed- form expression of the average utility function which represents the number of bits received correctly at the receiver per one Joule expended. Then, using this expression we study the existence, uniqueness of Nash equilibrium (NE), and the social desirability of NE in the Pareto sense.
Guidelines for the creation of brain-compatible cyber security educational material in Moodle 2.0
Most current approaches towards information security education do not have a sound theoretical basis. This could lead to the failure of these educational programs. Furthermore, the need for information security knowledge is no longer only of concern to organizations, but has also become a concern for individuals using online services for personal entertainment, social networking, banking, and other activities. Thus, there is a need for “cyber security” education for both individuals and organizations. Such cyber security educational programs should be based on sound pedagogical theories. One such a pedagogically sound approach that could potentially play a role in cyber security educational programs is “brain compatible learning”. This paper will perform a critical evaluation of an existing information security education course, and evaluate the subject matter in terms of brain compatible learning approaches. The aim of the paper is to propose a set of brain compatible learning guidelines for the creation of cyber security educational material. The paper will also argue in favour of the use e-learning as a delivery mechanism for such content. As such, the guidelines will be proposed in the context of a Moodle 2.0 e-learning environment.
OFDM code division multiplexing with unequal error protection and flexible data rate adaptation
An OFDM-CDM (orthogonal frequency division multiplexing code division multiplexing) system with adaptive symbol mapping is presented. This combination enables a robust transmission with flexible error protection and data rate adaptation for parallel data streams by exploiting additional diversity due to CDM. Performance results are presented for fading channels where OFDM-CDM with adaptive symbol mapping and soft interference cancellation is compared to conventional OFDM systems also taking into account channel coding with variable code rates.
Real-time communication in distributed environment-real-time packet filter approach
Recent modern operating system technology enables protocol processing in user space using in-kernel packet filter and user-level protocol processing library for flexibility without sacrificing the performance of traditional kernelized protocol processing. This technology can be adapted to build a highly preemptable protocol processing mechanism for distributed real-time environment. In this paper, we discuss the structural difference of various operating systems from the protocol processing point of view, and propose an extended mechanism of packet filter and user-level protocol processing library for real-time communication. Using this mechanism, priority of the client can be handed off to the server without priority inversion problem during protocol processing.
Let's Meet at the Mobile - Learning Dialogs with a Video Conferencing Software for Mobile Devices
Mobile phones and related gadgets in networks are omnipresent at our students, advertising itself as the platform for mobile, pervasive learning. Currently, these devices rapidly open and enhance, being soon able to serve as a major platform for rich, open multimedia applications and communication. In this report we introduce a video conferencing software, which seamlessly integrates mobile with stationary users into fully distributed multi-party conversations. Following the paradigm of flexible, user-initiated group communication, we present an integrated solution, which scales well for medium-size conferences and accounts for the heterogeneous nature of mobile and stationary participants. This approach allows for a spontaneous, location independent establishment of video dialogs, which is of particular importance in interactive learning scenarios. The work is based on a highly optimized realization of a H.264 codec.
Security issues in the development of a wireless blood-glucose monitoring system
This paper is a progress report on an ongoing research effort that makes use of wireless biometric data management, specifically a wireless blood-glucose monitoring system (WBgM). The goal of this research is to realize appropriate timely intervention by health-care providers in response to records of unregulated blood-glucose level in order to maintain near-normal levels. The focus of this paper is security-their is the possibility that medical information may be used consciously by malicious eavesdropper to the detriment of a patient in search of new employment or a new insurer. There is the possibility of malicious or accidental data corruption from database intrusion. Mobile data transmission is especially problematic in preventing these. We present requirements and our approach taken to realize a secure system architecture. Key techniques and their implementation details to maintain privacy, authentication, and data integrity are discussed here.
State abstraction discovery from irrelevant state variables
Abstraction is a powerful form of domain knowledge that allows reinforcement-learning agents to cope with complex environments, but in most cases a human must supply this knowledge. In the absence of such prior knowledge or a given model, we propose an algorithm for the automatic discovery of state abstraction from policies learned in one domain for use in other domains that have similar structure. To this end, we introduce a novel condition for state abstraction in terms of the relevance of state features to optimal behavior, and we exhibit statistical methods that detect this condition robustly. Finally, we show how to apply temporal abstraction to benefit safely from even partial state abstraction in the presence of generalization error.
Radiation Genes: a database devoted to microarrays screenings revealing transcriptome alterations induced by ionizing radiation in mammalian cells
The analysis of the great extent of data generated by using DNA microarrays technologies has shown that the transcriptional response to radiation can be considerably different depending on the quality, the dose range and dose rate of radiation, as well as the timing selected for the analysis. At present, it is very difficult to integrate data obtained under several experimental conditions in different biological systems to reach overall conclusions or build regulatory models which may be tested and validated. In fact, most available data is buried in different websites, public or private, in general or local repositories or in files included in published papers; it is often in various formats, which makes a wide comparison even more difficult. The Radiation Genes Database (http://www.caspur.it/RadiationGenes) collects microarrays data from various local and public repositories or from published papers and supplementary materials. The database classifies it in terms of significant variables, such as radiation quality, dose, dose rate and sampling timing, as to provide user-friendly tools to facilitate data integration and comparison.
Divisible Load Scheduling inWireless Sensor Networks with Information Utility
Optimal data scheduling strategies in a hierarchical wireless sensor network (WSN) are considered. Data aggregation in clusterheads is considered to find a closed form solution for the optimal amount of data that is to be reported by each sensor node using a newly introduced parameter, information utility. The optimal conditions for the feasible measurement instruction assignment time and for the minimum round time are derived and examined. Based on the optimal conditions, a performance evaluation is demonstrated via simulation study.
Low-Complexity Detections for Downlink MIMO MC-CDMA Systems
In this paper, we investigate and propose the detection techniques for spatially layered multiple-input multiple-output (MIMO) multi-carrier code division multiple access (MC-CDMA) systems. First, we propose a noise-predictive linear detector. It has the same bit error rate (BER) performance as symbol-level detector and reduces the complexity significantly when the system load is almost full. We also propose a partial minimum mean square error (MMSE)-ordered successive interference cancellation (OSIC) based on multi-user detection, which first detects the most powerful interfering data symbols transmitted through the determined transmit antenna and then cancels their contribution from the received signal by the multiplexed data symbol vector. The nulling and cancelling processes between the user data symbols from the same transmit antenna are not performed. The proposed algorithms are verified by computer simulation.
Automatic synthesis and technology mapping of combinational logic
SKOL, a system for the synthesis of combinational logic using a library of cells that emphasizes technology-mapping algorithms, is described. It combines current multilevel optimization techniques with a novel approach to technology mapping. Each factor (or the factorized Boolean equation) can be implemented by itself or collapsed into the higher level expression containing it, which is then implemented. An expression can be implemented in several ways, which differ in the degree of factorization. A number of selected implementations is evaluated and the one with minimal cost (area or delay) is chosen. The mapping algorithms are independent of the library of cells, which can be easily modified. Results from benchmark examples were better than or comparable to those for existing systems. >
Multichannel watermarking of color images
In the field of image watermarking, research has been mainly focused on grayscale image watermarking, whereas the extension to the color case is usually accomplished by marking the image luminance, or by processing each color channel separately. A DCT domain watermarking technique expressly designed to exploit the peculiarities of color images is presented. The watermark is hidden within the data by modifying a subset of full-frame DCT coefficients of each color channel. Detection is based on a global correlation measure which is computed by taking into account the information conveyed by the three color channels as well as their interdependency. To ultimately decide whether or not the image contains the watermark, the correlation value is compared to a threshold. With respect to existing grayscale algorithms, a new approach to threshold selection is proposed, which permits reducing the probability of missed detection to a minimum, while ensuring a given false detection probability. Experimental results, as well as theoretical analysis, are presented to demonstrate the validity of the new approach with respect to algorithms operating on image luminance only.
What to measure next to improve decision making? On top-down task driven feature saliency
Top-down attention is modeled as decision making based on incomplete information. We consider decisions made in a sequential measurement situation where initially only an incomplete input feature vector is available, however, where we are given the possibility to acquire additional input values among the missing features. The procecure thus poses the question what to do next? We take an information theoretical approach implemented for generality in a generative mixture model. The framework allows us reduce the decision about what to measure next in a classification problem to the estimation of a few one-dimensional integrals per missing feature. We demonstrate the viability of the framework on four well-known classification problems.
Offline Handwriting Recognition using Genetic Algorithm
Handwriting Recognition enables a person to scribble something on a piece of paper and then convert it into text. If we look into the practical reality there are enumerable styles in which a character may be written. These styles can be self combined to generate more styles. Even if a small child knows the basic styles a character can be written, he would be able to recognize characters written in styles intermediate between them or formed by their mixture. This motivates the use of Genetic Algorithms for the problem. In order to prove this, we made a pool of images of characters. We converted them to graphs. The graph of every character was intermixed to generate styles intermediate between the styles of parent character. Character recognition involved the matching of the graph generated from the unknown character image with the graphs generated by mixing. Using this method we received an accuracy of 98.44%.
Spatio-spectral sufficient statistic for mental imagery EEG signals
Classification of mental tasks from electroencephalogram (EEG) signals has important applications in brain-computer interfacing (BCI). However, classification of the highly redundant and high-dimensional EEG signal, with high spatial and spectral correlations, is quite challenging. Therefore, the discriminant information, especially that of the first and second data moments, need to be extracted in the form of uncorrelated features. This work addresses this need by approximating a linear minimal-dimension sufficient statistic of the EEG matrix data in both spatial and spectral domains. As a result of the two-dimensional spatio-temporal approach and the generalized sufficiency approximation, a significant improvement on the classification accuracy is achieved.
Privacy preserving OLAP
We present techniques for privacy-preserving computation of multidimensional aggregates on data partitioned across multiple clients. Data from different clients is perturbed (randomized) in order to preserve privacy before it is integrated at the server. We develop formal notions of privacy obtained from data perturbation and show that our perturbation provides guarantees against privacy breaches. We develop and analyze algorithms for reconstructing counts of subcubes over perturbed data. We also evaluate the tradeoff between privacy guarantees and reconstruction accuracy and show the practicality of our approach.
A New Conditioning Rule, Its Generalization and Evidential Reasoning
In Evidence theory, several conditioning rules for updating belief have been proposed, including Dempster's rule of conditioning. The paper views the conditioning rules proposed so far and proposes a new rule of conditioning based on three requirements. Then, it generalizes the rule to be applied to the case where condition is given by an uncertain belief. The paper also discusses a few interpretations of an equation used for evidential reasoning, one of which is interpreted as conditioning with an uncertain condition.
Channel assignment on strongly-simplicial graphs
Given a vector (/spl delta//sub 1/, /spl delta/2,..., /spl delta//sub t/) of non increasing positive integers, and an undirected graph G = (V, E), an L(/spl delta//sub 1/, /spl delta/2,..., /spl delta//sub t/)-coloring of G is a function f from the vertex set V to a set of nonnegative integers such that |f(u) - f (v)| /spl ges/ /spl delta//sub i/, if d(u, v) = i, 1 /spl les/ i /spl les/ t, where d(u,v) is the distance (i.e. the minimum number of edges) between the vertices u and v. This paper presents efficient algorithms for finding optimal L(1,..., 1)-colorings of trees and interval graphs. Moreover, efficient algorithms are also provided for finding approximate L(/spl delta//sub 1/, 1,..., 1)-colorings of trees and interval graphs, as well as approximate L(/spl delta//sub 1/, /spl delta//sub 2/) colorings of unit interval graphs.
Decomposition methods for multihour synthesis of private telecommunication networks
The optimal synthesis of private telecommunication networks subject to multiple nonsimultaneous demands is considered. A model is given for the multiservice network synthesis model with an accurate representation of the conversion and access costs, while maintaining a classical multicommodity flow representation of the network. This is done by increasing the original network to produce an augmented network. Two decomposition methods for solving the network synthesis problem are presented. The first one is a classical Lagrangian relaxation method and the second is a resource-decomposition-type technique. Some preliminary computational results in both cases are presented. These show that the two methods converge within reasonable computation times. The results also demonstrate that the multihour aspect of the synthesis of multiservice networks should be taken into account in order to realize fully the economies that are possible due to the noncoincidence of different types of demands. >
Does Web 3.0 come after Web 2.0? Deconstructing theoretical assumptions through practice
Current internet research has been influenced by application developers and computer engineers who see the development of the Web as being divided into three different stages: Web 1.0, Web 2.0 and Web 3.0. This article will argue that this understanding – although important when analysing the political economy of the Web – can have serious limitations when applied to everyday contexts and the lived experience of technologies. Drawing from the context of the Italian student movement, we show that the division between Web 1.0, Web 2.0 and Web 3.0 is often deconstructed by activists’ media practices. Therefore, we highlight the importance of developing an approach that – by focusing on practice – draws attention to the interplay between Web platforms rather than their transition. This approach, we believe, is essential to the understanding of the complex relationship between Web developments, human negotiations and everyday social contexts.
Integrated contextual representation for objects' identities and their locations
Visual context plays a prominent role in everyday perception. Contextual information can facilitate recognition of objects within scenes by providing predictions about objects that are most likely to appear in a specific setting, along with the locations that are most likely to contain objects in the scene. Is such identity-related (semantic) and location-related (spatial) contextual knowledge represented separately or jointly as a bound representation? We conducted a functional magnetic resonance imaging (fMRI) priming experiment whereby semantic and spatial contextual relations between prime and target object pictures were independently manipulated. This method allowed us to determine whether the two contextual factors affect object recognition with or without interacting, supporting a unified versus independent representations, respectively. Results revealed a Semantic Spatial interaction in reaction times for target object recognition. Namely, significant semantic priming was obtained when targets were positioned in expected (congruent), but not in unexpected (incongruent), locations. fMRI results showed corresponding interactive effects in brain regions associated with semantic processing (inferior prefrontal cortex), visual contextual processing (parahippocampal cortex), and object-related processing (lateral occipital complex). In addition, activation in fronto-parietal areas suggests that attention and memory-related processes might also contribute to the contextual effects observed. These findings indicate that object recognition benefits from associative representations that integrate information about objects' identities and their locations, and directly modulate activation in object-processing cortical regions. Such context frames are useful in maintaining a coherent and meaningful representation of the visual world, and in providing a platform from which predictions can be generated to facilitate perception and action.
A retrieval model based on an extended modal logic and its application to the RIME experimental approach
This paper focuses on the query processing module of RIME, an experimental prototype of an intelligent information retrieval system designed to manage high-precision queries on a corpus of medical reports. Though highly specific this particular corpus is representative of an important class of applications: information retrieval among full-text specialized documents which constitute critical sources of information in several organizations (medicine, law, space industry…). This experience allowed us to design and implement an elaborate model for the semantic content of the documents which is an extension of the Conceptual Dependency approach. The underlying retrieval model is inspired from the Logic model proposed by C.J. Van Rijsbergen, which has been considerably refined using an Extended Modal Logic. After presenting the context of the RIME project, we briefly describe the models designed for the internal representation of medical reports and queries. The main part of the paper is then devoted to the retrieval model and its application to the query processing module of RIME which has a natural language interface. Processing a query involves two main phases: the interpretation which transforms the natural language query into a search expression, and the evaluation phases which retrieves the corresponding medical reports. We focus here on the evaluation phases and show its relationship with the underlying retrieval model. Evaluations from practical experiments are also given, along with indications about current developments of the project.
Non-termination sets of simple linear loops
A simple linear loop is a simple while loop with linear assignments and linear loop guards. If a simple linear loop has only two program variables, we give a complete algorithm for computing the set of all the inputs on which the loop does not terminate. For the case of more program variables, we show that the non-termination set cannot be described by Tarski formulae in general.
Tracking people with twists and exponential maps
This paper demonstrates a new visual motion estimation technique that is able to recover high degree-of-freedom articulated human body configurations in complex video sequences. We introduce the use of a novel mathematical technique, the product of exponential maps and twist motions, and its integration into a differential motion estimation. This results in solving simple linear systems, and enables us to recover robustly the kinematic degrees-of-freedom in noise and complex self occluded configurations. We demonstrate this on several image sequences of people doing articulated full body movements, and visualize the results in re-animating an artificial 3D human model. We are also able to recover and re-animate the famous movements of Eadweard Muybridge's motion studies from the last century. To the best of our knowledge, this is the first computer vision based system that is able to process such challenging footage and recover complex motions with such high accuracy.
Developing Multi-Layer Information Infrastructures: Advancing Social Innovation through Public–Private Governance
Information infrastructures of businesses and government are increasingly interwoven. The development of these information infrastructures often has a technological focus and the concurrent social innovation is ill understood. To address this gap, we study public–private information infrastructure developments at three layers over a prolonged period of time. Stakeholders have to alter existing social practices to realize the potential of information infrastructures. New social practices need to be developed and sustaining innovations requires new governance mechanisms.
Morphological segmentation of Lidar Digital Elevation Models to extract stream channels in forested terrain
Our paper proposes an approach for the extraction of stream channels from Airborne Laser Swath Mapping (ALSM) data. Recent advances in technology have led to high-resolution topographic data acquisition by means of airborne lidar (i.e. ALSM), which can yield Digital Elevation Model (DEM) datasets with horizontal resolutions of 1 m and vertical rms errors in the range of 10 - 15 cm. The extraction of a stream network from a DEM plays a fundamental role in modeling spatially distributed hydrological processes and flow routing. We apply morphological filtering to an ALSM DEM to detect and characterize stream channels in forested terrain. Since the size and shape of morphological Structuring Elements (SEs) is known to strongly affect filtered results, we test for accuracy by developing a set of error measures over simulated terrain. We subsequently apply the filter to actual ALSM data. For linking disconnected stream segments, a measure of pixel connectedness known as the Connectivity Number is used. The method presented is shown to enable systematic characterization and comparisons of streams, even in heavily forested terrain.
Interconnections technologies for VLSI circuits
Abstract#R##N##R##N#VLSI technologies led to the possibility of integration of more than a million of active devices on a single silicon chip. A significant part of the effort in the geometrical circuitry shrinkage was set in development of suitable interconnection technologies. The goals persued were the improvement of the electrical properties (conductivity and contact resistance) with the simultaneous improvement of reliability performances.
An Event Notification Service based on XML Messaging on Different Transport Technologies
With the size and increasing complexity of telecom networks, there is a need to interconnect management systems at different levels. This requires information flow across many applications in a domain independent way. XML is a widely deployed standard which is being used for integration of network management system(NMS) with other applications. We examine the performance of different transport mechanisms, JMS, CORBA, HTTP and RMI for an XML message based event Notification Service. To improve the performance of XML message based event notifications, event grouping is examined and is found to perform well.
SSIM-Based Perceptual Rate Control for Video Coding
The quality of video is ultimately judged by human eye; however, mean squared error and the like that have been used as quality metrics are poorly correlated with human perception. Although the characteristics of human visual system have been incorporated into perceptual-based rate control, most existing schemes do not take rate-distortion optimization into consideration. In this paper, we use the structural similarity index as the quality metric for rate-distortion modeling and develop an optimum bit allocation and rate control scheme for video coding. This scheme achieves up to 25% bit-rate reduction over the JM reference software of H.264. Under the rate-distortion optimization framework, the proposed scheme can be easily integrated with the perceptual-based mode decision scheme. The overall bit-rate reduction may reach as high as 32% over the JM reference software.
Adaptive compensation techniques for communications systems with Tomlinson-Harashima precoding
To improve compensation to channel or interference changes, we propose adapting an auxiliary feedback filter (FBF) in the receiver of systems which use Tomlinson-Harashima (1971, 1972) precoding. We show how the auxiliary FBF can be adapted in conjunction with the receiver feedforward filter (FFF). Simulations demonstrate the performance advantage of our auxiliary FBF technique relative to FFF updating alone, and how the FFF combines interference suppression with despreading in wideband applications. Error propagation can be effectively avoided by using the auxiliary FBF values to decide when to update the precoder, while transient increases in mean-squared error are avoided by using the FBF values in the update equation.
A Robust Audit Mechanism to Prevent Malicious Behaviors in Multi-robot Systems
Market-based mechanisms can be used to coordinate self-interested multi-robot systems in fully distributed environments, where by self-interested we mean that each robot agent attempts to maximize a payoff function that accounts for both the resources consumed and the contribution made by the robot. In previous work, we have studied the effect of various market rules and bidding strategies on the global performance of the multi-robot system. However, rather than use a central monitoring and enforcement mechanisms, we rely on agents to self-report their actions. This assumes that the agents act honestly. In this paper, we drop the honesty assumption, raising the possibility that agents may exaggerate their contribution in order to increase their payoff. To address the problem of such malicious behavior, we propose an audit mechanism to maintain the integrity of reported payoffs. Our algorithm extends previous work on preventing free-riding in peer-to-peer networks. Specifically, we consider locality and mobility in multi-robot systems. We show that our approach efficiently detects malicious behaviors with a high probability.
Interactive emotional content communications system using portable wireless biofeedback device
In this paper, we implemented an interactive emotional content communication system using a portable wireless biofeedback device to support convenient emotion recognition and immersive emotional content representation for users. The newly designed system consists of the portable wireless biofeedback device and a novel emotional content rendering system. The former performs the acquisition and transmission of three different physiological signals (photoplethysmography, skin temperature, and galvanic skin response) to the remote emotional content rendering system via Bluetooth links in real time. The latter displays video content concurrently manipulated using the feedback of the user?s emotional state. The results of effectiveness of the system indicated that the response time of the emotional content communication system was nearly instant, the changes of between emotional contents and emotional states base on physiological signals was corresponded. The user?s concentration was increased by watching the measuredemotion- based rendered visual stimuli. In the near future, the users of this proposed system will be able to create further substantial user-oriented content based on emotional changes.
Intelligent semantic question answering system
The volume of information available on the World Wide Web and the rate of its growth requires new techniques to handle and organize this data. Ontologies are becoming the pivotal methodology to represent domain-specific conceptual knowledge and hence help in providing solutions for Question Answering (QA) systems. This paper introduces an approach for enhancing the capabilities of QA systems using semantic technologies. We implemented an approach to convert the natural language user queries to Resource Description Framework (RDF) triples and find relevant answers. The experiment results show that the proposed technique works very well for single word answers. We believe that with some modifications this approach can be expanded to a wider scale.
Temporal Edges: The Detection Of Motion And The Computation Of Optical Flow
A new method for the detection of motion and the computation of optical flow is presented. In the first step of the calculation the intensity history at each pixel is convolved with the second derivative in time of a temporal Gaussian smoothing function. The zero crossings in a single frame of the resulting function indicate the positions of moving edges. Spatial and temporal derivatives of the function at the zero-crossing locations are then used to compute the component of the flow that is normal to the zero-crossing contours. Both the detection of motion and the computation of the normal velocity are insensitive to slow temporal and spatial changes in the image intensity that are caused by illumination effects rather than motion. A framework in which to relate the present work to a number of gradient based flow measurement techniques is also presented.
On Averaging Multiview Relations for 3D Scan Registration
In this paper, we present an extension of the iterative closest point (ICP) algorithm that simultaneously registers multiple 3D scans. While ICP fails to utilize the multiview constraints available, our method exploits the information redundancy in a set of 3D scans by using the averaging of relative motions. This averaging method utilizes the Lie group structure of motions, resulting in a 3D registration method that is both efficient and accurate. In addition, we present two variants of our approach, i.e., a method that solves for multiview 3D registration while obeying causality and a transitive correspondence variant that efficiently solves the correspondence problem across multiple scans. We present experimental results to characterize our method and explain its behavior as well as those of some other multiview registration methods in the literature. We establish the superior accuracy of our method in comparison to these multiview methods with registration results on a set of well-known real datasets of 3D scans.
Effects of Input Shaping on Manual Control of Flexible and Time-Delayed Systems
OBJECTIVE: The objective was to study the performance of a manual tracking task with system flexibility and time delays in the input channel and to examine the effects of input shaping the human operator's commands. BACKGROUND: It has long been known that low-frequency, lightly damped vibration hinders performance of a manually controlled system. Recently, input shaping has been shown to improve the performance of such systems in a compensatory-display tracking task. It is unknown if similar improvements are seen with pursuit-display tasks, or how the improvement changes when time delays are added to the system. METHOD: A total of 18 novice participants performed a pursuit-view tracking experiment with a spring-centered joystick. Controlled elements included an integrator, an integrator with a lightly damped flexible mode, and an input-shaped integrator with a flexible mode. The input to these controlled elements was delayed between 0 and 1 s. Tracking performance was quantified by root mean square tracking error, and subjective difficulty was quantified by ratings on a Cooper-Harper scale. RESULTS: Performance was best with the undelayed integrator. Both time delay and flexibility degraded performance. Input shaping improved control of the flexible element, with a diminishing benefit as the time delay increased. Tracking error and subjective rating were significantly related. Some operators used a pulsive control strategy. CONCLUSION: Input shaping can improve the performance of a manually controlled system with flexibility, even when time delays are present. APPLICATION: This study is useful to designers of human-controlled systems, especially those with problematic flexibility and/or time delays. Language: en
The Transformer database: biotransformation of xenobiotics
As the number of prescribed drugs is constantly rising, drug–drug interactions are an important issue. The simultaneous administration of several drugs can cause severe adverse effects based on interactions with the same metabolizing enzyme(s). The Transformer database (http://bioinformatics.charite.de/transformer) contains integrated information on the three phases of biotransformation (modification, conjugation and excretion) of 3000 drugs and >350 relevant food ingredients (e.g. grapefruit juice) and herbs, which are catalyzed by 400 proteins. A total of 100 000 interactions were found through text mining and manual validation. The 3D structures of 200 relevant proteins are included. The database enables users to search for drugs with a visual display of known interactions with phase I (Cytochrome P450) and phase II enzymes, transporters, food and herbs. For each interaction, PubMed references are given. To detect mutual impairments of drugs, the drug-cocktail tool displays interactions between selected drugs. By choosing the indication for a drug, the tool offers suggestions for alternative medications to avoid metabolic conflicts. Drug interactions can also be visualized in an interactive network view. Additionally, prodrugs, including their mechanisms of activation, and further information on enzymes of biotransformation, including 3D models, can be viewed.
Combinedwavelet Domain and Motion Compensated Filtering Compliant with Video Codecs
In this paper, we introduce the idea of using motion estimation resources from a video codec for video denoising. This is not straightforward because the motion estimators aimed for video compression and coding, tolerate errors in the estimated motion field and hence are not directly applicable to video denoising. To solve this problem, we propose a novel motion field filtering step that refines the accuracy of the motion estimates to a degree that is required for denoising. We illustrate the use of the proposed motion estimation method within a wavelet-based video denoising scheme. The resulting video denoising method is of low-complexity and receives comparable results with respect to the latest video denoising methods.
Grey-box GUI Testing: Efficient Generation of Event Sequences
Graphical user interfaces (GUIs) encode, as event sequences, potentially unbounded ways to interact with software. During testing it becomes necessary to effectively sample the GUI’s event space. Ideally, for increasing the efficiency and effectiveness of GUI testing, one would like to sample the GUI’s event space by only generating sequences that (1) are allowed by the GUI’s structure, and (2) chain together only those events that have data dependencies between their event handlers. We propose a new model, called an eventdependency graph (EDG) of the GUI that captures data dependencies between the code of event handlers. We develop a mapping between an EDG and an existing black-box model of the GUI’s structure, called an event-flow graph (EFG). We automate the EDG construction in a tool that analyzes the bytecode of each event handler. We evaluate our“grey-box”approach using four open-source applications and compare it with the EFG approach. Our results show that using the EDG reduces the number of event sequences with respect to the EFG, while still achieving at least the same coverage. Furthermore, we are able to detect 2 new bugs in the subject applications.
An automatic registration method for frameless stereotaxy, image guided surgery, and enhanced reality visualization
There is a need for frameless guidance systems to help surgeons plan the exact location for incisions, to define the margins of tumors, and to precisely identify locations of neighboring critical structures. The authors have developed an automatic technique for registering clinical data, such as segmented magnetic resonance imaging (MRI) or computed tomography (CT) reconstructions, with any view of the patient on the operating table. The authors demonstrate on the specific example of neurosurgery. The method enables a visual mix of live video of the patient and the segmented three-dimensional (3-D) MRI or CT model. This supports enhanced reality techniques for planning and guiding neurosurgical procedures and allows us to interactively view extracranial or intracranial structures nonintrusively. Extensions of the method include image guided biopsies, focused therapeutic procedures, and clinical studies involving change detection over time sequences of images.
The fastest gradient waveforms for arbitrary and optimized k-space trajectories
A method for finding the fastest possible gradient waveforms for any given k-space trajectory is presented. It is an extension of our previously introduced solution. The original scheme provides an efficient and non-iterative method for designing the fastest freely rotatable gradient waveforms. Here, the hardware constraints are relaxed so that each axis is constrained independently. This produces the fastest possible non-rotatable waveforms that can be up to 10% faster than their previous counterparts. In addition, for circular trajectories we relax the path constraints. This results in new diamond-shaped trajectories, which are more optimized than circles for separable gradient sets, reducing the total travel time by up to an additional 11%. Analysis of performance for a variety of parameters including the sensitivity to field inhomogeneity compared to freely rotatable circle trajectories is presented.
The InterAction Database: Synergy of Science and Practive in Pharmacy
In social pharmacy and pharmacoepidemiology the distribution, use and performance of medication after registration is studied. In both fields, the pharmacists are the main source of data on drug use. To increase the value of research, we think it important to exchange ideas and suggestions between scientific researchers and pharmacists who work in community pharmacies. Hence, the department of Social Pharmacy and Pharmacoepidemiology of the University of Groningen sought close collaboration with some community pharmacies in the region, resulting in the InterAction project. The pharmacists deliver data to the InterAction database and are explicitly invited to raise questions and issues from their practice, and to participate in research. Consequently, science and practice benefit from each other's input and expertise. This paper describes the architecture and contents of the InterAction project. Additionally, the first experiences with the database as a laboratory for social pharmacy and pharmacoepidemiology are discussed.
Friendship prediction and homophily in social media
Social media have attracted considerable attention because their open-ended nature allows users to create lightweight semantic scaffolding to organize and share content. To date, the interplay of the social and topical components of social media has been only partially explored. Here, we study the presence of homophily in three systems that combine tagging social media with online social networks. We find a substantial level of topical similarity among users who are close to each other in the social network. We introduce a null model that preserves user activity while removing local correlations, allowing us to disentangle the actual local similarity between users from statistical effects due to the assortative mixing of user activity and centrality in the social network. This analysis suggests that users with similar interests are more likely to be friends, and therefore topical similarity measures among users based solely on their annotation metadata should be predictive of social links. We test this hypothesis on several datasets, confirming that social networks constructed from topical similarity capture actual friendship accurately. When combined with topological features, topical similarity achieves a link prediction accuracy of about 92p.
Examining the Role of Linguistic Knowledge Sources in the Automatic Identification and Classification of Reviews
This paper examines two problems in document-level sentiment analysis: (1) determining whether a given document is a review or not, and (2) classifying the polarity of a review as positive or negative. We first demonstrate that review identification can be performed with high accuracy using only unigrams as features. We then examine the role of four types of simple linguistic knowledge sources in a polarity classification system.
Development and Deployment of IPv6-Based SIP VoIP Networks
This paper presents an IPv6-based SIP VoIP network deployed in Taiwan. This deployment project is supported by NICI IPv6 R&D Division. The major contributions of this paper are exercising the ENUM deployment on IPv6 SIP network, developing the IPv6 SIP User Agent and the IPv6 SIP Analyzer.
An Integrated Approach of Variable Ordering and Logic Mapping into LUT-Array-Based PLD
This paper presents an approach of logic mapping into LUT-Array-Based PLD where Boolean functions in the form of the sum of generalized complex terms (SGCTs) can be mapped directly. While previous mapping approach requires predetermined variable ordering, our approach performs mapping and variable reordering simultaneously. For the purpose, we propose a directed acyclic graph based on the multiple valued decision diagram (MDD) and an algorithm to construct the graph. Our algorithm generates candidates of SGCT expressions for each node in a bottom-up manner and selects the variables in the current level by evaluating the sizes of SGCT expressions directly. Experimental results show that our approach reduces the number of terms maximum to 71 percent for the MCNC benchmark circuits.
An Error Model to Study the Behavior of Transient Errors in Sequential Circuits
In sequential logic circuits the transient errors that occur in a particular time frame will propagate to consecutive time frames thereby making the device more vulnerable. In this work we propose a probabilistic error model for sequential logic that can measure the expected output error probability, given a probabilistic input space, that account for both spatial dependencies and temporal correlations across the logic, using a time evolving causal network. We demonstrate our error model using MCNC and ISCAS benchmark circuits and validate it with HSpice simulations. Our observations show that, significantly low individual gate error probabilities produce at least 5 fold higher output error probabilities. The average error percentage of our results with reference to HSpice simulation results is only 4.43%. Our observations show that the order of temporal dependency of error varies for different sequential circuits.
Validity-guided fuzzy clustering evaluation for neural network-based time-frequency reassignment
This paper describes the validity-guided fuzzy clustering evaluation for optimal training of localized neural networks (LNNs) used for reassigning time-frequency representations (TFRs). Our experiments show that the validity-guided fuzzy approach ameliorates the difficulty of choosing correct number of clusters and in conjunction with neural network-based processing technique utilizing a hybrid approach can effectively reduce the blur in the spectrograms. In the course of every partitioning problem the number of subsets must be given before the calculation, but it is rarely known apriori, in this case it must be searched also with using validity measures. Experimental results demonstrate the effectiveness of the approach.
Diagnosability of Discrete Event Systems with Modular Structure
The diagnosis of unobservable faults in large and complex discrete event systems modeled by parallel composition of automata is considered. A modular approach is developed for diagnosing such systems. The notion of modular diagnosability is introduced and the corresponding necessary and sufficient conditions to ensure it are presented. The verification of modular diagnosability is performed by a new algorithm that incrementally exploits the modular structure of the system to save on computational effort. The correctness of the algorithm is proved. Online diagnosis of modularly diagnosable systems is achieved using only local diagnosers.
Field modifiable architecture with FPGAs and its design/verification/debugging methodologies
In the age of highly integrated system LSIs, design methodologies for shorter time-to-market and higher re-programmability after the chip fabrications are now key research issues because of the difficulty of complete verification before tape-out of LSI designs. In this paper, we first introduce an IP-based VLSI architecture that consists of a main processor and an additional hardware (both custom hard macros and FPGA on a single chip) specialized to be in charge of the specific instructions. We further replace the controller circuits of the specialized hardware with compact micro-controllers and memories by using IP libraries (hard macros), which results in the increase of the debuggability and the flexibility of design even for computations realized by hard macros. We call the proposed architecture as field modifiable architecture (FMA). Experimental results confirm that our architecture can achieve significant performance improvement in terms of execution cycles and that EC (engineering change) can be successfully accommodated "after" chip fabrications.
Tracking Articulated Motion using a Mixture of Autoregressive Models
We present a novel approach to modelling the non-linear and time- varying dynamics of human motion, using statistical methods to capture the char- acteristic motion patterns that exist in typical human activities. Our method is based on automatically clustering the body pose space into connected regions ex- hibiting similar dynamical characteristics, modelling the dynamics in each region as a Gaussian autoregressive process. Activities that would require large numbers of exemplars in example based methods are covered by comparatively few motion models. Different regions correspond roughly to different action-fragments and our class inference scheme allows for smooth transitions between these, thus mak- ing it useful for activity recognition tasks. The method is used to track activities including walking, running, etc., using a planar 2D body model. Its effectiveness is demonstrated by its success in tracking complicated motions like turns, without any key frames or 3D information.
Knowledge Analysis with Tree Patterns
Tree-structured knowledge representations are increasingly being used since the relationships between data objects can be represented in a more meaningful way. A number of tree mining algorithms were developed for mining different subtree types using different parameters. At this point in research it would be useful to discuss what kind of sub-problems can be solved within the current tree mining framework. In this paper we provide a general overview of the development in the area of tree mining and discuss motivations and useful application areas for each development. Implications of using different tree mining parameters and constraints are discussed. Such an overview will be particularly useful for those not so familiar with the area of tree mining as it can reveal useful applications within their domain of interest. It gives guidance as to which type of tree mining will be most useful for their particular application.
Local broadcasting in the physical interference model
In this work we analyze the complexity of local broadcasting in the physical interference model. We present two distributed randomized algorithms: one that assumes that each node knows how many nodes there are in its geographical proximity, and another, which makes no assumptions about topology knowledge. We show that, if the transmission probability of each node meets certain characteristics, the analysis can be decoupled from the global nature of the physical interference model, and each node performs a successful local broadcast in time proportional to the number of neighbors in its physical proximity. We also provide worst-case optimality guarantees for both algorithms and demonstrate their behavior in average scenarios through simulations.
Industrial implementation of a dynamic sampling algorithm in semiconductor manufacturing: approach and challenges
In a worldwide environment, sustaining high yield with a minimum number of quality controls is key for manufacturing plants to remain competitive. In high-mix semiconductor plants, where more than 200 products are concurrently run, the complexity of designing efficient control plans comes from the larger amount of data and number of production parameters to handle. Several sampling algorithms were proposed in the literature, but most of them are seen impracticable when coming to an industrial implementation. In this paper, we present and discuss the industrial implementation of a dynamic sampling algorithm in a high-mix semiconductor plant. We describe how the sampling algorithm has been modified, and point out the set of questions that have been raised by the industrial program. Results indicate that more than 30% of control operations on lots could be avoided without increasing the material at risk in production.
Quality Assessment of Pareto Set Approximations
This chapter reviews methods for the assessment and comparison of Pareto set approximations. Existing set quality measures from the literature are critically evaluated based on a number of orthogonal criteria, including invariance to scaling, monotonicity and computational effort. Statistical aspects of quality assessment are also considered in the chapter. Three main methods for the statistical treatment of Pareto set approximations deriving from stochastic generating methods are reviewed. The  dominance ranking method  is a generalization to partially-ordered sets of a standard non-parametric statistical test, allowing collections of Pareto set approximations from two or more stochastic optimizers to be directly compared statistically. The  quality indicator method  -- the dominant method in the literature -- maps each Pareto set approximation to a number, and performs statistics on the resulting distribution(s) of numbers. The  attainment function method  estimates the probability of attaining each goal in the objective space, and looks for significant differences between these probability density functions for different optimizers. All three methods are valid approaches to quality assessment, but give different information. We explain the scope and drawbacks of each approach and also consider some more advanced topics, including multiple testing issues, and using combinations of indicators. The chapter should be of interest to anyone concerned with generating and analysing Pareto set approximations.
Hybrid Filter Banks With Fractional Delays: Minimax Design and Application to Multichannel Sampling
This paper is motivated by multichannel sampling applications. We consider a hybrid filter banks consisting of a set of fractional delays operators, slow A/D converters with different antialiasing filters, digital expanders, and digital synthesis filters (to be designed). The synthesis filters are designed to minimize the maximum gain of a hybrid induced error system. We show that the induced error system is equivalent to a digital system. This digital system enables the design of stable synthesis filters using existing control theory tools such as model-matching and linear matrix inequalities. Moreover, the induced error is robust against delay estimate errors. Numerical experiments show the proposed approach yields better performance compared to existing techniques.
The drift table: designing for ludic engagement
The Drift Table is an electronic coffee table that displays slowly moving aerial photography controlled by the distribution of weight on its surface. It was designed to investigate our ideas about how technologies for the home could support ludic activities-that is, activities motivated by curiosity, exploration, and reflection rather than externally-defined tasks. The many design choices we made, for example to block or disguise utilitarian functionality, helped to articulate our emerging understanding of ludic design. Observations of the Drift Table being used in volunteers' homes over several weeks gave greater insight into how playful exploration is practically achieved and the issues involved in designing for ludic engagement.
Face-to-face and electronic communications in maintaining social networks: the influence of geographical and relational distance and of information content
Using data collected among 742 respondents, this article aims at gaining greater insight into (i) the interaction between face-to-face (F2F) and electronic contacts, (ii) the influence of information content and relational distance on the communication mode/ service choice and (iii) the influence of relational and geographical distance, in addition to other factors, on the frequency of F2F and electronic contacts with relatives and friends. The results show that the frequency of F2F contacts is positively correlated with that for electronic communication, pointing at a complementarity effect.With respect to information content and relational distance, we find, on the basis of descriptive analyses, that synchronous modes/services (F2F and telephone conversations) are used more for urgent matters and that asynchronous modes (in particular email) become more influential as the relational distance increases. Finally, ordered probit analyses confirm that the frequency of both F2F and electronic communication d...
Design of Spatial Data Broadcast for Mobile Navigation Applications
Existing mobile navigators can readily display any static data related to the area surrounding a user. However, their ability to display any dynamic data is limited. In this paper, we enable the viewing of dynamic data on a navigator using wireless data broadcast. The dynamic data are periodically broadcast via base stations of wireless systems, and can be filtered by navigators fetching desired data. We address several crucial issues related to the design of this application, including data organizing, indexing, caching, and querying. We simulate the performance of the resulting system by measuring the time and energy costs associated with retrieving the dynamic data.
Tool integration at the meta-model level: the Fujaba approach
Today’s development processes employ a variety of notations and tools, e.g., the Unified Modeling Language UML, the Standard Description Language SDL, requirements databases, design tools, code generators, model checkers, etc. For better process support, the employed tools may be organized within a tool suite or integration platform, e.g., Rational Rose or Eclipse. While these tool-integration platforms usually provide GUI adaption mechanisms and functional adaption via application programming interfaces, they frequently do not provide appropriate means for data integration at the meta-model level. Thus, overlapping and redundant data from different “integrated” tools may easily become inconsistent and unusable. We propose two design patterns that provide a flexible basis for the integration of different tool data at the meta-model level. To achieve consistency between meta-models, we describe rule-based mechanisms providing generic solutions for managing overlapping and redundant data. The proposed mechanisms are widely used within the Fujaba Tool Suite. We report about our implementation and application experiences .
TLS parameter estimation for filtering chaotic time series
We present a new algorithm for simultaneous filtering and parameter estimation of chaotic time series corrupted by additive measurement noise. This method is based on iteratively minimizing the total least squares (TLS) error in the phase-space using steepest descent. In contrast to prior work, we assume that the dynamic equations modeling the nonlinear time series are known, but the corresponding parameters are not. Specifically, the data is assumed to be generated from time series satisfying a set of coupled logistics equations corrupted by additive white Gaussian noise. This work is motivated in part by language modeling, where the dynamics of a conversation are argued to satisfy the coupled logistics equations and the time series data represents noisy measurements taken from protocols. Accurate estimation of the process parameters is critical, e.g., in diagnosing and treating different types of language disorders and also may prove valuable for testing and improving language understanding systems.
Industry track portals at BNP Paribas: a brief testimony (April 2005)
This paper intends to bear witness on the past, present and future of portals at BNP Paribas with a focus on: 1) the B2E intranet portal (Echo'net), 2) the French B2C home banking portal (BNPPARIBAS.NET). It introduce the challenges that confronted BNP Paribas in each case, reflect on lessons learnt and future trends and finally suggest what financial services firms expect from e-business infrastructure hardware and software vendors and service providers
Improvement of response properties of MR-fluid actuator by torque feedback control
Magnetorheological (MR) fluids are substances that respond to an applied magnetic field with a change in their theological behavior. Though they are functionally similar to electrorheological (ER) fluids, MR fluids exhibit much higher yield strengths for the applied magnetic fields than ER fluids for the applied electric fields. The devices using MR fluids have an ability to provide high-torque, low-inertia, a safe device and simple interface. In this study, we report on an actuator developed using the MR fluid, which consists of an input part, an output part and an MR fluid clutch between them. First, the basic experiments are examined to investigate the characteristics of the actuator. Next, the torque control system of the MR-fluid actuator is proposed. Finally, the closed-loop control experiments were carried out and it is confirmed that the torque-feedback control is effective for improving the response properties of MR actuators.
Schedulability-driven performance analysis of multiple mode embedded real-time systems
Providing multiple modes to support dynamically changing environments, standards, and new services is prevalent in embedded systems, especially in mobile radio systems. Because such a system frequently contains time-constrained tasks, it is important to analyze the temporal requirements as well as the functional correctness. This paper presents a method to analyze temporal requirements imposed on an embedded real-time system supporting multiple modes. While most performance analysis methods focus only on testing the feasibility of a task or a system, our method goes further by addressing the problem of locating hot spots of a system thereby helping the designer to choose among alternative designs or architectures. We formally define the analysis problem and show that it is very unlikely to be solved efficiently. We present a heuristic algorithm, which is accurate and fast enough to be used in iterative processes in system-level analysis and design. The analysis problem is extended to accommodate probabilistic behavior exhibited by soft real-time tasks.
Performance analysis of cognitive coexistence systems with sensing errors
In this paper, we focus on the performance analysis of the cognitive coexistence between Bluetooth and WLAN systems with sensing errors. The packet transmission rate and packet error probability are derived corresponding to the sensor operating points (consisting of the false alarm probability and the miss detection probability). Then, the optimization problem is established as maximizing the throughput of cognitive user with the constraint of the colliding probability with primary user. Simulation results illustrate that the proposed error analysis approach can obtain the optimal performance when considering the sensing errors. Moreover, the influence on the throughput of cognitive user caused by different sensor operating points is investigated by simulation.
The CHiC Interactive Task (CHiCi) at CLEF2013
The interactive task in Cultural Heritage in CLEF 2013 used a standardised interactive protocol, information retrieval system and interface to observe a set of participants remotely via the web as well as in the lab access an English language collection from the Europeana Digital Library. Both user response and log data were collected from the 208 participants.
Advanced fault tolerant bus for multicore system implemented in FPGA
In the paper, a technique for design of highly dependable communication structure in SRAM-based FPGA is presented. The architecture of the multicore system and the structure of fault tolerant bus with cache memories are demonstrated. The fault tolerant properties are achieved by the replication and utilization of the self checking techniques together with partial dynamic reconfiguration. The experimental results show that presented system has small overhead if the high number of function units are used. All experiments were done on the Virtex5 and Virtex6 platform.
Performance analysis of maximal ratio combining in the presence of multiple equal-power cochannel interferers in a Nakagami fading channel
The effect of cochannel interference on the performance of digital mobile radio systems in a Nakagami (1960) fading channel is studied. The performance of maximal ratio combining (MRC) diversity is analyzed in the presence of multiple equal-power cochannel interferers and additive white Gaussian noise. Closed-form expressions are derived for the average probability of error as well as outage probability of both coherent and noncoherent (differentially coherent) binary frequency-shift keying and binary phase-shift keying schemes in an environment with cochannel interference and noise. The results are expressed in terms of the confluent hypergeometric function of the second kind, a function that can be easily evaluated numerically. The analysis assumes an arbitrary number of independent and identically distributed Nakagami interferers.
Proof search for propositional abstract separation logics via labelled sequents
Abstract separation logics are a family of extensions of Hoare logic for reasoning about programs that mutate memory. These logics are "abstract" because they are independent of any particular concrete memory model. Their assertion languages, called propositional abstract separation logics, extend the logic of (Boolean) Bunched Implications (BBI) in various ways.   We develop a modular proof theory for various propositional abstract separation logics using cut-free labelled sequent calculi. We first extend the cut-fee labelled sequent calculus for BBI of Hou et al to handle Calcagno et al's original logic of separation algebras by adding sound rules for partial-determinism and cancellativity, while preserving cut-elimination. We prove the completeness of our calculus via a sound intermediate calculus that enables us to construct counter-models from the failure to find a proof. We then capture other propositional abstract separation logics by adding sound rules for indivisible unit and disjointness, while maintaining completeness and cut-elimination. We present a theorem prover based on our labelled calculus for these logics.
Storyboarding: an empirical determination of best practices and effective guidelines
Storyboarding is a common technique in HCI and design for demonstrating system interfaces and contexts of use. Despite its recognized benefits, novice designers still encounter challenges in the creation of storyboards. Furthermore, as computing becomes increasingly integrated into the environment, blurring the distinction between the system and its surrounding context, it is imperative to depict context explicitly in storyboards. In this paper, we present two formative studies designed to uncover the important elements of storyboards. These elements include the use of text, inclusion of people, level of detail, number of panels, and representation of the passage of time. We further present an empirical study to assess the effects of these elements on the understanding and enjoyment of storyboard consumers. Finally, we demonstrate how these guidelines were successfully used in an undergraduate HCI class.
Constraint Based Automated Synthesis of Nonmasking and Stabilizing Fault-Tolerance
We focus on constraint-based automated addition of nonmasking and stabilizing fault-tolerance to hierarchical programs. We specify legitimate states of the program in terms of constraints that should be satisfied in those states. To deal with faults that may violate these constraints, we add recovery actions while ensuring interference freedom among the recovery actions added for satisfying different constraints. Since the constraint-based{\em manual} design of fault-tolerance is well-known to be applicable in the manual design of nonmasking fault-tolerance, we expect our approach to have a significant benefit in automation of fault-tolerant programs. We illustrate our algorithms with three case studies:stabilizing mutual exclusion, stabilizing diffusing computation, and a data dissemination problem in sensor networks. With experimental results,we show that the complexity of synthesis is reasonable and that it can be reduced using the {\em structure} of the hierarchical systems. To our knowledge, this is the first instance where automated synthesis has been successfully used in synthesizing programs that are correct under fairness assumptions. Moreover, in two of the case studies considered in this paper, the structure of the recovery paths is too complex to permit existing heuristic based approaches for adding recovery.
Automatic Histogram Threshold Using Fuzzy Measures
In this paper, an automatic histogram threshold approach based on a fuzziness measure is presented. This work is an improvement of an existing method. Using fuzzy logic concepts, the problems involved in finding the minimum of a criterion function are avoided. Similarity between gray levels is the key to find an optimal threshold. Two initial regions of gray levels, located at the boundaries of the histogram, are defined. Then, using an index of fuzziness, a similarity process is started to find the threshold point. A significant contrast between objects and background is assumed. Previous histogram equalization is used in small contrast images. No prior knowledge of the image is required.
Ksensor: Multithreaded kernel-level probe for passive QoS monitoring
Traffic monitoring is an increasingly important discipline for nowadays networking, as Accounting, Security and Traffic Engineering lay on it. Besides, traffic bandwidth has increased exponentially in the last few years, and high-speed network monitoring has become a challenging task. Performance requirements are highly relevant for passive QoS monitoring systems. A low-level study of the capturing and processing stages on a traffic analysis system (TAS) has shown room for improvement. We provide an architecture able to cope with high-speed traffic monitoring using commodity hardware. Our system is intended to exploit the parallelism available in up-to- date workstations, which also introduces constraints for multithreaded QoS analysis. This paper presents a kernel-level framework (ksensor) that, keeping the previous requirements, removes some issues from user-level processing and effectively integrates QoS algorithms, improving the overall performance.
Chaos-Modulated Ramp IC for EMI Reduction in PWM Buck Converters Design and Analysis of Critical Issues
Various non-conventional methods have been employed in the past, to reduce the cost and weight of traditional conducted EMI filters and radiation screens for EMI suppression in switching power electronic converters. This paper points out various shortcomings of these methods which are mainly frequency modulation based, and describes the design of a ramp-generator IC based on a modified modulation scheme. This IC can be used on any voltage mode controlled converter and has a feature that enables the user to tune the same converter to various EMC norms. Test results from a prototype showing significant reduction in harmonic power level have been presented. Moreover, this paper discusses a theoretical formulation for calculating the output capacitor size to maintain ripple specifications, when operating under chaotic modulation.
Simultaneous placement and assignment for exploration in mobile backbone networks
This paper presents new algorithms for conducting cooperative sensing using a mobile backbone network. This hierarchical sensing approach combines backbone nodes, which have superior mobility and communication capability, with regular nodes, which are constrained in mobility and communication capability but which can sense the environment. In the framework of a cooperative exploration problem, a technique is developed for simultaneous placement and assignment of regular and mobile backbone nodes. This method, a generalization of existing techniques that only consider stationary regular nodes, optimally solves the simultaneous placement and assignment problem in computationally tractable time for problems of moderate size. For large-scale instances of this problem, a polynomial-time approximation algorithm is developed. This algorithm carries the benefit of a theoretical performance guarantee and also performs well in practice. Finally, the simultaneous placement and assignment technique is incorporated into a cooperative exploration algorithm, and its performance is shown to compare favorably with that of a benchmark based on existing assignment algorithms for mobile backbone networks.
The mystique of numbers: belief in quantitative approaches to segmentation and persona development
Quantitative market research and qualitative user-centered design research have long had an uneasy and complex relationship. A trend toward increasingly complex statistical segmentations and associated personas will once again increase the urgency of addressing paradigm differences to allow the two disciplines to collaborate effectively.   We present an instructive case in which qualitative field research helped contribute to abandoning a "state of the art" quantitative user segmentation that was used in an attempt to unify both marketing and user experience planning around a shared model of users. This case exposes risks in quantitative segmentation research, common fallacies in the evolving practice of segmentation and use of personas, and the dangers of excessive deference to quantitative research generally.
Analyzing the effects of disk-pointer corruption
The long-term availability of data stored in a file system depends on how well it safeguards on-disk pointers used to access the data. Ideally, a system would correct all pointer errors. In this paper, we examine how well corruption-handling techniques work in reality. We develop a new technique called type-aware pointer corruption to systematically explore how a file system reacts to corrupt pointers. This approach reduces the exploration space for corruption experiments and works without source code. We use type-aware pointer corruption to examine Windows NTFS and Linux ext3. We find that they rely on type and sanity checks to detect corruption, and NTFS recovers using replication in some instances. However, NTFS and ext3 do not recover from most corruptions, including many scenarios for which they possess sufficient redundant information, leading to further corruption, crashes, and unmountable file systems. We use our study to identify important lessons for handling corrupt pointers.
The nature of dialog: structural and lexical markers of dialogic teacher/learner interactions
In this paper I argue that dialog facilitates learning. As a consequence teaching and learning should be dialogic in principal. Against this background the question arises what dialog actually is and how it can be implemented for teaching and learning. A pilot corpus study of a dialog and a monolog corpus of selected teacher/learner interactions sheds light on some of the structural and lexical characteristics of dialogic teaching and learning. The analysis of the data discloses five communicative functions of selected keywords of the dialog corpus, which indicate how speakers are affiliated in dialogic interaction.
Evaluating the enjoyability of the ghosts in Ms Pac-Man
The video games industry is one of the fastest-growing industries in the world, bolstered by sophisticated technology in gaming consoles and modern trends such as mobile and social gaming. The goal of most video games is to entertain the gamer and in most games this stems from the interaction between the gamer and the non-player characters (NPCs): it is no longer sufficient for a game to be visually appealing but instead, the gamer must be challenged at the right level of difficulty to be engaged by the game. It is thus necessary to develop suitable NPCs that are fun to play against and the realm of computational intelligence offers a variety of techniques to do so. However, the perception of fun is clearly subjective and indeed, difficult to quantify. In this paper we make use of the Ms Pac-Man vs Ghosts gaming competition to gather and analyse data from human gamers regarding their preference of opponent: each gamer plays two games against different ghost teams, indicating their preference at the end. We subsequently use this data to establish which ghost teams are generally preferred and demonstrate that there are measurable differences between these ghost teams. These differences are sufficient to group the ghosts into different categories with a good degree of accuracy. This work is a first step in better understanding the attributes required by NPCs for players to be engaged.
Learning human multimodal dialogue strategies
We investigate the use of different machine learning methods in combination with feature selection techniques to explore human multimodal dialogue strategies and the use of those strategies for automated dialogue systems. We learn policies from data collected in a Wizard-of-Oz study where different human ‘wizards’ decide whether to ask a clarification request in a multimodal manner or else to use speech alone. We first describe the data collection, the coding scheme and annotated corpus, and the validation of the multimodal annotations. We then show that there is a uniform multimodal dialogue strategy across wizards, which is based on multiple features in the dialogue context. These are generic features, available at runtime, which can be implemented in dialogue systems. Our prediction models (for human wizard behaviour) achieve a weighted f-score of 88.6 per cent (which is a 25.6 per cent improvement over the majority baseline). We interpret and discuss the learned strategy. We conclude that human wizard behaviour is not optimal for automatic dialogue systems, and argue for the use of automatic optimization methods, such as Reinforcement Learning. Throughout the investigation we also discuss the issues arising from using small initial Wizard-of-Oz data sets, and we show that feature engineering is an essential step when learning dialogue strategies from such limited data.
Multi-objective hierarchical genetic algorithm for interpretable fuzzy rule-based knowledge extraction
A new scheme based on multi-objective hierarchical genetic algorithm (MOHGA) is proposed to extract interpretable rule-based knowledge from data. The approach is derived from the use of multiple objective genetic algorithm (MOGA), where the genes of the chromosome are arranged into control genes and parameter genes. These genes are in a hierarchical form so that the control genes can manipulate the parameter genes in a more effective manner. The effectiveness of this chromosome formulation enables the fuzzy sets and rules to be optimally reduced. Some important concepts about the interpretability are introduced and the fitness function in the MOGA will consider both the accuracy and interpretability of the fuzzy model. In order to remove the redundancy of the rule base proactively, we further apply an interpretability-driven simplification method to newborn individuals. In our approach, we first apply the fuzzy clustering to generate an initial rule-based model. Then the multi-objective hierarchical genetic algorithm and the recursive least square method are used to obtain the optimized fuzzy models. The accuracy and the interpretability of fuzzy models derived by this approach are studied and presented in this paper. We compare our work with other methods reported in the literature on four examples: a synthetic nonlinear dynamic system, a nonlinear static system, the Lorenz system and the Mackey-Glass system. Simulation results show that the proposed approach is effective and practical in knowledge extraction.
Cognitive Dust: Linking CSCW Theories to Creative Design Processes
Results from empirical research in Requirements Engineering lead us to characterise work processes, not as evolutionary and systematic, but as semi structured, emergent and creative. These same properties are found in Situated Action models where work is described as emergent and self defining. At another level within these processes, we use the term "cognitive dust" to describe the External and Distributed cognitive representations suspended in a small group workspace. These cognitive representations are components of communicative actions between humans, or between humans and their technological infrastructures, and are part of these creative processes. We use a multi modal sensor infrastructure, which is situated in a ubiquitous workspace, to observe and capture this cognitive dust. We hypothesise that, if an infrastructure can capture enough of these representations and extract enough semantic meaning to "understand" the communicative intentions, the infrastructure can dynamically support creative, unstructured activities. This paper sets out the conceptual, theoretical framework for this research by linking known CSCW theories with the emergent, unstructured or semi structured nature of creative work processes within small groups such as designers.
Error control using retransmission schemes in multicast transport protocols for real-time media
We analyze different retransmission (ARQ) schemes for error control in multicast protocols geared toward real-time, multimedia applications. We discuss why retransmission schemes are not inappropriate for such applications, but in fact can be quite effective. We present a quantitative analysis of such schemes, as well as simulation results, taking into account four different parameters (and not just the source throughput): (1) the probability of dropping a packet due to limited time for retransmissions; (2) the average time required to deliver a packet correctly to end receivers; (3) the number of times a packet will be retransmitted; and (4) the cost to the network, in terms of packet duplications, of retransmitting a packet. We reach the counter-intuitive conclusion that the optimum scheme, in terms of all four of the above parameters, in the most general scenarios (where several hosts with widely varying propagation delays and 'quality of connections' are participating in the session) is to immediately retransmit packets-preferably multicast-upon reception of a NACK from any receiver. We also demonstrate, again through quantitative analysis, the circumstances under which it would be beneficial (as well as those under which it would be counter-productive) to multicast control messages in the hope of suppressing duplicates and preventing the source from being overwhelmed by control messages.
Outer-loop vectorization: revisited for short SIMD architectures
Vectorization has been an important method of using data-level parallelism to accelerate scientific workloads on vector machines such as Cray for the past three decades. In the last decade it has also proven useful for accelerating multi-media and embedded applications on short SIMD architectures such as MMX, SSE and AltiVec. Most of the focus has been directed at innermost loops, effectively executing their iterations concurrently as much as possible. Outer loop vectorization refers to vectorizing a level of a loop nest other than the innermost, which can be beneficial if the outer loop exhibits greater data-level parallelism and locality than the innermost loop. Outer loop vectorization has traditionally been performed by interchanging an outer-loop with the innermost loop, followed by vectorizing it at the innermost position. A more direct unroll-and-jam approach can be used to vectorize an outer-loop without involving loop interchange, which can be especially suitable for short SIMD architectures.   In this paper we revisit the method of outer loop vectorization, paying special attention to properties of modern short SIMD architectures. We show that even though current optimizing compilers for such targets do not apply outer-loop vectorization in general, it can provide significant performance improvements over innermost loop vectorization. Our implementation of direct outer-loop vectorization, available in GCC 4.3, achieves speedup factors of 3.13 and 2.77 on average across a set of benchmarks, compared to 1.53 and 1.39 achieved by innermost loop vectorization, when running on a Cell BE SPU and PowerPC970 processors respectively. Moreover, outer-loop vectorization provides new reuse opportunities that can be vital for such short SIMD architectures, including efficient handling of alignment. We present an optimization tapping such opportunities, capable of further boosting the performance obtained by outer-loop vectorization to achieve average speedup factors of 5.26 and 3.64.
Designing gestures for hands and feet in daily life
In wearable computing environments, people handles various information anytime and anywhere with a wearing computer. In such situation, a gesture is one of powerful methods as input method because it needs no physical devices to touch and a user can input quickly. However, there are various restrictions for gesture input in daily life; gestures must be socially acceptable because a user has to gesture with unusual movements in a crowd, gestures must be flexible because a user cannot gesture when he/she has a bag with his hand that is used for a gesture. In this paper, we clarify the restrictions on gesture interfaces in daily life, then propose practical gestures for selecting simple menu items with hands and feet.
Modeling of heart systolic murmurs based on multivariate matching pursuit for diagnosis of valvular disorders
Heart murmurs are pathological sounds produced by turbulent blood flow due to certain cardiac defects such as valves disorders. Detection of murmurs via auscultation is a task that depends on the proficiency of physician. There are many cases in which the accuracy of detection is questionable. The purpose of this study is development of a new mathematical model of systolic murmurs to extract their crucial features for identifying the heart diseases. A high resolution algorithm, multivariate matching pursuit, was used to model the murmurs by decomposing them into a series of parametric time-frequency atoms. Then, a novel model-based feature extraction method which uses the model parameters was performed to identify the cardiac sound signals. The proposed framework was applied to a database of 70 heart sound signals containing 35 normal and 35 abnormal samples. We achieved 92.5% accuracy in distinguishing subjects with valvular diseases using a MLP classifier, as compared to the matching pursuit-based features with an accuracy of 77.5%.
Parallel implementation of the sparse-matrix/canonical grid method for the analysis of two-dimensional random rough surfaces (three-dimensional scattering problem) on a Beowulf system
Wave scattering from two-dimensional (2-D) random rough surfaces [three-dimensional (3-D) scattering problem] has been previously analyzed using the sparse-matrix/canonical grid (SM/CG) method. The computational complexity and memory requirement of the SM/CG method are O(N log N) per iteration and O(N), respectively, where N is the number of surface unknowns. Furthermore, the SM/CG method is FFT based, which facilitates the implementation on parallel processors. In this paper, we present a cost-effective solution by implementing the SM/CG method on a Beowulf system consisting of PCs (processors) connected by a 100 Base TX Ethernet switch. The workloads of computing the sparse-matrix-vector multiplication corresponding to the near interactions and the fast Fourier transform (FFT) operations corresponding to the far interactions in the SM/CG method can be easily distributed among all the processors. Both perfectly conducting and lossy dielectric surfaces of Gaussian spectrum and ocean spectrum are analyzed thereafter. When possible, speedup factors against a single processor are given. It is shown that the SM/CG method for a single realization of rough surface scattering can be efficiently adapted for parallel implementation. The largest number of surface unknowns solved in this paper is over 1.5 million. On the other hand, a problem of 131072 surface unknowns for a PEC random rough surface of 1024 square wavelengths only requires a CPU time of less than 20 min. We demonstrate that analysis of a large-scale 2-D random rough surface feasible for a single realization and for one incident angle is possible using the low-cost Beowulf system.
Active noise cancellation using aggressor-aware clamping circuit for robust on-chip communication
As the IC process technology scales the on-chip wiring network becomes denser. Increasing aspect ratios of the on-chip interconnects lead to higher coupling capacitances and ultimately higher cross-talk noise, which degrades signal integrity. In this paper we propose a clamping circuit for on-chip busses, which clamps a victim wire in an on-chip bus based on the states of its immediate aggressors. These clampers help the driver of the victim wire in draining the charge, which is induced due to cross-talk between aggressors and victim wires. This helps in decreasing the cross-talk peak noise and also the delay variability (referred to as delay noise). Simulation results for a 10 mm long communication bus (parallel wires) laid at minimum pitch in 0.13 /spl mu/m CMOS technology show that a reduction of 30%(17.6%), 37%(27.2%) and 26%(65.8%) in cross-talk peak noise amplitude (delay noise) is observed for point to point, parallel repeater inserted and staggered repeater inserted respectively when only immediate neighbours are considered (1/sup st/ order). Furthermore the aggressor-aware clamper is very effective in avoiding glitches, which may occur when more aggressors, in addition to the immediate ones are also switching simultaneously in the same.
High resolution two-dimensional ARMA spectral estimation
The authors present a practical algorithm for estimating the power spectrum of a 2-D homogeneous random field based on 2-D autoregressive moving average (ARMA) modeling. This algorithm is a two-step approach: first, the AR parameters are estimated by solving a version of the 2-D modified Yule-Walker equation, for which some existing efficient algorithms are available; then the MA spectrum parameters are obtained by simple computations. The potential capability and the high-resolution performance of the algorithm are demonstrated by using some numerical examples. >
Compiler verification in LF
A methodology for the verification of compiler correctness based on the LF logical framework as realized within the Elf programming language is presented. This technique is used to specify, implement, and verify a compiler from a simple functional programming language to a variant of the Categorical Abstract Machine (CAM). >
Blind Amplify-and-Forward Relaying in Multiple-Antenna Relay Networks
In this paper, we investigate the performance of a single-relay cooperative scenario where the source, relay, and destination terminals are equipped with multiple transmit/receive antennas. We particularly focus on the so-called blind amplify-and-forward relaying in which the availability of channel state information at the relay terminal is not required. Through the derivation of pairwise error probability, we quantify analytically the impact of multiple antenna deployment assuming various scenarios which involve relay location and power allocation assumptions imposed on the cooperating nodes.
Coupling of two 2-link robots with a passive joint for reconfigurable planar parallel robot
This paper proposes a reconfigurable planar parallel robot by coupling two 2R open kinematic chains (or 2-link robots), the first joints of which are passive. We show that they can reconfigure to a 5R closed kinematic chain which has the same number of actuators as its degrees of freedom. They can also reconfigure to a 4R closed kinematic chain plus one actuated link. The parallel robot has only two actuators but can have multiple functions by reconfigurations. Due to the passive joints, whether or not the 2R open kinematic chains can couple with each other is a non-trivial problem. We propose coupling sequences for forming the 4R and 5R closed kinematic chains and verify those experimentally.
On the Fingerprinting Capacity Under the Marking Assumption
We address the maximum attainable rate of fingerprinting codes under the marking assumption, studying lower and upper bounds on the value of the rate for various sizes of the attacker coalition. Lower bounds are obtained by considering typical coalitions, which represents a new idea in the area of fingerprinting and enables us to improve the previously known lower bounds for coalitions of size two and three. For upper bounds, the fingerprinting problem is modeled as a communications problem. It is shown that the maximum code rate is bounded above by the capacity of a certain class of channels, which are similar to the multiple-access channel (MAC). Converse coding theorems proved in the paper provide new upper bounds on fingerprinting capacity. It is proved that capacity for fingerprinting against coalitions of size two and three over the binary alphabet satisfies and , respectively. For coalitions of an arbitrary fixed size , we derive an upper bound on fingerprinting capacity in the binary case. Finally, for general alphabets, we establish upper bounds on the fingerprinting capacity involving only single-letter mutual information quantities.
Methods for reducing events in sequential circuit fault simulation
Methods are investigated for reducing events in sequential circuit fault simulation by reducing the number of faults simulated for each test vector. Inactive faults, which are guaranteed to have no effect on the output or the next state, are identified using local information from the fault-free circuit in one technique. In a second technique, the Star-algorithm is extended to handle sequential circuits and provides global information about inactive faults, based on the fault-free circuit state. Both techniques are integrated into the PROOFS synchronous sequential circuit fault simulator. An average 28% reduction in faulty circuit gate evaluations is obtained for the 19 ISCAS-89 benchmark circuits studied using the first technique, and 33% reduction for the two techniques combined. Execution times decrease by an average of 17% when the first technique is used. For the largest circuits, further improvements in execution time are made when the Star-algorithm is included. >
Tracking performance of the coherent and noncoherent discriminators in strong multipath
Signal multipath leads to undesirable tracking errors and inaccurate ranging information for GPS receivers. The extent of the tracking error in compromising the receiver discriminator performance depends on the multipath amplitude, delay, and phase relative to the direct path. Compared with the rural area, the GPS receiver in the semi-enclosed area, such as city canyons and building shadows, is subject to much weaker line of sight propagation environment which further compromise its performance. In this paper, we derive analytical expressions of the multipath effect on the GPS tracking errors for both coherent discriminator and noncoherent early-minus-late power discriminators in strong multipath environment.
Designing hardware with dynamic memory abstraction
Recent progress in program analysis has produced tools that are able to compute upper bounds on the use of dynamic memory. This opens up a space for the use of dynamic memory abstraction in high-level synthesis. In this paper, we explain how to design hardware using C programs with  malloc () and  free (). A compilation process is outlined for transforming C programs with heap operations into a hardware description language. As demonstrated by our experiments, this approach is feasible. Further, automatic parallelization of the generated circuits improves by a factor up to 1.9 in terms of clock frequency and a factor up to 2.7 in terms of clock cycles over the previous work.
A New Dynamic OVSF Code Allocation Method based on Adaptive Simulated Annealing Genetic Algorithm (ASAGA)
Orthogonal variable spreading factor (OVSF) codes are widely used to provide variable data rates for supporting different bandwidth requirements in wideband code division multiple access (WCDMA) systems. Many works in the literature have intensively investigated to find an optimal dynamic code assignment scheme for OVSF codes. Unlike earlier studies, which assign OVSF codes using conventional (CCA) or dynamic (DCA) code allocation schemes, in this paper, adaptive simulated annealing genetic algorithm (ASAGA) was applied which population is adaptively constructed according to existing traffic density in the OVSF code-tree. Also, the influences of the ASAGA parameters (selection, crossover and mutation techniques and cooling schedules) were examined on the dynamic OVSF code allocation problem. The simulation results show that the ASAGA provides reduced code blocking probability and improved spectral efficiency in the system when compared to the CCA and DCA schemes. ASAGA is also tested with its components SA and GA.
An analysis of multicast forwarding state scalability
Scalability of multicast forwarding state is likely to be a major issue facing inter-domain multicast deployment. We present a comprehensive analysis of the multicast forwarding state problem. Our goal is to understand the scaling trends of multicast forwarding state in the Internet, and to explore the intuitions that have motivated state reduction research. We conducted simulation experiments on both real and generated network topologies, with a range of parameters driven by multicast application characteristics. We found that the increase in peering among Internet backbone networks has led to more multicast forwarding state at a handful of core domains, but less state in the rest of the domains. We observed that scalability of multicast forwarding state with respect to session size follows a power law. Our findings show that distribution and concentration of multicast forwarding state in the Internet is significantly, impacted by the application characteristics. We investigated the proposals on non-branching multicast forwarding state elimination, and found substantial reduction is attainable even with very dense multicast sessions.
Customized Interface Generation Model Based on Knowledge and Template for Web Service
With the development of Service-Oriented Architecture, more and more researches have provided automatic and semi-automatic approaches to end-user. Users can construct their own applications with web services. However, it is hard for most end-users to customize the interfaces of applications with current service composition methods. To address this issue, an interface generation model was proposed to provide customized user interface and interaction workflow. In this model, knowledge was involved to instruct the workflow of interaction. Templates were adopted to describe the user interface. Some significant points, such as user definition, data profile, user interaction workflow, interface description, were discussed in detail. A prototype system was implemented. Some demos have been shown to verify the customized interface generation model. With this model, end-users can define the interfaces and interaction workflows of web services with rules and templates. It supplies the gap of user interface in service composition. Compared with the current interface generation in service composition, the proposed model is more flexible and more effective for end-users
Petri Nets and Ontologies: Tools for the "Learning Player" Assessment in Serious Games
Serious games are now an increasingly used tool in business training. The question of the effectiveness of such devices on learning is a research issue. The indicators provided at the end of a video game are insufficient to understand and follow the path of a learning player. It is therefore necessary to track not only the player's actions but also to provide tools in order to analyze and diagnose the knowledge acquisition of the learner. We developed an approach based on Petri nets, used to model the accurate behavior of the player. We complete this tool with ontology to explain learner's mistakes.
Epileptic EEG classification based on kernel sparse representation.
The automatic identification of epileptic EEG signals is significant in both relieving heavy workload of visual inspection of EEG recordings and treatment of epilepsy. This paper presents a novel method based on the theory of sparse representation to identify epileptic EEGs. At first, the raw EEG epochs are preprocessed via Gaussian low pass filtering and differential operation. Then, in the scheme of sparse representation based classification (SRC), a test EEG sample is sparsely represented on the training set by solving l1-minimization problem, and the represented residuals associated with ictal and interictal training samples are computed. The test EEG sample is categorized as the class that yields the minimum represented residual. So unlike the conventional EEG classification methods, the choice and calculation of EEG features are avoided in the proposed framework. Moreover, the kernel trick is employed to generate a kernel version of the SRC method for improving the separability between ictal and interictal classes. The satisfactory recognition accuracy of 98.63% for ictal and interictal EEG classification and for ictal and normal EEG classification has been achieved by the kernel SRC. In addition, the fast speed makes the kernel SRC suit for the real-time seizure monitoring application in the near future.
Self-Immunity Technique to Improve Register File Integrity Against Soft Errors
Continuous shrinking in feature size, increasing power density etc. increase the vulnerability of microprocessors against soft errors even in terrestrial applications. The register file is one of the essential architectural components where soft errors can be very mischievous because errors may rapidly spread from there throughout the whole system. Thus, register files are recognized as one of the major concerns when it comes to reliability. This paper introduces Self-Immunity, a technique that improves the integrity of the register file with respect to soft errors. Based on the observation that a certain number of register bits are not always used to represent a value stored in a register. This paper deals with the difficulty to exploit this obvious observation to enhance the register file integrity against soft errors. We show that our technique can reduce the vulnerability of the register file considerably while exhibiting smaller overhead in terms of area and power consumption compared to state-of-the-art in register file protection.
Measuring Semantic Associaiton in Domain Ontology
In domain ontology, semantic association (SA) is used to depict the correlation between two concepts. In this paper, we define semantic association degree (SAD) for measuring SA in the domain ontology. We first present a method to measure SAD of two direct related concepts by evaluating the semantic relationship between them, and then give another method to measure SAD of two indirect related concepts though SAD of two directed neighboring concepts. A set of comparison experiments show the benefit of our approaches.
The $h$ -Index and the Number of Citations: Two Fuzzy Integrals
In this paper, we review two of the most well-known citation indexes and establish their connections with the Choquet and Sugeno integrals. In particular, we show that the recently established h-index is a particular case of the Sugeno integral, and that the number of citations corresponds to the Choquet integral. In both cases, they use the same fuzzy measure. The results presented here permit one to envision new indexes defined in terms of fuzzy integrals using other types of fuzzy measures. A few considerations in this respect are also included in this paper. Indexes for taking into account recent research and the publisher credibility are outlined.
The soft error problem: an architectural perspective
Radiation-induced soft errors have emerged as a key challenge in computer system design. If the industry is to continue to provide customers with the level of reliability they expect, microprocessor architects must address this challenge directly. This effort has two parts. First, architects must understand the impact of soft errors on their designs. Second, they must select judiciously from among available techniques to reduce this impact in order to meet their reliability targets with minimum overhead. To provide a foundation for these efforts, this paper gives a broad overview of the soft error problem from an architectural perspective. We start with basic definitions, followed by a description of techniques to compute the soft error rate. Then, we summarize techniques used to reduce the soft error rate. This paper also describes problems with double-bit errors. Finally, this paper outlines future directions for architecture research in soft errors.
A 5.5-GHz 1-mW Full-Modulus-Range Programmable Frequency Divider in 90-nm CMOS Process
Operating up to 5.5 GHz with 1-mW power consumption, a 90-nm CMOS programmable frequency divider with eight stages of new static D-flip-flop-based (2/1) divider cells is presented, where the supply voltage of 1.0 V is employed. The divider achieves a full modulus range from 1 to 256 and operates over a wide range maintaining up to 4 GHz with -30-dBm input power. The divider also accomplishes a power efficiency of 12.8 GHz/mW with 0.5-V supply voltage. It is favorable for advanced processes.
Space curve recognition based on the wavelet transform and string-matching techniques
A technique for representing and recognising 3-D or space curves is presented. In the proposed algorithm, the space curves are represented by a set of two zero-crossing representations which are constructed based on the dyadic wavelet transform. These representations are then described in the form of an ordered set of complex numbers which is referred to as the compact representation of the space curves. A string-matching technique is adapted for comparing two curves using their compact representations. Experimental results show that the proposed technique can be used for recognising space curves under similarity transformation with and without additive noise.
Five Weeks in the Robot House Exploratory Human-Robot Interaction Trials in a Domestic Setting
This paper presents five exploratory trials investigating scenarios likely to occur when a personal robot shares a home with a person. The scenarios are: a human and robot working on a collaborative task, a human and robot sharing a physical space in a domestic setting, a robot recording and revealing personal information, a robot interrupting a human in order to serve them, and finally, a robot seeking assistance from a human through various combinations of physical and verbal cues. Findings indicate that participants attribute more blame and less credit to a robot than compared to themselves when working together on a collaborative task. Safety is a main concern when determining participants' comfort when sharing living space with their robot. Findings suggest that the robot should keep its interruption of the user's activities to a minimum. Participants were happy for the robot to store information which is essential for the robot to improve its functionality. However, their main concerns were related to the storing of sensitive information and security measures to safeguard such information.
Workstation based parallel test generation
Generation of test vectors for the VLSI devices used in contemporary digital system is becoming much more difficult as these devices increase in size. Automatic Test Pattern Generation (ATPG) techniques are commonly used to generate these tests. Since ATPG is an NP complete problem with complexity exponential to circuit size, the application of parallel processing techniques to accelerate the process of finding test patterns is an active area of research. This paper presents an approach to parallelization of the test generation problem that is targeted to a network-of-workstations environment. The system is based upon partitioning of the fault list across multiple processors and includes enhancements designed to address the main drawbacks of this technique, namely unequal load balancing and generation of redundant vectors. The technique is generalized enough that it can be applied to any test generation system regardless of the ATPG or fault simulation algorithm employed. Results were gathered to determine the impact of workstation processing load and network communications load on the performance of the system. >
Classroom Presenter: Enhancing Interactive Education with Digital Ink
Classroom Presenter is a Tablet PC-based interaction system that supports the sharing of digital ink on slides between instructors and students. Initial deployments show that using the technology can achieve a wide range of educational goals and foster a more participatory classroom environment.
Two odds-radio-based text classification algorithms
Since 1990's, the exponential growth of theseWeb documents has led to a great deal of interestin developing efficient tools and software toassist users in finding relevant information. Textclassification has been proved to be useful inhelping organize and search text information onthe Web. Although there have been existed anumber of text classification algorithms, most ofthem are either inefficient or too complex. In thispaper we present two Odds-Radio-Based textclassification algorithms, which are called ORand TF*OR respectively. We have evaluated ouralgorithm on two text collections and compared itagainst k-NN and SVM. Experimental resultsshow that OR and TF*OR are competitive withk-NN and SVM. Furthermore, OR and TF*OR ismuch simpler and faster than them. The resultsalso indicate that it is not TF but relevancefactors derived from Odds Radio that play thedecisive role in document categorization.
Congestion Adaptive Routing in Mobile Ad Hoc Networks
Mobility, channel error, and congestion are the main causes for packet loss in mobile ad hoc networks. Reducing packet loss typically involves congestion control operating on top of a mobility and failure adaptive routing protocol at the network layer. In the current designs, routing is not congestion-adaptive. Routing may let a congestion happen which is detected by congestion control, but dealing with congestion in this reactive manner results in longer delay and unnecessary packet loss and requires significant overhead if a new route is needed. This problem becomes more visible especially in large-scale transmission of heavy traffic such as multimedia data, where congestion is more probable and the negative impact of packet loss on the service quality is of more significance. We argue that routing should not only be aware of, but also be adaptive to, network congestion. Hence, we propose a routing protocol (CRP) with such properties. Our ns-2 simulation results confirm that CRP improves the packet loss rate and end-to-end delay while enjoying significantly smaller protocol overhead and higher energy efficiency as compared to AODV and DSR
Implementation and evaluation for dependable bus control using CPLD
Bus systems are used in computers as essential architecture, and dependability of bus systems should be accomplished reasonably for various applications. In this paper, we will present dependable bus operations with actual implementation and evaluation by CPLD. Most of the bus systems control transition of some classified phases with synchronous clock or guard time to avoid incorrect phase transition. However, these phase control methods may degrade system performance or cause incorrect operations. We design an asynchronous sequential circuit for bus phase control without clock or guard time. This circuit prevents incorrect phase transition at the time when large input delay or erroneous input occurs. We estimate probability of incorrect phase transition with single stuck-at fault on input signals. From the result of estimation, we also design checking system verifying outputs of initiator and target devices. Incorrect phase transition with single stuck-at fault occurred between both sequential circuits is inhibited completely by implementation of the system.
Portscan Detection with Sampled NetFlow
Sampling techniques are often used for traffic monitoring in high-speed links in order to avoid saturation of network resources. Although there is a wide existing research dealing with anomaly detection, few studies analyzed the impact of sampling on the performance of portscan detection algorithms. In this paper, we performed several experiments on two already existing portscan detection mechanisms to test whether they are robust enough to different sampling techniques. Unlike previous works, we found that flow sampling is not always better than packet sampling to continue detecting portscans reliably.
Morpheme-based chinese nested named entity recognition
Named entity recognition plays an important role in many natural language processing applications. While considerable attention has been pain in the past to research issues related to named entity recognition, few studies have been reported on the recognition of nested named entities. This paper presents a morpheme-based method to Chinese nested named entity recognition. To approach this task, we first employ the logistic regression model to extract multi-level entity morphemes from an entity-tagged corpus, and thus explore a variety of lexical features under the framework of conditional random fields to perform Chinese nested named entity recognition. Our experimental results on different data set show that our system is effective for most nested named entities under evaluation.
Voice Priority Queue Scheduling System Models for VoIP over WLANs
The Voice over Internet Protocol (VoIP) is a delay sensitive traffic due to real-time applications on networks. The assessment of voice flow quality in the VoIP is an essential requirement for technical and commercial motivation. The packets of VoIP streaming may experience drops because of the competition among the different kinds of traffic flow over the network. A VoIP application is also sensitive to delay and requires the voice packets to arrive on time from the sender to the receiver side without any delay over WLAN. The scheduling system model for VoIP traffic is an unresolved problem. In this research paper, the author proposes a new Voice Priority Queue (VPQ) scheduling system models and algorithms for the VoIP over WLANs to solve scheduling issues over IP-based networks. They present new contributions, through the three stages of the VPQ. The VPQ scheduling algorithm is provided as an essential technique in the VoIP communication networks to guarantee the QoS requirements. The design of the VPQ is managed by the limited bandwidth utilization and has been proven to have an efficient performance over WLANs.
Harmonic retrieval using higher order statistics: a deterministic formulation
Given a single record, the authors consider the problem of estimating the parameters of a harmonic signal buried in noise. The observed data are modeled as a sinusoidal signal plus additive Gaussian noise of unknown covariance. The authors define novel higher order statistics-referred to as "mixed" cumulants-that can be consistently estimated using a single record and are insensitive to colored Gaussian noise. Employing fourth-order mixed cumulants, they estimate the sinusoid parameters using a consistent, nonlinear matching approach. The algorithm requires an initial estimate that is obtained from a consistent, linear estimator. Finally, the authors examine the performance of the proposed method via simulations. >
A comparison between strand spaces and multiset rewriting for security protocol analysis
Formal analysis of security protocols is largely based on a set of assumptions commonly referred to as the Dolev-Yao model. Two formalisms that state the basic assumptions of this model are related here: strand spaces and multiset rewriting with existential quantification. Strand spaces provide a simple and economical approach to analysis of completed protocol runs by emphasizing causal interactions among protocol participants. The multiset rewriting formalism provides a very precise way of specifying finite-length protocols with unboundedly many instances of each protocol role, such as client, server, initiator, or responder. A number of modifications to each system are required to produce a meaningful comparison. In particular, we extend the strand formalism with a way of incrementally growing bundles in order to emulate an execution of a protocol with parametric strands. The correspondence between the modified formalisms directly relates the intruder theory from the multiset rewriting formalism to the penetrator strands. The relationship we illustrate here between multiset rewriting specifications and strand spaces thus suggests refinements to both frameworks, and deepens our understanding of the Dolev-Yao model.
Off-the-path flow handling mechanism forhigh-speed and programmable traffic management
In this paper, we propose a high-speed and programmable traffic management mechanism to enable easy and timely innovations. A control framework introduced by 4D, Tesseract, or OpenFlow, separates control functions from the switch nodes to a control server so that a variety of network control policies can be implemented outside of the switches. Within this framework, we propose a mechanism to enable flexible flow-based traffic management so that a variety of innovative traffic management schemes can be realized. Per-flow traffic management, however, requires packet-by-packet state updates, which can spoil this control framework. The proposed mechanism consists of a control server that monitors traffic conditions using sampled packets sent from the switches and calculates per-flow packet discarding rate, and switches that discard incoming packets according to the discarding rate. Packet sampling and discarding do not require packet-by-packet state handling at the switches and thus allows controls from a control server. We also propose a mechanism to compress the discarding information using a time series of bloom filters, so that frequent control updates are allowed. We tested the mechanism with per-flow WFQ emulation and the simulation results showed very good per-flow fairness. Furthermore, we found that the flow table is compressed 600 times smaller and that the processing cost at the server and the switches is small enough for use with 10 Gbps links.
PLDA: Parallel Latent Dirichlet Allocation for Large-Scale Applications
This paper presents PLDA, our parallel implementation of Latent Dirichlet Allocation on MPI and MapReduce. PLDA smooths out storage and computation bottlenecks and provides fault recovery for lengthy distributed computations. We show that PLDA can be applied to large, real-world applications and achieves good scalability. We have released  MPI-PLDA  to open source at http://code.google.com/p/plda under the Apache License.
A fast thresholded linear convolution representation of morphological operations
In this correspondence, we present a fast thresholded linear convolution representation of morphological operations. The thresholded linear convolution representation of dilation and erosion is first proposed. A comparison of the efficiency of the direct implementation of morphological operations and the thresholded linear convolution representation of morphological operations is subsequently conducted. Mathematical morphology has emerged as a powerful new tool for image processing. >
Facebook users have become much more private: A large-scale study
We investigate whether Facebook users have become more private in recent years. Specifically, we examine if there have been any important trends in the information Facebook users reveal about themselves on their public profile pages since early 2010. To this end, we have crawled the public profile pages of 1.4 million New York City (NYC) Facebook users in March 2010 and again in June 2011. We have found that NYC users in our sample have become dramatically more private during this period. For example, in March 2010 only 17.2% of users in our sample hid their friend lists, whereas in June 2011, just 15 months later, 52.6% of the users hid their friend lists. We explore privacy trends for several personal attributes including friend list, networks, relationship, high school name and graduation year, gender, and hometown. We find that privacy trends have become more pronounced for certain demographics. Finally, we attempt to determine the primary causes behind the dramatic decrease in the amount of information Facebook users reveal about themselves to the general public.
Improved compression by coupling of coding techniques and redundant transform
The techniques commonly used in image coding (JPEG, MPEG, ...) have as the main objective to compress as much as possible while retaining most of the information. These methods are often based on the use of the discrete cosine transform (DCT) and the wavelet transform (WT). Our purpose is to consider the necessary redundancy to achieve a good reception in the case of heavy interruptions of bits transmission. There is however a contradiction between an optimal compression and the redundancy required. It is thus necessary to master and compress the information to be transmitted as much as possible and to withstand the noise on the transmission channel. This article makes a contribution to this difficult problem: its originality resides in the coupling of orthogonal transforms and a redundant transform. Simulation results are provided using our method and the results are compared with that of DCT and WT based methods for Lena and Mountain images.
Principal Component Analysis for Large Scale Problems with Lots of Missing Values
Principal component analysis (PCA) is a well-known classical data analysis technique. There are a number of algorithms for solving the problem, some scaling better than others to problems with high dimensionality. They also differ in their ability to handle missing values in the data. We study a case where the data are high-dimensional and a majority of the values are missing. In case of very sparse data, overfitting becomes a severe problem even in simple linear models such as PCA. We propose an algorithm based on speeding up a simple principal subspace rule, and extend it to use regularization and variational Bayesian (VB) learning. The experiments with Netflix data confirm that the proposed algorithm is much faster than any of the compared methods, and that VB-PCA method provides more accurate predictions for new data than traditional PCA or regularized PCA.
On the guaranteed error correction capability of LDPC codes
We investigate the relation between the girth and the guaranteed error correction capability of gamma-left regular LDPC codes when decoded using the bit flipping (serial and parallel) algorithms. A lower bound on the number of variable nodes which expand by a factor of at least 3gamma/4 is found based on the Moore bound. An upper bound on the guaranteed correction capability is established by studying the sizes of smallest possible trapping sets.
Image recovery using a new nonlinear adaptive filter based on neural networks
This work defines a new nonlinear adaptive filter based on a feed-forward neural network with the capacity of significantly reducing the additive noise of an image. Even though measurements have been carried out using X-ray images with additive white Gaussian noise, it is possible to extend the results to other type of images. Comparisons have been carried out with the Weiner filter because it is the most effective option for reducing Gaussian noise. In most of the cases, image reconstruction using the proposed method has produced satisfactory results. Finally, some conclusions and future work lines are presented
Blob Analysis of the Head and Hands: A Method for Deception Detection
Behavioral indicators of deception and behavioral state are extremely difficult for humans to analyze. Blob analysis, a method for analyzing the movement of the head and hands based on the identification of skin color is presented. This method is validated with numerous skin tones. A proof-of-concept study is presented that uses blob analysis to explore behavioral state identification in the detection of deception.
Constrained Angular Motion Estimation in a Gyro-Free IMU
In this paper, we present an extended Kalman filter (EKF)-based solution for the estimation of the angular motion using a gyro-free inertial measurement unit (GF-IMU) built of twelve separate mono-axial accelerometers. Using such a GF-IMU produces a vector, which we call the angular information vector (AIV) that consists of 3D angular acceleration terms and six quadratic terms of angular velocities. We consider the multiple distributed orthogonal triads of accelerometers that consist of three nonplanar distributed triads equally spaced from a central triad as a specific case to solve. During research for the possible filter schemes, we derived equality constraints. Hence we incorporate the constraints in the filter to improve the accuracy of the angular motion estimation, which in turn improves the attitude accuracy (direction cosine matrix (DCM) or quaternion vector).
Subject searching in online catalogs: metaknowledge used by experienced searchers
This paper begins to identify and characterize the knowledge used by experienced librarians while searching for subject information in online catalogs. Ten experienced librarians performed the same set of six subject searches in an online catalog. Investigated was the knowledge used to solve retrieval problems. This knowledge represents expertise in the use of the catalog. Data were collected through the use of think-aloud protocols, transaction logs, and structured interviews. Knowledge was defined as knowledge of objects (factual knowledge), knowledge of events (experiential knowledge), knowledge of performance (process knowledge), and metaknowledge. Metaknowledge is the sense of whole derived from the integration of factual, process, and experiential knowledge about the search and the conditions under which it is performed. The focus of this paper is on metaknowledge. For evidence of metaknowledge the data were examined for explanations that participants gave for their actions and observations, and for ways that participants evaluated their own progress during the process of searching. Reasons and explanations given by searchers were related to all phases of the library information retrieval process from the user's receipt of material to policies for collection development, and not just events directly related to the performance of a particular search task.
Injection-Locked Clocking: A Low-Power Clock Distribution Scheme for High-Performance Microprocessors
We propose injection-locked clocking (ILC) to combat deteriorating clock skew and jitter, and reduce power consumption in high-performance microprocessors. In the new clocking scheme, injection-locked oscillators are used as local clock receivers. Compared to conventional clocking with buffered trees or grids, ILC can achieve better power efficiency, lower jitter, and much simpler skew compensation thanks to its built-in deskewing capability. Unlike other alternatives, ILC is fully compatible with conventional clock distribution networks. In this paper, a quantitative study based on circuit and microarchitectural-level simulations is performed. Alpha21264 is used as the baseline processor, and is scaled to 0.13 m and 3 GHz. Simulations show 20- and 23-ps jitter reduction, 10.1% and 17% power savings in two ILC configurations. A test chip distributing 5-GHz clock is implemented in a standard 0.18- m CMOS technology and achieved excellent jitter performance and a deskew range up to 80 ps.
State of the Art in Modeling and Deployment of Electronic Contracts
Modeling and deployment of e-contracts is a challenging task because of the involvement of both technological and business aspects. There are several frameworks and systems available in the literature. Some works mainly deal with the automatic handling of paper contracts and others provide monitoring and enactment of contracts. Because contracts evolve, it is useful to have a system that models and enacts the evolution of e-contracts.
CODEX: Exploration of semantic changes between ontology versions
Summary: Life science ontologies substantially change over time to meet the requirements of their users and to include the newest domain knowledge. Thus, an important task is to know what has been modified between two versions of an ontology (diff ). This diff should contain all performed changes as compact and understandable as possible. We present CODEX (Complex Ontology Diff Explorer), a tool that allows determining semantic changes between two versions of an ontology which users can interactively analyze in multiple ways. Availability and Implementation: CODEX is available under http: //www.izbi.de/codex and is supported by all major browsers. It is implemented in Java based on Google Web Toolkit technology. Additionally, users can access a web service interface to use the diff functionality in their applications and analyses.
Reliability Analysis in the Early Development of Real-Time Reactive Systems
The increasing trend toward complex software systems has highlighted the need to incorporate quality requirements earlier in the development process. Reliability is one of the important quality indicators of such systems. This paper proposes a reliability analysis approach to measure reliability in the early development of real-time reactive systems (RTRS). The goal is to provide decision support and detect the first signs of low or decreasing reliability as the system design evolves. The analysis is conducted in a formal development environment for RTRS, formalized mathematically and illustrated using a train-gate-controller case study.
Tools for Creating Documents in 'Preferred Format' for Visually Impaired People
A suite of tools is described that allow a word-processor operator to produce documents in large print, Braille and on cassette tape so that the documents can subsequently be read by visually impaired or blind users. The tools integrate with Microsoft Word, are written in Visual Basic for Applications and make use of the Word Object Model.
NCAC: Network Congestion Analyzer and Controller
The stability of the current Internet architecture depends mostly on end-to-end TCP congestion control mechanisms. The Network Congestion Analyzer and Controller (NCAC) is an effort to build a complete user interface application to NS-2 that provides graphical access to most of NS2's functionalities. Besides, the application provides powerful visual tools for monitoring and displaying network performance metrics calculated by the simulation. The application is developed in Java to benefit from Java's portability and platform independence. There are three main parts in NCAC: (1) NS-2 interface, (2) network animation and alarm, and (3) graph plotting application. The functionality of each part is discussed.
On the comparison of bilinear, cubic spline, and fuzzy interpolation techniques for robotic position measurements
This paper describes a novel technique for position error compensations of robots based on a fuzzy error interpolation method. A traditional robot calibration implements either model or modelless methods. The compensation of position error in a model-less method is to move the robot's end-effector to a target position in the robot workspace, and to find the target position error online based on the measured neighboring four-point errors around the target position. For this purpose, a stereo camera or other measurement device can be used to measure offline the position errors of the robot's end-effector at predefined grid points. By using the proposed fuzzy error interpolation technique, the accuracy of the position error compensation can be greatly improved, which is confirmed by the simulation results given in this paper. A comparison study among various interpolation methods, such as bilinear, cubic spline, and the fuzzy error interpolation technique is also made via simulation. The simulation results show that more accurate compensation results can be achieved using the fuzzy error interpolation technique compared with its bilinear and cubic spline counterparts.
Locally Discriminative Coclustering
Different from traditional one-sided clustering techniques, coclustering makes use of the duality between samples and features to partition them simultaneously. Most of the existing co-clustering algorithms focus on modeling the relationship between samples and features, whereas the intersample and interfeature relationships are ignored. In this paper, we propose a novel coclustering algorithm named Locally Discriminative Coclustering (LDCC) to explore the relationship between samples and features as well as the intersample and interfeature relationships. Specifically, the sample-feature relationship is modeled by a bipartite graph between samples and features. And we apply local linear regression to discovering the intrinsic discriminative structures of both sample space and feature space. For each local patch in the sample and feature spaces, a local linear function is estimated to predict the labels of the points in this patch. The intersample and interfeature relationships are thus captured by minimizing the fitting errors of all the local linear functions. In this way, LDCC groups strongly associated samples and features together, while respecting the local structures of both sample and feature spaces. Our experimental results on several benchmark data sets have demonstrated the effectiveness of the proposed method.
Automatic annotation of drosophila developmental stages using association classification and information integration
In current developmental research, one of the challenging tasks is to understand the spatio-temporal gene expression patterns and the relationships among different genes. In situ hybridization (ISH) assay which shows mRNA spatio-temporal expression patterns in cells and tissues directly is currently widely utilized in the bench work. With the increasing of available ISH images, automatic annotation systems are highly demanded. In this paper, an automatic classification system is proposed for annotating the in situ hybridization images with respect to the developmental stages. The embryo is first segmented from the original image, registered and normalized. The segmented embryo image is then divided into 100 blocks from which the pixel intensity and texture features are extracted and discretized. The multiple correspondence analysis (MCA) based association classification approach is proposed to generate classification rules for different stages based on the training data set. The testing instance is classified by applying the rules generated in the training process and a classification coordination module is incorporated to resolve the conflicts utilizing the weights derived from angle values in the MCA procedure. Experimental results show that our proposed method achieves promising results and outperforms other state-of-the-art algorithms.
Theoretical calculations of homoconjugation equilibrium constants in systems modeling acid-base interactions in side chains of biomolecules using the potential of mean force.
The potentials of mean force (PMFs) were determined for systems forming cationic and anionic homocomplexes composed of acetic acid, phenol, isopropylamine, n-butylamine, imidazole, and 4(5)-methylimidazole, and their conjugated bases or acids, respectively, in three solvents with different polarity and hydrogen-bonding propensity: acetonitrile (AN), dimethyl sulfoxide (DMSO), and water (H 2 O). For each pair and each solvent a series of umbrella-sampling molecular dynamics simulations with the AMBER force field, explicit solvent, and counterions added to maintain a zero net charge of a system were carried out and the PMF was calculated by using the Weighted Histogram Analysis Method (WHAM). Subsequently, homoconjugation-equilibrium constants were calculated by numerical integration of the respective PMF profiles. In all cases but imidazole stable homocomplexes were found to form in solution, which was manifested as the presence of contact minima corresponding to hydrogen-bonded species in the PMF curves. The calculated homoconjugation constants were found to be greater for complexes with the OHO bridge (acetic acid and phenol) than with the NHN bridge and they were found to decrease with increasing polarity and hydrogen-bonding propensity of the solvent (i.e., in the series AN > DMSO > H 2 O), both facts being in agreement with the available experimental data. It was also found that interactions with counterions are manifested as the broadening of the contact minimum or appearance of additional minima in the PMF profiles of the acetic acid-acetate, phenol/ phenolate system in acetonitrile, and the 4(5)-methylimidazole/4(5)-methylimidzole cation conjugated base system in dimethyl sulfoxide.
Cost-Effective Computer-Aided Manufacturing of Prototype Parts
Computer-aided manufacturing (CAM) can be made cost effective, even for one-of-a-kind jobs. To do so requires reevaluation of Computerized Numerical Control (CNC) from the point of view of a computer system, rather than a glorified machine shop, Good systematization, useful software and cooperative CNC shop staff can improve the productivity of a typical CNC shop by a factor of five.
Data-driven design of HMM topology for online handwriting recognition
Although HMM is widely used for online handwriting recognition, there is no simple and well-established method of designing the HMM topology. We propose a data-driven systematic method to design HMM topology. Data samples in a single pattern class are structurally simplified into a sequence of straight-line segments. Then the resulting multiple models of the class are combined to form an architecture of a multiple parallel-path HMM, which behaves as single HMM. To avoid excessive growing of the number of the states, parameter trying is applied such that structural similarity among patterns is reflected. Experiments on online Hangul recognition showed about 19% of error reductions, compared to the intuitive deisgn method.
Fast block size prediction for MPEG-2 to H.264/AVC transcoding
One objective in MPEG-2 to H.264 transcoding is to improve the H.264 compression ratio by using more accurate H.264 motion vectors. Motion re-estimation is by far the most time consuming process in video transcoding, and improving the searching speed is a challenging problem. We introduce a new transcoding scheme that uses the MPEG-2 DCT coefficients to predict the block size partitioning for H.264. Performance evaluations have shown that, for the same rate-distortion performance, our proposed scheme achieves an impressive reduction in the computational complexity of more than 82% compared to the full range motion estimation used by H.264.
A curvature-based approach to contour motion estimation
We present a novel method of velocity field estimation for points on moving contours in an image sequence. The method determines the corresponding point in the next image frame by considering curvature changes at each point on a contour. In previous methods, there are errors in estimation for the points which have low curvature variations since those methods compute the solutions by approximating the normal component of optical flow. The proposed method computes optical flow vectors of contour points by minimizing the curvature changes. As a first step, snakes are used to locate smooth curves in 2D imagery. Then, the extracted curves are tracked continuously. We excluded the rearranging process in snakes and allowed the snaxel distance to vary. Each point on a contour has a unique corresponding point in the nest frame. Experimental results showed that the proposed method computes accurate optical flow vectors for various moving contours.
The Case for Timing-Centric Distributed Software Invited Paper
This paper makes the case that the time is right to introduce temporal semantics into programming models for cyber-physical systems. Specifically, we argue for a programming model called PTIDES that provides a coordination language rooted in discreteevent semantics, supported by a lightweight runtime framework and tools for verifying concurrent software components. PTIDES leverages recent innovations in network time synchronization to deliver distributed real-time systems with determinate concurrent semantics, decentralized and robust control, and the potential for rigorous schedulability analysis.
Balancing Resource Utilization to Mitigate Power Density in Processor Pipelines
Power density is a growing problem in high-performance processors in which small, high-activity resources overheat. Two categories of techniques, temporal and spatial, can address power density in a processor. Temporal solutions slow computation and heating either through frequency and voltage scaling or through stopping computation long enough to allow the processor to cool; both degrade performance. Spatial solutions reduce heat by moving computation from a hot resource to an alternate resource (e.g., a spare ALU) to allow cooling. Spatial solutions are appealing because they have negligible impact on performance, but they require availability of spatial slack in the form of spare or underutilized resource copies. Previous work focusing on spatial slack within a pipeline has proposed adding extra resource copies to the pipeline, which adds substantial complexity because the resources that overheat, issue logic, register files, and ALUs, are the resources in some of the tightest critical paths in the pipeline. Previous work has not considered exploiting the spatial slack already existing within pipeline resource copies. Utilization can be quite asymmetric across resource copies, leaving some copies substantially cooler than others. We observe that asymmetric utilization within copies of three key back-end resources, the issue queue, register files, and ALUs, creates spatial slack opportunities. By balancing asymmetry in their utilization, we can reduce power density. Scheduling policies for these resources were designed for maximum simplicity before power density was a concern; our challenge is to address asymmetric heating while keeping the pipeline simple. Balancing asymmetric utilization reduces the need for other performance-degrading temporal power-density techniques. While our techniques do not obviate temporal techniques in high-resource-utilization applications, we greatly reduce their use, improving overall performance.
Efficient Vertical/Horizontal-Space 1D-DCT Processing Based on Massive-Parallel Matrix-Processing Engine
This paper reports an efficient discrete cosine transform (DCT) processing for the JPEG algorithm using a massive-parallel memory-embedded SIMD matrix processor. The matrix-processing engine has 2,048 2-bit processing elements, which are connected by a flexible switching network, and supports 2-bit 2,048-way bit-serial and word-parallel operations with a single command. For compatibility with this matrix-processing architecture, the conventional DCT algorithm has been improved in arithmetic order and the vertical/horizontal-space 1 dimensional (1D)-DCT processing has been further developed. Evaluation results of the matrix-engine-based DCT processing show that the necessary clock cycles per image blocks can be reduced by 87% in comparison to a conventional DSP architecture. The determined performances in MOPS and MOPS/mm are factors 8 and 5.6 better than with a conventional DSP, respectively. Moreover, the matrix-processing engine can reduce the number of total clock cycles for JPEG application about 49% in comparison to a conventional DSP architecture.
Parallel Online Ranking of Web Pages
Modern search engines use link structure of the World Wide Web in order to gain better results for ranking the results of users' queries. One of the most popular ranking algorithms which is based on link analysis is HITS. It generates very accurate outputs but because of huge amount of online computations, this algorithm is relatively slow. In this paper we introduce PHITS, a parallelized version of the HITS algorithm that is suitable for working with huge web graphs in a reason- able time. For implementing this algorithm, we use WebGraph framework and we focus on parallelizing access to web graph as the main bottleneck in the HITS algorithm. I. INTRODUCTION Search technology is one of the most important reasons for success of the web. The huge amount of information available on the web, its high growth rate, and its unstructured nature, all increase the need for search engines with high performance and accurate results. One of the major components of each search engine is its ranking algorithm. Traditional Information Retrieval (IR) systems usually use some models like VMS (4) and compute rank of results using content similarity measures between user's query and retrieved documents. But in the context of the web, there are some problems with these approaches. For example, spamming may lead to inefficient ranking. Some methods have been proposed to encounter these problems most of which uses some implicit information which is embedded in the web graph. These methods are known as Link-Analysis based algorithms. PageRank (5) and HITS (Hyperlink Induced Topic Search) (1) are the most well known algorithms in this category. PageRank, which is used by Google for ranking its results, is an offline and query-independent ranking algorithm. This means that the ranking is independent of the specific queries of users and therefore can be done once and used for all of the upcoming queries. On the other hand, HITS is an online and query-dependent algorithm. Being query dependent makes HITS more precise but it has some disadvantages too. In fact, required online computations for this algorithm is too much and the response time of the search engine after submitting queries by users is not acceptable. To overcome this problem, in this paper we will exploit the parallel processing methods to improve the execution performance of the algorithm. The rest of this paper is organized as follows. In section II, link-analysis based algorithms in general and HITS as a special case are discussed. At the end of this section, some of the variations and improvements for the HITS algorithm that are suggested in the literature are also described. Implementing the HITS algorithm and its parallel version, PHITS, are discussed in sections III and IV respectively. Finally, last section of this paper contains conclusion and some ideas for future work in this topic.
Authentication in distributed systems: theory and practice
We describe a theory of authentication and a system that implements it. Our theory is based on the notion of principal and a "speaks for" relation between principals. A simple principal either has a name or is a communication channel; a compound principal can express an adopted role or delegation of authority. The theory explains how to reason about a principal's authority by deducing the other principals that it can speak for; authenticating a channel is one important application. We use the theory to explain many existing and proposed mechanisms for security. In particular, we describe the system we have built. It passes principals efficiently as arguments or results of remote procedure calls, and it handles public and shared key encryption, name lookup in a large name space, groups of principals, loading programs, delegation, access control, and revocation.
Human Gait Recognition With Matrix Representation
Human gait is an important biometric feature. It can be perceived from a great distance and has recently attracted greater attention in video-surveillance-related applications, such as closed-circuit television. We explore gait recognition based on a matrix representation in this paper. First, binary silhouettes over one gait cycle are averaged. As a result, each gait video sequence, containing a number of gait cycles, is represented by a series of gray-level averaged images. Then, a matrix-based unsupervised algorithm, namely coupled subspace analysis (CSA), is employed as a preprocessing step to remove noise and retain the most representative information. Finally, a supervised algorithm, namely discriminant analysis with tensor representation, is applied to further improve classification ability. This matrix-based scheme demonstrates a much better gait recognition performance than state-of-the-art algorithms on the standard USF HumanID Gait database
Design for Testability Based on Single-Port-Change Delay Testing for Data Paths
This paper introduces a new concept of hierarchical testability called Single-Port-Change (SPC) two-pattern testability. We propose a non-scan design-for-testability (DFT) method which makes each path that needs to be tested in a data path SPC two-pattern testable. An SPC two-pattern test guarantees robust (resp. non-robust) test if the path is robust (resp. non-robust) testable. Since it is easy to find justification paths for SPC two-pattern tests at register-transfer level, the proposed DFT method can reduce hardware overhead compared to that of our previous DFT method for arbitrary two-pattern tests. Furthermore, we propose a method to reduce test generation effort by removing a subset of sequentially untestable paths from targets of test generation. Experimental results show that the proposed method can reduce hardware overhead without losing the quality of test.
DAIDS: An Architecture for Modular Mobile IDS
The popularity of mobile devices and the enormous number of third party mobile applications in the market have naturally lead to several vulnerabilities being identified and abused. This is coupled with the immaturity of intrusion detection system (IDS) technology targeting mobile devices. In this paper we propose a modular host-based IDS framework for mobile devices that uses behavior analysis to profile applications on the Android platform. Anomaly detection can then be used to categorize malicious behavior and alert users. The proposed system accommodates different detection algorithms, and is being tested at a major telecom operator in North America. This paper highlights the architecture, findings, and lessons learned.
An integrated data mining system to automate discovery of measures of association
Many data analysts require tools which can integrate their database management packages (e.g. Microsoft Access) with their data analysis ones (e.g. SAS, SPSS), and provide guidance for the selection of appropriate mining algorithms. In addition, the analysts need to extract and validate statistical results to facilitate data mining. In this paper, we describe an integrated data mining system called the Linear Correlation Discovery System (LCDS) that meets the above requirement. LCDS consists of four major sub-components, two of which, the selection assistant and the statistics coupler, we discussed in this paper. The former examines the scheme and instances to determine appropriate association measurement functions (e.g, chi-square, linear regression, ANOVA). The latter involves the appropriate statistical function on a sample data set, and extracts relevant statistical output such as /spl eta//sup 2/, and R/sup 2/ for effective mining of data. We also describe a new validation algorithm based on measuring the consistency of mining results applied to multiple test sets.
Configuring sensors by user learning for a locomotion aid interface
This article presents a study about a mobility aid system, which integrates adaptive control interface for a robotic walker. The objective is to give to the patient more flexibility in the choice of a technical aid. The specificity of our system is based on an auto-adaptive interface, which improves the configuration choice of the patient's driving commands. The results obtained from experimentations show the capabilities of our method for a technical aid. The learning process is applied on line to a neural network controller. The experiment is focused on sensors configuration and command of a powered walker interface.
Hermite normality tests
Nous presentons dans cet article une etude complete du test de normalite d'Hermite. Ce test utilise les proprietes des polynomes d'Hermite et une statistique de sphericite modifiee pour decider si un echantillon monodimensionnel, standardise et blanc est gaussien ou non. L'avantage majeur de cette approche est de definir une famille de statistiques qui va permettre d'adapter le choix d'un test particulier aux donnees. Nous avons etabli la distribution asymptotique du test d'Hermite sous l'hypothese nulle et sous l'hypothese alternative et etudie en details le cas particulier de tests a deux polynomes. Nous avons determine les tests asymptotiquement les plus puissants pour quelques distributions alternatives et effectue un grand nombre de simulations afin de comparer le test d'Hermite a trois autres tests. Les bons resultats obtenus nous encouragent a generaliser le test d'Hermite aux cas de donnees colorees et multivariees.
Modeling and Simulation of a Fuzzy Supervisory Controller for an Industrial Boiler
In this paper we compare and discuss the performance of a boiler evaporator system when the system is controlled by a traditional PID-type strategy and when the system is enhanced by using fuzzy logic blocks to provide set-points for the system. The strategy used in fuzzy logic controllers (FLCs) is called fuzzy supervisory control and it generates set-points for the conventional controllers. The boiler under test is a VU-60 industrial system that produces 180,000 pounds of steam per hour. The mathematical model of the plant is a scaled version model of that obtained for a thermoelectric unit. The new model simplifies the large-scale thermoelectric boiler model to an industrial small-scale type VU-60 boiler model based upon first principle mass and energy balance equations. The main change consists of representing only the behavior of the drum—evaporator system, having a partial model of the combustion process, with a simplified combustion control system and a three-element boiler feed-water controller. The control system for combustion and boiler feed-water receives a supervisory signal (or set-point tracking signal) that comes from the FLC to improve the performance of the overall control system. The behavior of the supervisory controller brings some advantages to the system performance, compared with the traditional control schemes. The comparison reflects fuel improvements from 2.5% to 6.5% depending upon the steam load ramp regime. The simulations are performed using the SIMULINK® shell running under the MATLAB ® platform.
Partial Functional Manipulation Based Wirelength Minimization
In-place flipping of rectangular blocks/cells can potentially reduce the wirelength of a floorplan/placement solution without changing the chip area, In a recent work [Hao 05], the flipping optimization is solved through a binary decision diagram (BDD) based approach. However, the BDD-based approach is not scalable for large SOC designs with many blocks due to memory and runtime blow-up. This paper presents a new approach using the partitioned ordered partial decision diagrams (POPDD) for wirelength minimization. POPDD is based on a novel compact partial functional representation between flip configurations and corresponding wirelengths. By controlling the number of nodes allowed per POPDD and the iterations, easy trade-off between runtime/memory and accuracy/optimality can be achieved. Experimental results clearly demonstrate the efficiency of the proposed approach.
Assessment of an Operational System for Crop Type Map Production Using High Temporal and Spatial Resolution Satellite Optical Imagery
Crop area extent estimates and crop type maps provide crucial information for agricultural monitoring and management. Remote sensing imagery in general and, more specifically, high temporal and high spatial resolution data as the ones which will be available with upcoming systems, such as Sentinel-2, constitute a major asset for this kind of application. The goal of this paper is to assess to what extent state-of-the-art supervised classification methods can be applied to high resolution multi-temporal optical imagery to produce accurate crop type maps at the global scale. Five concurrent strategies for automatic crop type map production have been selected and benchmarked using SPOT4 (Take5) and Landsat 8 data over 12 test sites spread all over the globe (four in Europe, four in Africa, two in America and two in Asia). This variety of tests sites allows one to draw conclusions applicable to a wide variety of landscapes and crop systems. The results show that a random forest classifier operating on linearly temporally gap-filled images can achieve overall accuracies above 80% for most sites. Only two sites showed low performances: Madagascar due to the presence of fields smaller than the pixel size and Burkina Faso due to a mix of trees and crops in the fields. The approach is based on supervised machine learning techniques, which need in situ data collection for the training step, but the map production is fully automatic.
Data driven frequency mapping for computationally scalable object detection
Nonlinear kernel Support Vector Machines achieve better generalizations, yet their training and evaluation speeds are prohibitively slow for real-time object detection tasks where the number of data points in training and the number of hypotheses to be tested in evaluation are in the order of millions. To accelerate the training and particularly testing of such nonlinear kernel machines, we map the input data onto a low-dimensional spectral (Fourier) feature space using a cosine transform, design a kernel that approximates the classification objective in a supervised setting, and apply a fast linear classifier instead of the conventional radial basis functions. We present a data driven hypotheses generation technique and a LogistBoost feature selection. Our experimental results demonstrate the computational improvements 20∼100× while maintaining a high classification accuracy in comparison to SVM linear and radial kernel basis function classifiers.
Towards computer-assisted photo-identification of humpback whales
This paper describes current work on a photo-id system for humpback whales. Individuals of this species can be uniquely identified by the light and dark pigmentation patches on their tails (flukes). We developed an interface that assists the user in segmenting the animal's tail from the sea and fitting an affine invariant coordinate grid to it. A numerical feature vector capturing the patch-distribution with respect to the grid is then automatically extracted and used to match the individual against a database of similarly processed images.
A novel fast approach for estimating error propagation in decision feedback detectors
The study of error-burst statistics is important for all detection systems, and more so for the decision feedback class. In data storage applications, many detection systems use decision feedback in one form or another. Fixed-delay tree search with decision feedback (FDTS/DF) and decision feedback equalization (DFE) are the direct forms, whereas the partial response detectors such as the reduced state sequence estimator (RSSE) and noise predictive maximum likelihood (NPML) detectors are the other forms. Although DF reduces the system complexity, it is inevitably linked with error propagation (EP), which can be quantified using error-burst statistics. Analytical evaluation of these statistics is difficult, if not impossible, because of the complexity of the problem. Hence, the usual practice is to use computer simulations. However, the computational time in traditional bit-by-bit simulations can be prohibitive at meaningful signal-to-noise ratios. In this paper, we propose a novel approach for fast estimation of error-burst statistics in FDTS/DF detectors, which is also applicable to other detection systems. In this approach, error events are initiated more frequently than natural by artificially injecting noise samples. These noise samples are generated using a transformation that results in significant reduction in computational complexity. Simulation studies show that the EP performance obtained by the proposed method matches closely with those obtained by bit-by-bit simulations, while saving as much as 99% of simulation time.
Incrementally maintaining classification using an RDBMS
The proliferation of imprecise data has motivated both researchers and the database industry to push statistical techniques into relational database management systems (RDBMSes). We study strategies to maintain model-based views for a popular statistical technique, classification, inside an RDBMS in the presence of updates (to the set of training examples). We make three technical contributions: (1) A strategy that incrementally maintains classification inside an RDBMS. (2) An analysis of the above algorithm that shows that our algorithm is optimal among all deterministic algorithms (and asymptotically within a factor of 2 of a non-deterministic optimal strategy). (3) A novel hybrid-architecture based on the technical ideas that underlie the above algorithm which allows us to store only a fraction of the entities in memory. We apply our techniques to text processing, and we demonstrate that our algorithms provide an order of magnitude improvement over non-incremental approaches to classification on a variety of data sets, such as the Citeseer and DBLife.
Optimal band selection for future satellite sensor dedicated to soil science
Hyperspectral imaging systems could be used for identifying the different soil types from the satellites. However, detecting the reflectance of the soils in all the wavelengths involves the use of a large number of sensors with high accuracy and also creates a problem in transmitting the data to earth stations for processing. The current sensors can reach a bandwidth of 20 nm and hence, the reflectance obtained using the sensors are the integration of reflectance obtained in each of the wavelength present in the spectral band. Moreover, not all spectral bands contribute equally to classification and hence, identifying the bands necessary to have a good classification is necessary to reduce sensor cost and problem in data transmission from the satellite. The work presents the spectral bands selected using a PCA-Based Forward Sequential band selection algorithm.
Positive Macroscopic Approximation for Fast Attribute Reduction
Attribute reduction is one of the challenging problems facing the effective application of computational intelligence technology for artificial intelligence. Its task is to eliminate dispensable attributes and search for a feature subset that possesses the same classification capacity as that of the original attribute set. To accomplish efficient attribute reduction, many heuristic search algorithms have been developed. Most of them are based on the model that the approximation of all the target concepts associated with a decision system is dividable into that of a single target concept represented by a pair of definable concepts known as lower and upper approximations. This paper proposes a novel model called macroscopic approximation, considering all the target concepts as an indivisible whole to be approximated by rough set boundary region derived from inconsistent tolerance blocks, as well as an efficient approximation framework called positive macroscopic approximation (PMA), addressing macroscopic approximations with respect to a series of attribute subsets. Based on PMA, a fast heuristic search algorithm for attribute reduction in incomplete decision systems is designed and achieves obviously better computational efficiency than other available algorithms, which is also demonstrated by the experimental results.
Managing multiple engineering projects in a manufacturing support environment
Business trends require front-line managers to integrate multiproject concepts with those of traditional single-project management since very rarely can one find major organizations managing just one project. A typical situation entails a limited pool of resources which is applied to the management of several projects, with people moving back and forth among different assignments in different projects. Yet, few studies on project management have started to explore the issue of how to manage an organization with multiple inter- or intradepartmental projects. Using a case study method, our exploratory research investigates the specific problems associated with the management of multiple engineering projects in a manufacturing support environment, with the intent to identify common factors of success. Knowing the factors of success is but the first step toward improving multi-project management. Our findings provide insight into how the most important multiple-project success factors in this environment differ from factors of success in traditional single-project management, and are consistent with other emerging research in product development environments. The differences center on resource allocation and flexibility. Some factors, such as ownership, staff experience, and communication, take on additional dimensions when considered in a multiple-versus a single-project environment. Division and assignment of resources, prioritization, and customized management style, which have little relevance in relation to single projects, are shown to play a major role in the success of multiproject management.
Performance analysis of a recursive fractional super-exponential algorithm
The super-exponential algorithm is a block-based technique for blind channel equalization and system identification. Due to its fast convergence rate, and no a priori parameterization other than the block length, it is a useful tool for linear equalization of moderately distortive channels. This paper presents a recursive implementation of the super-exponential algorithm for fractionally-sampled PAM signals. Although the resulting algorithm is still block-based, recursive propagation of several key variables allows the block length to be significantly reduced without compromising the algorithm's accuracy or speed, thereby enhancing its ability to track channel variations. The convergence rate is only mildly influenced by specific channel responses, and oversampling provides smaller output variance and almost perfect tolerance to sampling errors. Simulation results demonstrate the effectiveness of the proposed technique.
Free-viewpoint image generation using different focal length camera array
The availability of multi-view images of a scene makes new and exciting applications possible, including Free-Viewpoint TV (FTV). FTV allows us to change viewpoint freely in a 3D world, where the virtual viewpoint images are synthesized by Image-Based Rendering (IBR). In this paper, we introduce a FTV depth estimation method for forward virtual viewpoints. Moreover, we introduce a view generation method by using a zoom camera in our camera setup to improve virtual viewpoint-ts' image quality. Simulation results confirm reduced error during depth estimation using our proposed method in comparison with conventional stereo matching scheme. We have demonstrated the improvement in image resolution of virtually moved forward camera using a zoom camera setup.
Automatic location and tracking of the facial region in color video sequences
A novel technique is introduced to locate and track the facial area in videophone-type sequences. The proposed method essentially consists of two components: (i) a color processing unit, and (ii) a knowledge-based shape and color analysis module. The color processing component utilizes the distribution of skin-tones in the HSV color space to obtain an initial set of candidate regions or objects. The second component in the segmentation scheme, that is, the shape and color analysis module is used to correctly identify and select the facial region in the case where more than one object has been extracted. A number of fuzzy membership functions are devised to provide information about each object’s shape, orientation, location and average hue. An aggregation operator finally combines these measures and correctly selects the facial area. The suggested approach is robust with regard to di⁄erent skin types, and various types of object or background motion within the scene. Furthermore, the algorithm can be implemented at a low computational complexity due to the binary nature of the operations involved. Experimental results are presented for a series of CIF and QCIF video sequences. ( 1999 Elsevier Science B.V. All rights reserved.
On deriving the second-stage training set for trainable combiners
Unlike fixed combining rules, the trainable combiner is applicable to ensembles of diverse base classifier architectures with incomparable outputs. The trainable combiner, however, requires the additional step of deriving a second-stage training dataset from the base classifier outputs. Although several strategies have been devised, it is thus far unclear which is superior for a given situation. In this paper we investigate three principal training techniques, namely the re-use of the training dataset for both stages, an independent validation set, and the stacked generalization. On experiments with several datasets we have observed that the stacked generalization outperforms the other techniques in most situations, with the exception of very small sample sizes, in which the re-using strategy behaves better. We illustrate that the stacked generalization introduces additional noise to the second-stage training dataset, and should therefore be bundled with simple combiners that are insensitive to the noise. We propose an extension of the stacked generalization approach which significantly improves the combiner robustness.
Automated Service Composition with Adaptive Planning
Service-Oriented Computing is a cornerstone for the realization of user needs through the automatic composition of services from service descriptions and user tasks, i.e., high-level descriptions of the user needs. Yet, automatic service composition processes commonly assume that service descriptions and user tasks share the same abstraction level, and that services have been pre-designed to integrate. To release these strong assumptions and to augment the possibilities of composition, we add adaptation features into the service composition process using semantic descriptions and adaptive extensions to graph planning.
Base station scheduling of requests with fixed deadlines
We consider a packet switched wireless network in which a base station serves the mobiles. The packets for the mobiles arrive at the base station and have to be scheduled for transmission to the mobiles. The capacity of the channel from the base station to the mobiles is varying with time due to fading. We assume the mobiles can obtain different types of service based on the prices they are willing to pay. The objective of the base station is to schedule packets for transmission to mobiles to maximize the revenue earned. Our main result is that a simple greedy algorithm does at least 1/2 as good as the optimal offline algorithm that knows the complete future request pattern and channel conditions. We also show that no other online algorithm can do better.
THE SRI NIST 2008 speaker recognition evaluation system
The SRI speaker recognition system for the 2010 NIST speaker recognition evaluation (SRE) incorporates multiple subsystems with a variety of features and modeling techniques. We describe our strategy for this year's evaluation, from the use of speech recognition and speech segmentation to the individual system descriptions as well as the final combination. Our results show that under most conditions, the cepstral systems tend to perform the best, but that other, non-cepstral systems have the most complementarity. The combination of several subsystems with the use of adequate side information gives a 35% improvement on the standard telephone condition. We also show that a constrained cepstral system based on nasal syllables tends to be more robust to vocal effort variabilities.
Rehabilitation of handwriting skills in stroke patients using interactive games: a pilot study
This paper describes an interactive application that aims to support the rehabilitation of handwriting skills in people that suffer from paralysis after a stroke. The purpose of the application is to make the rehabilitation of handwriting skills fun and engaging. Four platform-independent games with adjustable levels of difficulty were created in order to target varying levels of skills. The application also features a performance history, audio-visual feedback, and posture reminders. It was evaluated with medical staff and patients from the Hoensbroeck Rehabilitation Centre in the Netherlands. The initial results indicated that the games are more motivating and fun than traditional pen and paper exercises. The feedback received from therapists supports our claim that the games are a useful addition to the rehabilitation of handwriting.
A Digital Switching Demodulator for Electrical Capacitance Tomography
In this paper, a digital switching demodulator is presented for use in ac-based electrical capacitance tomography systems. Implementing a switching phase-sensitive demodulator (PSD) digitally offers the following advantages: 1) Demodulation can be implemented using a programmable digital device, and hence, CMOS switches, which are used in a conventional switching PSD, are no longer needed; 2) compared with the widely used digital quadrature PSD, this proposed demodulator is simple in configuration because neither a reference signal nor multiplication is required; 3) according to the specific requirements, the new demodulator can be implemented in two operation modes, i.e., the amplitude mode and the phase-sensitive mode; and 4) because only subtractions and accumulations are needed, the proposed demodulator can be easily implemented with low-cost logic devices, e.g., a complex programmable logic device (CPLD). By simulation, the feasibility and effectiveness of the proposed demodulator have been confirmed. CPLD-based and field-programmable-gate-array-based capacitance measurement circuits are constructed, and the performances of different demodulation methods are compared. Both simulation and experiment show that the proposed demodulator can provide demodulation results with high signal-to-noise ratio. The system design can be simplified using the digital switching demodulator.
Psychophysical evaluation of in-situ ultrasound visualization
We present a novel psychophysical method for evaluating ultrasonography based on real-time tomographic reflection (RTTR), in comparison to conventional ultrasound (CUS). The method measures the user's perception of the location of an ultrasound-imaged target independently from assessing the action employed to reach it. Three experiments were conducted with the sonic flashlight (SF), an RTTR device, and CUS. The first two experiments determined subjects' perception of target location with a triangulation-by-pointing task. Depth perception with the SF was comparable to direct vision, while CUS caused considerable underestimation of target depth. Binocular depth information in the SF was shown to significantly contribute to its superiority. The third experiment tested subjects in an ultrasound-guided needle insertion task. Because the SF provides visualization of the target at its actual location, subjects performed insertions faster and more accurately by using the SF rather than CUS. Furthermore, the trajectory analysis showed that insertions with the SF generally went directly to the target along the desired path, while CUS often led to a large deviation from the correct path consistent with the observed underestimation of target depth. These findings lend great promise to the use of RTTR-based imaging in clinical practice and provide precise means of assessing efficacy.
Probabilistic model checking of complex biological pathways
Probabilistic model checking is a formal verification technique that has been successfully applied to the analysis of systems from a broad range of domains, including security and communication protocols, distributed algorithms and power management. In this paper we illustrate its applicability to a complex biological system: the FGF (Fibroblast Growth Factor) signalling pathway. We give a detailed description of how this case study can be modelled in the probabilistic model checker PRISM, discussing some of the issues that arise in doing so, and show how we can thus examine a rich selection of quantitative properties of this model. We present experimental results for the case study under several different scenarios and provide a detailed analysis, illustrating how this approach can be used to yield a better understanding of the dynamics of the pathway. Finally, we outline a number of exact and approximate techniques to enable the verification of larger and more complex pathways and apply several of them to the FGF case study.
A resistance-based approach to consensus algorithm performance analysis
We study the well known linear consensus algorithm by means of a LQ-type performance cost. We want to understand how the communication topology influences this algorithm. In order to do this, we recall the analogy between Markov Chains and electrical resistive networks. By exploiting this analogy, we are able to rewrite the performance cost as the average effective resistance on a suitable network. We use this result to show that if the communication graph fulfills some local properties, then its behavior can be approximated with that of a suitable grid, over which the behavior of the cost is known.
On the Use of the Source Reconstruction Method for Estimating Radiated EMI in Electronic Circuits
Electromagnetic interference (EMI) regulations are a very important issue in the design of almost any electronic circuit. Over the years, “cut-and-try” procedures have been adopted by electronic designers to make circuits comply with these regulations, mainly due to the lack of reliable theoretical models of radiated noise and clear design rules. To gain new insight into this field, a novel approach is presented in this paper based on a well-known technique in the field of antenna design, i.e., the source reconstruction method (SRM). Its application allows a set of equivalent currents to be obtained that behave exactly like the circuit under consideration with regard to radiated noise. From these currents, magnetic and electric radiated fields can be obtained at any point in space, even at 3 or 10 m away from the circuit where the regulations must be met. Moreover, the equivalent currents accurately represent noise sources in the circuit, thus permitting the elements responsible for generating radiated noise to be located. The aforementioned method would enable designers to reduce the use of anechoic-chamber facilities when testing their designs, thereby leaving the chamber only for final certification purposes.
MIMO Cooperative Diversity in a Transmit Power Limited Environment
This paper considers a fading relay channel where the total transmit power used is constrained to be equal to that of the standard single-hop channel. The relay channel used operates in what is termed as MIMO cooperative diversity mode, where the source transmits to both relay and destination terminals in the first instance. Both the source and relay then transmit to the destination in the second instance. Initially the cooperative diversity framework is introduced to consider system constraints so a direct and fair comparison with the single-hop case can be made. In-particular a power constraint is placed on the system and the optimal transmit power levels are derived and presented. The derived technique for finding the optimal transmit power levels is then used to demonstrate the advantages of using cooperative diversity in a wireless network. The results presented show that MIMO cooperative diversity offers a 3.4 dB increase in spectral efficiency at 5 % outage, with no additional cost incurred in transmit time, power or bandwidth.
An exact solution procedure for multi-item two-echelon spare parts inventory control problem with batch ordering in the central warehouse
We consider a multi-item two-echelon inventory system in which the central warehouse operates under a (Q,R) policy, and the local warehouses implement basestock policy. An exact solution procedure is proposed to find the inventory control policy parameters that minimize the system-wide inventory holding and fixed ordering cost subject to an aggregate mean response time constraint at each facility.
Utility-based admission control for mobile WiMAX networks
This paper presents a novel utility-based connection admission control (CAC) scheme for IEEE 802.16e broadband wireless access networks. We develop specific utility functions for real-time and non-real-time services coupled with a handover process. Given these utility functions we characterize the network utility with respect to the allocated bandwidth, and further propose a CAC algorithm which admits a connection that conducts to the greatest utility so as to maximize the total resource utilization. The simulation results demonstrate the effectiveness of the proposed CAC algorithm in terms of network utility.
Planetary-Scale Terrain Composition
Many interrelated planetary height map and surface image map data sets exist, and more data are collected each day. Broad communities of scientists require tools to compose these data interactively and explore them via real-time visualization. While related, these data sets are often unregistered with one another, having different projection, resolution, format, and type. We present a GPU-centric approach to the real-time composition and display of unregistered-but-related planetary-scale data. This approach employs a GPGPU process to tessellate spherical height fields. It uses a render-to-vertex-buffer technique to operate upon polygonal surface meshes in image space, allowing geometry processes to be expressed in terms of image processing. With height and surface map data processing unified in this fashion, a number of powerful composition operations may be uniformly applied to both. Examples include adaptation to nonuniform sampling due to projection, seamless blending of data of disparate resolution or transformation regardless of boundary, and the smooth interpolation of levels of detail in both geometry and imagery. Issues of scalability and precision are addressed, giving out-of-core access to giga-pixel data sources, and correct rendering at scales approaching one meter.
Continuous signature monitoring: low-cost concurrent detection of processor control errors
A low-cost approach to concurrent detection of processor control errors is presented that uses a simple hardware monitor and signatures embedded into the executing program. Existing signature-monitoring techniques detect a large portion of processor control errors at a fraction of the cost of duplication. Analytical methods developed in this study show that the new approach, continuous signature monitoring (CSM), makes major advances beyond existing techniques. CSM reduces the fraction of undetected control-flow errors by orders of magnitude, to less than 10/sup -6/, while the number of signatures reaches a theoretical minimum, being lowered by as much as three times to a range of 4-11%. Signature cost is reduced by placing CSM signatures at locations that minimize performance loss and (for some architectures) memory overhead. CSM exploits the program memory's SEC/DED code to decrease error-detection latency by as much as 1000 times, to 0.016 program memory cycles, without increasing memory overhead. This short latency allows transient faults to be tolerated. >
Traffic monitoring techniques for measurement based flow acceptance control
In this paper we describe the development of a measurement based flow acceptance control mechanism that will support guaranteed services in multiservice packet switched networks. Using simulation models we present simulation experiments with two monitoring techniques. One that monitors the percentile occupancy of queue and the other monitors the mean and variance of packet inter-arrival times and packet lengths of a continuous media packet stream. Both these techniques are considered in the development of the measurement based flow acceptance mechanism.
Using Fibonacci Compression Codes as Alternatives to Dense Codes
Recent publications advocate the use of various variable length codes for which each codeword consists of an integral number of bytes in compression applications using large alphabets. This paper shows that another tradeoff with similar properties can be obtained by Fibonacci codes. These are fixed codeword sets, using binary representations of integers based on Fibonacci numbers of order m ges 2. Fibonacci codes have been used before, and this paper extends previous work presenting several novel features. In particular, they compress better and are more robust, at the price of being slower.
Inter-finger coordination and postural synergies in robot hands via mechanical implementation of principal components analysis
Human hands employ characteristic patterns of actuation, or synergies, that contain much of the information required to describe an entire hand shape. In some cases, 80% or more of the total information can be described with only two scalar component values. Robotic hands, however, commonly only couple intra-finger joints, and rarely take advantage of this inter-finger coordination. In this paper, real-world data on a variety of human hand postures was collected using a data glove, and principal components analysis was used to calculate these synergies, resulting in what we call eigenpostures. A novel mechanism design is presented to combine the eigenpostures and drive a 17-degree-of-freedom 5-fingered robot hand. The hand uses only 2 DC motors to accurately recreate a wide range of hand shapes. We also present a design improvement that allows us to distinguish between high-precision and low-precision tasks, as well as greatly reduce overall error.
Developing Process Mediator for Web Service Interactions
Web service interactions lie in the core of SOA. Due to the autonomy, heterogeneity and continuous evolution of Web services, mediators are usually needed to support service interactions to overcome possible mismatches that may exist among business processes. In this paper, we introduce a space-based architecture for process mediator which considers both control-flow and data-flow, present possible mismatch patterns, and suggest how they can be automatically mediated. Our work can be used to perform runtime mediation and thus to facilitate service interactions.
Multiple-view object recognition in band-limited distributed camera networks
In this paper, we study the classical problem of object recognition in low-power, low-bandwidth distributed camera networks. The ability to perform robust object recognition is crucial for applications such as visual surveillance to track and identify objects of interest, and compensate visual nuisances such as occlusion and pose variation between multiple camera views. We propose an effective framework to perform distributed object recognition using a network of smart cameras and a computer as the base station. Due to the limited bandwidth between the cameras and the computer, the method utilizes the available computational power on the smart sensors to locally extract and compress SIFT-type image features to represent individual camera views. In particular, we show that between a network of cameras, high-dimensional SIFT histograms share a joint sparse pattern corresponding to a set of common features in 3-D. Such joint sparse patterns can be explicitly exploited to accurately encode the distributed signal via random projection, which is unsupervised and independent to the sensor modality. On the base station, we study multiple decoding schemes to simultaneously recover the multiple-view object features based on the distributed compressive sensing theory. The system has been implemented on the Berkeley CITRIC smart camera platform. The efficacy of the algorithm is validated through extensive simulation and experiments.
Change Detection for Bridges over Water in Airborne and Spaceborne SAR Data
The main advantages of SAR are the capability of imaging large areas in short time and delivering data at any day time and under nearly all weather conditions. This is especially important for disaster management and continuous long term monitoring applications. Key elements of man-made infrastructure are bridges. Especially for bridges over water, the SAR specific side looking imaging geometry can lead to special characteristics in the image. In this paper, the possibilities of extracting bridge features like width and height from SAR data, especially for bridges over water, are discussed. The feature extraction is based on the segmentation of parallel lines in an image. An approach is presented to exploit this feature extraction for change detection. The investigations are supported by SAR simulations, and real airborne and spaceborne data are presented.
Variational-based speckle noise removal of SAR imagery
In this paper we present a variational method for synthetic aperture radar (SAR) speckle removal. Variational method is a newly developed technique for the removal of SAR's multiplicative noise. For an image, we could define an energy functional. The energy evolves as the original image changes, and the minimum energy corresponds to the speckle reduced result. Partial differential equation (PDE) technique is used to get the minimal solution. Our energy functional makes use of the statistical information of the multiplicative noise since it follows a Gamma law with mean mu = 1 and variance sigma 2  = 1/M for M-look SAR. Our energy is a regularization term with two constraints. The regularization term is the integral for the norm of image gradient; two constraints are the mean of noise should be 1 and the variance of noise should be 1/M. We use the method of Lagrange multipliers, Euler-Lagrange equation and heat flow method to obtain the minimizer of the energy. ERS Precision Image (PRI) data are to demonstrate our algorithm. Numerical result shows that the speckle reduced image preserves edges and point targets while smoothes homogenous regions in the original image. The algorithm is computationally efficient and easy to implement.
Cluster-based distributed consensus
In this paper, we incorporate clustering techniques into distributed consensus algorithms for faster convergence and better energy efficiency. Together with a simple distributed clustering algorithm, we design cluster-based distributed consensus algorithms in forms of both fixed linear iteration and randomized gossip. The time complexity of the proposed algorithms is presented in terms of metrics of the original and induced graphs, through which the advantage of clustering is revealed. Our cluster-based algorithms are also shown to achieve an Omega(log n) gain in message complexity over the standard ones.
XIS-UML Profile for eXtreme Modeling Interactive Systems
The first version of the XIS profile addressed the development of interactive systems by defining models oriented only towards how the system should perform tasks. However, issues such as user-interface layouts, or the capture of interaction patterns, were not addressed by the profile, but only by the source-code generation process. This originated systems that, although functional, were considered by end-users as "difficult to use". In this paper we present the second version of the XIS UML profile, which is now a crucial component of the ProjectIT research project. This profile follows the "separation of concerns" principle by proposing an integrated set of views that address the various issues detected with the previous version of XIS. In addition, this profile also promotes the usage of extreme modeling, by relying on the extensive use of model-to-model transformation templates that are defined to accelerate the model development tasks
Forward acknowledgement: refining TCP congestion control
We have developed a Forward Acknowledgment (FACK) congestion control algorithm which addresses many of the performance problems recently observed in the Internet. The FACK algorithm is based on first principles of congestion control and is designed to be used with the proposed TCP SACK option. By decoupling congestion control from other algorithms such as data recovery, it attains more precise control over the data flow in the network. We introduce two additional algorithms to improve the behavior in specific situations. Through simulations we compare FACK to both Reno and Reno with SACK. Finally, we consider the potential performance and impact of FACK in the Internet.
In-service signal quality estimation for TDMA cellular systems
In-service interference plus noise power (I+N) and signal-to-interference plus noise power SI(I+N) estimation methods are examined for TDMA cellular systems. A simple (I+N) estimator is developed whose accuracy depends on the channel and symbol estimate error statistics. Improved (I+N) and S/(I+N) estimators are developed whose accuracy depends only on the symbol error statistics. The proposed estimators are evaluated through software simulation with an IS-54 frame structure. For high speed mobiles, it is demonstrated that S/(I+N) can be estimated to within 2 dB in less than a second.
Embedding agents within the intruder to detect parallel attacks
We carry forward the work described in our previous papers [5,18,20] on the application of data independence to the model checking of security protocols using CSP [19] and FDR [10]. In particular, we showed how techniques based on data independence [12,19] could be used to justify, by means of a finite FDR check, systems where agents can perform an unbounded number of protocol runs. Whilst this allows for a more complete analysis, there was one significant incompleteness in the results we obtained: while each individual identity could perform an unlimited number of protocol runs sequentially, the degree of parallelism remained bounded (and small to avoid state space explosion). In this paper, we report significant progress towards the solution of this problem, by means anticipated in [5], namely by “internalising” protocol roles within the “intruder” process. The internalisation of protocol roles (initially only server-type roles) was introduced in [20] as a state-space reduction technique (for which it is usually spectacularly successful). It was quickly noticed that this had the beneficial side-effect of making the internalised server arbitrarily parallel, at least in cases where it did not generate any new values of data independent type. We now consider the case where internal roles do introduce fresh values and address the issue of capturing their state of mind (for the purposes of analysis).
Can the Web Be Made Accessible for People with Intellectual Disabilities
This article presents the findings of a research project that aimed to contribute to the social inclusion of people with intellectual disabilities (ID) in the World Wide Web (the Web). The Inclusive New Media Design (INMD) project brought together thirty-one Web designers and developers with twenty-nine people with intellectual disabilities to explore the best practice for building Web sites accessible to the ID community. Specifically, the project took accessibility techniques identified in ID accessibility research, and investigated what would (or would not) make it possible for Web professionals to implement them. This article suggests some tentative answers to the question of whether a fully accessible Web can be built, one that includes people with ID. While the article outlines simple steps that can be taken to facilitate accessibility for people at the mild end of the ID spectrum, it also highlights a number of barriers that exist to implementing ID accessibility guidance, most notably the power holders and decision makers with whom Web designers work, who may not share the designers' commitment to accessibility.
A parallel implementation of a fast multipole based 3-D capacitance extraction program on distributed memory multicomputers
Very fast and accurate 3-D capacitance extraction is essential for interconnect optimization in ultra deep sub-micro designs (UDSM). Parallel processing provides an approach to reducing the simulation turn-around time. This paper examines the parallelization of the well known fast multipole based 3-D capacitance extraction program FASTCAP, which employs new preconditioning and adaptive techniques. To account for the complicated data dependencies in the unstructured problems, we propose a generalized cost function model, which can be used to accurately measure the workload associated with each cube in the hierarchy. We then present two adaptive partitioning schemes, combined with efficient communication mechanisms with bounded buffer size, to reduce the parallel processing overhead. The overall load balance is achieved through balancing the load at each level of the multipole computation. We report detailed performance results using a variety of standard benchmarks on 3-D capacitance extraction, on an IBM SP2.
Camera parameters auto-adjusting technique for robust robot vision
How to make vision system work robustly under dynamic light conditions is still a challenging research focus in computer/robot vision community. In this paper, a novel camera parameters auto-adjusting technique based on image entropy is proposed. Firstly image entropy is defined and its relationship with camera parameters is verified by experiments. Then how to optimize the camera parameters based on image entropy is proposed to make robot vision adaptive to the different light conditions. The algorithm is tested by using the omnidirectional vision in indoor RoboCup Middle Size League environment and the perspective camera in outdoor ordinary environment, and the results show that the method is effective and color constancy to some extent can be achieved.
New but not improved : a critical examination of revisions to the Regulation of Investigatory Powers Act 2000 encryption provisions
Considering the criminal uses of encryption, it has been asserted that national security and law enforcement endeavours must not be frustrated by potential evidence being hidden through digital encryption while the encryption key is withheld. The facilitation of state access to encryption keys through the Regulation of Investigatory Powers Act 2000 (‘RIPA 2000’) was intended to address precisely such a danger. However, since this statute’s enactment there have been significant shifts in law and policy relating to terrorism and child pornography as well as important technological developments. This article critically examines changes to the RIPA 2000 encryption provisions made by the Terrorism Act 2006 and the Policing and Crime Act 2009. It also considers emerging statistical data and cases including R v S(F) [2008]. It concludes that the underlying premises of the revised provisions are flawed, the provisions themselves ineffectual in practice and in the longer term potentially open to ‘mission creep’ to cover lesser offences.
The metaDESK: models and prototypes for tangible user interfaces
The metaDESK is a user interface platform demonstrating new interaction techniques we call "tangible user inter- faces." We explore the physical instantiation of interface elements from the graphical user interface paradigm, giving physical form to windows, icons, handles, menus, and controls. The design and implementation of the metaDESK display, sensor, and software architectures is discussed. A prototype application driving an interaction with geographi- cal space, Tangible Geospace, is presented to demonstrate these concepts.
A Motivation for "Ubuntu" to Enhance e-Learning Social Network Services in South Africa
This paper acknowledges the move from the content-centric to the network-centric approach to teaching and learning using Social Network Services (SNSs) in Higher Education. The South African cultural context is explained with a view to reconciling this with current SNSs. Finally, a way forward is proposed for developing SNSs particular to the Southern African cultural context.
Recent experiences utilizing TerraSAR-X for the monitoring of natural disasters in different parts of the world
PASCO in approximately 4 years, after the launch of TerraSAR-X (TSX), has successfully carried out a total 22 studies of disaster response for the worldwide and domestic cases. The outline and a part of the conducted cases are explained in detailed in this paper.
ÆTHER: an Authorization Management Architecture for Ubiquitous Computing ±
The ubiquitous computing paradigm suggests that we are going to be surrounded by countless wireless devices capable of providing services trans- parently. By definition, the nature of ubiquitous computing environments is open and extremely dynamic, making difficult the establishment of predefined security relationships between all of the participating entities. Authentication mechanisms can be employed to establish the identity of a pervasive computing entity but they suffer from scalability problems and have limited value in defin- ing authorization decisions among strangers. In this paper we propose AETHER, an authorization management architecture designed specifically to address trust establishment and access control in ubiquitous computing environments. Own- ers define attribute authority sets and access control policy entries that are em- bedded into their devices. Members of the attribute authority sets are trusted to issue credentials for the corresponding attributes that can then be used in order to gain access to protected resources. Our architecture supports dynamic mem- bership in these sets facilitating distributed administration, which is required in the context of the volatile nature of ubiquitous security relationships, and at- tribute mapping to allow roaming among authority domains. Moreover, we pre- sent the foundation of a logic model for our proposed architecture that is used to prove access control decisions.
Utilizing semantic caching in ubiquitous environment
Semantic caching is a dynamic caching strategy which deals with not only exact but also inexact similar queries. In this manner, each query will be carefully analyzed by the cache manager to identify the part that can be found in the cache from the part that needs to be retrieved from the server. This trimming process not only speeds up information retrieval but also saves on communication cost especially for mobile and wireless devices. Therefore, query trimming is a key problem in mobile and wireless environment, and devices in this environment have limited connection time, bandwidth, and battery power. However, the existing methods for query trimming have a number of limitations such as, inefficiency in time, space and the complexity of the algorithm used for trimming. These factors restrict the applicability of semantic caching for many applications. In this paper we investigate the shortcomings of query trimming process and propose a new solution to improve this process.
A study on using hierarchical basis error estimates in anisotropic mesh adaptation for the finite element method
A common approach for generating an anisotropic mesh is the M-uniform mesh approach where an adaptive mesh is generated as a uniform one in the metric specified by a given tensor M. A key component is the determination of an appropriate metric, which is often based on some type of Hessian recovery. Recently, the use of a global hierarchical basis error estimator was proposed for the development of an anisotropic metric tensor for the adaptive finite element solution. This study discusses the use of this method for a selection of different applications. Numerical results show that the method performs well and is comparable with existing metric tensors based on Hessian recovery. Also, it can provide even better adaptation to the solution if applied to problems with gradient jumps and steep boundary layers. For the Poisson problem in a domain with a corner singularity, the new method provides meshes that are fully comparable to the theoretically optimal meshes.
Believable judge bot that learns to select tactics and judge opponents
This paper describes our believable judge bot ICE-CIG2011 that has an ability to learn tactics from a judge player and an ability to judge an opponent character as a human or a bot. We conjecture that a bot with these two abilities should be considered human-like in a competition environment, such as BotPrize, where human players participate to compete not only for being the most human-like player but also the best judge. Main contributions of this work lie in our mechanisms for achieving these two abilities. To achieve the former ability, we develop a system and GUI that allow a selected judge player — whose role is to train ICE-CIG2011 — to control his or her character by only deciding which tactic to use under a given situation. We then obtain the judge's tactic log and use it for training tactic selection of ICE-CIG2011 with neuro evolution of augmenting topologies. To achieve the latter ability, we acquire additional logs when the judge character interacts with other opponent characters. In order to represent the play of a known (bot or human) character, we train a neural gas — a kind of self-organizing neural network — from its log. For an unknown character, once its neural gas is trained after a certain period of observation, ICE-CIG2011 decides if it is a human or bot by using the if-nearest-neighbor algorithm; this algorithm considers the majority in the labels of the if-nearest neural gases, of known characters, to the neural gas of that unknown character. Experimental results are given and discussed concerning these two abilities of ICE-CIG2011.
Type-based parametric analysis of program families
Previous research on static analysis for program families has focused on lifting analyses for single, plain programs to program families by employing idiosyncratic representations. The lifting effort typically involves a significant amount of work for proving the correctness of the lifted algorithm and demonstrating its scalability. In this paper, we propose a parameterized static analysis framework for program families that can automatically lift a class of type-based static analyses for plain programs to program families. The framework consists of a parametric logical specification and a parametric variational constraint solver. We prove that a lifted algorithm is correct provided that the underlying analysis algorithm is correct. An evaluation of our framework has revealed an error in a previous manually lifted analysis. Moreover, performance tests indicate that the overhead incurred by the general framework is bounded by a factor of 2.
Polar write once memory codes
A coding scheme for write once memory (WOM) using polar codes is presented. It is shown that the scheme achieves the capacity region of noiseless WOMs when an arbitrary number of multiple writes is permitted. The encoding and decoding complexities scale as O(N log N) where N is the blocklength. For N sufficiently large, the error probability decreases sub-exponentially in N. Some simulation results with finite length codes are presented.
Tolerance towards sensor failures: an application to a double inverted pendulum
In this paper we present a sensor fault tolerance control scheme that is applied to a double inverted pendulum. Sensor faults will affect the system when it is used in closed-loop feedback. The scheme uses a linear observer reconstruct the sensor fault and to subtract the reconstruction from the faulty sensor. The net result is then used for the closed-loop feedback. It was found that the scheme restored the performance to the fault-free scenario.
On the Expressiveness of Coordination Models
A number of different coordination models for specifying inter-process communication and synchronisation rely on a notion of shared dataspace. Many of these models are extensions of the Linda coordination model, which includes operations for adding, deleting and testing the presence/absence of data in a shared dataspace.#R##N##R##N#We compare the expressive power of three classes of coordination models based on shared dataspaces. The first class relies on Linda's communication primitives, while a second class relies on the more general notion of multi-set rewriting (e.g., like Bauhaus Linda or Gamma). Finally, we consider a third class of models featuring communication transactions that consist of sequences of Linda-like operations to be executed atomically (e.g., like in Shared Prolog or PoliS).
Manifold alignment for multitemporal hyperspectral image classification
While spectral and temporal advantages of multitemporal hyperspectral images provide opportunities for advancing classification of time varying phenomena, significant challenges are associated with high dimensionality and nonstationary signatures. While manifold learning retains critical geometry and develops a low dimension space where class clusters are recovered, spectral changes in temporal imagery impact the fidelity of the geometric representation of class dependent data. In this paper, we investigate a manifold alignment framework that exploits prior information while exploring similar local structures. The aim is to make use of common underlying geometries of two multitemporal images and embed the resemblances in a joint data manifold for classification tasks. Promising results support the advantages of the proposed manifold alignment approach.
Incremental rule learning based on example nearness from numerical data streams
Mining data streams is a challenging task that requires online systems based on incremental learning approaches. This paper describes a classification system based on decision rules that may store up-to-date border examples to avoid unnecessary revisions when virtual drifts are present in data. Consistent rules classify new test examples by covering and inconsistent rules classify them by distance as the nearest neighbor algorithm. In addition, the system provides an implicit forgetting heuristic so that positive and negative examples are removed from a rule when they are not near one another.
Elements of prescribed order, prescribed traces and systems of rational functions over finite fields
Let k ≥ 1 and f1 .... , fr ∈ Fqk (x) be a system of rational functions forming a strongly linearly independent set over a finite field Fq. Let γ1 ..... γr ∈ Fq be arbitrarily prescribed elements. We prove that for all sufficiently large extensions Fqkm, there is an element ξ ∈ Fqkm of prescribed order such that TrFqkm/Fq (fi (ξ))= γi for i = 1, ..., r, where TrFqkm/Fq is the relative trace map from Fqkm onto Fq. We give some applications to BCH codes, finite field arithmetic and ordered orthogonal arrays. We also solve a question of Helleseth et al. (Hypercubic 4 and 5-designs from Double-Error-Correcting codes, Des. Codes. Cryptgr. 28(2003). pp. 265-282) completely.
The experimental evaluation of knowledge acquisition techniques and methods: history, problems, and new directions
The special problems of experimentally evaluating knowledge acquisition and knowledge engineering tools, techniques and methods are outlined, and illustrated in detail with reference to two series of studies. The first is a series of experiments undertaken at Nottingham University under the aegis of the UK Alvey initiative and the ESPRIT project ACKnowledge. The second is the series of Sisyphus benchmark studies. A suggested programme of experimental evaluation is outlined which is informed by the problems with using Sisyphus for evaluation.
A domain-specific modeling language for scientific data composition and interoperability
Domain-Specific Modeling Languages (DSMLs) can offer assistance to domain experts, who may not be computer scientists, by providing notations and semantic constructs that align with abstractions from a particular domain. In this paper, we describe our design and application of a DSML in the area of data composition and interoperability. In particular, we introduce our recent effort to design a DSML to assist with interoperability issues across scientific software applications (e.g., composing scientific data in different file structures and integrating scientific data with data gathering devices). Currently, several different scientific data file specifications have been proposed (e.g., CID, netCDF, and HDF). Each file specification is optimized to manage a specific data type efficiently. Thus, each file specification has evolved with slightly different notions and implementation technologies. These differences led to the need for an environment that provides interoperability among the different specification formats. In this paper, we introduce our framework, supported by a DSML, that provides functionality to visually model the data composition and integration concepts independent from a particular data file specification.
Migrating Autonomic Self-Testing to the Cloud
The cloud computing model continues to gain much attention from software industry practitioners. As such, leading companies are investing in the development, packaging and delivery of cloud services over the Internet. However, although much work is being done to model and build cloud applications and services, there is significantly less research devoted to testing them. In this paper, we describe our research-in-progress towards migrating autonomic self-testing (AST) to the cloud. Our approach combines the development of an automated test harness for a cloud service, with the delivery of test support as-a-service (TSaaS). Both AST and TSaaS are supported by a virtual test environment, which utilizes the power of the cloud to enhance the self-testing process.
A CMOS fifth-order low-pass current-mode filter using a linear transconductor
In this paper, the design and analysis of a CMOS fifth-order low-pass GM-C filter are presented. It has a cutoff frequency of 4.3 MHz to accommodate the wideband CDMA standard. The transconductor used in this filter is based on a four-transistor cell operating in triode or saturation mode. It achieves high linearity range of /spl plusmn/ 1 V at /spl plusmn/ 1.5 V supply voltages. PSpice simulations show that total harmonic distortion at 1 Vpp and 1 MHZ is equal to 0.1% with 1.234 mW standby power dissipation. The proposed filter and the transconductor are simulated using 0.35 /spl mu/m technology.
Inverse problems theory and application: analysis of the two-temperature method for land-surface temperature and emissivity estimation
The two-temperature method (TTM) allows the separation of land-surface temperature and land-surface emissivity information from radiance measurements, and therefore, the solution can be uniquely determined by the data. However, the inverse problem is still an ill-posed problem, since the solution does not depend continuously on the data. Accordingly, we have used some mathematical tools, which are suited for analyses of ill-posed problems in order to show TTM properties, evaluate it, and optimize its estimations. Related to this last point, we have shown that it is necessary to constrain the problem, either by defining a region of physically admissible solutions and/or by using regularization methods, in order to obtain stable results. Besides, the results may be improved by using TTM with systems that possess a high temporal resolution, as well as by acquiring observations near the maximum and minimum of the diurnal temperature range.
Operating Rules Classification System of Water Supply Reservoir Based on LCS
Genetic algorithm-based learning classifier system (LCS) is a massively parallel, message-passing and rule-based machine learning system. But its potential self-adaptive learning capability has not been paid enough attention in reservoir operation research. In this paper, an operating rule classification system based on LCS , which learns through credit assignment (the bucket brigade algorithm) and rule discovery (the genetic algorithm), is established to extract water-supply reservoir operating rules. The proposed system acquires the online identification rate 95% for training samples and offline rate 85% for testing samples in a case study, and further discussions are made about the impacts on the performances or behaviors of the rule classification system from three aspects of obtained rules, training or testing samples and the comparisons between the rule classification system and the artificial neural network (ANN). The results indicate the learning classifier system is feasible and effective for the system to obtain the reservoir supply operating rules.
Assembling 2D blocks into 3D chips
Three-dimensional ICs promise to significantly extend the scale of system integration and facilitate new-generation electronics. However, progress in commercial 3D ICs has been slow. In addition to technology-related difficulties, industry experts cite the lack of a commercial 3D EDA tool-chain and design standards, high risk associated with a new technology, and high cost of transition from 2D to 3D ICs. To streamline the transition, we explore design styles that reuse existing 2D Intellectual Property (IP) blocks in 3D ICs. Currently, these design styles severely limit the placement of Through-Silicon Vias (TSVs) and constrain the reuse of existing 2D IP blocks in 3D ICs. To overcome this problem, we develop a methodology for using TSV islands and novel techniques for clustering nets to connect 2D IP blocks through TSV islands. Our empirical validation demonstrates 3D integration of traditional 2D circuit blocks without modifying their layout for this context.
Software engineering issues for small-scale parallelism
The availability of low-cost commodity multiprocessor machines change the nature of mainstream programming. This discipline is required to include small-scale, dual and quadruple processor machines, to remain competitive. These small-scale parallel systems require software engineering principles capable of encapsulating the complex parallel programming issues. This paper discusses a technique that provides a simple model for incorporating parallel programming in a scheduler. This model can dynamically adjust to single and small-scale multiple processor environments.
Pilotless Frame Synchronization for LDPC-Coded Transmission Systems
We present a pilotless frame synchronization approach that exploits feedback from a low-density parity-check (LDPC) code decoder. The synchronizer is based on syndrome checks using hard decisions from the channel observations. The bandwidth overhead associated with pilot symbols in conventional receiver architectures is eliminated while providing sufficient synchronization performance. An LDPC decoder coupled with our synchronizer exhibits negligible frame error rate degradation over a system with perfect synchronization. The complexity of the frame synchronizer is kept relatively low due to its XOR-based approach.
Third-octave analysis of multichannel amplitude compressed speech
Multiband amplitude compression has been studied as a compensation for the reduced auditory dynamic range often associated with sensorineural hearing loss, since it is capable of altering the dynamic range of speech as a function of frequency. Compression systems are generally characterized by their response to steady state tones and to simple dynamic stimuli, such as tone bursts. Level distributions of unprocessed and compressed materials are presented to demonstrate that these descriptions are inadequate to predict the processing of speech. A simple analysis of multiband compression which incorporates the interactions of static and dynamic properties is presented.
Depth image post-processing method by diffusion
Three-dimensional (3D) movies in theaters have become a massive commercial success during recent years, and it is likely that, with the advancement of display technologies and the production of 3D contents, TV broadcasting in 3D will play an important role in home entertainments in the not too distant future. 3D video contents contain at least two views from different perspectives for the left and the right eye of viewers. The amount of coded information is doubled if these views are encoded separately. Moreover, for multi-view displays (i.e. different perspectives of a scene in 3D are presented to the viewer at the same time through different angles), either video streams of all the required views must be transmitted to the receiver, or the displays must synthesize the missing views with a subset of the views. The latter approach has been widely proposed to reduce the amount of data being transmitted. The virtual views can be synthesized by the Depth Image Based Rendering (DIBR) approach from textures and associated depth images. However it is still the case that the amount of information for the textures plus the depths presents a significant challenge for the network transmission capacity. An efficient compression will, therefore, increase the availability of content access and provide a better video quality under the same network capacity constraints.In this thesis, the compression of depth images is addressed. These depth images can be assumed as being piece-wise smooth. Starting from the properties of depth images, a novel depth image model based on edges and sparse samples is presented, which may also be utilized for depth image post-processing. Based on this model, a depth image coding scheme that explicitly encodes the locations of depth edges is proposed, and the coding scheme has a scalable structure. Furthermore, a compression scheme for block-based 3D-HEVC is also devised, in which diffusion is used for intra prediction. In addition to the proposed schemes, the thesis illustrates several evaluation methodologies, especially, the subjective test of the stimulus-comparison method. It is suitable for evaluating the quality of two impaired images, as the objective metrics are inaccurate with respect to synthesized views.The MPEG test sequences were used for the evaluation. The results showed that virtual views synthesized from post-processed depth images by using the proposed model are better than those synthesized from original depth images. More importantly, the proposed coding schemes using such a model produced better synthesized views than the state of the art schemes. As a result, the outcome of the thesis can lead to a better quality of 3DTV experience.
Memory in the small: an application to provide task-based organizational memory for a scientific community
Many forms of organizational memory must exist embedded within the organizational processes and tasks. This paper argues that "memory-in-the small", memory utilized in the performance of an organizational task, can serve as an effective performance support mechanism. By basing organizational memory upon organizational tasks (and basing task support upon organizational memory), organizational memory systems can provide additional and necessary support services for organizations and communities. As an example of memory-in-the-small, this paper describes a software application called the ASSIST, that combines organizational memory with task performance for a scientific community. The ASSIST utilizes and stores the collective memory of astrophysicists about data analysis and is used world-wide by astrophysicists. The paper also considers the theoretical and architectural issues involved when combining organizational memory with task performance. >
CyloFold: secondary structure prediction including pseudoknots
Computational RNA secondary structure prediction approaches differ by the way RNA pseudoknot interactions are handled. For reasons of computational efficiency, most approaches only allow a limited class of pseudoknot interactions or are not considering them at all. Here we present a computational method for RNA secondary structure prediction that is not restricted in terms of pseudoknot complexity. The approach is based on simulating a folding process in a coarse-grained manner by choosing helices based on established energy rules. The steric feasibility of the chosen set of helices is checked during the folding process using a highly coarse-grained 3D model of the RNA structures. Using two data sets of 26 and 241 RNA sequences we find that this approach is competitive compared to the existing RNA secondary structure prediction programs pknotsRG, HotKnots and UnaFold. The key advantages of the new method are that there is no algorithmic restriction in terms of pseudoknot complexity and a test is made for steric feasibility. Availability: The program is available as web server at the site: http://cylofold.abcc.ncifcrf.gov.
Topological analysis of the power grid and mitigation strategies against cascading failures
This paper presents a complex systems overview of a power grid network. In recent years, concerns about the robustness of the power grid have grown because of several cascading outages in different parts of the world. In this paper, cascading effect has been simulated on three different networks, the IEEE 300 bus test system, the IEEE 118 bus test system, and the WSCC 179 bus equivalent model. Power Degradation has been discussed as a measure to estimate the damage to the network, in terms of load loss and node loss. A network generator has been developed to generate graphs with characteristics similar to the IEEE standard networks and the generated graphs are then compared with the standard networks to show the effect of topology in determining the robustness of a power grid. Three mitigation strategies, Homogeneous Load Reduction, Targeted Range-Based Load Reduction, and Use of Distributed Renewable Sources in combination with Islanding, have been suggested. The Homogeneous Load Reduction is the simplest to implement but the Targeted Range-Based Load Reduction is the most effective strategy.
Development of architecture and software technologies in high-performance low-power SoC design
For the success of an SoC design, a design platform and some key design technologies are needed. For this purpose, a research project in "Technology Development Program for Academia" was granted recently in Taiwan to develop the following technologies toward a high-performance low-power SoC design platform. First, a soft intellectual property (soft IP) and related RTOS, compiler, and integrated design environment (IDE) software of an Advanced Taiwan VLTW DSP Processor Core, which can be used as the Star IP of an SoC design platform, will be developed. Second, low-power and low-voltage digital and mixed-signal circuit design technologies will be developed based on the advanced MTCMOS process. Third, some key multimedia and communication soft or hard IPs will be developed. The developed advanced key technologies can be used as the technology driver to facilitate the design of SoC-based products, and they will also help to enhance the design capability of the Taiwan SoC industry. In this paper, we introduce the research project and focus on architecture and software technologies developing in one of the subitems.
Using information systems to structurally map workplace injury
Previous authors have identified mechanical, human and work organisation factors that may contribute to accident occurrences and have noted the value of site-specific data in accident analysis. However, major criticisms of site specific safety information systems have focused on the difficulties using traditional approaches for the identification of critical paths involving processes related to injury and non-injury incidents. This paper presents an idiographic approach to the study of accidents and through contextual analysis develops maps of work processes related to injury incidents in mining. The information used in the contextual maps includes data related to work area, activity and work role being performed, and equipment being used at the time of the injury incident. The computer algorithm consists of a series of contextual conditionals where the incidents and lost days recorded in the final category of each pathway meet all of the conditionals of the previous categories. The order of selection indicates the criticality of the path. The analysis resolves the processes related to the incident into their constitutive components, and then redescribes these processes as paths in order to reveal associations. These contextual paths illustrate the processes which are part of an established pattern of recurring regularities associated with injury incidents.
A PRAM-NUMA model of computation for addressing low-TLP workloads
It is possible to implement the parallel random access machine (PRAM) on a chip multiprocessor (CMP) efficiently with an emulated shared memory (ESM) architecture to gain easy parallel programmability crucial to wider penetration of CMPs to general purpose computing. This implementation relies on exploitation of the slack of parallel applications to hide the latency of the memory system instead of caches, sufficient bisection bandwidth to guarantee high throughput, and hashing to avoid hot spots in intercommunication. Unfortunately this solution can not handle workloads with low thread-level parallelism (TLP) efficiently because then there is not enough parallel slackness available for hiding the latency. In this paper we show that integrating nonuniform memory access (NUMA) support to the PRAM implementation architecture can solve this problem. The obtained PRAM-NUMA hybrid model is described and architectural implementation of it is outlined on our Eclipse ESM CMP framework.
DAVIM: Adaptable Middleware for Sensor Networks
Middleware services facilitate sensor-network application development. DAVIM is adaptable middleware that enables dynamic management of services and isolation between simultaneously running applications.
A survey on optimization metaheuristics
Metaheuristics are widely recognized as efficient approaches for many hard optimization problems. This paper provides a survey of some of the main metaheuristics. It outlines the components and concepts that are used in various metaheuristics in order to analyze their similarities and differences. The classification adopted in this paper differentiates between single solution based metaheuristics and population based metaheuristics. The literature survey is accompanied by the presentation of references for further details, including applications. Recent trends are also briefly discussed.
Mining formal concepts with a bounded number of exceptions from transactional data
We are designing new data mining techniques on boolean contexts to identify a priori interesting bi-sets (i.e., sets of objects or transactions associated to sets of attributes or items). A typical important case concerns formal concept mining (i.e., maximal rectangles of true values or associated closed sets by means of the so-called Galois connection). It has been applied with some success to, e.g., gene expression data analysis where objects denote biological situations and attributes denote gene expression properties. However in such real-life application domains, it turns out that the Galois association is a too strong one when considering intrinsically noisy data. It is clear that strong associations that would however accept a bounded number of exceptions would be extremely useful. We study the new pattern domain of α/β concepts, i.e., consistent maximal bi-sets with less than α false values per row and less than β false values per column. We provide a complete algorithm that computes all the α/β concepts based on the generation of concept unions pruned thanks to anti-monotonic constraints. An experimental validation on synthetic data is given. It illustrates that more relevant associations can be discovered in noisy data. We also discuss a practical application in molecular biology that illustrates an incomplete but quite useful extraction when all the concepts that are needed beforehand can not be discovered.
EpiToolKit—a web server for computational immunomics
Predicting the T-cell-mediated immune response is an important task in vaccine design and thus one of the key problems in computational immunomics. Various methods have been developed during the last decade and are available online. We present EpiToolKit, a web server that has been specifically designed to offer a problem-solving environment for computational immunomics. EpiToolKit offers a variety of different prediction methods for major histocompatibility complex class I and II ligands as well as minor histocompatibility antigens. These predictions are embedded in a user-friendly interface allowing refining, editing and constraining the searches conveniently. We illustrate the value of the approach with a set of novel tumor-associated peptides. EpiToolKit is available online at www. epitoolkit.org.
A Novel Breath Analysis System Based on Electronic Olfaction
Certain gases in the breath are known to be indicators of the presence of diseases and clinical conditions. These gases have been identified as biomarkers using equipments, such as gas chromatography and electronic nose (e-nose). GC is very accurate but is expensive, time consuming, and nonportable. E-nose has the advantages of low cost and easy operation, but is not particular for analyzing breath odor, and hence, has a limited application in diseases diagnosis. This paper proposes a novel system that is special for breath analysis. We selected chemical sensors that are sensitive to the biomarkers and compositions in human breath, developed the system, and introduced the odor signal preprocessing and classification method. To evaluate the system performance, we captured breath samples from healthy persons and patients known to be afflicted with diabetes, renal disease, and airway inflammation, respectively, and conducted experiments on medical treatment evaluation and disease identification. The results show that the system is not only able to distinguish between breath samples from subjects suffering from various diseases or conditions (diabetes, renal disease, and airway inflammation) and breath samples from healthy subjects, but in the case of renal failure is also helpful in evaluating the efficacy of hemodialysis (treatment for renal failure).
Improving business processes with business process modelling notation and business process execution language: an action research approach
The first purpose of this paper is to improve business processes in an industrial firm with business process modelling notation (BPMN) and business process execution language (BPEL). The second purpose is to provide a scientific approach for bringing business analysts and IT professionals together in a framework of an action research (AR). The research is conducted in one of the biggest distribution companies in Iran. BPMN is applied to model as-is and to-be situations of sale and distribution process. Both as-is and to-be models were simulated to compare their performance indexes. Moreover, models were implemented using BPEL, so that the model can be used for automating and improving the process. In this study, AR methodology was used to find a resolution of an organisational issue with those who experience this issue directly and to improve scientific knowledge and real-life contexts simultaneously.
Virtual marionettes: a system and paradigm for real-time 3D animation
This paper describes a computer graphics system that enables users to define virtual marionette puppets, operate them using relatively simple hardware input devices, and display the scene from a given viewpoint on the computer screen. This computerized marionette theater has the potential to become a computer game for children, an interaction tool over the Internet, enabling the creation of simultaneously viewed and operated marionette show by users on the World Wide Web, and, most importantly, a versatile and efficient professional animation system.
A Call Admission Control Algorithm Based on Utility Fairness for Low Earth Orbit Satellite Networks
A fair and adaptive call admission control algorithm for multimedia low Earth orbit (LEO) satellite networks was proposed. Based on current call dropping probability of destination cell, this algorithm reserves bandwidth for handoff calls using double threshold method. To avoid the discrimination of quality of service (QoS) caused by the allocation based on fair bandwidth, this algorithm adopts the bandwidth allocation rule based on fair QoS. Simulation results show that the proposed algorithm can accurately and adaptively reserve bandwidth, present satisfactory call blocking probability and greatly reduce handoff call dropping probability, while guarantees the high bandwidth utilization.
Co-Training for Domain Adaptation
Domain adaptation algorithms seek to generalize a model trained in a source domain to a new target domain. In many practical cases, the source and target distributions can differ substantially, and in some cases crucial target features may not have support in the source domain. In this paper we introduce an algorithm that bridges the gap between source and target domains by slowly adding to the training set both the target features and instances in which the current algorithm is the most confident. Our algorithm is a variant of co-training [7], and we name it CODA (Co-training for domain adaptation). Unlike the original co-training work, we do not assume a particular feature split. Instead, for each iteration of co-training, we formulate a single optimization problem which simultaneously learns a target predictor, a split of the feature space into views, and a subset of source and target features to include in the predictor. CODA significantly out-performs the state-of-the-art on the 12-domain benchmark data set of Blitzer et al. [4]. Indeed, over a wide range (65 of 84 comparisons) of target supervision CODA achieves the best performance.
Multi-agent Resource Allocation Algorithm Based on the XSufferage Heuristic for Distributed Systems
Distributed computing systems provide a highly dynamic behavior which originates from heterogeneous computing and storage resources, heterogeneous users and the variety of submitted applications and finally from the heterogeneous communication that takes part among the systems entities. As such applying global optima oriented allocation algorithms usually produces poor results and heuristics are used instead. We concentrated our experiments around the Sufferage heuristic and its adaptive cluster-aware version XSufferage. Both Sufferage and XSufferage use a centralized design and produce good results for low levels of dynamism and deterministic environments. In real life distributed environments, both heuristics produce poor results. We expose the Sufferage heuristic through a distributed architecture based on a cooperative set of entities, which form a Multi-Agent System, such that the results could be improved. We implemented a new algorithm, based on this architecture, called Distributed XSufferage. In order to test the new algorithm, a series of experiments were developed by simulating two real life Grid environments. A complex set of performance metrics were collected -- flow time, make span, throughput -- both resource and cluster level, utilization -- both resource and cluster level and resources and clusters mean loads. Algorithms produce their allocation solution based on estimates and modeling of system's resources and as such are sensitive to estimation errors. Throughout our experiments DX Sufferage was more robust to such errors compared to the original Sufferage and, respectively, XSufferage heuristics.
Hierarchical A* Parsing with Bridge Outside Scores
Hierarchical A* (HA*) uses of a hierarchy of coarse grammars to speed up parsing without sacrificing optimality. HA* prioritizes search in refined grammars using Viterbi outside costs computed in coarser grammars. We present Bridge Hierarchical A* (BHA*), a modified Hierarchial A* algorithm which computes a novel outside cost called a bridge outside cost. These bridge costs mix finer outside scores with coarser inside scores, and thus constitute tighter heuristics than entirely coarse scores. We show that BHA* substantially outperforms HA* when the hierarchy contains only very coarse grammars, while achieving comparable performance on more refined hierarchies.
Strings of congruent primes in short intervals
Fix �> 0, and let p1 =2 ,p 2 =3 ,... be the sequence of all primes. We prove that if (q, a )=1 , then there are infinitely many pairs pr ,p r+1 such that pr ≡ pr+1 ≡ a mod q and pr+1 − pr <� log pr. � � � �
Leveraging speculative architectures for runtime program validation
Program execution can be tampered with by malicious attackers through exploiting software vulnerabilities. Changing the program behavior by compromising control data and decision data has become the most serious threat in computer system security. Although several hardware approaches have been presented to validate program execution, they either incur great hardware overhead or introduce false alarms. We propose a new hardware-based approach by leveraging the existing speculative architectures for runtime program validation. The on-chip branch target buffer (BTB) is utilized as a cache of the legitimate control flow transfers stored in a secure memory region. In addition, the BTB is extended to store the correct program path information. At each indirect branch site, the BTB is used to validate the decision history of previous conditional branches and monitor the following execution path at runtime. Implementation of this approach is transparent to the upper operating system and programs. Thus, it is applicable to legacy code. Because of good code locality of the executable programs and effectiveness of branch prediction, the frequency of control-flow validations against the secure off-chip memory is low. Our experimental results show a negligible performance penalty and small storage overhead.
A proposal of cell selection algorithm for LTE handover optimization
A large number of cells will be deployed to provide high speed services in any places using the Long-Term Evolution (LTE) system. The management of such a large number of cells increases the operating expenditure (OPEX). Self-organizing networks (SON) attracted interest as an effective way to reduce OPEX. One of the main targets in SON is the self-optimization of handover (HO) that realizes mobility robustness. HO optimization algorithms adjust HO parameters between the serving and the reconnected cells based on the HO failure logs and cell selection, which is the procedure used to select a suitable reconnected cell, is very important for HO optimization algorithms. In this paper, we propose a cell selection scheme to enhance the performance of HO optimization. In the proposed scheme, both the uplink and downlink channel quality is considered when selecting a suitable reconnected cell. Through the computer simulation, we can see that the proposed scheme reduces the HO failure rate and the number of HO failures by 3 percentage points and 38%, respectively, compared to the conventional scheme based solely on downlink channel quality.
Clustering Large Optical Networks for Distributed and Dynamic Multicast
For a large-scale mesh network with dynamic traffic, maintaining the global state information in a centralized fashion is impractical. Hence, distributed schemes are needed to organize nodes and to manage state information in a more localized manner. One such effective scheme for organizing nodes is to cluster the nodes into a hierarchical structure. In this paper, we address the problem of determining the appropriate clustering of nodes for providing scalability in wavelength division multiplexed (WDM) optical networks with dynamic traffic. We present an on-line load-based (or bandwidth availability based) clustering technique that determines clusters adaptively in response to current network conditions. We also consider the problem of dynamic multicast on clustered networks with wavelength conversion capability. We introduce a heuristic using an auxiliary graph model to address routing, wavelength assignment, and traffic grooming jointly. Simulation results demonstrate the feasibility of our approach.
Detecting patterns of change using enhanced parallel coordinates visualization
Analyzing data to find trends, correlations, and stable patterns is an important problem for many industrial applications. We propose a new technique based on parallel coordinates visualization. Previous work on parallel coordinates method has shown that they are effective only when variables that are correlated and/or show similar patterns are displayed adjacently. Although current parallel coordinates tools allow the user to manually rearrange the order of variables, this process is very time-consuming when the number of variables is large. Automated assistance is needed. We propose an edit-distance based technique to rearrange variables so that interesting patterns can be easily detected. Our system, V-Miner, includes both automated methods for visualizing common patterns and a query tool that enables the user to describe specific target patterns to be mined/displayed by the system. Following an overview of the system, a case study is presented to explain how Motorola engineers have used V-Miner to identify significant patterns in their product test and design data.
Phase Diversity for an Antenna-Array System With a Short Interelement Separation
This paper presents a simple procedure of obtaining a diversity gain in an antenna-array system with a short interelement separation, typically less than the carrier wavelength. The new technique provides a diversity gain through a noncoherent combination of received signals at each antenna element. The diversity gain arises because, as the number of signal components of the received signal at each antenna element becomes large enough and as the arrival angle of each signal component is distinct from one another, which is a general signal circumstance in most practical code division multiple access (CDMA) signal environments, the amplitudes of the received signals become nearly independent due to the phase difference among the received signals. The diversity gain was referred to as ldquophase diversityrdquo in this paper. The proposed technique is first theoretically analyzed to estimate the performance in terms of pseudorandom-noise-code acquisition, which is verified through extensive computer simulations. Then, through the experimental results that are obtained from a CDMA array-antenna base station system, it has been shown that the performance of noncoherent detection is proportionally improved to the number of antenna elements.
Evaluating Usability and Efficaciousness of an E-learning System: A Quantitative, Model-Driven Approach
Using an e-learning system as a case example of complex information systems, we tested a conceptual framework that expands upon the Technology Acceptance Model (TAM) and incorporate measures of self-efficacy. A survey instrument was developed to gather large samples of users' perceptions on system usability and usefulness. Advanced statistical tests such as structural equation modeling were carried out. The paper concludes with a discussion on efficacy of evaluation techniques and design implications for e-learning systems.
Conception D'un Environnement Sous Matlab Pour le Marquage D'images en Jpeg2000
Le standard de compression JPEG2000 se caracterise par la diversite des options d'encodage menant a de bons compromis compression/qualite. Cependant, une extension integrant des elements de securite est fortement attendue. Le marquage numerique (watermarking, de l'expression anglaise) est une technologie prometteuse pour securiser la circulation des contenus multimedias. L'incorporation d'un marquage dans la chaine de compression presente l'avantage de prise en compte de la distorsion introduite par la marque dans le processus global de compression, en plus de la possibilite de reduire la complexite d'implementation en exploitant les traitements deja effectues pour la compression. Le travail realise consiste a la conception et au developpement d'un environnement sous MATLAB offrant differentes ouvertures sur l'architecture JPEG2000 et permettant d'incorporer une ou plusieurs methodes de marquage. L'environnement a ete developpe a partir de JJ2000, implementation du standard en langage JAVA. Comme premiere experimentation, un marquage robuste a ete incorpore au module de transformee en ondelettes du codeur et du decodeur JPEG2000.
Analyzing the Energy-Time Trade-Off in High-Performance Computing Applications
Although users of high-performance computing are most interested in raw performance both energy and power consumption has become critical concerns. One approach to lowering energy and power is to use high-performance cluster nodes that have several power-performance states so that the energy-time trade-off can be dynamically adjusted. This paper analyzes the energy-time trade-off of a wide range of applications-serial and parallel-on a power-scalable cluster. We use a cluster of frequency and voltage-scalable AMD-64 nodes, each equipped with a power meter. We study the effects of memory and communication bottlenecks via direct measurement of time and energy. We also investigate metrics that can, at runtime, predict when each type of bottleneck occurs. Our results show that, for programs that have a memory or communication bottleneck, a power-scalable cluster can save significant energy with only a small time penalty. Furthermore, we find that, for some programs, it is possible to both consume less energy and execute in less time by increasing the number of nodes while reducing the frequency-voltage setting of each node
Using Evolutionary Algorithms for Signal Integrity Checks of High-Speed Data Buses
Today's high performance computer systems must have fast, reliable access to memory and I/O devices. Unfortunately, inter-symbol interference, transmission line effects and other noise sources can distort data transfers. Engineers must therefore determine if bus designs have signal integrity  . e., the bus can transfer data with minimal amplitude or timing distortion. One method of determining signal quality on buses is to conduct a set of data transfers and measure various signal parameters at the receiver end. But the tests must be conducted with stressful test patterns that maximize inter-symbol interference to help identify any potential problems. In this paper we describe how an evolutionary algorithm was used to evolve such test patterns. All test results were obtained intrinsically
A Binary Communication Channel With Memory Based on a Finite Queue
A model for a binary additive noise communication channel with memory is introduced. The channel noise process, which is generated according to a ball sampling mechanism involving a queue of finite length M, is a stationary ergodic Mth-order Markov source. The channel properties are analyzed and several of its statistical and information-theoretical quantities (e.g., block transition distribution, autocorrelation function (ACF), capacity, and error exponent) are derived in either closed or easily computable form in terms of its four parameters. The capacity of the queue-based channel (QBC) is also analytically and numerically compared for a variety of channel conditions with the capacity of other binary models, such as the well-known Gilbert-Elliott channel (GEC), the Fritchman channel, and the finite-memory contagion channel. We also investigate the modeling of the traditional GEC using this QBC model. The QBC parameters are estimated by minimizing the Kullback-Leibler divergence rate between the probability of noise sequences generated by the GEC and the QBC, while maintaining identical bit-error rates (BER) and correlation coefficients. The accuracy of fitting the GEC via the QBC is evaluated in terms of ACF, channel capacity, and error exponent. Numerical results indicate that the QBC provides a good approximation of the GEC for various channel conditions; it thus offers an interesting alternative to the GEC while remaining mathematically tractable.
A new approach for a topographic feature-based characterization of digital elevation data
Triangular Irregular Network (TIN) and Regular Square Grid (RSG) are widely used for representing 2.5 dimensional spatial data. However, these models are not defined from the topographic properties of the terrain (i.e., ridge lines, valley lines, saddle points, etc.). This paper introduces a three-step feature-based approach for topographic properties extraction on scattered elevation data modeled by a TIN. Firstly, a segmentation process extracts homogeneous morphological areas bounded by critical lines and points. Secondly, these lines and points are displaced using a deformable process in order to derive the terrain feature points, lines and areas. Thirdly, a classification process labels any topographic feature. This three-step approach relies on the definition of an adapted model of representation (SPIN) and data structure (DCFL2). The proposed approach is validated on a real case study (Seolak mountain in South Korea). Consistent results with the morphology of terrain are displayed.
Ultrahigh Dimensional Feature Selection: Beyond The Linear Model
Variable selection in high-dimensional space characterizes many contemporary problems in scientific discovery and decision making. Many frequently-used techniques are based on independence screening; examples include correlation ranking (Fan & Lv, 2008) or feature selection using a two-sample t-test in high-dimensional classification (Tibshirani et al., 2003). Within the context of the linear model, Fan & Lv (2008) showed that this simple correlation ranking possesses a sure independence screening property under certain conditions and that its revision, called iteratively sure independent screening (ISIS), is needed when the features are marginally unrelated but jointly related to the response variable. In this paper, we extend ISIS, without explicit definition of residuals, to a general pseudo-likelihood framework, which includes generalized linear models as a special case. Even in the least-squares setting, the new method improves ISIS by allowing feature deletion in the iterative process. Our technique allows us to select important features in high-dimensional classification where the popularly used two-sample t-method fails. A new technique is introduced to reduce the false selection rate in the feature screening stage. Several simulated and two real data examples are presented to illustrate the methodology.
DNS Resource Record Analysis of URLs in E-Mail Messages for Improving Spam Filtering
In recent years, spam mails intending for ``One-click fraud" or ``Phishing" have become increasing. As one anti-spam technology, DNSBL based on the URLs or their corresponding IP addresses in the messages is well used. However, some spam mails that cannot be filtered by conventional DNSBLs get appearing since the spammers create websites using various techniques such as botnet, fast-flux and Wildcard DNS record. To improve the accuracy of filtering spam mails using these techniques, we analyzed DNS record features corresponding to the domain name from the URLs in actual spam mails. According to the result of this analysis, we confirmed that abuse of Wildcard DNS record is one effective criterion for spam filtering.
Recovery of Digital Information Using Bacterial Foraging Optimization Based Nonlinear Channel Equalizers
Transmission and storing of high density digital information plays an important role in the present age of information technology. These binary data are distorted while reading out of the recording medium or arriving at the receiver end due to inter symbol interference in the channel. The adaptive channel equalizer alleviates this distortion and reconstructs the transmitted data faithfully. The bacterial foraging optimization (BFO) is a recently developed efficient and derivative free evolutionary computing tool used for optimization purpose. In the present paper we propose a novel nonlinear channel equalizer using BFO algorithm. The recovery performance of the new equalizer is obtained through computer simulation study using nonlinear channels. It is shown that the proposed equalizer offers superior performance both in terms of bit-error-rate and convergence speed compared to the GA based equalizers. In addition it requires substantially less computation during training.
A fault-tolerant approach to secure information retrieval
Several private information retrieval (PIR) schemes were proposed to protect users' privacy when sensitive information stored in database servers is retrieved. However, existing PIR schemes assume that any attack to the servers does not change the information stored and any computational results. We present a novel fault-tolerant PIR scheme (called FT-PIR) that protects users' privacy and at the same time ensures service availability in the presence of malicious server faults. Our scheme neither relies on any unproven cryptographic assumptions nor the availability of tamper-proof hardware. A probabilistic verification function is introduced into the scheme to detect corrupted results. Unlike previous PIR research that attempted mainly to demonstrate the theoretical feasibility of PIR, we have actually implemented both a PIR scheme and our FT-PIR scheme in a distributed database environment. The experimental and analytical results show that only modest performance overhead is introduced by FT-PIR while comparing with PIR in the fault-free cases. The FT-PIR scheme tolerates a variety of server faults effectively. In certain fail-stop fault scenarios, FT-PIR performs even better than PIR. It was observed that 35.82% less processing time was actually needed for FT-PIR to tolerate one server fault.
Autonomous land vehicle navigation using millimeter wave radar
This paper discusses the use of a 77 GHz millimeter wave radar as a guidance sensor for autonomous land vehicle navigation. A test vehicle has been fitted with a radar and encoders that give steer angle and velocity. An extended Kalman filter optimally fuses the radar range and bearing measurements with vehicle control signals to give estimated position and variance as the vehicle moves around a test site. The effectiveness of this data fusion is compared with encoders alone and with a satellite positioning system. Consecutive scans have been combined to give a radar image of the surrounding environment. Data in this format are invaluable for future work on collision detection and map building navigation.
WiBro Net.-Based Five Senses Multimedia Technology Using Mobile Mash-Up
In this paper, we suggest and implement an enhanced Wireless Broadband (WiBro) Net.-based five senses multimedia technology using Web-map-oriented mobile Mash-up. The WiBro Net. in this applicative technology supports and allows Web 2.0-oriented various issues such as user-centric multimedia, individual multimedia message exchange between multi-users, and a new media-based information and knowledge sharing / participation without spatiotemporal-dependency. To inspect applicability and usability of the technology, we accomplish various experiments, which include 1) WiBro Net.-based real-time field tests and 2) ISO 9241/11 and /10-based surveys on the user satisfaction by relative experience in comparison with the AP-based commercialized mobile service. As a result, this application provides higher data rate transmission in UP-DOWN air-link and wider coverage region. Also, its average System Usability Scale (SUS) scores estimated at 83.58%, and it relatively held competitive advantage in the specific item scales such as system integration, individualization, and conformity with user expectations.
Generalizing Analytic Shrinkage for Arbitrary Covariance Structures
Analytic shrinkage is a statistical technique that offers a fast alternative to cross-validation for the regularization of covariance matrices and has appealing consistency properties. We show that the proof of consistency requires bounds on the growth rates of eigenvalues and their dispersion, which are often violated in data. We prove consistency under assumptions which do not restrict the covariance structure and therefore better match real world data. In addition, we propose an extension of analytic shrinkage -orthogonal complement shrinkage- which adapts to the covariance structure. Finally we demonstrate the superior performance of our novel approach on data from the domains of finance, spoken letter and optical character recognition, and neuroscience.
ChariTime — Concepts of Analysis and Design of an Agent-Oriented System for Appointment Management
Conceptual modeling is an important basis for developing general purpose systems for information and service management. In this paper, we present concepts for the analysis and design of a system of agents in which agents represent interests of individuals or groups. In particular, we develop a semantic model of agent domain knowledge by introducing analytical concepts for service scheduling which is the envisioned application of our prototypical system ChariTime.
Monitoring cross-channel correlation solar scan measurements using the Iowa X-band polarimetric radars
The sun is a convenient and frequently employed external radiation source for calibrating weather radar antenna and receiver characteristics. However, changes in solar activity can be a major source of error in interpreting the results of solar calibration for lower frequency bands. The cross-correlation of horizontal and vertical polarization signals, which is zero for perfectly unpolarized electromagnetic radiation, could give non-zero estimates if a quiet sun is not observed by the radar. In this paper, solar scan measurements are made at X-band to detect the effect of solar activity on this cross-correlation coefficient. To facilitate mitigation of instrument-wide errors, we employ multiple XPOL radars to simultaneously observe the sun. Though our experiments during a limited period show that the cross-channel correlation estimates obtained by X-band weather radars remain relatively unaffected by the variations in solar flux, the paper makes suggestions on improving the results.
A Context-based Ontological Structure for Knowledge Sharing and Customization
Several research projects have recently surfaced to automate the discovery and acquisition of knowledge from information and to facilitate information sharing and reusability in the global network. However, some of these research attempts are limited in scope with respect to proper identification and delivery of customized information. This paper addresses some of these challenges by proposing a high-level ontological and context-based architecture that enables information customization and knowledge organization. The proposed model aims at managing knowledge items through the use of stand-alone computational layers. Ontology is used in this research to describe knowledge representation and structure, whereas context reflects knowledge adaptability to its hosting environment.
Verification of security properties of payment protocol using AVISPA
Emerging e-commerce activity is giving scope for the design of many new protocols, and to gain confidence, these protocol need to be verified for its designed properties. Specifically protocol used in ecommerce transactions needs to be verified for their security properties. Verification of these protocols is done using the formal verification tools. AVISPA is one of the evolving tools used mainly for verifying security properties. A newly designed electronic payment protocol is verified for its correctness and security properties. This paper presents the use of AVISPA for verifying the security properties of the newly evolved electronic transaction protocol.
On Kalman Filtering for Detectable Systems With Intermittent Observations
We consider the problem of Kalman filtering when observations are available according to a Bernoulli process. It is known that there exists a critical probability  pc  such that, if measurements are available with probability greater than  pc , then the expected prediction covariance is bounded for all initial conditions; otherwise, it is unbounded for some initial conditions. We show that, when the system observation matrix restricted to the observable subspace is invertible, the known lower bound on  pc  is, in fact, the exact critical probability. This result is based on a novel decomposition of positive semidefinite matrices.
Probabilistic temporal databases, I: algebra
Dyreson and Snodgrass have drawn attention to the fact that, in many temporal database applications, there is often uncertainty about the start time of events, the end time of events, and the duration of events.  When the granularity of time is small (e.g., milliseconds), a statement such as “Packet  p  was shipped sometime during the first 5 days of January, 1998” leads to a massive amount of uncertainty (5×24×60×60×1000) possibilities.  As noted in Zaniolo et al. [1997], past attempts to deal with uncertainty in databases have been restricted to relatively small amounts of uncertainty in attributes.  Dyreson and Snodgrass have taken an important first step towards solving this problem.  In this article, we first introduce the   syntax of Temporal-Probabilistic (TP) relations and then show how they can be converted to an explicit, significantly more space-consuming form, called Annotated Relations.  We then present a  theoretical annotated temporal algebra  (TATA).  Being explicit, TATA is convenient for specifying how the algebraic operations should behave, but is impractical to use because annotated relations are overwhelmingly large.  Next, we present a  temporal probabilistic algebra  (TPA).  We show that our definition of the TP-algebra provides a correct implementation of TATA despite the fact that it operates on implicit, succinct TP-relations instead of overwhemingly large annotated relations. Finally, we report on timings for an implementation of the TP-Algebra built on   top of ODBC.
A Conceptual Design of an Adaptive and Collaborative E-Work Environment
Emerging work models increasingly take the form of loosely structured, often self-organising networks of nimble and virtual knowledge work teams within and between organisations. To model such work patterns requires a different approach from that of traditional workflow management systems. This paper presents the conceptual design of a prototype adaptive and collaborative e-Work environment - e-Workbench, which we are currently developing to enable future collaborative workspaces to adapt to emerging knowledge work models. We argue that with appropriate knowledge of tasks, workspaces will be able to adapt to work, and automatically retrieve contextually relevant knowledge elements from the Web in order to contribute creatively to problem solving and semantically manage shared information among collaborating workers. Our goal is to enable e-Workbench to become, not only a working environment but also, a collaborator and a co-worker as a result of its knowledge of work and creative participation in problem solving
Statistical distributions of DCT coefficients and their application to an interframe compression algorithm for 3-D medical images
Displacement estimated interframe (DEI) coding, a coding scheme for 3-D medical image data sets such as X-ray computed tomography (CT) or magnetic resonance (MR) images, is presented. To take advantage of the correlation between contiguous slices, a displacement-compensated difference image based on the previous image is encoded. The best fitting distribution functions for the discrete cosine transform (DCT) coefficients obtained from displacement compensated difference images are determined and used in allocating bits and optimizing quantizers for the coefficients. The DEI scheme is compared with 2-D block discrete cosine transform (DCT) as well as a full-frame DCT using the bit allocation technique of S. Lo and H.K. Huang (1985). For X-ray CT head images, the present bit allocation and quantizer design, using an appropriate distribution model, resulted in a 13-dB improvement in the SNR compared to the full-frame DCT using the bit allocation technique. For an image set with 5-mm slice thickness, the DEI method gave about 5% improvement in the compression ratio on average and less blockiness at the same distortion. The performance gain increases to about 10% when the slice thickness decreases to 3 mm. >
CodeRank: a new family of software metrics
The concept of pagerank has proved successful in allowing search engines to identify important pages in the World Wide Web. In this paper, we describe the application of the pagerank concept to the domain of software in order to derive a new family of metrics, CodeRank, which captures aspects of software not readily obtainable from other metrics. We have implemented a tool, CODERANKER, to compute values of CodeRank metrics using a full semantic model which we have developed. We present some results and discuss the use of CodeRank metrics in their interpretation.
New multiparty authentication services and key agreement protocols
Many modern computing environments involve dynamic peer groups. Distributed simulation, multiuser games, conferencing applications, and replicated servers are just a few examples. Given the openness of today's networks, communication among peers (group members) must be secure and, at the same time, efficient. This paper studies the problem of authenticated key agreement in dynamic peer groups with the emphasis on efficient and provably secure key authentication, key confirmation, and integrity. It begins by considering two-party authenticated key agreement and extends the results to group Diffie-Hellman (1976) key agreement. In the process, some new security properties (unique to groups) are encountered and discussed.
Fusing Asynchronous Feature Streams for On-line Writer Identification
In this paper, we present a new approach to improving the performance of a writer identification system by fusing asynchronous feature streams. Different feature streams are extracted from on-line handwritten text acquired from a whiteboard. The feature streams are used to train a text and language independent writer identification system based on Gaussian mixture models (GMMs). From a stroke consisting of n points, n point-based feature vectors and one stroke-based feature vector are extracted. The resulting feature streams thus have an unequal number of feature vectors. We evaluate different methods to directly fuse the feature streams and show that, by means of feature fusion, we can improve the performance of the writer identification system on a data set produced by 200 different writers.
Scalable BGP QoS Extension with Multiple Metrics
Being an important and yet a challenging problem, the QoS-based routing in the converging Internet has received significant attention from the research community. However, most of the QoS-based routing research is conducted in the context of intra-domain routing, leaving QoS-based inter-domain routing relatively open. In this paper, we specifically investigate how to extend the current inter-domain routing protocol (BGP) to support multiple QoS metrics. With multiple metrics, a BGP speaker has to advertise multiple routes for each destination to its peer. This will increase the routing message overhead and make QoS-aware BGP unscalable to large networks. Therefore, our focus in this paper is to bring QoS extensions to the original BGP while simultaneously providing high performance and scalability. In response to this, we propose path reduction algorithms, i.e., Contribution Based Reduction (CBR) algorithms, to reduce the number of routes advertised by BGP speakers while maximizing the routing success ratio. Extensive simulations show that our schemes achieve high performance with low complexity in terms of message overhead and computation, making the QoS extension to BGP scalable.
Optimized submerged batch fermentation strategy for systems scale studies of metabolic switching in Streptomyces coelicolor A3(2)
Background: Systems biology approaches to study metabolic switching in Streptomyces coelicolor A3(2) depend on cultivation conditions ensuring high reproducibility and distinct phases of culture growth and secondary metabolite production. In addition, biomass concentrations must be sufficiently high to allow for extensive time-series sampling before occurrence of a given nutrient depletion for transition triggering. The present study describes for the first time the development of a dedicated optimized submerged batch fermentation strategy as the basis for highly time-resolved systems biology studies of metabolic switching in S. coelicolor A3(2). Results: By a step-wise approach, cultivation conditions and two fully defined cultivation media were developed and evaluated using strain M145 of S. coelicolor A3(2), providing a high degree of cultivation reproducibility and enabling reliable studies of the effect of phosphate depletion and L-glutamate depletion on the metabolic transition to antibiotic production phase. Interestingly, both of the two carbon sources provided, D-glucose and L-glutamate, were found to be necessary in order to maintain high growth rates and prevent secondary metabolite production before nutrient depletion. Comparative analysis of batch cultivations with (i) both L-glutamate and D-glucose in excess, (ii) L-glutamate depletion and D-glucose in excess, (iii) L-glutamate as the sole source of carbon and (iv) D-glucose as the sole source of carbon, reveal a complex interplay of the two carbon sources in the bacterium's central carbon metabolism. Conclusions: The present study presents for the first time a dedicated cultivation strategy fulfilling the requirements for systems biology studies of metabolic switching in S. coelicolor A3(2). Key results from labelling and cultivation experiments on either or both of the two carbon sources provided indicate that in the presence of D-glucose, L-glutamate was the preferred carbon source, while D-glucose alone appeared incapable of maintaining culture growth, likely due to a metabolic bottleneck at the oxidation of pyruvate to acetyl-CoA.
Scalable, absolute position recovery for omni-directional image networks
We describe a linear-time algorithm that recovers absolute camera positions for networks of thousands of terrestrial images spanning hundreds of meters in outdoor urban scenes, under uncontrolled lighting. The algorithm requires no human input or interaction. For real data, it recovers camera pose globally consistent on average to roughly five centimeters, or about four pixels of epipolar alignment. The paper's principal contributions include an extension of Markov chain Monte Carlo estimation techniques to the case of unknown numbers of feature points, unknown occlusion and deocclusion, large scale (thousands of images, and hundreds of thousands of point features), and large dimensional extent (tens of meters of inter-camera baseline, and hundreds of meters of baseline overall). Also, a principled method is given to manage uncertainty on the sphere; a new use of the Hough transform is proposed; and a method for aggregating local baseline constraints into a globally consistent pose set is described.
Last mile problem in overlay design
Performance of overlay networks is dependent on last-mile connections, since they require that data traverse these last-mile bottlenecks at each forwarding step. This requires several times more upstream bandwidth than downstream, further exaggerating the asymmetry between down-stream and upstream bandwidth in last-mile technologies. This imbalance can cause packet queuing at the outgoing network interface of forwarding nodes, increasing latency and causing packet losses. We describe a model of a last-mile constrained overlay network and formulate and use it to solve a simplified latency- and bandwidth-bounded overlay construction problem. We observe that queueing delay may be a significant component of the end-to-end delay and approaches ignoring this may potentially result in an overlay network violating the delay and/or loss bounds. We observe that allowing a small amount of loss, it is possible to support a significantly large number of nodes. For a given end to end delay and loss bound we identify feasible degree (fan out) of each nodes. Our study sheds insights which provide engineering guidelines for designing overlays accounting for last mile problem in the Internet.
A learning adaptive Bollinger band system
This paper introduces a novel forecasting algorithm that is a blend of micro and macro modelling perspectives when using Artificial Intelligence (AI) techniques. The micro component concerns the fine-tuning of technical indicators with population based optimization algorithms. This entails learning a set of parameters that optimize some economically desirable fitness function as to create a dynamic signal processor which adapts to changing market environments. The macro component concerns combining the heterogeneous set of signals produced from a population of optimized technical indicators. The combined signal is derived from a Learning Classifier System (LCS) framework that combines population based optimization and reinforcement learning (RL). This research is motivated by two factors, that of non-stationarity and cyclical profitability (as implied by the adaptive market hypothesis [10]). These two properties are not necessarily in contradiction but they do highlight the need for adaptation and creation of new models, while synchronously being able to consult others which were previously effective. The results demonstrate that the proposed system is effective at combining the signals into a coherent profitable trading system but that the performance of the system is bounded by the quality of the solutions in the population.
Pitch Estimation using Models of Voiced Speech on Three Levels
We present an algorithm for estimating the fundamental frequency in speech signals. Our approach incorporates models of voiced speech on three levels. First, we estimate the pitch for each time frame based on its harmonic structure using non-negative matrix factorization. The second level utilizes temporal pitch continuity to extract partial pitch contours. Thirdly, we incorporate statistics of the succession of voiced segments to aggregate partial contours to the final contour of an utterance. We evaluate our approach on the Keele database. The experimental results show the robustness of our method for noisy speech, and the good performance for clean speech in comparison with state-of-the-art algorithms.
On the family of ML spectral estimates for mixed spectrum identification
A recently developed point spectrum identification procedure based on a family of AR and ML spectral estimates is exploited to arrive at a mixed spectrum identification procedure. To this end, a variety of properties of the AR and ML estimates as a function of model order are described. These properties relate to amplitude convergence, resolution and a characterization of the AR spectral artifact which is used to arrive at improved continuous spectral estimates. A variety of examples are presented. >
Using Linda for supercomputing on a local area network
A distributed parallel processing system based on the LINDA programming constructs has been implemented on a local area network of computers. This system allows a single application program to utilize many machines on the network simultaneously. Several applications have been implemented on the network at Sandia National Laboratories and have achieved performances considerably faster than that of a Cray-1S. Several collections of machines have been used including up to eleven DEC VAXes, three Sun/3 workstations, and a PC.
Fault modeling and pattern-sensitivity testing for a multilevel DRAM
Multilevel dynamic random-access memory (MLDRAM) attempts to increase the storage density of semiconductor memory without further reducing the lithographic dimensions. It does so by using more than two possible signal voltages on each cell capacitor thus permitting more than one bit to be stored in each cell. Birk's MLDRAM scheme has several promising properties, including robust locally-generated data signal and reference signal generation, and fast flash-conversion sensing. This paper describes a fault model for Birk's MLDRAM that was developed by considering the behaviors produced by likely defects at the schematic level. The resulting behaviors include faults that are detectable as observable logical errors, faults that can be detected by current measurements, and faults that, in the worst case, can only be detected by testing for degraded noise margins. All Boolean faults in the fault model can be detected by an efficient test whose length grows linearly in the number of cells. The narrower noise margins in MLDRAM will make it more vulnerable to pattern sensitivities. We also developed a linear test that evaluates worst-case sensing conditions.
Bit error rate analysis in IEEE 802.15.3a UWB channels
In this paper, we present a computable bit error rate (BER) expression for the binary signals in the IEEE 802.15.3a ultra-wideband (UWB) channel. In the literature, the impacts of the RAKE receiver's finger numbers and lognormal shadowing on the BER performance have not been reported yet. We propose a characteristic function (CF) based BER formula to overcome the convergence problem of the existing moment generating function (MGF) approach when the BER calculation takes account of the shadowing effect. Simulation results demonstrate that the proposed CF-based computable BER formula can accurately estimate the complete effects of the cluster and ray arrival processes, the lognormal fading as well as shadowing, and the finger numbers at RAKE receivers.
Application-Driven Voltage-Island Partitioning for Low-Power System-on-Chip Design
Among the different methods of reducing power for core-based system-on-chip (SoC) designs, the  voltage-island   technique  has gained in popularity. Assigning cores to the different supply voltages and floorplanning to create contiguous voltage islands are two important steps in the design process. We propose a new application-driven approach to voltage partitioning and island creation with the objective of reducing overall SoC power, area, and floorplanner runtime. Given an application power-state machine (PSM), we first identify the suitable range of supply voltages for each core. Then, we generate the discrete voltage assignment table using a heuristic technique. Next, we describe a methodology of reducing the large number of available choices from the voltage assignment table down to a useful set using the application PSM. We partition the cores into islands, using a cost function that gradually shifts from a power-based assignment to a connectivity-based one. Compared with previously reported techniques, a 9.4% reduction in power and 8.7% reduction in area are achieved using our approach, with an average runtime improvement of 2.4 times.
Query scheduling in multi query optimization
Complex queries are becoming commonplace, with the growing use of decision support systems. Decision support queries often have a lot of common sub-expressions within each query, and queries are often run as a batch. Multi query optimization aims at exploiting common sub-expressions, to reduce the evaluation cost of queries, by computing them once and then caching them for future use, both within individual queries and across queries in a batch. In case cache space is limited, the total size of sub-expressions that are worth caching may exceed available cache space. Prior work in multi query optimization involves choosing a set of common sub-expressions that fit in available cache space, and once computed, retaining their results across the execution of all queries in a batch. Such optimization algorithms do not consider the possibility of dynamically changing the cache contents. This may lead to sub-expressions occupying cache space even if they are not used by subsequent queries. The available cache space can be best utilized by evaluating the queries in an appropriate order and changing the cache contents as queries are executed. We present several algorithms that consider these factors, in order to reduce the cost of query evaluation.
DDoS detection and traceback with decision tree and grey relational analysis
In Distributed Denial-of-Service (DDoS) Attack, an attacker breaks into many innocent computers (called zombies). Then, the attacker sends a large number of packets from zombies to a server, to prevent the server from conducting normal business operations. We design a DDoS-detection system based on a decision-tree technique and, after detecting an attack, to trace back to the attacker's locations with a traffic-flow pattern-matching technique. Our system could detect DDoS attacks with the false positive ratio about 1.2-2.4%, false negative ratio about 2-10%, and find the attack paths in traceback with the false negative rate 8-12% and false positive rate 12-14%.
Comparative interactomics analysis of protein family interaction networks using PSIMAP (protein structural interactome map)
Motivation: Many genomes have been completely sequenced. However, detecting and analyzing their protein--protein interactions by experimental methods such as co-immunoprecipitation, tandem affinity purification and Y2H is not as fast as genome sequencing. Therefore, a computational prediction method based on the known protein structural interactions will be useful to analyze large-scale protein--protein interaction rules within and among complete genomes.#R##N##R##N#Results: We confirmed that all the predicted protein family interactomes (the full set of protein family interactions within a proteome) of 146 species are scale-free networks, and they share a small core network comprising 36 protein families related to indispensable cellular functions. We found two fundamental differences among prokaryotic and eukaryotic interactomes: (1) eukarya had significantly more hub families than archaea and bacteria and (2) certain special hub families determined the topology of the eukaryotic interactomes. Our comparative analysis suggests that a very small number of expansive protein families led to the evolution of interactomes and seemed tohave played a key role in species diversification.#R##N##R##N#Contact: jong@kribb.re.kr#R##N##R##N#Supplementary information: http://interactomics.org
Exploring board game design using digital technologies
This talk presents results of a case study from a course called "History of Games" offered at the School of Art + Design at New Jersey Institute of Technology. After analyzing various traditional board games and their mechanics, students explore the possibility of producing their own original board games by altering various existing game structures through application of new technologies such as digital prototyping, including laser cutting and 3-D printing, and microcontroller technologies. In principle, we can fully emulate the playing of a board game such as Monopoly inside a computer, digitally. However, there is a certain quality of physicality with traditional board games that cannot be experienced through games in fully digital environments. The existence of tangible game pieces, boards, and real human players can produce cooperation, engagement, and tensions unlike those in video games and AR-based applications. Through this project-based course, students further explore how new technologies can help in developing new types of games.
Using Call-Context to Prevent the Emergence of Chaotic Workflow Behaviors in Overload Situations
By defining workflows, existing services are put into novel contexts and exposed to different workloads, which in turn can result in unexpected behaviors. This paper examines the chaotic behavior of sequential workflows in overload situations and discusses the use of call-contexts as a means of avoiding them.
An analytical and experimental investigation of flexible manipulator performance
A critical modeling of a flexible manipulator will involve a thorough understanding of the issues specific to flexible manipulator performance and an experimental investigation of a physical system. A comprehensive dynamic model of a flexible manipulator has been presented in this paper together with a preliminary experimental investigation that examines the performance of this comprehensive model. The analytical model recognizes in particular, the coupling characteristics of the deformations in a flexible manipulator configuration and realizes an efficient and dedicated finite element scheme to model general spatial manipulator configurations with revolute and prismatic pairs. The experimental results indicate a favorable level of performance for the application of the special finite element for a practical manipulator.
BIDDLE: a bidirectional data driven Lisp engine
The authors propose BIDDLE, a direct execution architecture for Lisp based on both data- and demand-driven principles. A priority queuing mechanism is used to control the parallelism and the workload of the processing elements. Also introduced is a novel mechanism whereby the sequential semantics of Lisp can be enforced in such a way as not to reduce the parallelism too drastically. BIDDLE is, therefore, aimed at achieving a balance between eager and lazy evaluation, sequential semantics, and parallelism. At present, BIDDLE exists only on paper and is a long way from hard ware implementation. >
Ant algorithms for discrete optimization
This article presents an overview of recent work on ant algorithms, that is, algorithms for discrete optimization that took inspiration from the observation of ant colonies' foraging behavior, and introduces the ant colony optimization (ACO) metaheuristic. In the first part of the article the basic biological findings on real ants are reviewed and their artificial counterparts as well as the ACO metaheuristic are defined. In the second part of the article a number of applications of ACO algorithms to combinatorial optimization and routing in communications networks are described. We conclude with a discussion of related work and of some of the most important aspects of the ACO metaheuristic.
Vehicular networking for intelligent and autonomous traffic management
Traffic congestion has become a daily problem that most people suffer. This not only impacts the productivity of the population but also poses a safety risk. Most of the technologies for intelligent highways focus on safety measures and increased driver awareness, and expect a centralized management for the traffic flow. This paper presents a new approach for enabling autonomous and adaptive traffic management through vehicular networks. By allowing data exchange between vehicles about route choices, congestions and traffic alerts, a vehicle makes a decision on the best course of action. Unlike centralized schemes that provide recommendations, our VANET-based Autonomous Management (VAM) approach factors in the destination and routes of nearby vehicles in deciding on whether rerouting is advisable. In addition, VAM leverages the presence of smart traffic lights and enables coordination between vehicles and lightcontrollers in order to ease congestion. The collective effect of all vehicles will be an autonomous reshape of the traffic pattern based on their destinations and road conditions. The simulation results demonstrate the advantage of VAM.
Peer-to-peer support for low-latency Massively Multiplayer Online Games in the cloud
Cloud gaming has recently been proposed as an alternative to traditional video game distribution. With this approach, the entire game is stored, run and rendered on a remote server. Player input is forwarded to the server via the Internet and the game's output is returned as a video stream. This adds network delay, which can negatively impact the gameplay. The delay is acceptable as long as the user is located geographically close to the cloud servers. However, for Massively Multiplayer Online Games (MMOGs), this delay is added on top of the existing delay between MMOG client and server. As MMOGs are highly delay-sensitive, this can significantly degrade their playability. To deal with this issue, we propose to use peer-to-peer techniques to distribute the MMOG server functionality and place it at the cloud server centers. This allows us to reduce the additional delay introduced by running the MMOG clients in the cloud.
Estimating crowd densities and pedestrian flows using wi-fi and bluetooth
The rapid deployment of smartphones as all-purpose mobile computing systems has led to a wide adoption of wireless communication systems such as Wi-Fi and Bluetooth in mobile scenarios. Both communication systems leak information to the surroundings during operation. This information has been used for tracking and crowd density estimations in literature. However, an estimation of pedestrian flows has not yet been evaluated with respect to a known ground truth and, thus, a reliable adoption in real world scenarios is rather difficult. With this paper, we fill in this gap. Using ground truth provided by the security check process at a major German airport, we discuss the quality and feasibility of pedestrian flow estimations for both Wi-Fi and Bluetooth captures. We present and evaluate three approaches in order to improve the accuracy in comparison to a naive count of captured MAC addresses. Such counts only showed an impractical Pearson correlation of 0.53 for Bluetooth and 0.61 for Wi-Fi compared to ground truth. The presented extended approaches yield a superior correlation of 0.75 in best case. This indicates a strong correlation and an improvement of accuracy. Given these results, the presented approaches allow for a practical estimation of pedestrian flows.
Alleviating the Constant Stochastic Variance Assumption in Decision Research: Theory, Measurement, and Experimental Test
Analysts often rely on methods that presume constant stochastic variance, even though its degree can differ markedly across experimental and field settings. This reliance can lead to misestimation of effect sizes or unjustified theoretical or behavioral inferences. Classic utility-based discrete-choice theory makes sharp, testable predictions about how observed choice patterns should change when stochastic variance differs across items, brands, or conditions. We derive and examine the implications of assuming constant stochastic variance for choices made under different conditions or at different times, in particular, whether substantive effects can arise purely as artifacts. These implications are tested via an experiment designed to isolate the effects of stochastic variation in choice behavior. Results strongly suggest that the stochastic component should be carefully modeled to differ across both available brands and temporal conditions, and that its variance may be relatively greater for choices made for the future. The experimental design controls for several alternative mechanisms (e.g., flexibility seeking), and a series of related models suggest that several econometrically detectable explanations like correlated error, state dependence, and variety seeking add no explanatory power. A series of simulations argues for appropriate flexibility in discrete-choice specification when attempting to detect temporal stochastic inflation effects.
The Design of a Low-Power Asynchronous DES Coprocessor for Sensor Network Encryption
Sensor network nodes have a very tight power budget and the power efficiency is the biggest design concern in sensor network circuits. A general-purpose processor (e.g. an ARM processor) is not efficient to execute encryption algorithms because it has no special instructions to support encryption operations, for example very often-used permutation operations. In the paper, we propose a low-power ASIC encryption coprocessor for sensor network nodes. A DES algorithm is used because the algorithm does not include power-hungry and complex mathematic operations, such as multiplication, division and addition. An asynchronous logic style is used to design the coprocessor. With an asynchronous controller, a global clock is not necessary when idle, resulting in zero standby dynamic power. Using the DES coprocessor, the power consumed by encryption can be saved by 4 orders of magnitude than a pure software calculation.
The effect of a stand-alone ethics course in Chilean engineering students' attitudes
Engineering ethics education is taking on increasing importance worldwide, but in Chile the percentage of universities that have a mandatory course concerning ethics is still small. Traditionally, Chilean universities with existing ethics courses teach them using a philosophical or theological perspective, limited to occidental theories, and usually from a Christian point of view. This article studies the impact of a new methodology and technique to teach ethics in Chile: case-based, non-normative, and with a critical-descriptive approach. An empirical study is conducted to assess the relative impact of an ethics class on students individual and inherent moral values and attitudes, and understand the factors that contribute to this impact. Results indicate that even though the importance of religion in Chile is decreasing, it is still a major source of students� ethical principles and moral values. In addition, results suggest that a change in moral values develops when discussions among groups with different points of view occur.
Functional validation of fault-tolerant asynchronous algorithms
The paper presents an alternative approach to the formal specification and validation of distributed asynchronous algorithms. It begins with a syntactically correct description of the algorithm whose correctness is then to be validated. The validation of the algorithm is based on the process-oriented discrete simulation and permits a partial correctness validation of the algorithm implemented by a program. The suggested method enables to model independent activity of several processors (using pseudo-parallel processes) in simulation time and to model communication channels with defined time behavior and failure semantics. Using the approach it is easy to add other processes like model of system's environment, fault injector and state observer. The method is described with the aid of a simple C-based validation tool called C-Sim. The utilization of C-Sim requires only slight changes in C-coded implementation of the verified algorithm. An example of validation of distributed election algorithm with the presence of faults is presented.
An event-driven SIR model for topic diffusion in web forums
Social media is being increasingly used as a communication channel. Among social media, web forums, where people in online communities disseminate and receive information by interaction, provide a good environment to examine information diffusion. In this research, we aim to understand the mechanisms and properties of the information diffusion in the web forum. For that, we model topic-level information diffusion in web forums using the baseline epidemic model, the SIR(Susceptible, Infective, and Recovered) model, frequently used in previous research to analyze disease outbreaks and knowledge diffusion. In addition, we propose an event-driven SIR model that reflects the event effect on information diffusion in the web forum. The proposed model incorporates the effect of news postings on the web forum. We evaluate two models using a large longitudinal dataset from the web forum of a major company. The event-SIR model outperforms the SIR model in fitting on major spikey topics that have peaks of author participation.
An intersection algorithm based on Delaunay triangulation
A robust method for finding points of intersection of line segments in a 2-D plane is presented. The plane is subdivided by Delaunay triangulation to localize areas where points of intersection exist and to guarantee the topological consistency of the resulting arrangement. The subdivision is refined by inserting midpoints recursively until the areas containing points of intersection are sufficiently localized. The method is robust in the sense that it does not miss points of intersection that are easily detectable when costly line-pair checking is performed. The algorithm is adaptive in the sense that most of the computational cost is incurred for the areas where finding points of intersection is difficult. >
Earth System Science Workbench: a data management infrastructure for earth science products
The Earth System Science Workbench (ESSW) is a non-intrusive data management infrastructure for researchers who are also data publishers. An implementation of ESSW to track the processing of locally received satellite imagery is presented, demonstrating the Workbench's transparent and robust support for archiving and publishing data products. ESSW features a Lab Notebook metadata service, an ND-WORM (No Duplicate-Write Once Read Many) storage service, and Web user interface tools. The Lab Notebook logs processes (experiments) and their relationships via a custom API to XML documents stored in a relational database. The ND-WORM provides a managed storage archive for the Lab Notebook by keeping unique file digests and name-space meta-data, also in a relational database. ESSW Notebook tools allow project searching and ordering, and file and meta-data management.
Highly-available services using the primary-backup approach
The authors derive lower bounds and the corresponding optimal protocols for three parameters for synchronous primary-backup systems. They compare their results with similar results for active replication in order to determine whether the common folklore on the virtues of the two approaches can be shown formally. They also extend some of their results to asynchronous primary-backup systems. They implement an important subclass of primary-backup protocols that they call 0-blocking. These protocols are interesting because they introduce no additional protocol related delay into a failure-free service request. Through implementing these protocols the authors hope to determine the appropriateness of their theoretical system model and uncover other practical advantages or limitations of the primary-backup approach. >
Efficient composition of media object for multimedia scene rendering
In an audiovisual scene consisting of high capacity multimedia objects, when a scene change, which an object is inserted, deleted, or replaced in real time by user interaction, is required, the object priority order compositor for an multimedia player according to the present invention performs rendering of objects based on priority order of objects. The compositor searches priority order of objects, and makes it possible to arbitrarily change order of objects and further to perform rendering of only objects requiring reconstruction. Thus, the object priority order compositor provides re-usability and availability of multimedia data. Further, the object priority order compositor renders efficient multimedia data processing possible by providing a user with a dynamic scene.
Percolation based synthesis
A new approach called Percolation Based Synthesis for the scheduling phase of High Level Synthesis (HLS) is presented. We discuss some new techniques (which are implemented in our tools) for compaction of flow graphs beyond basic blocks limits, which can produce order of magnitude speed ups versus serial execution. Our algorithm applies to programs with  conditional jumps, loops  and  multicycle pipelined operations . In order to schedule under resource constraints we start by first finding the  optimal schedule  (without constraints) and then add heuristics to map the optimal schedule onto the given system. We argue that starting from an optimal schedule is one of the most important factors in scheduling because it offers the user flexibility to tune the heuristics and gives him a good bound for the resource constrained schedule. This scheduling algorithm is integrated with synthesis tool which uses VHDL as input description and produces a  structural netlist  of generic register-transfer components and a  unit based control table  as output. We show that our algorithm obtains better results than previously published algorithms.
Using SDL for implementing a wireless medium access control protocol
Specification and Description Language (SDL) is a high abstraction level system design language with a clear graphical notation. Because of formal presentation, an SDL model can be automatically converted into source C code for implementation. However, the high abstraction level creates a conceptual gap between a general SDL model and its implementation in a final operational platform. The paper studies the SDL development of an embedded Medium Access Control (MAC) protocol for a wireless local area network (WLAN) demonstrator. The SDL design flow for the protocol is first started by architectural design without target platform dependencies. Functionality is added to the model using the top-down design approach. Functional simulations are used for verifying the operation of the protocol. Next, the performance is estimated using performance simulations in a workstation environment. Performance improvements can be achieved by optimising the SDL model.
AGUIA: Agents Guidance for Intelligence Amplification in Goal Oriented Tasks
All I know is that I know nothing (Socratic Ignorance). The world is an increasingly complex with problems that require swift resolution. Although knowledge is widely available, be it stored in companies’ databases or spread over the Internet, humans have intrinsic limitations for handling very large volumes of information or keeping track of frequent updates in a constantly changing world. Moreover, human reasoning is highly intuitive and potentially biased. Decision-making is often based on rules of thumb instead of systematic analysis with full understanding of decisions’ context. Computer systems that manage knowledge to thoroughly explore the context and the range of alternatives may improve human decision-making by making people aware of possible misconceptions and biases. Computer systems are also limited in their potential usage due to the frame problem. Systems are not aware of their ignorance, thus they cannot surplus human intelligence, but they may be useful complement to human’s intelligence. The objective of this paper is to present the AGUIA model for amplifying human intelligence, based on agent’s technology, for task-oriented contexts. AGUIA uses domain ontology and task scripts for handling formal and semiformal knowledge bases, thereby helping to systematically (1) explore the range of alternatives, (2) interpret the problem and the context, and (3) maintain “awareness” of the problem. As for humans, knowledge is a fundamental resource for AGUIA performance. AGUIA’s knowledge base keeps updating its content, in the background, during interaction with humans, either through identified individuals or through anonymous mass contribution. The feasibility and benefits of AGUIA were demonstrated in many different fields, such as engineering design, fault diagnosis, accident investigation and online interaction with the government. The experiments considered a set of criteria including: product cost, number of explored alternatives, users’ problem understanding and users’ awareness of problem context changes. Results indicate AGUIA can actually improve human problem solving capacity in many different areas.
Globally asymptotically stable filters for source localization and navigation aided by direction measurements
This paper presents a set of filters with globally asymptotically stable error dynamics for source localization and navigation, in 3-D, based on direction measurements from the agent (or vehicle) to the source, in addition to relative velocity readings of the agent. Both the source and the agent are allowed to have constant unknown drift velocities and the relative drift velocity is also explicitly estimated. The observability of the system is studied and realistic simulation results are presented, in the presence of measurement noise, that illustrate the performance of the achieved solutions. Comparison results with the Extended Kalman Filter are also provided and similar performances are achieved.
Learning and Leveraging the Relationship between Architecture-Level Measurements and Individual User Satisfaction
The ultimate goal of computer design is to satisfy the end-user. In particular computing domains, such as interactive applications, there exists a variation in user expectations and user satisfaction relative to the performance of existing computer systems. In this work, we leverage this variation to develop more efficient architectures that are customized to end-users. We first investigate the relationship between microarchitectural parameters and user satisfaction. Specifically, we analyze the relationship between hardware performance counter (HPC) readings and individual satisfaction levels reported by users for representative applications. Our results show that the satisfaction of the user is strongly correlated to the performance of the underlying hardware. More importantly, the results show that user satisfaction is highly user-dependent. To take advantage of these observations, we develop a framework called Individualized Dynamic Voltage and Frequency Scaling (iDVFS). We study a group of users to characterize the relationship between the HPCs and individual user satisfaction levels. Based on this analysis, we use artificial neural networks to model the function from HPCs to user satisfaction for individual users. This model is then used online to predict user satisfaction and set the frequency level accordingly. A second set of user studies demonstrates that iDVFS reduces the CPU power consumption by over 25% in representative applications as compared to the Windows XP DVFS algorithm.
Performance of Different Mobile Payment Service Concepts Compared with a NFC-Based Solution
The paper compares the performance of different traditional mobile payment service concepts with a state of the art NFC-based mobile payment solution. The goal is to evaluate the different mobile payment concepts, not their software implementation, from a performance and end-to-end service duration time point of view. Overall, there have been five different mobile payment services developed, implemented and benchmarked for the concept comparison, namely: Interactive Voice Response, Short Message Service, Wireless Application Protocol, One Time Password Generator as well as a solution based on Near Field Communication.
Electrical Power Monitoring System Using Thermochron Sensor and 1-Wire Communication Protocol
Maintaining quality and reliability of operation of remotely located electrical machines as well as power supply equipment by monitoring their load is an important task in modern practical electrical engineering. This paper presents a low-cost efficient, robust and relatively simple load monitoring electronic system based on the use of special encapsulated temperature sensors-data loggers (so-called Thermochron iButtons) combined with application of 1-wire data communications implemented on the power supply line, dedicated data communication line with off-line data transfer.
Undergraduate students' gender differences in IT skills and attitudes
The worldwide concern about the gender gap in information technology and the lack of woman participation in computer science has been attributed to the different cultural influences to which boys and girls are subject. In The University of Hong Kong, girls achieved greater improvements in their computer skills than their male counterparts after completing one year of studies. Recognising their own progress has, in turn, boosted their confidence in using IT. The young women's estimates of their skill levels have doubled over the years from 1998 to 2000. Despite this recorded acceleration at the end of the academic years, girls were less confident of their abilities and possessed lower IT skill levels than boys before starting their university education, as found in surveys of freshmen's computer skills. This study compares the responses of student participants of the HKU/IBM Notebook Computer Programme, which started in 1998, in the self-reported IT skills and attitudes of male and female students, in surveys conducted both at the beginning and again at the end of the freshman year. It also examines the achievement scores of the IT Proficiency Tests and the 'Foundations to Information Technology' courses administered for the student IT requirement for graduation.
Construction and completion of flux balance models from pathway databases
Motivation: Flux balance analysis (FBA) is a well-known technique for genome-scale modeling of metabolic flux. Typically, an FBA formulation requires the accurate specification of four sets: biochemical reactions, biomass metabolites, nutrients and secreted metabolites. The development of FBA models can be time consuming and tedious because of the difficulty in assembling completely accurate descriptions of these sets, and in identifying errors in the composition of these sets. For example, the presence of a single non-producible metabolite in the biomass will make the entire model infeasible. Other difficulties in FBA modeling are that model distributions, and predicted fluxes, can be cryptic and difficult to understand.#R##N##R##N#Results: We present a multiple gap-filling method to accelerate the development of FBA models using a new tool, called MetaFlux, based on mixed integer linear programming (MILP). The method suggests corrections to the sets of reactions, biomass metabolites, nutrients and secretions. The method generates FBA models directly from Pathway/Genome Databases. Thus, FBA models developed in this framework are easily queried and visualized using the Pathway Tools software. Predicted fluxes are more easily comprehended by visualizing them on diagrams of individual metabolic pathways or of metabolic maps. MetaFlux can also remove redundant high-flux loops, solve FBA models once they are generated and model the effects of gene knockouts. MetaFlux has been validated through construction of FBA models for Escherichia coli and Homo sapiens.#R##N##R##N#Availability: Pathway Tools with MetaFlux is freely available to academic users, and for a fee to commercial users. Download from: biocyc.org/download.shtml.#R##N##R##N#Contact: mario.latendresse@sri.com#R##N##R##N#Supplementary information:Supplementary data are available at Bioinformatics online.
Practice Prize Report---The Power of CLV: Managing Customer Lifetime Value at IBM
Customer management activities at firms involve making consistent decisions over time, about: a which customers to select for targeting, b determining the level of resources to be allocated to the selected customers, and c selecting customers to be nurtured to increase future profitability. Measurement of customer profitability and a deep understanding of the link between firm actions and customer profitability are critical for ensuring the success of the above decisions. We present the case study of how IBM used customer lifetime value CLV as an indicator of customer profitability and allocated marketing resources based on CLV. CLV was used as a criterion for determining the level of marketing contacts through direct mail, telesales, e-mail, and catalogs for each customer. In a pilot study implemented for about 35,000 customers, this approach led to reallocation of resources for about 14% of the customers as compared to the allocation rules used previously which were based on past spending history. The CLV-based resource reallocation led to an increase in revenue of about $20 million a tenfold increase without any changes in the level of marketing investment. Overall, the successful implementation of the CLV-based approach resulted in increased productivity from marketing investments. We also discuss the organizational and implementation challenges that surrounded the adoption of CLV in this firm.
Multi-User Joint Tx/Iterative Rx MMSE-FDE and Successive MUI Cancellation for Uplink DS-CDMA
Uplink multi-user direct sequence-code division multi-access (DS-CDMA) suffers from strong multi-user interference (MUI) and self inter-chip interference (ICI) caused by severe frequency-selective fading. In this paper, we propose a joint Tx/iterative Rx frequency-domain equalization (FDE) based on minimum mean square error (MMSE) criterion and successive MUI cancellation (MUIC) for DS-CDMA uplink. In the proposed scheme, each user applies one-tap Tx FDE before transmitting signal. At the base station, joint one-tap Rx FDE and successive MUIC is iteratively performed. The FDE weights of users and base station are jointly optimized based on the MMSE criterion in order to reduce MUI and ICI while exploiting channel frequency-selectivity. Computer simulation results show that the proposed scheme provides much improved bit error rate (BER) performance than the conventional iterative Rx MMSE-FDE with successive MUIC.
Basic primitives for molecular diagram sketching
A collection of primitive operations for molecular diagram sketching has been developed. These primitives compose a concise set of operations which can be used to construct publication-quality 2 D coordinates for molecular structures using a bare minimum of input bandwidth. The input requirements for each primitive consist of a small number of discrete choices, which means that these primitives can be used to form the basis of a user interface which does not require an accurate pointing device. This is particularly relevant to software designed for contemporary mobile platforms. The reduction of input bandwidth is accomplished by using algorithmic methods for anticipating probable geometries during the sketching process, and by intelligent use of template grafting. The algorithms and their uses are described in detail.
Efficient and complete remote authentication scheme with smart cards
A complete remote authentication scheme should provide the following security properties: (1) mutual authentication, (2) session key exchange, (3) protection of user anonymity, (4) support of immediate revocation capability, (5) low communication and computation cost, (6) resistance to various kinds of attacks, (7) freely choosing and securely changing passwords by users, and (8) without storing password or verification tables in servers. However, none of the existing schemes meets all the requirements. In this paper, along the line of cost effective approach using hash functions for authentication, we propose an efficient and practical remote user authentication scheme with smart cards to support the above complete security properties.
Multimonostatic Shape Reconstruction of Two-Dimensional Dielectric Cylinders by a Kirchhoff-Based Approach
The inverse problem of reconstructing the shape of dielectric cylinders by aspect-limited multimonostatic multifrequency electromagnetic scattering data is dealt with. The problem is formulated as a linear one by means of the physical-optics approximation distributional approach. The difference with respect to the case of perfectly electrical conducting scatterers is pointed out, since the penetrability of the scatterers is taken into account by considering the contribution of the “shadowed” side to the local reflection coefficient. The adopted model allows one to predict that both the “illuminated” and “shadowed” sides of the scatterer provide contribution to the reconstructed image but with a delocalization depending on the relative dielectric permittivity. The numerical results confirm this expectation and show the effectiveness of the approach.
Robust and low complexity packet detector design for MB-OFDM UWB
Multiband orthogonal frequency division multiplexing (MB-OFDM) ultra wideband (UWB) systems have drawn much attention for its high spectrum efficiency and multiple access capability. However, its large throughput requirement and low power spectral density result in high hardware complexity and high power consumption, which are challenges of designing the packet detector. In this paper, a novel detection method is proposed with very good detection performance in low SNR. Low cost and low power schemes are also introduced in circuit design to save 70% area and 71% power. The proposed packet detector is synthesized with UMC 0.13µm library at 132MHz clock frequency. The hardware cost is 56.9K gates and the power consumption is only 11.7mW.
Microfluidic device for continuous magnetophoretic separation of red blood cells
This paper presents a microfluidic device for magnetophoretic separation red blood cells from blood under continuous flow. The separation method consist of continuous flow of a blood sample (diluted in PBS) through a microfluidic channel which presents on the bottom ldquodotsrdquo of ferromagnetic layer. By applying a magnetic field perpendicular on the flowing direction, the ferromagnetic ldquodotsrdquo generate a gradient of magnetic field which amplifies the magnetic force. As a result, the red blood cells are captured on the bottom of the microfluidic channel while the rest of the blood is collected at the outlet. Experimental results show that an average of 95 % of red blood cells is trapped in the device.
Polynomial Time Construction Algorithm of BCNC for Network Coding in Cyclic Networks
Network coding in cyclic networks meets more problems than in acyclic networks. Recently, S.-Y.R.Li et al.proposed a framework of convolutional network coding (CNC) as well as its four properties for cyclic networks with theoretic fundamentals of discrete valuation ring (DVR). The four properties, Convolutional Multicast (CM), Convolutional Broadcast (CB), Convolutional Dispersion (CD) and Basic Convolutional NetworkCode (BCNC), are notions of increasing strength in this order with regard to linear independence among the global encoding kernels. The existence of a BCNC then implies the existence ofthe rest. That is, BCNC is the best convolutional network code in terms of linear independence. However, the code construction algorithm of BCNC was not presented explicitly. To the best of our knowledge, this is the first paper to propose a polynomial time construction algorithm of BCNC for network coding in cyclic networks, which can deal with different characteristics of cycles in terms of topology, including link cycles but flow acyclic, simpleflow cycles and knots. Finally, polynomial time complexity of the algorithm was proved as well as its effectiveness.
Using Context Ontologies for Addressing and Routing in Mobile Ad Hoc Networks
This paper presents a new way of addressing and routing for mobile ad hoc networks on the basis of contextual information such as air pressure, brightness, wind direction and strength, or GPS position. The most common use case of context-based addressing is group communication: A participant sends a message to an a priori unspecified set of recipients, but indicates the context in which the message could be useful for a potential receiver. In contrast to infrastructure networks the sender no longer designates the receiver of its message with a distinct identifier. Instead, each recipient using his local context decides by himself, whether the message is useful for him and whether it should be sent out again. The modeling of the necessary application knowledge is done as ontologies in OWL (Web Ontology Language). As an example scenario, a wind gust warning on highways using a vehicular ad hoc network (VANET) is described: the warning message should be sent to all vehicles on the same route containing the place where the wind was detected. The models are applied in a prototypical example scenario in order to show the performance of the approach through a simulation, using the JiST/SWANS simulator for mobile ad hoc networks. The results show that the number of messages that are necessary to warn all vehicles in a given environment of the wind danger can be reduced by half – as opposed to a simple flooding of the network.
Local histogram of figure/ground segmentations for dynamic background subtraction
We propose a novel feature, local histogram of figure/ground segmentations, for robust and efficient background subtraction (BGS) in dynamic scenes (e.g., waving trees, ripples in water, illumination changes, camera jitters, etc.). We represent each pixel as a local histogram of figure/ground segmentations, which aims at combining several candidate solutions that are produced by simple BGS algorithms to get a more reliable and robust feature for BGS. The background model of each pixel is constructed as a group of weighted adaptive local histograms of figure/ground segmentations, which describe the structure properties of the surrounding region. This is a natural fusion because multiple complementary BGS algorithms can be used to build background models for scenes. Moreover, the correlation of image variations at neighboring pixels is explicitly utilized to achieve robust detection performance since neighboring pixels tend to be similarly affected by environmental effects (e.g., dynamic scenes). Experimental results demonstrate the robustness and effectiveness of the proposedmethod by comparing with four representatives of the state of the art in BGS.
Search space boundary extension method in real-coded genetic algorithms
In real-coded genetic algorithms (GAs), some crossover operators do not work well on functions which have their optimum at the corner of the search space. To cope with this problem, we have proposed a boundary extension method which allows individuals to be located within a limited space beyond the boundary of the search space. In this paper, we give an analysis of the boundary extension methods from the viewpoint of sampling bias and perform a comparative study on the effect of applying two boundary extension methods, namely the boundary extension by mirroring (BEM) and the boundary extension with extended selection (BES). We were able to confirm that to use sampling methods which have smaller sampling bias had good performance on both functions which have their optimum at or near the boundaries of the search space, and functions which have their optimum at the center of the search space. The BES/SD/A (BES by shortest distance selection with aging) had good performance on functions which have their optimum at or near the boundaries of the search space. We also confirmed that applying the BES/SD/A did not cause any performance degradation on functions which have their optimum at the center of the search space.
A New Implementation of a Half-Duplex Decode-and-Forward Cooperative Algorithm Using Complete Complementary Code Sets.
This paper introduces an implementation of a halfduplex decode-and-forward cooperative algorithm using the complete complementary code (CCC) sets. These code sets have an impulsive autocorrelation sum among each set and a cross correlation sum along the set size that vanishes for all shifts. Each user is assigned a set of spreading codes to spread his data and send each of the resulting signals on a different sub-band. The decode-and-forward cooperative procedure is applied and a receiver consisting of parallel branches matched filter followed by a λ-MRC combiner is used for detection. It is demonstrated that the probability of error performance of the proposed algorithm is very close to the performance of the analytical closed-form probability of error in similar transmission conditions. The algorithm is also compared with a noncooperative multiband direct-sequence code division multiple access (MB DS-CDMA) algorithm using the complete complementary sets. The simulation results illustrate the enhanced performance of the proposed algorithm under different channel assumptions.
Efficient lower bounds and heuristics for the variable cost and size bin packing problem
We consider the variable cost and size bin packing problem, a generalization of the well-known bin packing problem, where a set of items must be packed into a set of heterogeneous bins characterized by possibly different volumes and fixed selection costs. The objective of the problem is to select bins to pack all items at minimum total bin-selection cost. The paper introduces lower bounds and heuristics for the problem, the latter integrating lower and upper bound techniques. Extensive numerical tests conducted on instances with up to 1000 items show the effectiveness of these methods in terms of computational effort and solution quality. We also provide a systematic numerical analysis of the impact on solution quality of the bin selection costs and the correlations between these and the bin volumes. The results show that these correlations matter and that solution methods that are un-biased toward particular correlation values perform better.
An efficiently revised sustain driver for AC plasma display
A new sustain driver employing a step-up function is presented to achieve the faster rise-time of sustain voltage, which Ls suitable for widely used address-and-display-period-separated (ADS) driving method, and most of all for cost saving. The proposed sustain driver can reduce the number of switching devices by 25 (%) compared to the prior approach improving the overall system efficiency about 10 (%). And brightness decrease problem resulted from the lack of the number of the sustain pulse can be solved without a limitation to sustain pulse width. Operational principle and its features are illustrated comparing with conventional approaches. The validity of the proposed sustain driver is verified through experiments using a prototype equipped with a 7.5 (inch) diagonal panel, which is operated at 200 (kHz) switching frequency.
Short note: An automated system for the statistical analysis of sediment texture and structure at the micro scale
A macro has been developed that allows for automated statistical analyses of particles. It can be used with binary images from any source, and is developed for use with lacustrine, marine, Aeolian, and marine sediments. The macro code is freely available, and runs on open-source software. It has been specially designed to accommodate very small regions of interest relative to particle size, and to produce continuous downcore stratigraphies. The macro runs quickly, requires no user-interaction, is easily modifiable, saves 'raw data' for algorithm checking and further analyses, and can run in a mode that quantifies cracks and other disturbances on images. The macro allows for rapid analyses of relatively long sedimentary sequences, and provides relevant sedimentary interpretations of transport and depositional processes. Examples of output from two lacustrine sediment records and one marine record are shown.
Development of A Hierarchical Saving Power System for Campus
Saving power is the major goal in every area either in industry or in campus. However, there are more uncertainties in campus. For instances, the usages of air conditioners in the classroom are irregular events. In the other hand, the lighting in the parking area is the regular event. For the regular event, we use the unstable green energy like wind or solar to save the electric cost. For the irregular event, we use the stable electricity but managed by dynamic power dispatch system to efficiently handle usages of air conditioners in the classroom in order to reduce cost. This dispatch system is the integration of Internet, network, coded radio frequency, database, interfacing technology, and campus administration computerization servers. By using the existing media and devices, the set-up of this system is without extra expensive cost. This hybrid strategy is simulated in a University with about 15000 students and more than 100 classrooms equipped with air conditioners. We can estimate to save more than 10% power than usual.
Architectural Adaptation Addressing the Criteria of Multiple Quality Attributes in Mission-Critical Systems
Mission-critical software claims safe and robust adaptations that comply with rigorous criteria of multiple critical quality attributes. Existing adaptation approaches pay little attention to comprehensively capture mission goals and explicitly specify adaptation requirements. We propose an approach to using scenario-based analysis to elicit and specify the criteria of multiple quality attributes as adaptation invariants, and design corresponding architecture variants as facilities implementing adaptations. We also present how to make adaptation decisions at runtime.
Simulation-based ATPG for low power testing of crosstalk delay faults in asynchronous circuits
A new multi-objective genetic algorithm has been proposed for testing crosstalk delay faults in asynchronous sequential circuits that reduces average power dissipation during test application. The proposed Elitist Non-dominated Sorting Genetic Algorithm ENGA-based Automatic Test Pattern Generation ATPG for crosstalk induced delay faults generates test pattern set that has high fault coverage and low switching activity. Redundancy is introduced in ENGA-based ATPG by modifying the fault dropping phase and hence a very good reduction in transition activity is achieved. Tests are generated for several asynchronous SIS benchmark circuits. Experimental results demonstrate that ENGA gives higher fault coverage, reduced transitions and compact test vectors for most of the asynchronous benchmark circuits when compared with those generated by Weighted Sum Genetic Algorithm WSGA.
Field Measurements of a Hybrid DVB-SH Single Frequency Network With an Inclined Satellite Orbit
Field measurements of a DVB-SH network with both satellite and terrestrial transmitters are presented. The system is a Single Frequency Network transmitting video streams to vehicular terminals with up to 4 branches of receiver antenna diversity. The signal is transmitted in the S-band at 2.1859 GHz with both time and frequency synchronization of the terrestrial repeaters with the satellite's signal. The time and frequency variations are cancelled out at the satellite gateway, but because these can not be canceled at all locations, and because the satellite's position in space is variable (in an inclined orbit) the terrestrial repeaters are made to cancel the residual time and frequency variation. The field measurements include data taken in late 2008 through 2009 with multiple repeaters located in several cities including Las Vegas, NV, Raleigh and Durham, NC, as well as various morphologies, and with elevation angles to the satellite ranging from 25 °  to 52 °  . This paper presents coverage data for these various morphologies, modulation and code rates, as well as various MPE-iFEC interleaver settings used to ameliorate the effects of long shadowed intervals such as when the vehicle goes under highways or bridges where there signal is obscured for several seconds. We observed excellent performance in hybrid mode with better than 99% of the measured seconds error free. In satellite-only mode, the MPE-iFEC interleaver raised the performance from 81% to 91% averaged over all environments, including dense urban.
ASSESSING THE FEASIBILITY OF SELF-ORGANIZING MAPS FOR DATA MINING FINANCIAL INFORMATION
Analyzing financial performance in today’s information-rich society can be a daunting task. With the evolution of the Internet, access to massive amounts of financial data, typically in the form of financial statements, is widespread. Managers and stakeholders are in need of a data-mining tool allowing them to quickly and accurately analyze this data. An emerging technique that may be suited for this application is the self-organizing map. The purpose of this study was to evaluate the performance of self-organizing maps for analyzing financial performance of international pulp and paper companies. For the study, financial data, in the form of seven financial ratios, was collected, using the Internet as the primary source of information. A total of 77 companies, and six regional averages, were included in the study. The time frame of the study was the period 1995-00. An example analysis was performed, and the results analyzed based on information contained in the annual reports. The results of the study indicate that self-organizing maps can be feasible tools for the financial analysis of large amounts of financial data.
An Energy-Aware Video Streaming System for Portable Computing Devices
In this demonstration, we show an energy-aware video streaming system which allows users to play back video for the specified duration within the remaining battery amount. In the system, we execute a proxy server on an intermediate node in the network. It receives the video stream from a content server, transcodes it to the videos with appropriate quality, and forwards it to a PDA or a laptop PC. Here, suitable parameter values of the video (such as picture size, frame rate and bitrate) which enable playback for the specified duration are automatically calculated on the proxy using our battery consumption model. The system also allows users to play back video segments with different qualities based on the importance specified to each video segment.
A TOOL KIT FOR LEXICON BUILDING
This paper describes a set of interactive routines that can be used to create, maintain, and update a computer lexicon. The routines are available to the user as a set of commands resembling a simple operating system. The lexicon produced by this system is based on lexical-semantic relations, but is compatible with a variety of other models of lexicon structure. The lexicon builder is suitable for the generation of moderate-sized vocabularies and has been used to construct a lexicon for a small medical expert system. A future version of the lexicon builder will create a much larger lexicon by parsing definitions from machine-readable dictionaries.
Exact phase transition of backtrack-free search with implications on the power of greedy algorithms
Backtracking is a basic strategy to solve constraint satisfaction problems (CSPs). A satisfiable CSP instance is backtrack-free if a solution can be found without encountering any dead-end during a backtracking search, implying that the instance is easy to solve. We prove an exact phase transition of backtrack-free search in some random CSPs, namely in Model RB and in Model RD. This is the first time an exact phase transition of backtrack-free search can be identified on some random CSPs. Our technical results also have interesting implications on the power of greedy algorithms, on the width of superlinear dense random hypergraphs and on the exact satisfiability threshold of random CSPs.
Autonomous climbing of spiral staircases with humanoids
In this paper, we present an approach to enable a humanoid robot to autonomously climb up spiral staircases. This task is substantially more challenging than climbing straight stairs since careful repositioning is needed. Our system globally estimates the pose of the robot, which is subsequently refined by integrating visual observations. In this way, the robot can accurately determine its relative position with respect to the next step. We use a 3D model of the environment to project edges corresponding to stair contours into monocular camera images. By detecting edges in the images and associating them to projected model edges, the robot is able to accurately locate itself towards the stairs and to climb them. We present experiments carried out with a Nao humanoid equipped with a 2D laser range finder for global localization and a low-cost monocular camera for short-range sensing. As we show in the experiments, the robot reliably climbs up the steps of a spiral staircase.
Lightweight map matching for indoor localisation using conditional random fields
Indoor tracking and navigation is a fundamental need for pervasive and context-aware smartphone applications. Although indoor maps are becoming increasingly available, there is no practical and reliable indoor map matching solution available at present. We present MapCraft, a novel, robust and responsive technique that is extremely computationally efficient (running in under 10 ms on an Android smartphone), does not require training in different sites, and tracks well even when presented with very noisy sensor data. Key to our approach is expressing the tracking problem as a conditional random field (CRF), a technique which has had great success in areas such as natural language processing, but has yet to be considered for indoor tracking. Unlike directed graphical models like Hidden Markov Models, CRFs capture arbitrary constraints that express how well observations support state transitions, given map constraints. Extensive experiments in multiple sites show how MapCraft outperforms state-of-the art approaches, demonstrating excellent tracking error and accurate reconstruction of tortuous trajectories with zero training effort. As proof of its robustness, we also demonstrate how it is able to accurately track the position of a user from accelerometer and magnetometer measurements only (i.e. gyro- and WiFi-free). We believe that such an energy-efficient approach will enable always-on background localisation, enabling a new era of location-aware applications to be developed.
Medial-Guided Fuzzy Segmentation
Segmentation is generally regarded as partitioning space at the boundary of an object so as to represent the object's shape, pose, size, and topology. Some images, however, contain so much noise that distinct boundaries are not forthcoming even after the object has been identified. We have used statistical methods based on medial features in Real Time 3D echocardiography to locate the left ventricular axis, even though the precise boundaries of the ventricle are simply not visible in the data. We then produce a fuzzy labeling of ventricular voxels to represent the shape of the ventricle without any explicit boundary. The fuzzy segmentation permits calculation of total ventricular volume as well as determination of local boundary equivalencies, both of which are validated against manual tracings on 155 left ventricles. The method uses a medial-based compartmentalization of the object that is generalizable to any shape.
Cryptographically secure identity certificates
We present FACECERTS, a simple, inexpensive, and cryptographically secure identity certification system. A FACECERT is a printout of person's portrait photo, an arbitrary textual message, and a 2D color bar-code which encodes an RSA signature of the message hash and the compressed representation of the face encompassed by the photo. The signature is created using the private key of the party issuing the ID. Verification is performed by a simple, intelligent, and off-line scanning device that contains the public key of the issuer. The system does not require smart cards. More interestingly, the ID does not need to be printed by a high-end printer, it can be printed anywhere. We present a novel algorithm for compressing faces and investigate the reliability of the crucial components of the system.
Use of communication technologies in South Korean universities
South Korean universities are currently at various stages of advancement in their use of electronic communication. Networking provision for universities is expected to improve considerably over the next few years. There is therefore a need to examine their present usage of communication technologies, how this depends on the level of facilities available and what the implications are for their future development. The study described here investigates usage at a stratified sample of South Korean universities via a questionnaire survey. It also examines the factors affecting use via a series of interviews. The results suggest that, though infrastructural limitations are important, optimal deployment of communication technologies will require organisational changes within the university system.
A 62.5 ns holographic reconfiguration of an optically differential reconfigurable gate array
To date, holographic configuration speeds have remained limited to 16 mus because of issues related to the architecture of optically reconfigurable gate array VLSIs (ORGA-VLSIs). Therefore, to improve the issue, optically differential reconfigurable gate array VLSIs (ODRGA-VLSIs) have been developed and have achieved zero-overhead and nanosecond optical reconfiguration. Moreover, the architecture of an ODRGA-VLSI has the advantage of accelerating the reconfiguration speed compared to that of other ORGAs. However, nanosecond holographic configurations and, in particular, rapid holographic reconfiguration exploiting the advantages of ODRGA-VLSIs have not been reported. Therefore, this paper presents results of the world's fastest 62.5 ns holographic reconfiguration, exploiting advantages of the ODRGA-VLSI.
Performance of coherent ASK lightwave systems with finite intermediate frequency
The impact of finite intermediate frequency (IF) on the performance of heterodyne ASK lightwave systems is examined and quantified in the presence of laser phase noise and shot noise. For negligible linewidths, it is shown that certain finite choices of IF (R/sub b/,3R/sub b//2,2R/sub b/,5R/sub b//2, etc.) lead to the same ideal bit-error-rate (BER) performance as infinite choices of IF. Results indicate that for negligible linewidths the worst case sensitivity penalty is 0.9 dB for proper heterodyne detection and occurs when f/sub IF/=1.25 R/sub b/. For nonnegligible linewidths (e.g., when /spl Delta//spl nu/T/spl ges/0.04) the sensitivity penalty is always less than 0.9 dB for finite choices of IF. The analysis presented does lead to a closed-form signal-to-noise ratio (SNR) expression at the decision gate of the receiver which can readily be used for BER and sensitivity penalty computations. The SNR expression provided includes all the key system parameters of interest such as system bit rate (R/sub b/), the peak IF SNR (/spl xi/), laser linewidth (/spl Delta//spl nu/), and the IF filter expansion factor (/spl alpha/). The findings of this work suggest that the number of channels in a multichannel heterodyne ASK lightwave system can be increased substantially by properly choosing a small value for the IF at the expense of a small penalty <1 dB. On the negative side, IF frequency stabilization becomes a more critical requirement in multichannel systems employing small values of IF.
Simulation and Measurement of Narrow-Band Antennas for Small Terminals
In this paper both normal-band and narrow-band PIFA antennas for small terminals are compared through numerical simulations and measurements for different UMTS bandwidths. It is found that using different antennas for the transmitting and receiving regions of the frequency duplex it is possible to achieve a significant improvement in terms of isolation. Measurement results show also that ohmic losses lower the total efficiency in the narrow-band case especially at low frequencies.
An extension to the ordered subcarrier selection algorithm (OSSA)
In this letter, we propose an extension to the ordered subcarrier selection algorithm (OSSA) for orthogonal frequency division multiplexing (OFDM) systems. The result is a simple algorithm for minimizing the bit error rate of the OFDM system at a fixed throughput. The proposed algorithm employs multiple modulations (non-uniform bit loading) within an OFDM symbol. However, unlike existing bit loading algorithms that have a very high computational complexity, the proposed algorithm is based only on the ordered statistics of the subcarrier gains and is consequently very simple. After ordering the subcarriers based on their gains, progressively higher order modulations are used with increasing gains. The key aspect here that greatly simplifies the algorithm is that the modulation used on a subcarrier depends only on the position of its gain in the ordered set and not on the actual values of the gains. We show an analytical approach for determining the parameters of the algorithm.
Classifiers for Motion
In this paper, we present a supervised learning based approach for sub-pixel motion estimation. The novelty of this work is the learning based method itself which tries to learn the shifts from a large training database. Integer pixel shift is sub-divided and discretized to levels in both the horizontal and vertical direction. We pose the problem of motion estimation in a polar coordinate system. Shift estimation in the x and y direction has been posed as a problem of estimating r and thetas. The ordinal property of r has been used, and consequently, we employ a ranking based approach for estimating r. For thetas estimation we employ multi-class classification techniques. We demonstrate how very simplistic features can be used to differentiate between different sub-pixel shifts
Performance analysis of a novel traffic scheduling algorithm in slotted optical networks
This paper considers the scheduling problem in a new slotted optical network called Time-Domain Wavelength Interleaved network (TWIN). The TWIN architecture possesses interesting properties, which may offer solutions for next-generation optical networks. Besides, better Quality of Service (QoS) could be achieved in TWIN by minimizing two parameters: queueing delay and delay variance. However, to the best of our knowledge, most of the existing scheduling algorithms in TWIN ignored consideration of QoS and focused mainly on maximizing the throughput. In this paper, we formulate the scheduling problem into an Integer Linear Programming (ILP) problem and propose a novel heuristic - Destination Slot Set (DSS) algorithm to solve it fast and efficiently. Besides, we derive an analytical model for TWIN and investigate the performance of DSS in it. By means of simulations, we demonstrate that our analytical model approximates the TWIN network very well; moreover, DSS incurs smaller queueing delay and delay variance, which ensures better QoS.
Motion filter vector quantization
Motion-compensated prediction of video is formulated as a novel vector quantization scheme called motion filter vector quantization (MFVQ). In MFVQ, the motion vector and the pixel-intensity interpolation filter are combined into a motion filter and the entire filter is vector quantized. A codebook design algorithm is proposed for designing unit gain and entropy constrained MFVQ codebooks. The algorithm is tested under two application configurations, MFVQ with static codebook and MFVQ with forward-adaptive codebook, and is shown to furnish up to a dB of PSNR gain.
Towards a real-time navigation strategy for a mobile robot
Describes the design of a real-time obstacle avoidance and navigation strategy for a mobile robot, using simulated time of flight infrared data. Algorithms have been developed in order to overcome the undesirable effect of potential traps within the field, and a new approach for dealing with sensing whilst moving is demonstrated. The authors show simulated results of a vehicle moving under artificial potential force equations, which is designed to achieve modification of behaviour at a speed close to the normal operational speed of a real mobile. >
Development of a telepresence controlled ambidextrous robot for space applications
A master-slave system is developed to evaluate the effectiveness of telepresence in space telerobotics applications. An operator uses the master's telepresence and virtual reality equipment to control the slave. The slave is a dual-arm, dual-hand robot equipped with a stereo camera platform designed to provide an operator-centered perspective of the remote environment. The initial integration and tests of the system presented several operational issues that are resolved through a variety of shared control techniques and optimized algorithms. The resulting system provides a very flexible capability and is used to perform a range of tasks: grasping and handling toots, manipulating electronics controls, manipulating soft flexible material, and performing planetary geology tasks that involve a variety of manipulation and tool-use skills. Using this system an operator is able to complete most of these tasks in less than 2 minutes.
Natural hand posture recognition based on Zernike moments and hierarchical classifier
View-independence and user-independence are two fundamental requirements for hand posture recognition during natural human-robot interaction. However only a few research concerns on the two issues simultaneously. The difficulty for natural gesture-based human-robot interaction lies in that appearances of the same hand posture vary with different users from different viewing directions. In this paper, we propose a systematic feature selection approach based on Zernike moments and Isomap dimensionality reduction. A hierarchical classifier based on multivariate decision tree and piecewise linearization is developed to deal with the irregular distribution of the same hand postures. The proposed method is compared with other commonly used ones in hand posture recognition. Experimental results indicate that the proposed method can effectively identify different hand postures, irrespective of viewing directions and users.
Unsupervised Feature Learning for Visual Sign Language Identification
Prior research on language identification focused primarily on text and speech. In this paper, we focus on the visual modality and present a method for identifying sign languages solely from short video samples. The method is trained on unlabelled video data (unsupervised feature learning) and using these features, it is trained to discriminate between six sign languages (supervised learning). We ran experiments on short video samples involving 30 signers (about 6 hours in total). Using leave-one-signer-out cross-validation, our evaluation shows an average best accuracy of 84%. Given that sign languages are underresourced, unsupervised feature learning techniques are the right tools and our results indicate that this is realistic for sign language identification.
Association Control in Mobile Wireless Networks
As mobile nodes roam in a wireless network, they continuously associate with different access points and perform handoff operations. However, frequent handoffs can potentially incur unacceptable delays and even interruptions for interactive applications. To alleviate these negative impacts, we present novel association control algorithms that can minimize the frequency of handoffs occurred to mobile devices. Specifically, we show that a greedy LookAhead algorithm is optimal in the offline setting, where the user's future mobility is known. Inspired by such optimality, we further propose two online algorithms, namely LookBack and Track, that operate without any future mobility information. Instead, they seek to predict the lifetime of an association using randomization and statistical approaches, respectively. We evaluate the performance of these algorithms using both analysis and trace-driven simulations. The results show that the simple LookBack algorithm has surprisingly a competitive ratio .of (log k + 2), where k is the maximum number of APs that a user can hear at any time, and the Track algorithm can achieve near-optimal performance in practical scenarios.
Debugging the Internet of Things: The Case of Wireless Sensor Networks
The Internet of Things (IoT) has the strong potential to support a human society interacting more symbiotically with its physical environment. Indeed, the emergence of tiny devices that sense environmental cues and trigger actuators after consulting logic and human preferences promises a more environmentally aware and less wasteful society. However, the IoT inherently challenges software development processes, particularly techniques for ensuring software reliability. Researchers have developed debugging tools for wireless sensor networks (WSNs), which can be viewed as the enablers of perception in the IoT. These tools gather run-time information on individual sensor node executions and node interactions and then compress that information.
Two empirical studies of computer-supported collaborative learning in science: methodological and affective implications
In this paper the results and implications of two studies of computer-supported collaborative learning are presented and implications discussed. The first study was an experimental study in a British secondary school, while the second study followed a group of primary school children in a naturalistic context. Assessing learning situations is discussed with an emphasis on the affective factors. The differences between the products, the interactions and the outcomes of learning situations are discussed along with the research methodology. There is an emphasis on pre- and post-testing, naturalistic and experimental studies and time-based analyses.
Acquisition of Project-Specific Assets with Bayesian Updating
We study the impact of learning on the optimal policy and the time-to-decision in an infinite-horizon Bayesian sequential decision model with two irreversible alternatives: exit and expansion. In our model, a firm undertakes a small-scale pilot project to learn, via Bayesian updating, about the project's profitability, which is known to be in one of two possible states. The firm continuously observes the project's cumulative profit, but the true state of the profitability is not immediately revealed because of the inherent noise in the profit stream. The firm bases its exit or expansion decision on the posterior probability distribution of the profitability. The optimal policy is characterized by a pair of thresholds for the posterior probability. We find that the time-to-decision does not necessarily have a monotonic relation with the arrival rate of new information.
A RULE REPOSITORY FOR ACTIVE DATABASE SYSTEMS
Active Database Systems (ADBSs) provides a good infrastructure to define and execute active rules. Nevertheless, this infrastructure offered by ADBSs does not completely satisfy the necessities of rules management that demands current business applications. Rules also need to be stored in appropriate structures to facilitate their management, as the existing structures for data in these systems. This work proposes a rule repository, composed by structures that allow the storage and organization of rules, in order to facilitate their management. For this purpose, a rule classification with the main rule types existing in the literature is presented, and then, it represents the characteristics and anatomy of each type in a meta-model, with the goal of analyzing the data that must be stored about rules. The rule repository, proposed in this paper, has been built based on this meta-model.
Decentralized group formation
Imagine a network of entities, being it replica servers aiming to minimize the probability of data loss, players of online team-based games and tournaments, or companies that look into co-branding opportunities. The objective of each entity in any of these scenarios is to find a few suitable partners to help them achieve a shared goal: replication of the data for fault tolerance, winning the game, successful marketing campaign. All information attainable by the entities is limited to the profiles of other entities that can be used to assess the pairwise fitness. How can they create teams without help of any centralized component and without going into each other’s way? We propose a decentralized algorithm that helps nodes in the network to form groups of a specific size. The protocol works by finding an approximation of a weighted k-clique matching of the underlying graph of assessments. We discuss the basic version of the protocol, and explain how dissemination via gossiping helps in improving its scalability. We evaluate its performance through extensive simulations.
Efficient Blind Compressed Sensing Using Sparsifying Transforms with Convergence Guarantees and Application to MRI
Natural signals and images are well-known to be approximately sparse in transform domains such as Wavelets and DCT. This property has been heavily exploited in various applications in image processing and medical imaging. Compressed sensing exploits the sparsity of images or image patches in a transform domain or synthesis dictionary to reconstruct images from undersampled measurements. In this work, we focus on blind compressed sensing, where the underlying sparsifying transform is a priori unknown, and propose a framework to simultaneously reconstruct the underlying image as well as the sparsifying transform from highly undersampled measurements. The proposed block coordinate descent type algorithms involve highly efficient optimal updates. Importantly, we prove that although the proposed blind compressed sensing formulations are highly nonconvex, our algorithms are globally convergent (i.e., they converge from any initialization) to the set of critical points of the objectives defining the formulations. These critical points are guaranteed to be at least partial global and partial local minimizers. The exact point(s) of convergence may depend on initialization. We illustrate the usefulness of the proposed framework for magnetic resonance image reconstruction from highly undersampled k-space measurements. As compared to previous methods involving the synthesis dictionary model, our approach is much faster, while also providing promising reconstruction quality.
Optimizing local pickup and delivery with uncertain loads
The local pickup and delivery problem (LPDP) is an essential operational problem in intermodal industry. While the problem with deterministic settings is already difficult to solve, in reality, there exist a set of loads, called uncertain loads, which are unknown at the beginning of the day. But customers may call in during the day to materialize these loads. In this paper, we call the LPDP considering these uncertain loads as the stochastic LPDP. The problem description and the mathematical modeling of stochastic LPDP are discussed. Then, a simulation-based optimization approach is proposed to solve the problem, which features in a fast solution generation procedure and an intelligent simulation budget allocation framework. The numerical examples show the best strategy to consider the stochastic loads in the planning process and validate the benefits compared to its deterministic counterpart.
Object-Based Coding for Plenoptic Videos
A new object-based coding system for a class of dynamic image-based representations called plenoptic videos (PVs) is proposed. PVs are simplified dynamic light fields, where the videos are taken at regularly spaced locations along line segments instead of a 2-D plane. In the proposed object-based approach, objects at different depth values are segmented to improve the rendering quality. By encoding PVs at the object level, desirable functionalities such as scalability of contents, error resilience, and interactivity with an individual image-based rendering (IBR) object can be achieved. Besides supporting the coding of texture and binary shape maps for IBR objects with arbitrary shapes, the proposed system also supports the coding of grayscale alpha maps as well as depth maps (geometry information) to respectively facilitate the matting and rendering of the IBR objects. Both temporal and spatial redundancies among the streams in the PV are exploited to improve the coding performance, while avoiding excessive complexity in selective decoding of PVs to support fast rendering speed. Advanced spatial/temporal prediction methods such as global disparity-compensated prediction, as well as direct prediction and its extensions are developed. The bit allocation and rate control scheme employing a new convex optimization-based approach are also introduced. Experimental results show that considerable improvements in coding performance are obtained for both synthetic and real scenes, while supporting the stated object-based functionalities.
Adaptive reconstruction method of missing textures based on inverse projection via sparse representation
This paper presents an adaptive reconstruction method of missing textures based on an inverse projection via sparse representation. The proposed method approximates original and corrupted textures in lower-dimensional subspaces by using the sparse representation technique. Then, this approach effectively solves problems of not being able to directly estimate an inverse projection for reconstructing missing textures. Furthermore, even if target textures contain missing areas, the proposed method enables adaptive generation of the subspaces by monitoring errors caused in their known neighboring textures by the estimated inverse projection. Consequently, since the optimal inverse projection is adaptively estimated for each texture, successful reconstruction of the missing areas can be expected. Experimental results show impressive improvement of the proposed reconstruction technique over previously reported reconstruction techniques.
Large margin non-linear embedding
It is common in classification methods to first place data in a vector space and then learn decision boundaries. We propose reversing that process: for fixed decision boundaries, we "learn" the location of the data. This way we (i) do not need a metric (or even stronger structure) - pairwise dissimilarities suffice; and additionally (ii) produce low-dimensional embeddings that can be analyzed visually. We achieve this by combining an entropy-based embedding method with an entropy-based version of semi-supervised logistic regression. We present results for clustering and semi-supervised classification.
Scientific Misconduct: Three Forms that Directly Harm Others as the Modus Operandi of Mill’s Tyranny of the Prevailing Opinion
Scientific misconduct is usually assumed to be self-serving. This paper, however, proposes to distinguish between two types of scientific misconduct: ‘type one scientific misconduct’ is self-serving and leads to falsely positive conclusions about one’s own work, while ‘type two scientific misconduct’ is other-harming and leads to falsely negative conclusions about someone else’s work. The focus is then on the latter type, and three known issues are identified as specific forms of such scientific misconduct: biased quality assessment, smear, and officially condoning scientific misconduct. These concern the improper ways how challenges of the prevailing opinion are thwarted in the modern world. The central issue is pseudoskepticism: uttering negative conclusions about someone else’s work that are downright false. It is argued that this may be an emotional response, rather than a calculated strategic action. Recommendations for educative and punitive measures are given to prevent and to deal with these three forms of scientific misconduct.
Yield Curve Shapes and the Asymptotic Short Rate Distribution in Affine One-Factor Models
We consider a model for interest rates where the short rate is given under the risk-neutral measure by a time-homogeneous one-dimensional affine process in the sense of Duffie, Filipovic, and Schachermayer. We show that in such a model yield curves can only be normal, inverse, or humped (i.e., endowed with a single local maximum). Each case can be characterized by simple conditions on the present short rate rt. We give conditions under which the short rate process converges to a limit distribution and describe the risk-neutral limit distribution in terms of its cumulant generating function. We apply our results to the Vasicek model, the CIR model, a CIR model with added jumps, and a model of Ornstein–Uhlenbeck type.
Multimedia descriptions based on MPEG-7: extraction and applications
The amount of digital multimedia content available to consumers is growing because of the existence of digital capturing devices such as digital cameras, camcorders and the advent of digital video broadcast. With this increase in content, it becomes important for users to be able to browse and search for content in a timely manner. Descriptions and annotations of the content are needed to enable searching and browsing of content. MPEG-7 is a recent ISO standard for multimedia content description. In this paper, we present three descriptors, which we had proposed to MPEG-7, and have been accepted to the standard. In addition, we describe algorithms for automatically extracting these descriptors. We also present the indexing and retrieval algorithms that we developed for these descriptors; these algorithms are fast and scalable for large databases. The results presented in the paper for image and video segment matching using these descriptors show their usefulness in real applications.
3D shape reconstruction using volume intersection techniques
Volume intersection algorithms are used to reconstruct incomplete objects from their silhouettes. An imagined light source is moved about the data and the cumulative amount of "light" seen at each point an space is interpreted as indicating the likelihood that the point is inside the object. The object data need not be uniformly distributed nor exclusively surface data. Explicit distinction between noise, surface and interior data is avoided. The novel concept of a localised viewing region is introduced to overcome the inherent inability of volume intersection algorithms to reconstruct concave surfaces. Algorithms for 2D pixel and 3D voxel data are described and applied to 3D ultrasound data.
Characterizing and Mitigating Inter-domain Policy Violations in Overlay Routes
The Internet is a complex structure arising from the interconnection of numerous autonomous systems (AS), each exercising its own administrative policies to reflect the commercial agreements behind the interconnection. However, routing in service overlay networks is quite capable of violating these policies to its advantage. To prevent these violations, we see an impending drive in the current Internet to detect and filter overlay traffic. In this paper, we first present results from a case study overlay network, constructed on top of Planetlab, that helps us gain insights into the frequency and characteristics of the different inter-domain policy violations. We further investigate the impact of two types of overlay traffic filtering that aim to prevent these routing policy violations: blind filtering and policy- aware filtering. We show that such filtering can be detrimental to the performance of overlay routing. We next consider two approaches that allow the overlay network to realize the full advantage of overlay routing in this context. In the first approach, overlay nodes are added so that good overlay paths do not represent inter-domain policy violations. In the second approach, the overlay acquires transit permits from certain ASes that allow certain policy violations to occur. We develop a single cost-sharing framework that allows the incorporation of both approaches into a single strategy. We formulate and solve an optimization problem that aims to determine how the overlay network should allocate a given budget between paying for additional overlay nodes and paying for transit permits to ASes. We illustrate the use of this approach on our case study overlay network and evaluate its performance under varying network characteristics.
A Sensor Fusion Framework Using Multiple Particle Filters for Video-Based Navigation
This paper presents a sensor-fusion framework for video-based navigation. Video-based navigation offers the advantages over existing approaches. With this type of navigation, road signs are directly superimposed onto the video of the road scene, as opposed to those superimposed onto a 2-D map, as is the case with conventional navigation systems. Drivers can then follow the virtual signs in the video to travel to the destination. The challenges of video-based navigation require the use of multiple sensors. The sensor-fusion framework that we propose has two major components: (1) a computer vision module for accurately detecting and tracking the road by using partition sampling and auxiliary variables and (2) a sensor-fusion module using multiple particle filters to integrate vision, Global Positioning Systems (GPSs), and Geographical Information Systems (GISs). GPS and GIS provide prior knowledge about the road for the vision module, and the vision module, in turn, corrects GPS errors.
An evolution programme for the resource-constrained project scheduling problem
This paper describes an implementation of an evolution programme for the resource-constrained project scheduling problem. In essentials, the problem consists of two issues; (a) to determine the order of activities without violating precedence constraints and (b) subsequently to determine earliest start time for each activity according to available resources. How to determine the order of activation is critical to the problem, because if the order is determined, a schedule can be easily constructed with some determining procedures. The basic ideas of the proposed approach are; (a) using an evolution programme to evolve an appropriate order of activities and (b) using a fit-in-best procedure to calculate the earliest start times of activities. A new approach is addressed to guide how to design genetic operators; one operator is designed to perform a wide spread search to try to explore the area beyond local optima, whereas the other is designed to perform an intensive search to try to find an improved solut...
A methodology for evaluating the accuracy of wave field rendering techniques
In this paper we propose a methodology for assessing the accuracy of techniques of wave field rendering through loudspeaker arrays. In order to measure the rendered wave field we adopt a solution based on a circular harmonic analysis of the sound field captured by a virtual microphone array. As a result of this analysis stage, we are able to compare the target, the theoretical and the measured wave fields, which may differ due to the non-ideality in the loudspeaker array or in the environment that generates some spurious reverberations. Moreover, in order to quantify the error between target, theoretical and measured wave fields, we define some evaluation metrics, based on RMSE and modal analysis of the acquired wave fields. We show some experimental results on real data.
Rectangular v-Splines
This article describes and presents examples of some techniques for the representation and interactive design of surfaces based on a parametric surface representation that user v-spline curves. These v-spline curves, similar in mathematical structure to v-splines, were developed as a more computationally efficient alternative to splines in tension. Although splines in tension can be modified to allow tension to be applied at each control point, the procedure is computationally expensive. The v-spline curve, however, uses more computationally tractable piecewise cubic curves segments, resulting in curves that are just as smoothly joined as those of a standard cubic spline. After presenting a review of v-splines and some new properties, this article extends their application to a rectangular grid of control points. Three techniques and some application examples are presented.
Downlink Joint Base-station Assignment and Packet Scheduling Algorithm for Cellular CDMA/TDMA Networks
In this paper using a utility-based approach, down-link packet transmission in a CDMA/TDMA cellular network is formulated as an optimization problem. A utility function corresponds to each packet served by a base-station that is an increasing function of the packet experienced delay and the channel gain, and a decreasing function of the base-station load. Unlike previous works, in this paper, the optimization objective is to maximize the total network utility instead of the base-station utility. We show that this optimization results in joint base-station assignment and packet scheduling. Therefore, in addition to multi-user diversity, the proposed method also exploits multi-access-point diversity and soft capacity. A polynomial time heuristic algorithm is then proposed to solve the optimization problem. Simulation results indicate a significant performance improvement in terms of packet-drop-ratio and achieved throughput.
Enhanced routing-aware adaptive MAC with traffic differentiation and smoothed contention window in wireless ad-hoc networks
In wireless mobile ad-hoc networks, data packets have to be relayed hop by hop from a given source node to a destination node. This means that some or all of the mobile nodes must accept to forward information for the benefit of other nodes. It has been shown by F. Nait-Abdesselam, et al., (2003) that this ability of forwarding packets leads to a new unfairness problem in wireless ad-hoc networks, where a node which is forwarding other node's packets gets less bandwidth, for its own use, than a node which is not participating to the routing service. The proposed RAMAC scheme by F. Nait-Abdesselam, et al., (2003), has shown all its effectiveness to cope with this unfairness problem. However, an extra bandwidth is sometimes allowed to the routing node's own traffic comparing to other nonrouting node's own traffics, and the routing node's routed traffic gets much less bandwidth. This is explained by the fact that at the upper layer (for instance the IP layer) does not differentiate between the routing and the own traffics, and that the multiplicative factor used to compute the new contention window is too aggressive. In this paper, an enhanced RAMAC scheme is proposed, by taking into account the differentiation on top of the MAC layer, between the and routed traffics within a routing node, and by smoothing the multiplicative factor used to compute the new contention window. The simulation results obtained showed a good improvement of the original RAMAC scheme, leading to better approximate equal bandwidth share among all the mobile nodes in the wireless ad-hoc network.
Meaning of Pearson Residuals Linear Algebra View
Marginal distributions play an central role in statistical analysis of a contingency table. However, when the number of partition becomes large, the contribution from marginal distributions decreases. This paper focuses on a formal analysis of marginal distributions in a contingency table. The main approach is to take the difference between two matrices with the same sample size and the same marginal distributions, which we call difference matrix. The important nature of the difference matrix is that the determinant is equal to 0: when the rank of a matrix is r, the difference between a original matrix and the expected matrix will become r - 1 at most. Since the sum of rows or columns of the will become zero, which means that the information of one rank corresponds to information on the frequency of a contingency matrix. Interestingly, if we take an expected matrix whose elements are the expected values based on marginal distributions, the difference between an original matrix and expected matrix can be represented by linear combination of determinants of 2 times 2 submatrices.
An experimental study of Quartets MaxCut and other supertree methods
Background#R##N#Supertree methods represent one of the major ways by which the Tree of Life can be estimated, but despite many recent algorithmic innovations, matrix representation with parsimony (MRP) remains the main algorithmic supertree method.
On-line knowledge-based simulation for FMS: a state of the art survey
Simulation modeling has been widely used to study many aspects of FMS design, planing and control. Yet, simulation modeling still offers other capabilities that are proven to be effective for the on-line control of FMS. An example of it is the training of neural nets off-line, so that it will later control the decision making process when the FMS is in operation. if the neural net runs out of knowledge, it turns back to a simulation model to learn new situations. In this paper, we review the various ways in which on-line knowledge-based simulation for FMS has been approached. This paper represents the early stages of on-going research efforts at Florida International University.
Study on the Management Models of Urban Multi-Ethnic Community in Northwestern Cities of China - The Case of Lanzhou
Urban multi-ethnic community is a special type of community which has a high degree of heterogeneity. Thus a study on the management of these communities will be of theoretical and practical significance. This paper took an urban multi-ethnic community of Lanzhou as example, investigating on the management of multi-ethnic communities in northwestern cities. Based on the theory of social relationship, using qualitative and case study methods, this paper has probed into the management models, causes, further development and other issues s in multi-ethnic communities in the Northwestern cities of China.
A Cross-Layer Adaptation for VoIP over Infrastructure Mesh Network
The deployment of wireless mesh paradigm was meant to extend Internet access without a consideration of delay sensitive applications. None the less, since voice over IP (VoIP) services are rapidly increasing in popularity, IEEE 802.11 based wireless mesh networks are challenged with the provision of guaranteed quality VoIP calls. In this paper, the disquiet on VoIP systems caused by physical (PHY) and medium access control (MAC) anomaly in the current wireless mesh deployment is addressed through a cross-layer scheme. The scheme is aimed at enhancing VoIP call capacity by mitigating PHY and MAC overheads through aggregation of packets of the same next hop. Through simulations, it is shown that the proposed scheme has significant performance improvements while leaving the IEEE 802.11 standard intact.
Combining variants of iterative flattening search
Iterative flattening search (ifs) is an iterative improvement heuristic schema for makespan minimization in scheduling problems. Given an initial solution, ifsiteratively interleaves a relaxation-step, which randomly retracts some search decisions, and an incremental solving step (or flattening-step) to recompute a new solution. The process continues until a stop condition is met and the best solution found is returned. In recent work we have created a uniform software framework to analyze component techniques that have been proposed in ifsapproaches. In this paper we combine basic components to obtain hybrid variants and perform a detailed experimental evaluation of their performance. Specifically, we examine the utility of: (1) operating with different relaxation strategies and (2) using different searching strategies to built a new solution. We present a two-step experimental evaluation: (a) an extensive explorative evaluation with a spectrum of parameter combination; (b) a time-intensive evaluation of the best ifscombinations emerged from the previous. The experimental results shed light on weaknesses and strengths of the different variants improving the current understanding of this family of meta-heuristics.
A Multielement Tactile Feedback System for Robot-Assisted Minimally Invasive Surgery
A multi-element tactile feedback (MTF) system has been developed to translate the force distribution, in magnitude and position, from 3times2 sensor arrays on surgical robotic end-effectors to the fingers via 3times2 balloon tactile displays. High detection accuracies from perceptual tests (> 96%) suggest that MTF may be an effective means to improve robotic control.
Can Brand Extension Signal Product Quality
This paper asks whether brand extension can serve as a signal of product quality given that it costs less than a new brand. (Existing literature has assumed either that brand extension is cost-neutral or that it costs more.) I show that it can as a perfect Bayesian equilibrium, but the argument is unconvincing. For one thing, the separating equilibrium is not unique; a pooling equilibrium also exists in which brand extension signals nothing. For another, the separating equilibrium relies on off-equilibrium beliefs that are poorly motivated in the model. I propose a refinement of the perfect Bayesian equilibrium that resolves both issues. Empirical off-equilibrium beliefs require that consumers' off-equilibrium beliefs be justifiable on the basis of their prior beliefs and product performance observations. With empirical off-equilibrium beliefs, two necessary conditions for brand extension to signal product quality are identified: (i) consumers must perceive old and new products of the firm to be positively correlated in quality, and (ii) at least some consumers must identify with brands and not the firm behind the brands. Even with these conditions in place, the signaling argument is fragile: firm observability of past performance diminishes brand extension's signaling capability; an arbitrarily small probability of failure for good products eliminates it. My results suggest that, going forward, the case for brand extension must rest on foundations other than signaling product quality.
Community detection using a neighborhood strength driven Label Propagation Algorithm
Studies of community structure and evolution in large social networks require a fast and accurate algorithm for community detection. As the size of analyzed communities grows, complexity of the community detection algorithm needs to be kept close to linear. The Label Propagation Algorithm (LPA) has the benefits of nearly-linear running time and easy implementation, thus it forms a good basis for efficient community detection methods. In this paper, we propose new update rule and label propagation criterion in LPA to improve both its computational efficiency and the quality of communities that it detects. The speed is optimized by avoiding unnecessary updates performed by the original algorithm. This change reduces significantly (by order of magnitude for large networks) the number of iterations that the algorithm executes. We also evaluate our generalization of the LPA update rule that takes into account, with varying strength, connections to the neighborhood of a node considering a new label. Experiments on computer generated networks and a wide range of social networks show that our new rule improves the quality of the detected communities compared to those found by the original LPA. The benefit of considering positive neighborhood strength is pronounced especially on real-world networks containing sufficiently large fraction of nodes with high clustering coefficient.
Semidefinite Programming Based Algorithms for the Sparsest Cut Problem
In this paper we analyze a known relaxation for the Spars- est Cut problem based on positive semidefinite constraints, and we present a branch and bound algorithm and heuristics based on this relaxation. The relaxed formulation and the algorithms were tested on small and moderate sized instances. It leads to values very close to the optimum solution values. The exact algorithm could obtain solutions for small and moderate sized instances, and the best heuristics obtained optimum or near optimum solutions for all tested instances. The semidefinite relaxation gives a lower bound C/W and each heuristic produces a cut S with a ratio c S /w S , where either c S  is at most a factor of C or w S  is at least a factor of W. We solved the semidefinite relaxation using a semi-infinite cut generation with a commercial linear programming package adapted to the sparsest cut problem. We showed that the proposed strategy leads to a better performance compared to the use of a known semidefinite programming solver.
Asynchronous comparison-based decoders for delay-insensitive codes
A comparison-based decoder detects the arrival of a code word by comparing the received checkbits with the checkbits computed using the received data. Implementation issues underlying comparison-based decoders for systematic delay-insensitive (DI) or unordered codes is the subject of this paper. We show that if the decoder is to be implemented using asynchronous logic, i.e., if the gate and wire delays are arbitrary (unbounded but finite), then it is impossible to design a comparison-based decoder for any code that is more efficient than a dual-rail code. In other words, the encoded word must contain at least twice as many bits as the data. In addition, the codes should satisfy two other properties, called the initial condition and the all-zero lower triangle (AZLT) property, for the realization of a delay-insensitive comparison-based decoder. The paper shows that comparison-based decoders for codes that have the requisite level of redundancy and that satisfy the two properties can be implemented using asynchronous logic.
Analysis of an adaptive SIC for near-far resistant DS-CDMA
A new multiuser detection scheme is proposed which employs adaptive minimum mean square error (MMSE) detection in combination with successive interference cancellation (SIC). Through theoretical analysis and numerical examples, it is shown that the proposed detector provides superior performance to existing ones in terms of asymptotic multiuser efficiency (AME) and bit error rate (BER).
Rex: A randomized EXclusive region based scheduling scheme for mmWave WPANs with directional antenna
Millimeter-wave (mmWave) transmissions are promising technologies for high data rate (multi-Gbps) Wireless Personal Area Networks (WPANs). In this paper, we first introduce the concept of exclusive region (ER) to allow concurrent transmissions to explore the spatial multiplexing gain of wireless networks. Considering the unique characteristics of mmWave communications and the use of omni-directional or directional antennae, we derive the ER conditions which ensure that concurrent transmissions can always outperform serial TDMA transmissions in a mmWave WPAN. We then propose REX, a randomized ER based scheduling scheme, to decide a set of senders that can transmit simultaneously. In addition, the expected number of flows that can be scheduled for concurrent transmissions is obtained analytically. Extensive simulations are conducted to validate the analysis and demonstrate the effectiveness and efficiency of the proposed REX scheduling scheme. The results should provide important guidelines for future deployment of mmWave based WPANs.
Year 5 pupils reading an interactive storybook on CD-ROM: losing the plot?
The use of “interactive storybooks” in the primary classroom may facilitate small group and individual reading with minimal teacher intervention. This small-scale study examines whether small groups of Year 5 pupils, without teacher supervision, progress linearly through an “interactive storybook” and whether such diversions as cued animations affect pupil comprehension. The study finds that more intensive choice of diversions affects some pupils' comprehension.
Using texture mapping with mipmapping to render a VLSI layout
This paper presents a method of using texture mapping with mipmapping to render a VLSI layout.  Texture mapping is used to save already rasterized areas of the layout from frame to frame, and to take advantage of any hardware accelerated capabilities of the host platform. Mipmapping is used to select which textures to display so that the amount of information sent to the display is bounded, and the image rendered on the display is filtered correctly.  Additionally, two caching schemes are employed.  The first, used to bound memory consumption, is a general purpose cache that holds textures spatially close to the user's current viewpoint. The second, used to speed up the rendering process, is a cache of heavily used sub-designs that are precomputed so rasterization on the fly is not necessary.  An experimental implementation shows that real-time navigation can be achieved on arbitrarily large designs. Results also show how this technique ensures that image quality does not degrade as the number of polygons drawn increases, avoiding the aliasing artifacts common in other layout systems.
Matrix inversion on CPU–GPU platforms with applications in control theory
In this paper we tackle the inversion of large-scale dense matrices via conventional matrix factorizations (LU, Cholesky, LDL T ) and the Gauss-Jordan method on hybrid platforms consisting of a multi-core CPU and a many-core graphics processor (GPU). Specifically, we introduce the different matrix inversion algorithms using a unified framework based on the notation from the FLAME project; we develop hybrid implementations for those matrix operations underlying the algorithms, alternative to those in existing libraries for singleGPU systems; and we perform an extensive experimental study on a platform equipped with state-of-the-art general-purpose architectures from Intel and a “Fermi” GPU from NVIDIA that exposes the efficiency of the different inversion approaches. Our study and experimental results show the simplicity and performance advantage of the GJE-based inversion methods, and the difficulties associated with the symmetric indefinite case.
Opportunistic Relaying in In-Home PLC Networks
We consider the use of a relay to provide capacity improvements and range extension for in-home power line communication networks. In particular, we focus on opportunistic relaying where the relay is exploited only if it provides improved capacity w.r.t. the use of direct transmission between the source and the destination. The relay applies a decode and forward scheme and the channel is shared in a time division multiple access mode. The performance is studied in statistically representative in-home power line communication (PLC) networks via the use of a statistical topology model together with the application of transmission line theory for the computation of the channel transfer function among network nodes. The statistical topology model allows determining the capacity improvements as a function of the relay position. Furthermore, we determine the optimal time slot duration for each considered relay configuration, as well as we propose the use of a globally optimal time slot duration that maximizes the average network capacity. The numerical results show that significant capacity improvement can be obtained via opportunistic relaying in in-home PLC networks. The gains are more significant for low SNR scenarios and for networks composed by sub-networks each connected to the main panel via a circuit breaker that introduces signal attenuation.
SPEECH OGLE: Indexing Uncertainty for Spoken Document Search
The paper presents the Position Specific Posterior Lattice (PSPL), a novel lossy representation of automatic speech recognition lattices that naturally lends itself to efficient indexing and subsequent relevance ranking of spoken documents.In experiments performed on a collection of lecture recordings --- MIT iCampus data --- the spoken document ranking accuracy was improved by 20% relative over the commonly used baseline of indexing the 1-best output from an automatic speech recognizer.The inverted index built from PSPL lattices is compact --- about 20% of the size of 3-gram ASR lattices and 3% of the size of the uncompressed speech --- and it allows for extremely fast retrieval. Furthermore, little degradation in performance is observed when pruning PSPL lattices, resulting in even smaller indexes --- 5% of the size of 3-gram ASR lattices.
Performance evaluation for different suggested wireless ATM systems deployed at a wireless channel with abrupt fading conditions
In this paper we suggest and compare the performance of different systems that can be used for wireless asynchronous transfer mode (WATM) to improve performance in terms of cell loss ratio and throughput under a certain fading wireless channel for a certain period of time. The type of fading considered in this paper is a result of a sudden and a sharp change in the signal level due to a sudden change in the weather condition (e.g. lightning) or due to a multipath effect caused by a mobile obstacle. It was found that compressing the ATM cell header and then applying the Bose-Chaudhuri-Hocquenghem BCH (i+18,i,3) code to the compressed header during the fading period, when the bit error rate ranges between 10/sup -3/ and 10/sup -1/, results in an optimum performance in terms of cell loss ratio and throughput.
i-Miner: a Web usage mining framework using hierarchical intelligent systems
Recently Web mining has become a hot research topic, which combines two of the prominent research areas comprising of data mining and the World Wide Web (WWW). Web usage mining attempts to discover useful knowledge from the secondary data obtained from the interactions of the users with the Web. Web usage mining has become very critical for effective Web site management, business and support services, personalization, network traffic flow analysis and so on. Our previous study on Web usage mining using a concurrent neuro-fuzzy approach has shown that the usage trend analysis very much depends on the performance of the clustering of the number of requests. In this paper, a novel approach 'intelligent-miner' (i-Miner) is introduced to optimize the concurrent architecture of a fuzzy clustering algorithm (to discover data clusters) and a fuzzy inference system to analyze the trends. In the concurrent neuro-fuzzy approach, self-organizing maps were used to cluster the Web user requests. A hybrid evolutionary FCM approach is proposed in this paper to optimally segregate similar user interests. The clustered data is then used to analyze the trends using a Takagi-Sugeno fuzzy inference system learned using a combination of evolutionary algorithm and neural network learning. Empirical results clearly shows that the proposed technique is efficient with lesser number of if-then rules and improved accuracy at the expense of complicated algorithms and extra computational cost.
Preceding car tracking using belief functions and a particle filter
This article presents a preceding car rear view tracking algorithm which utilizes a particle filter and belief function data fusion. Most of tracking applications resort to only one source of information, making the system dependent on the source reliability. To achieve more robust and longer tracking, multiple source data fusion is a solution. Belief functions are a powerful tool for data fusion. Using bridges between probability theory and belief function theory, data fusion information can be incorporated inside a particle filter. The efficiency of the proposed method is demonstrated on natural on-road sequences.
Performance Analysis for BICM Transmission over Gaussian Mixture Noise Fading Channels
Bit-interleaved coded modulation (BICM) has been adopted in many systems and standards for spectrally efficient coded transmission. The analytical evaluation of BICM performance parameters, in particular bit-error rate (BER), has received considerable attention in the recent past. In this paper, we derive BER approximations for BICM transmission over general fading channels impaired by Gaussian mixture noise (GMN). To this end, we build upon the saddlepoint approximation of the pairwise error probability (PEP) and a recently established approximation for the probability density function (PDF) of bit-wise reliability metrics for nonfading additive white Gaussian noise (AWGN) channels. We extend this PDF approximation to the case of GMN, and obtain closed-form expressions for its Laplace transform for fading GMN channels. The latter allows us to express the PEP and thus BER via the saddlepoint approximation. For the special case of fading AWGN channels the presented approximations are closed form, since the saddlepoint is well approximated by 1/2 for BICM decoding. Furthermore, we derive closed-form PEP expressions also for GMN channels in the high signal-to-noise ratio regime and establish the diversity and coding gain for BICM transmission over fading GMN channels. Selected numerical results for the BER of convolutional coded BICM highlight the usefulness of the proposed approximations and the differences between AWGN and GMN channels.
A Staggered FEC System for Seamless Handoff in Wireless LANs: Implementation Experience and Experimental Study
We report the implementation experience and experimental evaluation of a staggered adaptive forward error correction (FEC) system for video multicast over wireless LANs. In the system, the parity packets generated by a cross-packet FEC code are transmitted at a time delay from the original video packets, i.e. staggercasting video stream and FEC stream in different multicast groups. The delay provides temporal diversity to improve the robustness of video multicast, especially to enable the clients to correct burst packet loss using FEC and to achieve seamless handoff. A wireless client dynamically joins the FEC multicast groups based upon its channel conditions and handoff events. We have implemented the system including the streaming server and client proxy. A novel software architecture is designed to integrate the FEC functionality in the clients without requirement for changing the existing video player software. We conduct extensive experiments to investigate the impact of FEC overhead and the delay between the video stream and FEC stream to the video quality under different interference levels and mobile handoff durations. The efficacy of staggered adaptive FEC system on improving video multicast quality is demonstrated in real system implementation.
An adaptive feedforward compensation algorithm for active vibration control
Adaptive feedforward broadband vibration (or noise) compensation is currently used when an image of the disturbance is available. However in most of the systems there is a “positive” feedback coupling between the compensator system and the measurement of the image of the disturbances. The paper proposes a new algorithm taking in account this coupling effect and the corresponding analysis. The algorithm has been applied to an active vibration control (AVC) system and real time results are presented.
Explaining robust additive utility models by sequences of preference swaps
As decision-aiding tools become more popular everyday—but at the same time more sophisticated—it is of utmost importance to develop their explanatory capabilities. Some decisions require careful explanations, which can be challenging to provide when the underlying mathematical model is complex. This is the case when recommendations are based on incomplete expression of preferences, as the decision-aiding tool has to infer despite this scarcity of information. This step is key in the process but hardly intelligible for the user. The robust additive utility model is a necessary preference relation which makes minimal assumptions, at the price of handling a collection of compatible utility functions, virtually impossible to exhibit to the user. This strength for the model is a challenge for the explanation. In this paper, we come up with an explanation engine based on sequences of preference swaps, that is, pairwise comparison of alternatives. The intuition is to confront the decision maker with “elementary” comparisons, thus building incremental explanations. Elementary here means that alternatives compared may only differ on two criteria. Technically, our explanation engine exploits some properties of the necessary preference relation that we unveil in the paper. Equipped with this, we explore the issues of the existence and length of the resulting sequences. We show in particular that in the general case, no bound can be given on the length of explanations, but that in binary domains, the sequences remain short.
Extending the Applicability of Recommender Systems: A Multilayer Framework for Matching Human Resources
Recommender systems (RS) so far have been applied to many fields of e-commerce in order to assist users in finding the products that best meet their preferences. However, while the application of RS to the search for objects is well established, this is not the case for the search for subjects. This is astonishing as a growing number of people make personal and professional information digitally available to others by managing profiles in CV databases, social networking platforms and other online services. In order to address this new field of application for RS, we integrate own prior research into a unified multilayer framework supporting the matching of individuals for recruitment and team staffing processes. By this means we enhance RS research and make a next step towards the development of empirically and theoretically grounded decision support for the human resources function
Multicast for small conferences
This paper describes a concept to support scalable multicast communications for small audio/video conferencing groups on the Internet. The solution presented in this paper is based on extensions of IPv6 and the session description protocol (SDP). A goal of the concept called multicast for small conferences (MSC) is the smooth deployment in the Internet.
Transformational Construction of Correct Pointer Algorithms
This paper shows how to use the transformation of Paterson and Hewitt to improve the memory and operations used in a pointer algorithm. That transformation scheme normally is only of theoretical interest because of the inefficient performance of the transformed function. However we present a method how it can be used to decrease the amount of selective updates in memory while preserving the original runtime performance. This leads to a general transformation framework for the derivation of a class of pointer algorithms.
Network and content aware information management
The presented approach addresses the problem of query and persistent query (subscription) resolution, taking into consideration distribution across multi-domains, network infrastructure and content management. This approach is particularly suitable for information-centric and cloud computing applications based around a mobile-device infrastructure.
Outsourcing Decisions of Small and Medium-Sized Enterprises: A Multiple-Case Study Approach in the German Software Industry
Outsourcing of software development tasks has become a major issue for large software enterprises over the last decades. Nowadays, small and medium-sized enterprises (SMEs) follow this trend and outsource parts of their software development as well. However, most of the existing literature deals with large enterprises whereas the situation of SMEs is being neglected. Especially sourcing decisions and the organizational as well as operational setup may differ between large enterprises and SMEs. We choose an exploratory multiple-case study approach focusing on the German software market in order to shed light on these aspects of the sourcing behavior of SMEs. This paper addresses three complementing research questions. Besides the question of which phases of the software development process qualify for outsourcing, we explore the organizational setup of SMEs' outsourcing scenarios. In addition, we seek to find out which characteristics a software component has to fulfill in order to qualify for outsourcing.
Efficient incremental analysis of on-chip power grid via sparse approximation
In this paper, a new sparse approximation technique is proposed for incremental power grid analysis. Our proposed method is motivated by the observation that when a power grid network is locally updated during circuit design, its response changes locally and, hence, the incremental "change" of the power grid voltage is almost zero at many internal nodes, resulting in a unique sparse pattern. An efficient Orthogonal Matching Pursuit (OMP) algorithm is adopted to solve the proposed sparse approximation problem. In addition, several numerical techniques are proposed to improve the numerical stability of the proposed solver, while simultaneously maintaining its high efficiency. Several industrial circuit examples demonstrate that when applied to incremental power grid analysis, our proposed approach achieves up to 130× runtime speed-up over the traditional Algebraic Multi-Grid (AMG) method, without surrendering any accuracy.
LCARS: the next generation programming context
In this paper, we present a high-level graphical language to develop pervasive applications based on a unique interface design. The language supports a wide range of programming constructs. Its graphical notation is based on the LCARS design, which is appealing to different target groups, based on their specific interests and requirements. We show that users can easily create pervasive applications using an LCARS-based user interface. The first step is to describe the technical context in which the application will execute. Based on this technical context, the UI offers a context-specific set of visual primitives. By composing these visual primitives on the screen, the user can specify the behavior of the application.
Hierarchical Part-Template Matching for Human Detection and Segmentation
Local part-based human detectors are capable of handling partial occlusions efficiently and modeling shape articulations flexibly, while global shape template-based human detectors are capable of detecting and segmenting human shapes simultaneously. We describe a Bayesian approach to human detection and segmentation combining local part-based and global template-based schemes. The approach relies on the key ideas of matching a part-template tree to images hierarchically to generate a reliable set of detection hypotheses and optimizing it under a Bayesian MAP framework through global likelihood re-evaluation and fine occlusion analysis. In addition to detection, our approach is able to obtain human shapes and poses simultaneously. We applied the approach to human detection and segmentation in crowded scenes with and without background subtraction. Experimental results show that our approach achieves good performance on images and video sequences with severe occlusion.
Using Google Drive to facilitate a blended approach to authentic learning
Abstract#R##N##R##N#While technology has the potential to create opportunities for transformative learning in higher education, it is often used to merely reinforce didactic teaching that aims to control access to expert knowledge. Instead, educators should consider using technology to enhance communication and provide richer, more meaningful platforms for the social construction of knowledge. By using technology to engage in shared learning experiences that extend beyond the walls of the classroom, we can create opportunities to develop the patterns of thinking that students need to participate in complex, real world situations. We used authentic learning as a framework to guide the implementation of a case-based, blended module in a South African physiotherapy department. Google Drive was used as a collaborative online authoring environment in which small groups of students used clinical cases to create their own content, guided by a team of facilitators. This paper describes an innovative approach to clinical education using authentic learning as a guiding framework, and Google Drive as an implementation platform. We believe that this approach led to the transformation of student learning practices, altered power relationships in the classroom and facilitated the development of critical attitudes towards knowledge and authority.
Quantitative Comparison of the Error-Containment Capabilities of a Bus and a Star Topology in CAN Networks
There has been an increasing interest in using star topologies in field-bus communications, e.g., in Time Triggered Protocol for SAE classC applications (TTP/C), FlexRay, or controller area network (CAN), due to increased fault resilience and potential error-containment advantages. In this context, an innovative CAN-compliant star topology, CANcentrate, has been developed, whose hub includes enhanced fault-treatment mechanisms. However, despite this interest toward stars, it is still necessary to quantify their real dependability benefits. For this purpose and for the particular case of CAN, this paper presents models for the dependability features of CAN and CANcentrate using Stochastic Activity Networks (SANs). It quantitatively compares their reliability and error-containment capabilities under permanent hardware faults. These models rely on assumptions that ensure that results are not biased toward CANcentrate, which, in some cases, is too detrimental for it. Thus, despite not reflecting the full CANcentrate potential, results quantitatively confirm the improvement of error-containment it achieves over CAN. Additionally, the way in which the nodes' ability to contain their own errors affects the relevance of using a star topology has been quantified. Although this paper refers to the case of CAN, conclusions regarding the justification of using a star topology depending on this ability can be extrapolated to other field-bus technologies.
Analysis and Design of Single-Stage AC/DC $LLC$ Resonant Converter
Analysis and design of a single-stage  LLC  resonant converter are proposed. A single-stage converter uses only one control signal to drive two power converters, a power factor corrector (PFC) converter and a dc/dc converter, for reducing the cost of the system. However, this simplicity induces power imbalance between two converters, and then, the bus voltage between two converters drifts and becomes unpredictable. To ensure that the bus capacitor voltage can be kept in a tolerable region, the characteristics of a PFC converter and an  LLC  tank are investigated, and then, a design procedure is proposed correspondingly. Finally, a single-stage  LLC  resonant converter is implemented to verify the analysis.
Agent-Based Petroleum Offshore Monitoring Using Sensor Networks
This paper investigates the architecture and design of agent-based sensor networks for petroleum offshore monitoring. A few challenges to monitor the reservoir, wellbore and wellhead are identified. Moreover, the necessary components for a reliable, precise, and accurate monitoring are suggested. The paper describes the architecture of the routing agent and discusses the cross layer optimization issues for query processing. The paper also provides the software design and components for a web-based continuous monitoring application.
The Study of 3D Reconstruction Method Based on Dynamic Threshold Method and Improved Ray Casting Algorithm
This article carries on a new method which can improve quality and speed of 3D visualization. This method, first based on dynamic threshold method, divides region of interest ROI from the original image; then extends bounding box algorithm to 3D space, and combines it with ray casting algorithm. We validate the validity and robustness of this method in 3D reconstruction, experimenting with 3D lung parenchyma's image, vascular image and bones' image, which used to be 2D medical images about chest CT.
CAE-L: An Ontology Modelling Cultural Behaviour in Adaptive Education
The presentation of learning materials in Adaptive Education Hypermedia is influenced by several factors such as learning style, background knowledge and cultural background, to name a few. In this paper, we introduce the notion of the CAE-L Ontology for modelling stereotype cultural artefacts in adaptive education. The Ontology design is based on the user study gathered from the respondents to the CAE questionnaire which determines the cultural artefacts that influence a learner?s behaviour within an educational environment. We present a brief overview of the implementation and discuss the stereotype presentation styles from three different countries, namely China, Ireland and UK.
Solar powered unmanned aerial vehicle for continuous flight: Conceptual overview and optimization
An aircraft that is capable of continuous flight offers a new level of autonomous capacity for unmanned aerial vehicles. We present an overview of the components and concepts of a small scale unmanned aircraft that is capable of sustaining powered flight without a theoretical time limit. We then propose metrics that quantify the robustness of continuous flight achieved and optimization criteria to maximize these metrics. Finally, the criteria are applied to a fabricated and flight tested small scale high efficiency aircraft prototype to determine the optimal battery and photovoltaic array mass for robust continuous flight.
Measuring Oscillating Walking Paths with a LIDAR
This work describes the analysis of different walking paths registered using a Light Detection And Ranging (LIDAR) laser range sensor in order to measure oscillating trajectories during unsupervised walking. The estimate of the gait and trajectory parameters were obtained with a terrestrial LIDAR placed 100 mm above the ground with the scanning plane parallel to the floor to measure the trajectory of the legs without attaching any markers or modifying the floor. Three different large walking experiments were performed to test the proposed measurement system with straight and oscillating trajectories. The main advantages of the proposed system are the possibility to measure several steps and obtain average gait parameters and the minimum infrastructure required. This measurement system enables the development of new ambulatory applications based on the analysis of the gait and the trajectory during a walk.
Identification of Normalised Coprime Plant Factors from Closed-loop Experimental Data
Recently introduced methods of iterative identification and control design are directed towards the design of high performing and robust control systems. These methods show the necessity of identifying approximate models from closed loop plant experiments. In this paper a method is proposed to approximately identify normalized coprime plant factors from closed loop data. The fact that normalized plant factors are estimated gives specific advantages both from an identification and from a robust control design point of view. It will be shown that the proposed method leads to identified models that are specifically accurate around the bandwidth of the closed loop system. The identification procedure fits very naturally into a recently developed the iterative identification/control design scheme based on H∞ robustness optimization.
Current Situation of Digitalized Ship Navigation System for Safety
Automatic Identification System (AIS) has to be on board in every ship over 500 gross tonnages (GT) in Japan from July 1, 2008. This is a response of amendment of maritime relating laws in accordance with SOLAS (International Convention for the Safety of Life at Sea) Convention and IMO (International Maritime Organization) requires all ships over 500 GT or upward are fitted with ship borne AIS. AIS is a system which makes it possible to get precise on-line information from a large area about ships and their movements. AIS is based on a ship borne radio wave of VHF, it continuously and automatically transmits fixed, dynamic and voyage-related information and receives corresponding information from other ships near by. VTS (Vessel Transport Service) is provided by collection of AIS information from ships. Most of the VTS stations around the area of Gulf of Finland have equipped with coastal surveillance system with radar for early warnings to endangering ships. Combinations of AIS and VTS, in addition, practical lane-separation system have also been activated under international agreement of Russia, Estonia and Finland. These three systems are well combined and organised then significant effects of reducing risks of ship borne accidents have been observed. Practices of Gulf of Finland should be introduced other congested sea lanes.
Inference of gene predictor set using Boolean satisfiability
The inference of gene predictors in the gene regulatory network (GRN) has become an important research area in the genomics and medical disciplines. Accurate predicators are necessary for constructing the GRN model and to enable targeted biological experiments that attempt to validate or control the regulation process. In this paper, we implement a SAT-based algorithm to determine the gene predictor set from steady state gene expression data (attractor states). Using the attractor states as input, the states are ordered into attractor cycles. For each attractor cycle ordering, all possible predictors are enumerated and a conjunctive normal form (CNF) expression is generated which encodes these predictors and their biological constraints. Each CNF is solved using a SAT solver to find candidate predictor sets. Statistical analysis of the resulting predictor sets selects the most likely predictor set of the GRN, corresponding to the attractor data. We demonstrate our algorithm on attractor state data from a melanoma study [1] and present our predictor set results.
Integration of Chemical and Visual Sensors for Identifying an Odor Source in Near Shore Ocean Conditions
This paper develops new multiple sensor-based algorithms for identifying a chemical odor source in a near-shore and ocean environment via an autonomous underwater vehicle (AUV). Those algorithms implement two modules: source declaration and source verification, which are embedded in a subsumption architecture for chemical plume tracing (CPT). The source declaration module is based on chemical events detected by a chemical sensor, in combination with measured vehicle locations and fluid flow directions, while the source verification module uses a fuzzy color segmentation algorithm to process an image taken when the odor source is declared.
HIVSetSubtype: software for subtype classification of HIV-1 sequences
An automated web based tool for assigning HIV-1 pure and recombinant subtypes within unaligned sequences is presented. The system combines the BLAST search algorithm and the recombination identification program for genetic subtyping of HIV-1. The software was validated through combined analysis of simulated and other HIV-1 real data.
Robust intensity-based 3D-2D registration of ct and X-ray images for precise estimation of cup alignment after total hip arthroplasty
The widely used procedure of evaluation of cup orientation following THA using single standard anteroposterior radiograph is known inaccurate, largely due to the wide variability in individual pelvic position relative to X-ray plate. 3D-2D image registration methods have been introduced to estimate the transformation between a CT volume and the radiograph for an accurate estimation of the cup orientation relative to an anatomical reference extracted from the CT data. However, the robustness of these methods is questionable. This paper presents a robust image similarity measure which is derived from a variational approximation to mutual information and allows for incorporation of spatial information via energy minimization. Experimental results on estimating cup alignment from single X-ray radiograph with gonadal shielding demonstrate the robustness and the accuracy of the present approach.
Image Denoising with Shrinkage and Redundant Representations
Shrinkage is a well known and appealing denoising technique. The use of shrinkage is known to be optimal for Gaussian white noise, provided that the sparsity on the signals representation is enforced using a unitary transform. Still, shrinkage is also practiced successfully with nonunitary, and even redundant representations. In this paper we shed some light on this behavior. We show that simple shrinkage could be interpreted as the first iteration of an algorithm that solves the basis pursuit denoising (BPDN) problem. Thus, this work leads to a novel iterative shrinkage algorithm that can be considered as an effective pursuit method. We demonstrate this algorithm, both on synthetic data, and for the image denoising problem, where we learn the image prior parameters directly from the given image. The results in both cases are superior to several popular alternatives.
Global tracking for robot manipulators using a simple causal pd controller plus feedforward
This paper shows that a well-known causal PD controller plus feedforward solves the global output feedback tracking control problem of robot manipulators, by requiring only the existence of the robot natural damping, no matter how small. To this end, we first demonstrate that a robot controlled by a causal PD is globally input-to-state stable (ISS) with respect to a bounded input disturbance. Then, we prove that the addition of a feedforward compensation renders the error system uniformly globally asymptotically stable. Furthermore, we present a possible extension to more general nonlinear systems and also to uncertain systems.
Prototype-oriented development of high-performance systems
We discuss the problem of developing performance-oriented software and the need for methodologies. We then present the EDPEPPS (Environment for Design and Performance Evaluation of Portable Parallel Software) approach to the problem of designing and evaluating high-performance (parallel) applications. The EDPEPPS toolset is based on a rapid prototyping philosophy, where the designer synthesises a model of the intended software which may be simulated, and the performance is subsequently analysed using visualisation. The toolset combines a graphical design tool, a simulation facility, and a visualisation tool. The same design is used to produce a code suitable for simulation and real execution.
A Strategy for Vision-Based Controlled Pushing of Microparticles
In this paper, a strategy for controlled pushing is presented for microassembly of 4.5 mum polystyrene particles on a flat glass substrate using an atomic force microscope probe tip. Real-time vision based feedback from a CCD camera mounted to a high resolution optical microscope is used to track particle positions relative to the tip and target position. Tip-particle system is modeled in 2D as a nonholonomic differential drive robot. Effectiveness of the controller is demonstrated through experiments performed using a single goal position as well as linking a series of target positions to form a single complex trajectory. Cell decomposition and wavefront expansion algorithms are implemented to autonomously locate a navigable path to a specified target position. Control strategy alleviates problem of slipping and spinning during pushing.
Crowd character complexity on Big Hero 6
On Disney's  Big Hero 6 , we needed to create the city of  San Fransokyo  with unparalleled levels of visual complexity. The cityscape has more buildings and more geometry than any prior Disney film. Inhabiting this city are hundreds of unique characters, each performing a high caliber of animation individually and as a group. These challenges prompted a major upgrade to our existing crowd pipeline and the development of several new technologies in authoring crowd characters, generating crowd animation cycles, and instancing crowds for rendering.
Evaluating real-time Java features and performance for real-time embedded systems
This paper provides two contributions to the study of programming languages and middleware for real-time and embedded applications. First, we present the empirical results from applying the RTJPerf benchmarking suite to evaluate the efficiency and predictability of several implementations of the real-time specification for Java (RTSJ). Second, we describe some of the techniques used to develop jRate, which is an open-source ahead-of-time-compiled implementation of RTSJ we are developing. Our results indicate that RTSJ implementations are maturing to the point where they can be applied to a variety of real-time embedded applications.
Biologically inspired design principles for Scalable, Robust, Adaptive, Decentralized search and automated response (RADAR)
Distributed search problems are ubiquitous in Artificial Life (ALife). Many distributed search problems require identifying a rare and previously unseen event and producing a rapid response. This challenge amounts to finding and removing an unknown needle in a very large haystack. Traditional computational search models are unlikely to find, nonetheless, appropriately respond to, novel events, particularly given data distributed across multiple platforms in a variety of formats and sources with variable and unknown reliability. Biological systems have evolved solutions to distributed search and response under uncertainty. Immune systems and ant colonies efficiently scale up massively parallel search with automated response in highly dynamic environments, and both do so using distributed coordination without centralized control. These properties are relevant to ALife, where distributed, autonomous, robust and adaptive control is needed to design robot swarms, mobile computing networks, computer security systems and other distributed intelligent systems. They are also relevant for searching, tracking the spread of ideas, and understanding the impact of innovations in online social networks. We review design principles for Scalable Robust, Adaptive, Decentralized search with Automated Response (Scalable RADAR) in biology. We discuss how biological RADAR scales up efficiently, and then discuss in detail how modular search in the immune system can be mimicked or built upon in ALife. Such search mechanisms are particularly useful when components have limited capacity to communicate and when physical distance makes communication more costly.
Combining Grid and Cloud Resources by Use of Middleware for SPMD Applications
Distributed computing environments have evolved from in-house clusters to Grids and now Cloud platforms. We, as others, provide HPC benchmarks results over Amazon EC2 that show a lower performance of Cloud resources compared to private resources., So, it is not yet clear how much of impact Clouds will have in high performance computing (HPC). But hybrid Grid/Cloud computing may offer opportunities to increase overall applications performance, while benefiting from in-house computational resources extending them by Cloud ones only whenever needed. In this paper, we advocate the usage of Proactive, a well established middleware in the grid community, for mixed Grid/Cloud computing, extended with features to address Grid/Cloud issues with little or, no effort for application developers. We also introduce a framework, developed in the context of the Disco Grid project, based upon the Proactive middleware to couple HPC domain-decomposition SPMD applications in heterogeneous multi-domain environments. Performance results, coupling Grid and Cloud resources for the execution of such, kind of highly communicating and processing intensive applications, have shown an overhead of about 15%, which is a non-negligible value, but lower enough to consider using such environments to achieve a better cost-performance trade-off than using exclusively Cloud resources.
The evolution of a hierarchical partitioning algorithm for large-scale scientific data: three steps of increasing complexity
As scientific data sets grow exponentially in size, the need for scalable algorithms that heuristically partition the data increases. In this paper, we describe the three-step evolution of a hierarchical partitioning algorithm for large-scale spatio-temporal scientific data sets generated by massive simulations. The first version of our algorithm uses a simple top-down partitioning technique, which divides the data by using a four-way bisection of the spatio-temporal space. The shortcomings of this algorithm lead to the second version of our partitioning algorithm, which uses a bottom-up approach. In this version, a partition hierarchy is constructed by systematically agglomerating the underlying Cartesian grid that is placed on the data. Finally, the third version of our algorithm utilizes the intrinsic topology of the data given in the original scientific problem to build the partition hierarchy in a bottom-up fashion. Specifically, the topology is used to heuristically agglomerate the data at each level of the partition hierarchy. Despite the growing complexity in our algorithms, the third version of our algorithm builds partition hierarchies in less time and is able to build trees for larger size data sets as compared to the previous two versions.
Determinantal equations for secant varieties and the Eisenbud-Koh-Stillman conjecture
We address special cases of a question of Eisenbud on the ideals of secant varieties of Veronese re-embeddings of arbitrary varieties. Eisenbud's question generalizes a conjecture of Eisenbud, Koh and Stillman (EKS) for curves. We prove that set-theoretic equations of small secant varieties to a high degree Veronese re-embedding of a smooth variety are determined by equations of the ambient Veronese variety and linear equations. However this is false for singular varieties, and we give explicit counter-examples to the EKS conjecture for singular curves. The techniques we use also allow us to prove a gap and uniqueness theorem for symmetric tensor rank. We put Eisenbud's question in a more general context about the behaviour of border rank under specialisation to a linear subspace, and provide an overview of conjectures coming from signal processing and complexity theory in this context.
An Approach to Supply Simulations of the Functional Environment of ECUs for Hardware-in-the-Loop Test Systems Based on EE-architectures Conform to AUTOSAR
Today’s vehicles include a complex network of programmableelectronic control units with software components. Avehicle’s electric and electronic (EE) architecture has to bemodeled in an early design phase to evaluate design alternatives.The tool PREEvision offers possibilities to modelEE-architectures considering feature function networks,function networks, component networks as well as wiringharness and the respective mappings.The software architecture specified by AUTOSAR separateshardware dependent and hardware independent softwaremodules. This allows the mapping of hardware independentsoftware applications to different hardware platforms.Hardware-in-the-Loop (HiL) is an established technologyfor testing electronic control units (ECU) and to assurequality. HiL-test-systems (HiL-TS) simulate the ECU’sfunctional environment (car, driver, road, tires, etc.) andadditionally offers the possibility to insert logical faults aswell as electrical faults (short circuit, open load, etc.).Mostly, this HiL-simulation is individually engineered forevery single ECU.This paper introduces a concept for the automated supportof such simulations. This includes the derivation of relevantinformation from the model of the EE-architecture as wellas the portation of the AUTOSAR software architecture tothe HiL-TS. Following this concept, engineering costs canbe reduced and the quality and correctness of the simulationincreased.
Constriction of Mutual Information Based Matching-Suitable Features for SAR Image Aided Navigation
Mutual information based matching-suitable features are studied in this paper, for the selection of good matching areas with high success probability in SAR matching aided navigation system. Based on the analysis of SAR imaging and multi-source matching, four feature constructing guide lines are proposed first. Then ten candidate features are designed under mutual information measurement. Effectiveness of these features is tested and compared through experiments under real SAR data. And features, such as reference image complexity and absolute value roughness et.al., which show stable monotony and good convergence, can be use as good matching-suitable features in practices.
Blockwise zero mapping image coding
A coding algorithm of low addressing and implementation complexity is proposed. It is based on partitioning uniformly quantized wavelet coefficients into multiscale blocks with each block classified as either an all-zero block or a non-zero block. The non-zero blocks are coded by a new method called zero-mapping whose outputs are further compressed by a first-order arithmetic coder. The performance of the proposed coding algorithm compares favorably with that of some well-known coding algorithms, particularly for images with considerable high frequency components.
Demo: Distributed video coding applications in wireless multimedia sensor networks
Novel distributed video coding (DVC) architectures developed by the IBBT DVC group realize state-of-the-art video coding efficiency under stringent energy restrictions, while supporting error-resilience and scalability. Therefore, these architectures are particularly attractive for application scenarios involving low-complexity energy-constrained wireless visual sensors. This demo presents the scenarios, which are considered to be the most promising areas of integration for IBBT's DVC systems, considering feasibility and commercial applicability.
HPC-Europa: towards uniform access to European HPC infrastructures
One of the goals of the HPC-Europa project is to provide users with a Single Point of Access (SPA) to the resources of HPC centers in Europe. To this end, the HPC-Europa Portal is being built to provide transparent, uniform, flexible and intuitive user access to HPC-Europa resources. In this paper, we present a mechanism that enables end-users to transparently access the diverse services available in the HPC-Europa environment. The uniform job submission interface that uses this mechanism, utilizing the job specification description language (JSDL), is described. We also present the architecture of the SPA, based on the GridSphere portal framework. Finally, we discuss the various interoperability problems encountered, in particular those concerning job submission, security and accounting.
Modeling the faulty behaviour of digital designs using a feed forward neural network approach
Cosmic rays lead to soft errors and faulty behavior in electronic circuits. Knowing about their faulty behavior before fabrication would be helpful. This research proposes an approach for modeling the faulty behaviour of digital circuits. It could be applied in a design flow before circuit fabrication. This is achieved by extracting information about faulty behaviour of circuits from low-level models expressed in the VHDL language. Afterwards the extracted information is used to train high-level artificial neural networks models expressed in C/C++ or MATLAB TM  languages. The trained neural network models are able to replicate the behaviour of circuits in presence of faults. The methodology is based on experiments done with two benchmarks, the ISCAS-C17 and a 4-bit multiplier. Results show that the neural network approach leads to models that are more accurate than a previously reported signature generation method. For the C17, using only 30% of the dataset generated with the LIFTING fault simulator, the neural network is able to replicate the output of the circuit in presence of faults with a mean absolute modeling error below 6%.
Optimal/Near-Optimal Dimensionality Reduction for Distributed Estimation in Homogeneous and Certain Inhomogeneous Scenarios
We consider distributed estimation of a deterministic vector parameter from noisy sensor observations in a wireless sensor network (WSN). The observation noise is assumed uncorrelated across sensors. To meet stringent power and bandwidth budgets inherent in WSNs, local data dimensionality reduction is performed at each sensor to reduce the number of messages sent to a fusion center (FC). The problem of interest is to jointly design the compression matrices associated with those sensors, aiming at minimizing the estimation error at the FC. Such a dimensionality reduction problem is investigated in this paper. Specifically, we study a homogeneous environment where all sensors have identical noise covariance matrices and an inhomogeneous environment where the noise covariance matrices across the sensors have the same correlation structure but with different scaling factors. Given a total number of messages sent to the FC, theoretical lower bounds on the estimation error of any compression strategy are derived for both cases. Compression strategies are developed to approach or even attain the corresponding theoretical lower bounds. Performance analysis and simulations are carried out to illustrate the optimality and effectiveness of the proposed compression strategies.
Incentive Compatible Configuration for Wireless Multicast: A Game Theoretic Approach
Multicast and broadcast service (MBS) is a service offered by the base station (BS) to multiple receivers requesting the same information. Such a BS must be kept updated with the receivers' feedback information (e.g., packet loss rates) to configure the MBS. We propose MBS operation schemes that are dominant-strategy incentive compatible in game theory, i.e., the schemes induce dominant-strategy equilibria, where all selfish receivers reveal their true information. Moreover, the induced equilibria are Pareto efficient and max-min fair. To conclude, the proposed schemes can elicit true feedback information from the receivers, avoiding any manipulation and, thereby, ensuring an efficient and fair system operation.
Quantization, channel compensation, and energy allocation for estimation in wireless sensor networks
In clustered networks of wireless sensor motes, each mote collects noisy observations of the environment, quantizes these observations into a local estimate of finite length, and forwards them through one or more noisy wireless channels to the Cluster Head (CH). The measurement noise is assumed to be zero-mean and have finite variance. Each wireless hop is assumed to be a Binary Symmetric Channel (BSC) with a known crossover probability. We propose a novel scheme that uses dithered quantization and channel compensation to ensure that each motes' local estimate received by the CH is unbiased. The CH then fuses these unbiased local estimates into a global one using a Best Linear Unbiased Estimator (BLUE). The energy allocation problem at each mote and among different sensor motes are also discussed. Simulation results show that the proposed scheme can achieve much smaller mean square error (MSE) than two other common schemes while using the same amount of energy. The sensitivity of the proposed scheme to errors in estimates of the crossover probability of the BSC channel is studied by both analysis and simulation.
A direct algorithm to compute rational solutions of first order linear q-difference systems
We present an algorithm to compute rational function solutions to a first order system of linear q-difference equations with rational coefficients. We make use of the fact that q-difference equations bear similarity to differential equations at the point 0 and to difference equations at other points. This allows the combining of known algorithms for the differential and the difference cases. This algorithm does not require preliminary uncoupling of the given system.
Product Model Derivation by Model Transformation in Software Product Lines
Product derivation is an essential part of the Software Product Line (SPL) development process. The paperproposes a model transformation for deriving automatically a UML model of a specific product from the UML model of a product line. This work is a part of a larger project aiming to integrate performance analysis in the SPL model-driven development. The SPL source model is expressed in UML extended with two separate profiles: a "product line" profile from literature for specifying the commonality and variability between products, and the MARTE profile recently standardized by OMG for performance annotations. The automatic derivation of a concrete product model based on a given feature configuration is enabled through the mapping between features from the feature model and their realizations in the design model. The paper proposes an efficient mapping technique that aims to minimize the amount of explicit feature annotations in the UML design model of SPL. Implicit feature mapping is inferred during product derivation from the relationships between annotated and non-annotated model elements as defined in the UML metamodel and well formedness rules. The transformation is realized in the Atlas Transformation Language (ATL) and illustrated with an ecommerce case study that models structural and behavioural SPL views.
Web Browsing, Mobile Computing and Academic Performance
Students in two different courses at a major research university (one a Communication course, the other a Computer Science course) were given laptop computers with wireless network access during the course of a semester. Students’ Web browsing on these laptops (including: URLs, dates, and times) was recorded 24 hours/day, 7 days/week in a log file by a proxy server during most of a semester (about 15 weeks). For each student, browsing behavior was quantified and then correlated with academic performance. The emergence of statistically significant results suggests that quantitative characteristics of browsing behavior—even prior to examining browsing content—can be useful predictors of meaningful behavioral outcomes. Variables such as Number of browsing sessions and Length of browsing sessions were found to correlate with students’ final grades; the valence and magnitude of these correlations were found to interact with Course (i.e., whether student was enrolled in the Communication or Computer Science course), Browsing Context (i.e., setting in which browsing took place: during class, on the wireless network between classes, or at home) and Gender. The implications of these findings in relation to previous studies of laptop use in education settings are discussed.
Optimization of transceivers with bit allocation to maximize bit rate for MIMO transmission
There have been many results on designing transceivers for MIMO channels. In early results, the transceiver is designed for a given bit allocation. In this paper we will jointly design the transceiver and bit allocation for maximizing bit rate. By using a high bit rate assumption, we will see that the optimal transceiver and bit allocation can be obtained in a closed form using simple Hadamard inequality and the Poincare separation theorem. In the simulation, we will demonstrate the usefulness of the joint design. Simulation results, in which a high bit rate assumption is not used in allocating bits, show that a higher bit rate can be achieved compared to previously reported methods.
MoRePriv: mobile OS support for application personalization and privacy
Privacy and personalization of mobile experiences are inherently in conflict: better personalization demands knowing more about the user, potentially violating user privacy. A promising approach to mitigate this tension is to migrate personalization to the client, an approach dubbed  client-side personalization . This paper advocates for  operating system support  for client-side personalization and describes MoRePriv, an operating system  service  implemented in the Windows Phone OS. We argue that personalization support should be as ubiquitous as location support, and should be provided by a unified system within the OS, instead of by individual apps.   We aim to provide a solution that will stoke innovation around mobile personalization. To enable easy application personalization, MoRePriv approximates users' interests using  personae  such as technophile or business executive. Using a number of case studies and crowd-sourced user studies, we illustrate how more complex personalization tasks can be achieved using MoRePriv.   For privacy protection, MoRePriv distills sensitive user information to a coarse-grained  profile , which limits the potential damage from information leaks. We see MoRePriv as a way to increase end-user privacy by enabling client-side computing, thus minimizing the need to share user data with the server. As such, MoRePriv shepherds the ecosystem towards a better privacy stance by  nudging  developers away from today's privacy-violating practices. Furthermore, MoRePriv can be  combined  with privacy-enhancing technologies and is complimentary to recent advances in data leak detection.
Hierarchical implicit deregistration with forced registrations in 3G wireless networks
Deregistration due to the departures of mobile users from their current visiting registration area may cause significant traffic in the wireless cellular networks. In this paper, we propose a hierarchical implicit deregistration scheme with forced registration in third-generation wireless cellular networks to reduce the remote/international roaming signaling traffic when home-location registers (HLRs), gateway-location registers (GLRs), and the visitor-location registers (VLRs) form a three-level database hierarchy. In this scheme, if a mobile phone arrives and the GLR/VLR is full, a random record is deleted and the reclaimed storage is reassigned to the new arriving mobile phone. When a call arrives and the callee's record is missing in the GLR/VLR, forced registration is executed to restore the GLR/VLR record before the call-setup operation proceeds. An analytic model is proposed to carry out the performance evaluation for the proposed scheme. Our results show that the proposed scheme not only reduces the local deregistration traffic between the GLR and the VLR, but also reduces the remote/international deregistration traffic between the HLR and the GLR, especially when the ratio of the cost of the remote/international traffic between GLR and HLR to the cost of local traffic between the VLR and the GLR is high.
Global optimization of extended hand-eye calibration
This paper introduces simultaneous global optimization of both camera orientation and vehicle wheel circumference without requiring any information about the translations in the system. The main contribution are new objective function bounds to integrate this problem into a branch-and-bound parameter space search. The presented method constitutes the first guaranteed globally optimal estimator for both components of the problem with respect to a cost function based on reprojection errors. The algorithm operates directly on image measurements and does not depend on any structure and motion preprocessing to estimate camera poses. The complete system is implemented and validated on both synthetic and real automotive datasets.
Blog Popularity Mining Using Social Interconnection Analysis
Analyzing interconnections among blog communities reveals blogger behaviors that could help in assessing blog quality. A new approach to ranking blogs uses a social blog network model based on blogs' interconnection structure and a popularity ranking method, called BRank. Experiments show that the proposed method can discriminate blogs with various degrees of popularity in the blogosphere.
Do changes in movements after tool use depend on body schema or motor learning
In a recent study, Cardinali et al. (2009) showed that training the use of a tool affected kinematic characteristics of subsequent free movements (i.e., movement were slower, for instance), which they interpreted as that the use of a tool affects the body schema. The current study examined whether these results can also be explained in terms of motor learning where movement characteristics during tool use persist in the free movements. Using a different tool we replicated parts of the study of Cardinali et al: As did Cardinali et al. we found that tool use after-effects can be found in subsequent free movements. Importantly, we showed that the tooling movement was very slow compared to the free hand movement. We concluded that it can not be ruled out yet that after-effects of tool use originate from a general slowing down of movement speed that persists in free hand movements.
Discovering golden nuggets: data mining in financial application
With the increase of economic globalization and evolution of information technology, financial data are being generated and accumulated at an unprecedented pace. As a result, there has been a critical need for automated approaches to effective and efficient utilization of massive amount of financial data to support companies and individuals in strategic planning and investment decision-making. Data mining techniques have been used to uncover hidden patterns and predict future trends and behaviors in financial markets. The competitive advantages achieved by data mining include increased revenue, reduced cost, and much improved marketplace responsiveness and awareness. There has been a large body of research and practice focusing on exploring data mining techniques to solve financial problems. In this paper, we describe data mining in the context of financial application from both technical and application perspectives. In addition, we compare different data mining techniques and discuss important data mining issues involved in specific financial applications. Finally, we highlight a number of challenges and trends for future research in this area.
Fault and simple power attack resistant RSA using Montgomery modular multiplication
Side channel attacks and more specifically fault, simple power attacks, constitute a pragmatic, potent mean of braking a cryptographic algorithm like RSA. For this reason, many researchers have proposed modifications on the arithmetic operation functions required for RSA in order to thwart those attacks. However, these modifications are applied on theoretic — algorithmic level and do not necessary result in high performance RSA designs. This paper constitute the first complete attempt for an efficient design approach on a fault and simple power attack resistant RSA based on the well known, for its high performance, Montgomery multiplication algorithm. To achieve this, a fault and simple power attack resistant modular exponentiation algorithm is proposed that is based on the Montgomery modular multiplication. In order to optimize this algorithm's performance we also propose a modified version of Montgomery modular multiplication algorithm that employs value precomputation and carry save logic in all input, output and intermediate values. We introduce a hardware architecture based on the proposed Montgomery modular multiplication algorithm and use it as a building block for the design of a fault and simple power attack resistant modular exponentiation unit. This unit is optimized by taking advantage of the inherit parallelism in the proposed fault and simple power attack resistant modular exponentiation algorithm. Realizing the proposed unit in FPGA technology very advantageous results are found when compared against other well known designs even though our design bears an extra computation cost due to its fault and simple power attack resistance characteristic.
A fast and efficient algorithm to map prerequisites of landslides in sensitive clays based on detailed soil and topographical information
We present an algorithm developed for GIS-applications in order to produce maps of landside susceptibility in postglacial and glacial sediments in Sweden. The algorithm operates on detailed topographic and Quaternary deposit data. We compare our algorithm to two similar computational schemes based on a global visibility operator and a shadow-casting algorithm. We find that our algorithm produces more reliable results in the vicinity of stable material than the global visibility algorithm. We also conclude that our algorithm is more computationally efficient than the other two methods, which is important when we may want to assess the effects of uncertainty in the data by evaluating many different models. Our method also provides the possibility to take other data into account. We show how different soil types with different geotechnical properties may be modelled. Our algorithm may also take depth information, i.e. the thicknesses of the deposits into account. We thus propose that our method may be used to provide more refined maps than the overview maps in areas where more detailed geotechnical/geological data have been acquired. The efficiency of our algorithm suggests that it may replace any global visibility operators used in other applications or processing schemes of gridded map data.
Assessing the Value of Enterprise Identity Management (EIdM) Towards a Generic Evaluation Approach
The introduction of enterprise identity management systems (EldMS) in organisations is a costly and challenging endeavour, where organisations have to face various costs for the planning, the implementation, and the operation of such systems. In the planning phase it is important that organisational aspects are incorporated into the development of an enterprise identity management (EldM) solution, instead of purely focussing on the technological or financial issues. Indeed, without a proper assessment of the costs and the organisational settings (such as stakeholders, processes), companies will not see the benefit for introducing EldM as additional layer into their IT infrastructure and their business processes. This paper proposes initial ideas for a generic approach, based on the balanced scorecard, for assessing the value of investing in the introduction of EldMS. In the decision process, such an instrument can be used for decision support purposes and the planning phase on a tactical level. Furthermore, the organisational aspects are discussed and possible solutions for integrating all relevant parties into the planning process are presented.
Reactive NUMA: a design for unifying S-COMA and CC-NUMA
This paper proposes and evaluates a new approach to directory-based cache coherence protocols called  Reactive NUMA  (R-NUMA). An R-NUMA system combines a conventional CC-NUMA coherence protocol with a more-recent Simple-COMA (S-COMA) protocol. What makes R-NUMA novel is the way it dynamically reacts to program and system behavior to switch between CC-NUMA and S-COMA and exploit the best aspects of both protocols. This reactive behavior allows each node in an R-NUMA system to independently choose the best protocol for a particular page, thus providing much greater performance stability than either CC-NUMA or S-COMA alone. Our evaluation is both qualitative and quantitative. We first show the theoretical result that R-NUMA's worst-case performance is bounded within a small constant factor (i.e., two to three times) of the best of CC-NUMA and S-COMA. We then use detailed execution-driven simulation to show that, in practice, R-NUMA usually performs better than either a pure CC-NUMA or pure S-COMA protocol, and no more than 57% worse than the best of CC-NUMA and S-COMA, for our benchmarks and base system assumptions.
Robust Exponential Stability of Markovian Jump Impulsive Stochastic Cohen-Grossberg Neural Networks With Mixed Time Delays
This paper is concerned with the problem of exponential stability for a class of Markovian jump impulsive stochastic Cohen-Grossberg neural networks with mixed time delays and known or unknown parameters. The jumping parameters are determined by a continuous-time, discrete-state Markov chain, and the mixed time delays under consideration comprise both time-varying delays and continuously distributed delays. To the best of the authors' knowledge, till now, the exponential stability problem for this class of generalized neural networks has not yet been solved since continuously distributed delays are considered in this paper. The main objective of this paper is to fill this gap. By constructing a novel Lyapunov-Krasovskii functional, and using some new approaches and techniques, several novel sufficient conditions are obtained to ensure the exponential stability of the trivial solution in the mean square. The results presented in this paper generalize and improve many known results. Finally, two numerical examples and their simulations are given to show the effectiveness of the theoretical results.
The type and effect discipline
The type and effect discipline, a framework for reconstructing the principal type and the minimal effect of expressions in implicitly typed polymorphic functional languages that support imperative constructs, is introduced. The type and effect discipline outperforms other polymorphic type systems. Just as types abstract collections of concrete values, effects denote imperative operations on regions. Regions abstract sets of possibly aliased memory locations. Effects are used to control type generalization in the presence of imperative constructs while regions delimit observable side effects. The observable effects of an expression range over the regions that are free in its type environment and its type; effects related to local data structures can be discarded during type reconstruction. The type of an expression can be generalized with respect to the variables that are not free in the type environment or in the observable effect. >
Acoustic-to-articulatory inversion using an episodic memory
This paper presents a new acoustic-to-articulatory inversion method-based on an episodic memory, which is an interesting model for two reasons. First, it does not rely on any assumptions about the mapping function but rather it relies on real synchronized acoustic and articulatory data streams. Second, the memory structurally embeds the naturalness of the articulatory dynamics. In addition, we introduce the concept of generative episodic memory, which enables the production of unseen articulatory trajectories according to the acoustic signals to be inverted. The proposed memory is evaluated on the MOCHA corpus. The results show its effectiveness and are very encouraging since they are comparable to those of recently proposed methods.
Home Network Framework Based on OSGi Service Platform Using SSL Component Bundle
It becomes increasingly common to use distributed services according to development of wire/wireless network. Home network area is also based on distributed environment which use a lot of services. Various technologies already have been used in home network. However, the research of information security part is insufficient. Thus, in this paper, we enhance our previous work by applying SSL component as a bundle format. This framework includes efficient user access control based on OSGi service platform from the former researches. This paper can cover the venerability between home gateway and authorization server in existing OSGi service platform. Therefore we provide the advantages of pre-studied framework and user information protection simultaneously.
Analyzing an embedded sensor with timed automata in uppaal
An infrared sensor is modeled and analyzed in Uppaal. The sensor typifies the sort of component that engineers regularly integrate into larger systems by writing interface hardware and software.   In all, three main models are developed. In the first model, the timing diagram of the sensor is interpreted and modeled as a timed safety automaton. This model serves as a specification for the complete system. A second model that emphasizes the separate roles of driver and sensor is then developed. It is validated against the timing diagram model using an existing construction that permits the verification of timed trace inclusion, for certain models, by reachability analysis (i.e., model checking). A transmission correctness property is also stated by means of an auxiliary automaton and shown to be satisfied by the model.   A third model is created from an assembly language driver program, using a direct translation from the instruction set of a processor with simple timing behavior. This model is validated against the driver component of the second timing diagram model using the timed trace inclusion validation technique. The approach and its limitations offer insight into the nature and challenges of programming in real time.
Social Motivations To Use Gamification: An Empirical Study Of Gamifying Exercise
This paper investigates how social factors predict attitude toward gamification and intention to continue using gamified services, as well as intention to recommend gamified services. The paper employs structural equation modelling for analyses of data (n=107) gathered through a survey that was conducted among users of one of the world’s largest gamification applications for physical exercise. The results indicate that social factors are strong predictors for attitudes towards gamification, and, further, continued use intentions and intentions to recommend the related service.
A global test for groups of genes: testing association with a clinical outcome
Motivation: This paper presents a global test to be used for the analysis of microarray data. Using this test it can be determined whether the global expression pattern of a group of genes is significantly related to some clinical outcome of interest. Groups of genes may be any size from a single gene to all genes on the chip (e.g. known pathways, specific areas of the genome or clusters from a cluster analysis).#R##N##R##N#Result: The test allows groups of genes of different size to be compared, because the test gives one p-value for the group, not a p-value for each gene. Researchers can use the test to investigate hypotheses based on theory or past research or to mine gene ontology databases for interesting pathways. Multiple testing problems do not occur unless many groups are tested. Special attention is given to visualizations of the test result, focussing on the associations between samples and showing the impact of individual genes on the test result.#R##N##R##N#Availability: An R-package globaltest is available from http://www.bioconductor.org
Information Security Governance control through comprehensive policy architectures
Information Security Governance has become one of the key focus areas of strategic management due to its importance in the overall protection of the organization's information assets. A properly implemented Information Security Governance framework should ideally facilitate the implementation of (directing), and compliance to (control), Strategic level management directives. These Strategic level management directives are normally interpreted, disseminated and implemented by means of a series of information security related policies. These policies should ideally be disseminated and implemented from the Strategic management level, through the Tactical level to the Operational level where eventual execution takes place. Control is normally exercised by capturing data at the lowest levels of execution and measuring compliance against the Operational level policies. Through statistical and summarized analyses of the Operational level data into higher levels of extraction, compliance at the Tactical and Strategic levels can be facilitated. This scenario of directing and controlling defines the basis of sound Information Security Governance. Unfortunately, information security policies are normally not disseminated onto the Operational level. As a result, proper controlling is difficult and therefore compliance measurement against all information security policies might be problematic. The objective of this paper is to argue towards a more complete information security policy architecture that will facilitate complete control, and therefore compliance, to ensure sound Information Security Governance.
Darwin: customizable resource management for value-added network services
The Internet is rapidly changing from a set of wires and switches that carry packets into a sophisticated infrastructure that delivers a set of complex value-added services to end users. Services can range from bit transport all the way up to distributed value-added services like video teleconferencing, data mining, and distributed interactive simulations. Before such services can be supported in a general and dynamic manner we have to develop appropriate resource management mechanisms. These resource management mechanisms must make it possible to identify and allocate resources that meet service or application requirements, support both isolation and controlled dynamic sharing of resources across organizations sharing physical resources, and be customizable so services and applications can tailor resource usage to optimize their performance. The Darwin project is developing a set of customizable resource management mechanisms that support value-added services, In this paper we present these mechanisms, describe their implementation in a prototype system, and describe the results of a series of proof-of-concept experiments.
Block-Quantized Support Vector Ordinal Regression
Support vector ordinal regression (SVOR) is a recently proposed ordinal regression (OR) algorithm. Despite its theoretical and empirical success, the method has one major bottleneck, which is the high computational complexity. In this brief, we propose a both practical and theoretical guaranteed algorithm, block-quantized support vector ordinal regression (BQSVOR), where we approximate the kernel matrix  K  with [( K )\tilde] that is composed of  k   2  constant blocks. We provide detailed theoretical justification on the approximation accuracy of BQSVOR. Moreover, we prove theoretically that the OR problem with the block-quantized kernel matrix [( K )\tilde] could be solved by first separating the data samples in the training set into  k  clusters with kernel  k -means and then performing SVOR on the  k  cluster representatives. Hence, the algorithm leads to an optimization problem that scales only with the number of clusters, instead of the data set size. Finally, experiments on several real-world data sets support the previous analysis and demonstrate that BQSVOR improves the speed of SVOR significantly with guaranteed accuracy.
Formality, Agility, Security, and Evolution in Software Development
Combining formal and agile techniques in software development has the potential to minimize change-related problems.
Non-negative pre-image in machine learning for pattern recognition
Moreover, in order to have a physical interpretation, some constraints should be incorporated in the signal or image processing technique, such as the non-negativity of the solution. This paper deals with the non-negative pre-image problem in kernel machines, for nonlinear pattern recognition. While kernel machines operate in a feature space, associated to the used kernel function, a pre-image technique is often required to map back features into the input space. We derive a gradient-based algorithm to solve the pre-image problem, and to guarantee the non-negativity of the solution. Its convergence speed is significantly improved due to a weighted stepsize approach. The relevance of the proposed method is demonstrated with experiments on real datasets, where only a couple of iterations are necessary.
