Symmetric feedback capacity of the Gaussian interference channel to within one bit
We characterize the symmetric capacity of the two-user Gaussian interference channel with feedback to within 1 bit/s/Hz. The result makes use of a deterministic model to provide insights into the Gaussian channel. We derive a new outer bound to show that a proposed scheme can achieve the symmetric capacity to within one bit for all channel parameters. One consequence of the result is that feedback provides unbounded gain, i.e., the gain becomes arbitrarily large for certain channel parameters. It is a surprising result because feedback has been so far known to provide no gain in memoryless point-to-point channels and only power gain (bounded gain) in the multiple access channels. The gain comes from using feedback to fully exploit the side information provided by the broadcast nature of the wireless medium.
Parallel processing of spatial joins using R-trees
We show that spatial joins are very suitable to be processed on a parallel hardware platform. The parallel system is equipped with a so called shared virtual memory which is well suited for the design and implementation of parallel spatial join algorithms. We start with an algorithm that consists of three phases: task creation, task assignment and parallel task execution. In order to reduce CPU and I/O cost, the three phases are processed in a fashion that preserves spatial locality. Dynamic load balancing is achieved by splitting tasks into smaller ones and reassigning some of the smaller tasks to idle processors. In an experimental performance comparison, we identify the advantages and disadvantages of several variants of our algorithm. The most efficient one shows an almost optimal speed up under the assumption that the number of disks is sufficiently large.
A Fault-Tolerant Strategy for Improving the Reliability of Service Composition
Service composition is an important means for integrating the individual Web services for creating new value added systems that satisfy complex demands. Since, Web services exist in the heterogeneous environments on the Internet, study on how to guarantee the reliability of service composition in a distributed, dynamic and complex environment becomes more and more important. This paper proposes a service composition net(SCN) and fault-tolerant strategy to improve the reliability of service composition. The strategy consists of static strategy, dynamic strategy and exception handling mechanism, which can be used to dynamically adjust component service for achieving good reliability as well as good overall performance. SCN is adopted to model different components of service composition. The fault detection and fault recovery mechanisms are also considered. Based on the constructed model, theories of Petri nets help prove the consistency of processing states and the effectiveness of the strategy. A case study of Export Service illustrates the feasibility of proposed method.
Cybersim: geographic, temporal, and organizational dynamics of malware propagation
Cyber-infractions into a nation's strategic security envelope pose a constant and daunting challenge. We present the modular CyberSim tool which has been developed in response to the need to realistically simulate at a national level, software vulnerabilities and resulting malware propagation in online social networks. CyberSim suite (a) can generate realistic scale-free networks from a database of geo-coordinated computers to closely model social networks arising from personal and business email contacts and online communities; (b) maintains for each host a list of installed software, along with the latest published vulnerabilities; (c) allows to designate initial nodes where malware gets introduced; (d) simulates using distributed discrete event-driven technology, the spread of malware exploiting a specific vulnerability, with packet delay and user online behavior models; (e) provides a graphical visualization of spread of infection, its severity, businesses affected etc to the analyst. We present sample simulations on a national level network with millions of computers.
Preface to the special issue: Commutativity of algebraic diagrams
The problem of the commutativity of algebraic (categorical) diagrams has attracted the attention of researchers for a long time. For example, the related notion of coherence was discussed in Mac Lane's homology book Mac Lane (1963), see also his AMS presidential address Mac Lane (1976). Researchers in category theory view this problem from a specific angle, and for them it is not just a question of convenient notation, though it is worth mentioning the important role that notation plays in the development of science (take, for example, the progress made after the introduction of symbolic notation in logics or matrix notation in algebra). In 1976, Peter Freyd published the paper 'Properties Invariant within Equivalence Types of Categories' (Freyd 1976), where the central role is played by the notion of a 'diagrammatic property'. We may also recall the process of 'diagram chasing', and its applications in topology and algebra. But before we can use diagrams (and the principal property of a diagram is its commutativity), it is vital for us to be able to check whether a diagram is commutative.
Adaptive Estimation for Spectral-Temporal Characterization of Energetic Transient Events
We describe a new approach for performing pseudo-imaging of point energy sources from spectral-temporal sensor data. Pseudo-imaging, which involves the automatic localization, spectrum estimation, and identification of energetic sources, can be difficult for dim sources and/or noisy images, or in data containing multiple sources which are closely spaced such that their signatures overlap. The new approach is specifically designed for these difficult cases. It is developed within the framework of modeling field theory (MFT), a biologically-inspired neural network system that has demonstrated practical value in many diverse areas. MFT performs an efficient optimization over the space of all model parameters and mappings between image pixels and sources, or clutter. The optimized set of parameters is then used for detection, localization and identification of the multiple sources in the data. The paper includes results computed from experimental spectrometer data.
Syntax-based alignment of multiple translations: extracting paraphrases and generating new sentences
We describe a syntax-based algorithm that automatically builds Finite State Automata (word lattices) from semantically equivalent translation sets. These FSAs are good representations of paraphrases. They can be used to extract lexical and syntactic paraphrase pairs and to generate new, unseen sentences that express the same meaning as the sentences in the input sets. Our FSAs can also predict the correctness of alternative semantic renderings, which may be used to evaluate the quality of translations.
Exception diagnosis in agent-based grid computing
Diagnosing exceptions in multi-agent systems (MAS) is a complex task due to the distributed nature of the data and control in such systems. This complexity is exacerbated in open environments where independently developed autonomous agents interact with each other in order to achieve their goals. Inevitably, exceptions would occur in such MAS and these exceptions can arise at one of three levels, namely environmental, knowledge and social levels. In this paper we propose a novel exception diagnosis system that is able to analyse and detect exceptions effectively. The proposed architecture consists of specialised exception diagnosis agents called sentinel agents. The sentinel agents are equipped with knowledge of observable abnormal situations, their underlying causes, and resolution strategies associated with these causes. The sentinel agent applies a heuristic classification approach to collect related data from affected agents in order to uncover the underlying causes of the observed symptoms. We illustrate and evaluate our proposed architecture using an agent-based grid computing case study.
A hybrid global optimisation algorithm based on locally filled functions and cluster analysis
In this paper we will extend the definition of a filled function and propose a new definition of a locally filled function. The difference between the locally filled function and the classical filled function is illustrated by an example. The existence of a locally filled function is also studied in theory. Based on the locally filled function and cluster analysis technique we will present a hybrid global optimisation algorithm. The algorithm integrates the deterministic and stochastic searching techniques and has a very powerful globally searching ability. Numerical performance of the new hybrid algorithm is demonstrated by two examples about the Shubert I and Sine-Square I functions.
Multipath Aware TCP (MATCP)
On the Internet many different paths exist between each source and destination. When single path routing is used these paths can be under utilized, not used fairly or not used at all. One way to overcome this is to allow multipath routing. But when multiple paths are used TCP congestion control can be negatively affected and cause poor goodput performance due to the reordering of packets. We proposeMATCP (Multipath Aware TCP) which makes modifications to TCP that allows it to monitor and select which path it takes through the network for each flow. MATCP is compared to single path routing and is validated using extensive simulation. MATCP is found to greatly improve fairness between flows while providing equal or better utilization of links than single best path networks.
A model for mapping between printed and digital document instances
The first steps towards bridging the paper-digital divide have been achieved with the development of a range of technologies that allow printed documents to be linked to digital content and services. However, the static nature of paper and limited structural information encoded in classical paginated formats make it difficult to map between parts of a printed instance of a document and logical elements of a digital instance of the same document, especially taking document revisions into account. We present a solution to this problem based on a model that combines metadata of the digital and printed instances to enable a seamless mapping between digital documents and their physical counterparts on paper. We also describe how the model was used to develop iDoc, a framework that supports the authoring and publishing of interactive paper documents.
Bias analysis of source localization using the maximum likelihood estimator
The nonlinear nature of the source localization problem creates bias to a location estimate. The bias could play a significant role in limiting the performance of localization and tracking when multiple measurements at different instants are available. This paper performs bias analysis of the source location estimate obtained by the maximum likelihood estimator, where the positioning measurements can be TOA, TDOA, or AOA. The effect of bias to the mean-square localization error is examined and the amounts of bias introduced by the three types of measurements are contrasted.
Schedulability analysis for automated implementations of real-time object-oriented models
The increasing complexity of real time software has led to a recent trend in the use of high level modeling languages for development of real time software. One representative example is the modeling language ROOM (real time object oriented modeling), which provides features such as object orientation, state machine description of behaviors, formal semantics for executability of models, and possibility of automated code generation. However these modeling languages largely ignore the timeliness aspect of real time systems, and fail to provide any guidance for a designer to a priori predict and analyze temporal behavior. We consider schedulability analysis for automated implementations of ROOM models, based on the ObjecTime toolset. This work builds on results presented by M. Saksena (1997), where we developed some guidelines for the design and implementation of real time object oriented models. Using the guidelines, we have modified the run time system library provided by the ObjecTime toolset to make it amenable to schedulability analysis. Based on the modified toolset, we show how a ROOM model can be analyzed for schedulability, taking into account the implementation overheads and structure. The analysis is validated experimentally, first using simple periodic models, and then using a large case study of a train tilting system.
Multimodal people detection and tracking in crowded scenes
This paper presents a novel people detection and tracking method based on a multi-modal sensor fusion approach that utilizes 2D laser range and camera data. The data points in the laser scans are clustered using a novel graph-based method and an SVM based version of the cascaded AdaBoost classifier is trained with a set of geometrical features of these clusters. In the detection phase, the classified laser data is projected into the camera image to define a region of interest for the vision-based people detector. This detector is a fast version of the Implicit Shape Model (ISM) that learns an appearance codebook of local SIFT descriptors from a set of hand-labeled images of pedestrians and uses them in a voting scheme to vote for centers of detected people. The extension consists in a fast and detailed analysis of the spatial distribution of voters per detected person. Each detected person is tracked using a greedy data association method and multiple Extended Kalman Filters that use different motion models. This way, the filter can cope with a variety of different motion patterns. The tracker is asynchronously updated by the detections from the laser and the camera data. Experiments conducted in real-world outdoor scenarios with crowds of pedestrians demonstrate the usefulness of our approach.
Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model
We aim to produce predictive models that are not only accurate, but are also interpretable to human experts. Our models are decision lists, which consist of a series of if ...then...statements (e.g., if high blood pressure, then stroke) that discretize a high-dimensional, multivariate feature space into a series of simple, readily interpretable decision statements. We introduce a generative model called Bayesian Rule Lists that yields a posterior distribution over possible decision lists. It employs a novel prior structure to encourage sparsity. Our experiments show that Bayesian Rule Lists has predictive accuracy on par with the current top algorithms for prediction in machine learning. Our method is motivated by recent developments in personalized medicine, and can be used to produce highly accurate and interpretable medical scoring systems. We demonstrate this by producing an alternative to the CHADS2 score, actively used in clinical practice for estimating the risk of stroke in patients that have atrial fibrillation. Our model is as interpretable as CHADS2, but more accurate.
Stability vs. Effectiveness: Improved Sentence-Level Combination of Machine Translation Based on Weighted MBR
We describe an improved strategy to combine the outputs of machine translation on sentence-level balancing the stability and the effectiveness of the combination. The new method alternates the classical MBR-based sentence-level combination with weighted Minimum Bayes Risk (wMBR). During the calculation of the risk, we weight the hypotheses with the performance of the MT system, which is measured by the automatic evaluation metrics on the development data. In experiments, the wMBR-based method stably achieve better results than other sentence-level methods and get the best position in CWMT08 evaluation track outperforming the other word-level and sentence-level combination systems.
Joint source and channel coding using trellis coded CPM: soft decoding
Joint source and channel (JSC) coding using combined trellis coded quantization (TCQ) and continuous phase modulation (CPM) is studied. The channel is assumed to be the additive white Gaussian noise (AWGN) channel. Optimal soft decoding for JSC coding using jointly designed TCQ/CPM is studied in this paper. The soft decoder is based on the a posteriori probability (APP) algorithm for trellis coded CPM. It is shown that the systems with soft decoding outperform the systems with hard decoding especially when the systems operate at low to medium signal-to-noise ratio (SNR). Furthermore, a TCQ design algorithm for the noisy channel is developed. It has been demonstrated that the combined TCQ/CPM systems are both power and bandwidth efficient compared with the combined TCQ/TCM/8PSK systems. The novelty of this work is the use of a soft decoder and the APP algorithm for combined TCQ/CPM systems.
Google PhD Forum at PerCom 2009
Sixteen PhD students presented their research at the PhD Forum at PerCom 2009. The PhD forum offered the students an opportunity for interaction with and feedback from senior researchers in pervasive computing.
Adaptive estimation of human posture using a component-based model
To detect a human body and recognize its posture, a component-based approach is less susceptible to changes in posture and lighting conditions. This paper proposes a component-based human-body model that comprises ten components and their flexible links. Each component contains geometrical information, appearance information, and information on the links with other components. The proposed method in this paper uses hierarchical links between components of human body, so that it allows to make coarse-to-fine searches and makes human-body matching more time-efficient. To adaptively estimate the posture in change of posture and illumination, we update the component online every time a new human body is incoming.
Impact of fading wireless channel on the performance of game theoretic power control algorithms for CDMA wireless data
Our goal in this paper is to study the performance of the game-theoretic power control algorithms for wireless data introduced by Saraydar et al [1] in two realistic channels: (a1) fast flat fading channel and (a2) Slow flat fading channel. The fading coefficients under both (a1) and (a2) are studied under an appropriate small scale channel model that is used in the CDMA cellular systems, namely Nakagami channel model. To do so, we derive a closed- form expression of the average utility function which represents the number of bits received correctly at the receiver per one Joule expended. Then, using this expression we study the existence, uniqueness of Nash equilibrium (NE), and the social desirability of NE in the Pareto sense.
Guidelines for the creation of brain-compatible cyber security educational material in Moodle 2.0
Most current approaches towards information security education do not have a sound theoretical basis. This could lead to the failure of these educational programs. Furthermore, the need for information security knowledge is no longer only of concern to organizations, but has also become a concern for individuals using online services for personal entertainment, social networking, banking, and other activities. Thus, there is a need for “cyber security” education for both individuals and organizations. Such cyber security educational programs should be based on sound pedagogical theories. One such a pedagogically sound approach that could potentially play a role in cyber security educational programs is “brain compatible learning”. This paper will perform a critical evaluation of an existing information security education course, and evaluate the subject matter in terms of brain compatible learning approaches. The aim of the paper is to propose a set of brain compatible learning guidelines for the creation of cyber security educational material. The paper will also argue in favour of the use e-learning as a delivery mechanism for such content. As such, the guidelines will be proposed in the context of a Moodle 2.0 e-learning environment.
OFDM code division multiplexing with unequal error protection and flexible data rate adaptation
An OFDM-CDM (orthogonal frequency division multiplexing code division multiplexing) system with adaptive symbol mapping is presented. This combination enables a robust transmission with flexible error protection and data rate adaptation for parallel data streams by exploiting additional diversity due to CDM. Performance results are presented for fading channels where OFDM-CDM with adaptive symbol mapping and soft interference cancellation is compared to conventional OFDM systems also taking into account channel coding with variable code rates.
Real-time communication in distributed environment-real-time packet filter approach
Recent modern operating system technology enables protocol processing in user space using in-kernel packet filter and user-level protocol processing library for flexibility without sacrificing the performance of traditional kernelized protocol processing. This technology can be adapted to build a highly preemptable protocol processing mechanism for distributed real-time environment. In this paper, we discuss the structural difference of various operating systems from the protocol processing point of view, and propose an extended mechanism of packet filter and user-level protocol processing library for real-time communication. Using this mechanism, priority of the client can be handed off to the server without priority inversion problem during protocol processing.
Let's Meet at the Mobile - Learning Dialogs with a Video Conferencing Software for Mobile Devices
Mobile phones and related gadgets in networks are omnipresent at our students, advertising itself as the platform for mobile, pervasive learning. Currently, these devices rapidly open and enhance, being soon able to serve as a major platform for rich, open multimedia applications and communication. In this report we introduce a video conferencing software, which seamlessly integrates mobile with stationary users into fully distributed multi-party conversations. Following the paradigm of flexible, user-initiated group communication, we present an integrated solution, which scales well for medium-size conferences and accounts for the heterogeneous nature of mobile and stationary participants. This approach allows for a spontaneous, location independent establishment of video dialogs, which is of particular importance in interactive learning scenarios. The work is based on a highly optimized realization of a H.264 codec.
Security issues in the development of a wireless blood-glucose monitoring system
This paper is a progress report on an ongoing research effort that makes use of wireless biometric data management, specifically a wireless blood-glucose monitoring system (WBgM). The goal of this research is to realize appropriate timely intervention by health-care providers in response to records of unregulated blood-glucose level in order to maintain near-normal levels. The focus of this paper is security-their is the possibility that medical information may be used consciously by malicious eavesdropper to the detriment of a patient in search of new employment or a new insurer. There is the possibility of malicious or accidental data corruption from database intrusion. Mobile data transmission is especially problematic in preventing these. We present requirements and our approach taken to realize a secure system architecture. Key techniques and their implementation details to maintain privacy, authentication, and data integrity are discussed here.
State abstraction discovery from irrelevant state variables
Abstraction is a powerful form of domain knowledge that allows reinforcement-learning agents to cope with complex environments, but in most cases a human must supply this knowledge. In the absence of such prior knowledge or a given model, we propose an algorithm for the automatic discovery of state abstraction from policies learned in one domain for use in other domains that have similar structure. To this end, we introduce a novel condition for state abstraction in terms of the relevance of state features to optimal behavior, and we exhibit statistical methods that detect this condition robustly. Finally, we show how to apply temporal abstraction to benefit safely from even partial state abstraction in the presence of generalization error.
Radiation Genes: a database devoted to microarrays screenings revealing transcriptome alterations induced by ionizing radiation in mammalian cells
The analysis of the great extent of data generated by using DNA microarrays technologies has shown that the transcriptional response to radiation can be considerably different depending on the quality, the dose range and dose rate of radiation, as well as the timing selected for the analysis. At present, it is very difficult to integrate data obtained under several experimental conditions in different biological systems to reach overall conclusions or build regulatory models which may be tested and validated. In fact, most available data is buried in different websites, public or private, in general or local repositories or in files included in published papers; it is often in various formats, which makes a wide comparison even more difficult. The Radiation Genes Database (http://www.caspur.it/RadiationGenes) collects microarrays data from various local and public repositories or from published papers and supplementary materials. The database classifies it in terms of significant variables, such as radiation quality, dose, dose rate and sampling timing, as to provide user-friendly tools to facilitate data integration and comparison.
Divisible Load Scheduling inWireless Sensor Networks with Information Utility
Optimal data scheduling strategies in a hierarchical wireless sensor network (WSN) are considered. Data aggregation in clusterheads is considered to find a closed form solution for the optimal amount of data that is to be reported by each sensor node using a newly introduced parameter, information utility. The optimal conditions for the feasible measurement instruction assignment time and for the minimum round time are derived and examined. Based on the optimal conditions, a performance evaluation is demonstrated via simulation study.
Low-Complexity Detections for Downlink MIMO MC-CDMA Systems
In this paper, we investigate and propose the detection techniques for spatially layered multiple-input multiple-output (MIMO) multi-carrier code division multiple access (MC-CDMA) systems. First, we propose a noise-predictive linear detector. It has the same bit error rate (BER) performance as symbol-level detector and reduces the complexity significantly when the system load is almost full. We also propose a partial minimum mean square error (MMSE)-ordered successive interference cancellation (OSIC) based on multi-user detection, which first detects the most powerful interfering data symbols transmitted through the determined transmit antenna and then cancels their contribution from the received signal by the multiplexed data symbol vector. The nulling and cancelling processes between the user data symbols from the same transmit antenna are not performed. The proposed algorithms are verified by computer simulation.
Automatic synthesis and technology mapping of combinational logic
SKOL, a system for the synthesis of combinational logic using a library of cells that emphasizes technology-mapping algorithms, is described. It combines current multilevel optimization techniques with a novel approach to technology mapping. Each factor (or the factorized Boolean equation) can be implemented by itself or collapsed into the higher level expression containing it, which is then implemented. An expression can be implemented in several ways, which differ in the degree of factorization. A number of selected implementations is evaluated and the one with minimal cost (area or delay) is chosen. The mapping algorithms are independent of the library of cells, which can be easily modified. Results from benchmark examples were better than or comparable to those for existing systems. >
Multichannel watermarking of color images
In the field of image watermarking, research has been mainly focused on grayscale image watermarking, whereas the extension to the color case is usually accomplished by marking the image luminance, or by processing each color channel separately. A DCT domain watermarking technique expressly designed to exploit the peculiarities of color images is presented. The watermark is hidden within the data by modifying a subset of full-frame DCT coefficients of each color channel. Detection is based on a global correlation measure which is computed by taking into account the information conveyed by the three color channels as well as their interdependency. To ultimately decide whether or not the image contains the watermark, the correlation value is compared to a threshold. With respect to existing grayscale algorithms, a new approach to threshold selection is proposed, which permits reducing the probability of missed detection to a minimum, while ensuring a given false detection probability. Experimental results, as well as theoretical analysis, are presented to demonstrate the validity of the new approach with respect to algorithms operating on image luminance only.
What to measure next to improve decision making? On top-down task driven feature saliency
Top-down attention is modeled as decision making based on incomplete information. We consider decisions made in a sequential measurement situation where initially only an incomplete input feature vector is available, however, where we are given the possibility to acquire additional input values among the missing features. The procecure thus poses the question what to do next? We take an information theoretical approach implemented for generality in a generative mixture model. The framework allows us reduce the decision about what to measure next in a classification problem to the estimation of a few one-dimensional integrals per missing feature. We demonstrate the viability of the framework on four well-known classification problems.
Offline Handwriting Recognition using Genetic Algorithm
Handwriting Recognition enables a person to scribble something on a piece of paper and then convert it into text. If we look into the practical reality there are enumerable styles in which a character may be written. These styles can be self combined to generate more styles. Even if a small child knows the basic styles a character can be written, he would be able to recognize characters written in styles intermediate between them or formed by their mixture. This motivates the use of Genetic Algorithms for the problem. In order to prove this, we made a pool of images of characters. We converted them to graphs. The graph of every character was intermixed to generate styles intermediate between the styles of parent character. Character recognition involved the matching of the graph generated from the unknown character image with the graphs generated by mixing. Using this method we received an accuracy of 98.44%.
Spatio-spectral sufficient statistic for mental imagery EEG signals
Classification of mental tasks from electroencephalogram (EEG) signals has important applications in brain-computer interfacing (BCI). However, classification of the highly redundant and high-dimensional EEG signal, with high spatial and spectral correlations, is quite challenging. Therefore, the discriminant information, especially that of the first and second data moments, need to be extracted in the form of uncorrelated features. This work addresses this need by approximating a linear minimal-dimension sufficient statistic of the EEG matrix data in both spatial and spectral domains. As a result of the two-dimensional spatio-temporal approach and the generalized sufficiency approximation, a significant improvement on the classification accuracy is achieved.
Privacy preserving OLAP
We present techniques for privacy-preserving computation of multidimensional aggregates on data partitioned across multiple clients. Data from different clients is perturbed (randomized) in order to preserve privacy before it is integrated at the server. We develop formal notions of privacy obtained from data perturbation and show that our perturbation provides guarantees against privacy breaches. We develop and analyze algorithms for reconstructing counts of subcubes over perturbed data. We also evaluate the tradeoff between privacy guarantees and reconstruction accuracy and show the practicality of our approach.
A New Conditioning Rule, Its Generalization and Evidential Reasoning
In Evidence theory, several conditioning rules for updating belief have been proposed, including Dempster's rule of conditioning. The paper views the conditioning rules proposed so far and proposes a new rule of conditioning based on three requirements. Then, it generalizes the rule to be applied to the case where condition is given by an uncertain belief. The paper also discusses a few interpretations of an equation used for evidential reasoning, one of which is interpreted as conditioning with an uncertain condition.
Channel assignment on strongly-simplicial graphs
Given a vector (/spl delta//sub 1/, /spl delta/2,..., /spl delta//sub t/) of non increasing positive integers, and an undirected graph G = (V, E), an L(/spl delta//sub 1/, /spl delta/2,..., /spl delta//sub t/)-coloring of G is a function f from the vertex set V to a set of nonnegative integers such that |f(u) - f (v)| /spl ges/ /spl delta//sub i/, if d(u, v) = i, 1 /spl les/ i /spl les/ t, where d(u,v) is the distance (i.e. the minimum number of edges) between the vertices u and v. This paper presents efficient algorithms for finding optimal L(1,..., 1)-colorings of trees and interval graphs. Moreover, efficient algorithms are also provided for finding approximate L(/spl delta//sub 1/, 1,..., 1)-colorings of trees and interval graphs, as well as approximate L(/spl delta//sub 1/, /spl delta//sub 2/) colorings of unit interval graphs.
Decomposition methods for multihour synthesis of private telecommunication networks
The optimal synthesis of private telecommunication networks subject to multiple nonsimultaneous demands is considered. A model is given for the multiservice network synthesis model with an accurate representation of the conversion and access costs, while maintaining a classical multicommodity flow representation of the network. This is done by increasing the original network to produce an augmented network. Two decomposition methods for solving the network synthesis problem are presented. The first one is a classical Lagrangian relaxation method and the second is a resource-decomposition-type technique. Some preliminary computational results in both cases are presented. These show that the two methods converge within reasonable computation times. The results also demonstrate that the multihour aspect of the synthesis of multiservice networks should be taken into account in order to realize fully the economies that are possible due to the noncoincidence of different types of demands. >
Does Web 3.0 come after Web 2.0? Deconstructing theoretical assumptions through practice
Current internet research has been influenced by application developers and computer engineers who see the development of the Web as being divided into three different stages: Web 1.0, Web 2.0 and Web 3.0. This article will argue that this understanding – although important when analysing the political economy of the Web – can have serious limitations when applied to everyday contexts and the lived experience of technologies. Drawing from the context of the Italian student movement, we show that the division between Web 1.0, Web 2.0 and Web 3.0 is often deconstructed by activists’ media practices. Therefore, we highlight the importance of developing an approach that – by focusing on practice – draws attention to the interplay between Web platforms rather than their transition. This approach, we believe, is essential to the understanding of the complex relationship between Web developments, human negotiations and everyday social contexts.
Integrated contextual representation for objects' identities and their locations
Visual context plays a prominent role in everyday perception. Contextual information can facilitate recognition of objects within scenes by providing predictions about objects that are most likely to appear in a specific setting, along with the locations that are most likely to contain objects in the scene. Is such identity-related (semantic) and location-related (spatial) contextual knowledge represented separately or jointly as a bound representation? We conducted a functional magnetic resonance imaging (fMRI) priming experiment whereby semantic and spatial contextual relations between prime and target object pictures were independently manipulated. This method allowed us to determine whether the two contextual factors affect object recognition with or without interacting, supporting a unified versus independent representations, respectively. Results revealed a Semantic Spatial interaction in reaction times for target object recognition. Namely, significant semantic priming was obtained when targets were positioned in expected (congruent), but not in unexpected (incongruent), locations. fMRI results showed corresponding interactive effects in brain regions associated with semantic processing (inferior prefrontal cortex), visual contextual processing (parahippocampal cortex), and object-related processing (lateral occipital complex). In addition, activation in fronto-parietal areas suggests that attention and memory-related processes might also contribute to the contextual effects observed. These findings indicate that object recognition benefits from associative representations that integrate information about objects' identities and their locations, and directly modulate activation in object-processing cortical regions. Such context frames are useful in maintaining a coherent and meaningful representation of the visual world, and in providing a platform from which predictions can be generated to facilitate perception and action.
A retrieval model based on an extended modal logic and its application to the RIME experimental approach
This paper focuses on the query processing module of RIME, an experimental prototype of an intelligent information retrieval system designed to manage high-precision queries on a corpus of medical reports. Though highly specific this particular corpus is representative of an important class of applications: information retrieval among full-text specialized documents which constitute critical sources of information in several organizations (medicine, law, space industry…). This experience allowed us to design and implement an elaborate model for the semantic content of the documents which is an extension of the Conceptual Dependency approach. The underlying retrieval model is inspired from the Logic model proposed by C.J. Van Rijsbergen, which has been considerably refined using an Extended Modal Logic. After presenting the context of the RIME project, we briefly describe the models designed for the internal representation of medical reports and queries. The main part of the paper is then devoted to the retrieval model and its application to the query processing module of RIME which has a natural language interface. Processing a query involves two main phases: the interpretation which transforms the natural language query into a search expression, and the evaluation phases which retrieves the corresponding medical reports. We focus here on the evaluation phases and show its relationship with the underlying retrieval model. Evaluations from practical experiments are also given, along with indications about current developments of the project.
Non-termination sets of simple linear loops
A simple linear loop is a simple while loop with linear assignments and linear loop guards. If a simple linear loop has only two program variables, we give a complete algorithm for computing the set of all the inputs on which the loop does not terminate. For the case of more program variables, we show that the non-termination set cannot be described by Tarski formulae in general.
Tracking people with twists and exponential maps
This paper demonstrates a new visual motion estimation technique that is able to recover high degree-of-freedom articulated human body configurations in complex video sequences. We introduce the use of a novel mathematical technique, the product of exponential maps and twist motions, and its integration into a differential motion estimation. This results in solving simple linear systems, and enables us to recover robustly the kinematic degrees-of-freedom in noise and complex self occluded configurations. We demonstrate this on several image sequences of people doing articulated full body movements, and visualize the results in re-animating an artificial 3D human model. We are also able to recover and re-animate the famous movements of Eadweard Muybridge's motion studies from the last century. To the best of our knowledge, this is the first computer vision based system that is able to process such challenging footage and recover complex motions with such high accuracy.
Developing Multi-Layer Information Infrastructures: Advancing Social Innovation through Public–Private Governance
Information infrastructures of businesses and government are increasingly interwoven. The development of these information infrastructures often has a technological focus and the concurrent social innovation is ill understood. To address this gap, we study public–private information infrastructure developments at three layers over a prolonged period of time. Stakeholders have to alter existing social practices to realize the potential of information infrastructures. New social practices need to be developed and sustaining innovations requires new governance mechanisms.
Morphological segmentation of Lidar Digital Elevation Models to extract stream channels in forested terrain
Our paper proposes an approach for the extraction of stream channels from Airborne Laser Swath Mapping (ALSM) data. Recent advances in technology have led to high-resolution topographic data acquisition by means of airborne lidar (i.e. ALSM), which can yield Digital Elevation Model (DEM) datasets with horizontal resolutions of 1 m and vertical rms errors in the range of 10 - 15 cm. The extraction of a stream network from a DEM plays a fundamental role in modeling spatially distributed hydrological processes and flow routing. We apply morphological filtering to an ALSM DEM to detect and characterize stream channels in forested terrain. Since the size and shape of morphological Structuring Elements (SEs) is known to strongly affect filtered results, we test for accuracy by developing a set of error measures over simulated terrain. We subsequently apply the filter to actual ALSM data. For linking disconnected stream segments, a measure of pixel connectedness known as the Connectivity Number is used. The method presented is shown to enable systematic characterization and comparisons of streams, even in heavily forested terrain.
Interconnections technologies for VLSI circuits
Abstract#R##N##R##N#VLSI technologies led to the possibility of integration of more than a million of active devices on a single silicon chip. A significant part of the effort in the geometrical circuitry shrinkage was set in development of suitable interconnection technologies. The goals persued were the improvement of the electrical properties (conductivity and contact resistance) with the simultaneous improvement of reliability performances.
An Event Notification Service based on XML Messaging on Different Transport Technologies
With the size and increasing complexity of telecom networks, there is a need to interconnect management systems at different levels. This requires information flow across many applications in a domain independent way. XML is a widely deployed standard which is being used for integration of network management system(NMS) with other applications. We examine the performance of different transport mechanisms, JMS, CORBA, HTTP and RMI for an XML message based event Notification Service. To improve the performance of XML message based event notifications, event grouping is examined and is found to perform well.
SSIM-Based Perceptual Rate Control for Video Coding
The quality of video is ultimately judged by human eye; however, mean squared error and the like that have been used as quality metrics are poorly correlated with human perception. Although the characteristics of human visual system have been incorporated into perceptual-based rate control, most existing schemes do not take rate-distortion optimization into consideration. In this paper, we use the structural similarity index as the quality metric for rate-distortion modeling and develop an optimum bit allocation and rate control scheme for video coding. This scheme achieves up to 25% bit-rate reduction over the JM reference software of H.264. Under the rate-distortion optimization framework, the proposed scheme can be easily integrated with the perceptual-based mode decision scheme. The overall bit-rate reduction may reach as high as 32% over the JM reference software.
Adaptive compensation techniques for communications systems with Tomlinson-Harashima precoding
To improve compensation to channel or interference changes, we propose adapting an auxiliary feedback filter (FBF) in the receiver of systems which use Tomlinson-Harashima (1971, 1972) precoding. We show how the auxiliary FBF can be adapted in conjunction with the receiver feedforward filter (FFF). Simulations demonstrate the performance advantage of our auxiliary FBF technique relative to FFF updating alone, and how the FFF combines interference suppression with despreading in wideband applications. Error propagation can be effectively avoided by using the auxiliary FBF values to decide when to update the precoder, while transient increases in mean-squared error are avoided by using the FBF values in the update equation.
A Robust Audit Mechanism to Prevent Malicious Behaviors in Multi-robot Systems
Market-based mechanisms can be used to coordinate self-interested multi-robot systems in fully distributed environments, where by self-interested we mean that each robot agent attempts to maximize a payoff function that accounts for both the resources consumed and the contribution made by the robot. In previous work, we have studied the effect of various market rules and bidding strategies on the global performance of the multi-robot system. However, rather than use a central monitoring and enforcement mechanisms, we rely on agents to self-report their actions. This assumes that the agents act honestly. In this paper, we drop the honesty assumption, raising the possibility that agents may exaggerate their contribution in order to increase their payoff. To address the problem of such malicious behavior, we propose an audit mechanism to maintain the integrity of reported payoffs. Our algorithm extends previous work on preventing free-riding in peer-to-peer networks. Specifically, we consider locality and mobility in multi-robot systems. We show that our approach efficiently detects malicious behaviors with a high probability.
Interactive emotional content communications system using portable wireless biofeedback device
In this paper, we implemented an interactive emotional content communication system using a portable wireless biofeedback device to support convenient emotion recognition and immersive emotional content representation for users. The newly designed system consists of the portable wireless biofeedback device and a novel emotional content rendering system. The former performs the acquisition and transmission of three different physiological signals (photoplethysmography, skin temperature, and galvanic skin response) to the remote emotional content rendering system via Bluetooth links in real time. The latter displays video content concurrently manipulated using the feedback of the user?s emotional state. The results of effectiveness of the system indicated that the response time of the emotional content communication system was nearly instant, the changes of between emotional contents and emotional states base on physiological signals was corresponded. The user?s concentration was increased by watching the measuredemotion- based rendered visual stimuli. In the near future, the users of this proposed system will be able to create further substantial user-oriented content based on emotional changes.
Intelligent semantic question answering system
The volume of information available on the World Wide Web and the rate of its growth requires new techniques to handle and organize this data. Ontologies are becoming the pivotal methodology to represent domain-specific conceptual knowledge and hence help in providing solutions for Question Answering (QA) systems. This paper introduces an approach for enhancing the capabilities of QA systems using semantic technologies. We implemented an approach to convert the natural language user queries to Resource Description Framework (RDF) triples and find relevant answers. The experiment results show that the proposed technique works very well for single word answers. We believe that with some modifications this approach can be expanded to a wider scale.
Temporal Edges: The Detection Of Motion And The Computation Of Optical Flow
A new method for the detection of motion and the computation of optical flow is presented. In the first step of the calculation the intensity history at each pixel is convolved with the second derivative in time of a temporal Gaussian smoothing function. The zero crossings in a single frame of the resulting function indicate the positions of moving edges. Spatial and temporal derivatives of the function at the zero-crossing locations are then used to compute the component of the flow that is normal to the zero-crossing contours. Both the detection of motion and the computation of the normal velocity are insensitive to slow temporal and spatial changes in the image intensity that are caused by illumination effects rather than motion. A framework in which to relate the present work to a number of gradient based flow measurement techniques is also presented.
On Averaging Multiview Relations for 3D Scan Registration
In this paper, we present an extension of the iterative closest point (ICP) algorithm that simultaneously registers multiple 3D scans. While ICP fails to utilize the multiview constraints available, our method exploits the information redundancy in a set of 3D scans by using the averaging of relative motions. This averaging method utilizes the Lie group structure of motions, resulting in a 3D registration method that is both efficient and accurate. In addition, we present two variants of our approach, i.e., a method that solves for multiview 3D registration while obeying causality and a transitive correspondence variant that efficiently solves the correspondence problem across multiple scans. We present experimental results to characterize our method and explain its behavior as well as those of some other multiview registration methods in the literature. We establish the superior accuracy of our method in comparison to these multiview methods with registration results on a set of well-known real datasets of 3D scans.
Effects of Input Shaping on Manual Control of Flexible and Time-Delayed Systems
OBJECTIVE: The objective was to study the performance of a manual tracking task with system flexibility and time delays in the input channel and to examine the effects of input shaping the human operator's commands. BACKGROUND: It has long been known that low-frequency, lightly damped vibration hinders performance of a manually controlled system. Recently, input shaping has been shown to improve the performance of such systems in a compensatory-display tracking task. It is unknown if similar improvements are seen with pursuit-display tasks, or how the improvement changes when time delays are added to the system. METHOD: A total of 18 novice participants performed a pursuit-view tracking experiment with a spring-centered joystick. Controlled elements included an integrator, an integrator with a lightly damped flexible mode, and an input-shaped integrator with a flexible mode. The input to these controlled elements was delayed between 0 and 1 s. Tracking performance was quantified by root mean square tracking error, and subjective difficulty was quantified by ratings on a Cooper-Harper scale. RESULTS: Performance was best with the undelayed integrator. Both time delay and flexibility degraded performance. Input shaping improved control of the flexible element, with a diminishing benefit as the time delay increased. Tracking error and subjective rating were significantly related. Some operators used a pulsive control strategy. CONCLUSION: Input shaping can improve the performance of a manually controlled system with flexibility, even when time delays are present. APPLICATION: This study is useful to designers of human-controlled systems, especially those with problematic flexibility and/or time delays. Language: en
The Transformer database: biotransformation of xenobiotics
As the number of prescribed drugs is constantly rising, drug–drug interactions are an important issue. The simultaneous administration of several drugs can cause severe adverse effects based on interactions with the same metabolizing enzyme(s). The Transformer database (http://bioinformatics.charite.de/transformer) contains integrated information on the three phases of biotransformation (modification, conjugation and excretion) of 3000 drugs and >350 relevant food ingredients (e.g. grapefruit juice) and herbs, which are catalyzed by 400 proteins. A total of 100 000 interactions were found through text mining and manual validation. The 3D structures of 200 relevant proteins are included. The database enables users to search for drugs with a visual display of known interactions with phase I (Cytochrome P450) and phase II enzymes, transporters, food and herbs. For each interaction, PubMed references are given. To detect mutual impairments of drugs, the drug-cocktail tool displays interactions between selected drugs. By choosing the indication for a drug, the tool offers suggestions for alternative medications to avoid metabolic conflicts. Drug interactions can also be visualized in an interactive network view. Additionally, prodrugs, including their mechanisms of activation, and further information on enzymes of biotransformation, including 3D models, can be viewed.
Combinedwavelet Domain and Motion Compensated Filtering Compliant with Video Codecs
In this paper, we introduce the idea of using motion estimation resources from a video codec for video denoising. This is not straightforward because the motion estimators aimed for video compression and coding, tolerate errors in the estimated motion field and hence are not directly applicable to video denoising. To solve this problem, we propose a novel motion field filtering step that refines the accuracy of the motion estimates to a degree that is required for denoising. We illustrate the use of the proposed motion estimation method within a wavelet-based video denoising scheme. The resulting video denoising method is of low-complexity and receives comparable results with respect to the latest video denoising methods.
Grey-box GUI Testing: Efficient Generation of Event Sequences
Graphical user interfaces (GUIs) encode, as event sequences, potentially unbounded ways to interact with software. During testing it becomes necessary to effectively sample the GUI’s event space. Ideally, for increasing the efficiency and effectiveness of GUI testing, one would like to sample the GUI’s event space by only generating sequences that (1) are allowed by the GUI’s structure, and (2) chain together only those events that have data dependencies between their event handlers. We propose a new model, called an eventdependency graph (EDG) of the GUI that captures data dependencies between the code of event handlers. We develop a mapping between an EDG and an existing black-box model of the GUI’s structure, called an event-flow graph (EFG). We automate the EDG construction in a tool that analyzes the bytecode of each event handler. We evaluate our“grey-box”approach using four open-source applications and compare it with the EFG approach. Our results show that using the EDG reduces the number of event sequences with respect to the EFG, while still achieving at least the same coverage. Furthermore, we are able to detect 2 new bugs in the subject applications.
An automatic registration method for frameless stereotaxy, image guided surgery, and enhanced reality visualization
There is a need for frameless guidance systems to help surgeons plan the exact location for incisions, to define the margins of tumors, and to precisely identify locations of neighboring critical structures. The authors have developed an automatic technique for registering clinical data, such as segmented magnetic resonance imaging (MRI) or computed tomography (CT) reconstructions, with any view of the patient on the operating table. The authors demonstrate on the specific example of neurosurgery. The method enables a visual mix of live video of the patient and the segmented three-dimensional (3-D) MRI or CT model. This supports enhanced reality techniques for planning and guiding neurosurgical procedures and allows us to interactively view extracranial or intracranial structures nonintrusively. Extensions of the method include image guided biopsies, focused therapeutic procedures, and clinical studies involving change detection over time sequences of images.
The fastest gradient waveforms for arbitrary and optimized k-space trajectories
A method for finding the fastest possible gradient waveforms for any given k-space trajectory is presented. It is an extension of our previously introduced solution. The original scheme provides an efficient and non-iterative method for designing the fastest freely rotatable gradient waveforms. Here, the hardware constraints are relaxed so that each axis is constrained independently. This produces the fastest possible non-rotatable waveforms that can be up to 10% faster than their previous counterparts. In addition, for circular trajectories we relax the path constraints. This results in new diamond-shaped trajectories, which are more optimized than circles for separable gradient sets, reducing the total travel time by up to an additional 11%. Analysis of performance for a variety of parameters including the sensitivity to field inhomogeneity compared to freely rotatable circle trajectories is presented.
The InterAction Database: Synergy of Science and Practive in Pharmacy
In social pharmacy and pharmacoepidemiology the distribution, use and performance of medication after registration is studied. In both fields, the pharmacists are the main source of data on drug use. To increase the value of research, we think it important to exchange ideas and suggestions between scientific researchers and pharmacists who work in community pharmacies. Hence, the department of Social Pharmacy and Pharmacoepidemiology of the University of Groningen sought close collaboration with some community pharmacies in the region, resulting in the InterAction project. The pharmacists deliver data to the InterAction database and are explicitly invited to raise questions and issues from their practice, and to participate in research. Consequently, science and practice benefit from each other's input and expertise. This paper describes the architecture and contents of the InterAction project. Additionally, the first experiences with the database as a laboratory for social pharmacy and pharmacoepidemiology are discussed.
Friendship prediction and homophily in social media
Social media have attracted considerable attention because their open-ended nature allows users to create lightweight semantic scaffolding to organize and share content. To date, the interplay of the social and topical components of social media has been only partially explored. Here, we study the presence of homophily in three systems that combine tagging social media with online social networks. We find a substantial level of topical similarity among users who are close to each other in the social network. We introduce a null model that preserves user activity while removing local correlations, allowing us to disentangle the actual local similarity between users from statistical effects due to the assortative mixing of user activity and centrality in the social network. This analysis suggests that users with similar interests are more likely to be friends, and therefore topical similarity measures among users based solely on their annotation metadata should be predictive of social links. We test this hypothesis on several datasets, confirming that social networks constructed from topical similarity capture actual friendship accurately. When combined with topological features, topical similarity achieves a link prediction accuracy of about 92p.
Examining the Role of Linguistic Knowledge Sources in the Automatic Identification and Classification of Reviews
This paper examines two problems in document-level sentiment analysis: (1) determining whether a given document is a review or not, and (2) classifying the polarity of a review as positive or negative. We first demonstrate that review identification can be performed with high accuracy using only unigrams as features. We then examine the role of four types of simple linguistic knowledge sources in a polarity classification system.
Development and Deployment of IPv6-Based SIP VoIP Networks
This paper presents an IPv6-based SIP VoIP network deployed in Taiwan. This deployment project is supported by NICI IPv6 R&D Division. The major contributions of this paper are exercising the ENUM deployment on IPv6 SIP network, developing the IPv6 SIP User Agent and the IPv6 SIP Analyzer.
An Integrated Approach of Variable Ordering and Logic Mapping into LUT-Array-Based PLD
This paper presents an approach of logic mapping into LUT-Array-Based PLD where Boolean functions in the form of the sum of generalized complex terms (SGCTs) can be mapped directly. While previous mapping approach requires predetermined variable ordering, our approach performs mapping and variable reordering simultaneously. For the purpose, we propose a directed acyclic graph based on the multiple valued decision diagram (MDD) and an algorithm to construct the graph. Our algorithm generates candidates of SGCT expressions for each node in a bottom-up manner and selects the variables in the current level by evaluating the sizes of SGCT expressions directly. Experimental results show that our approach reduces the number of terms maximum to 71 percent for the MCNC benchmark circuits.
An Error Model to Study the Behavior of Transient Errors in Sequential Circuits
In sequential logic circuits the transient errors that occur in a particular time frame will propagate to consecutive time frames thereby making the device more vulnerable. In this work we propose a probabilistic error model for sequential logic that can measure the expected output error probability, given a probabilistic input space, that account for both spatial dependencies and temporal correlations across the logic, using a time evolving causal network. We demonstrate our error model using MCNC and ISCAS benchmark circuits and validate it with HSpice simulations. Our observations show that, significantly low individual gate error probabilities produce at least 5 fold higher output error probabilities. The average error percentage of our results with reference to HSpice simulation results is only 4.43%. Our observations show that the order of temporal dependency of error varies for different sequential circuits.
Validity-guided fuzzy clustering evaluation for neural network-based time-frequency reassignment
This paper describes the validity-guided fuzzy clustering evaluation for optimal training of localized neural networks (LNNs) used for reassigning time-frequency representations (TFRs). Our experiments show that the validity-guided fuzzy approach ameliorates the difficulty of choosing correct number of clusters and in conjunction with neural network-based processing technique utilizing a hybrid approach can effectively reduce the blur in the spectrograms. In the course of every partitioning problem the number of subsets must be given before the calculation, but it is rarely known apriori, in this case it must be searched also with using validity measures. Experimental results demonstrate the effectiveness of the approach.
Diagnosability of Discrete Event Systems with Modular Structure
The diagnosis of unobservable faults in large and complex discrete event systems modeled by parallel composition of automata is considered. A modular approach is developed for diagnosing such systems. The notion of modular diagnosability is introduced and the corresponding necessary and sufficient conditions to ensure it are presented. The verification of modular diagnosability is performed by a new algorithm that incrementally exploits the modular structure of the system to save on computational effort. The correctness of the algorithm is proved. Online diagnosis of modularly diagnosable systems is achieved using only local diagnosers.
Field modifiable architecture with FPGAs and its design/verification/debugging methodologies
In the age of highly integrated system LSIs, design methodologies for shorter time-to-market and higher re-programmability after the chip fabrications are now key research issues because of the difficulty of complete verification before tape-out of LSI designs. In this paper, we first introduce an IP-based VLSI architecture that consists of a main processor and an additional hardware (both custom hard macros and FPGA on a single chip) specialized to be in charge of the specific instructions. We further replace the controller circuits of the specialized hardware with compact micro-controllers and memories by using IP libraries (hard macros), which results in the increase of the debuggability and the flexibility of design even for computations realized by hard macros. We call the proposed architecture as field modifiable architecture (FMA). Experimental results confirm that our architecture can achieve significant performance improvement in terms of execution cycles and that EC (engineering change) can be successfully accommodated "after" chip fabrications.
Tracking Articulated Motion using a Mixture of Autoregressive Models
We present a novel approach to modelling the non-linear and time- varying dynamics of human motion, using statistical methods to capture the char- acteristic motion patterns that exist in typical human activities. Our method is based on automatically clustering the body pose space into connected regions ex- hibiting similar dynamical characteristics, modelling the dynamics in each region as a Gaussian autoregressive process. Activities that would require large numbers of exemplars in example based methods are covered by comparatively few motion models. Different regions correspond roughly to different action-fragments and our class inference scheme allows for smooth transitions between these, thus mak- ing it useful for activity recognition tasks. The method is used to track activities including walking, running, etc., using a planar 2D body model. Its effectiveness is demonstrated by its success in tracking complicated motions like turns, without any key frames or 3D information.
Knowledge Analysis with Tree Patterns
Tree-structured knowledge representations are increasingly being used since the relationships between data objects can be represented in a more meaningful way. A number of tree mining algorithms were developed for mining different subtree types using different parameters. At this point in research it would be useful to discuss what kind of sub-problems can be solved within the current tree mining framework. In this paper we provide a general overview of the development in the area of tree mining and discuss motivations and useful application areas for each development. Implications of using different tree mining parameters and constraints are discussed. Such an overview will be particularly useful for those not so familiar with the area of tree mining as it can reveal useful applications within their domain of interest. It gives guidance as to which type of tree mining will be most useful for their particular application.
Local broadcasting in the physical interference model
In this work we analyze the complexity of local broadcasting in the physical interference model. We present two distributed randomized algorithms: one that assumes that each node knows how many nodes there are in its geographical proximity, and another, which makes no assumptions about topology knowledge. We show that, if the transmission probability of each node meets certain characteristics, the analysis can be decoupled from the global nature of the physical interference model, and each node performs a successful local broadcast in time proportional to the number of neighbors in its physical proximity. We also provide worst-case optimality guarantees for both algorithms and demonstrate their behavior in average scenarios through simulations.
Industrial implementation of a dynamic sampling algorithm in semiconductor manufacturing: approach and challenges
In a worldwide environment, sustaining high yield with a minimum number of quality controls is key for manufacturing plants to remain competitive. In high-mix semiconductor plants, where more than 200 products are concurrently run, the complexity of designing efficient control plans comes from the larger amount of data and number of production parameters to handle. Several sampling algorithms were proposed in the literature, but most of them are seen impracticable when coming to an industrial implementation. In this paper, we present and discuss the industrial implementation of a dynamic sampling algorithm in a high-mix semiconductor plant. We describe how the sampling algorithm has been modified, and point out the set of questions that have been raised by the industrial program. Results indicate that more than 30% of control operations on lots could be avoided without increasing the material at risk in production.
Quality Assessment of Pareto Set Approximations
This chapter reviews methods for the assessment and comparison of Pareto set approximations. Existing set quality measures from the literature are critically evaluated based on a number of orthogonal criteria, including invariance to scaling, monotonicity and computational effort. Statistical aspects of quality assessment are also considered in the chapter. Three main methods for the statistical treatment of Pareto set approximations deriving from stochastic generating methods are reviewed. The  dominance ranking method  is a generalization to partially-ordered sets of a standard non-parametric statistical test, allowing collections of Pareto set approximations from two or more stochastic optimizers to be directly compared statistically. The  quality indicator method  -- the dominant method in the literature -- maps each Pareto set approximation to a number, and performs statistics on the resulting distribution(s) of numbers. The  attainment function method  estimates the probability of attaining each goal in the objective space, and looks for significant differences between these probability density functions for different optimizers. All three methods are valid approaches to quality assessment, but give different information. We explain the scope and drawbacks of each approach and also consider some more advanced topics, including multiple testing issues, and using combinations of indicators. The chapter should be of interest to anyone concerned with generating and analysing Pareto set approximations.
Hybrid Filter Banks With Fractional Delays: Minimax Design and Application to Multichannel Sampling
This paper is motivated by multichannel sampling applications. We consider a hybrid filter banks consisting of a set of fractional delays operators, slow A/D converters with different antialiasing filters, digital expanders, and digital synthesis filters (to be designed). The synthesis filters are designed to minimize the maximum gain of a hybrid induced error system. We show that the induced error system is equivalent to a digital system. This digital system enables the design of stable synthesis filters using existing control theory tools such as model-matching and linear matrix inequalities. Moreover, the induced error is robust against delay estimate errors. Numerical experiments show the proposed approach yields better performance compared to existing techniques.
The drift table: designing for ludic engagement
The Drift Table is an electronic coffee table that displays slowly moving aerial photography controlled by the distribution of weight on its surface. It was designed to investigate our ideas about how technologies for the home could support ludic activities-that is, activities motivated by curiosity, exploration, and reflection rather than externally-defined tasks. The many design choices we made, for example to block or disguise utilitarian functionality, helped to articulate our emerging understanding of ludic design. Observations of the Drift Table being used in volunteers' homes over several weeks gave greater insight into how playful exploration is practically achieved and the issues involved in designing for ludic engagement.
Face-to-face and electronic communications in maintaining social networks: the influence of geographical and relational distance and of information content
Using data collected among 742 respondents, this article aims at gaining greater insight into (i) the interaction between face-to-face (F2F) and electronic contacts, (ii) the influence of information content and relational distance on the communication mode/ service choice and (iii) the influence of relational and geographical distance, in addition to other factors, on the frequency of F2F and electronic contacts with relatives and friends. The results show that the frequency of F2F contacts is positively correlated with that for electronic communication, pointing at a complementarity effect.With respect to information content and relational distance, we find, on the basis of descriptive analyses, that synchronous modes/services (F2F and telephone conversations) are used more for urgent matters and that asynchronous modes (in particular email) become more influential as the relational distance increases. Finally, ordered probit analyses confirm that the frequency of both F2F and electronic communication d...
Design of Spatial Data Broadcast for Mobile Navigation Applications
Existing mobile navigators can readily display any static data related to the area surrounding a user. However, their ability to display any dynamic data is limited. In this paper, we enable the viewing of dynamic data on a navigator using wireless data broadcast. The dynamic data are periodically broadcast via base stations of wireless systems, and can be filtered by navigators fetching desired data. We address several crucial issues related to the design of this application, including data organizing, indexing, caching, and querying. We simulate the performance of the resulting system by measuring the time and energy costs associated with retrieving the dynamic data.
Tool integration at the meta-model level: the Fujaba approach
Today’s development processes employ a variety of notations and tools, e.g., the Unified Modeling Language UML, the Standard Description Language SDL, requirements databases, design tools, code generators, model checkers, etc. For better process support, the employed tools may be organized within a tool suite or integration platform, e.g., Rational Rose or Eclipse. While these tool-integration platforms usually provide GUI adaption mechanisms and functional adaption via application programming interfaces, they frequently do not provide appropriate means for data integration at the meta-model level. Thus, overlapping and redundant data from different “integrated” tools may easily become inconsistent and unusable. We propose two design patterns that provide a flexible basis for the integration of different tool data at the meta-model level. To achieve consistency between meta-models, we describe rule-based mechanisms providing generic solutions for managing overlapping and redundant data. The proposed mechanisms are widely used within the Fujaba Tool Suite. We report about our implementation and application experiences .
TLS parameter estimation for filtering chaotic time series
We present a new algorithm for simultaneous filtering and parameter estimation of chaotic time series corrupted by additive measurement noise. This method is based on iteratively minimizing the total least squares (TLS) error in the phase-space using steepest descent. In contrast to prior work, we assume that the dynamic equations modeling the nonlinear time series are known, but the corresponding parameters are not. Specifically, the data is assumed to be generated from time series satisfying a set of coupled logistics equations corrupted by additive white Gaussian noise. This work is motivated in part by language modeling, where the dynamics of a conversation are argued to satisfy the coupled logistics equations and the time series data represents noisy measurements taken from protocols. Accurate estimation of the process parameters is critical, e.g., in diagnosing and treating different types of language disorders and also may prove valuable for testing and improving language understanding systems.
Industry track portals at BNP Paribas: a brief testimony (April 2005)
This paper intends to bear witness on the past, present and future of portals at BNP Paribas with a focus on: 1) the B2E intranet portal (Echo'net), 2) the French B2C home banking portal (BNPPARIBAS.NET). It introduce the challenges that confronted BNP Paribas in each case, reflect on lessons learnt and future trends and finally suggest what financial services firms expect from e-business infrastructure hardware and software vendors and service providers
Improvement of response properties of MR-fluid actuator by torque feedback control
Magnetorheological (MR) fluids are substances that respond to an applied magnetic field with a change in their theological behavior. Though they are functionally similar to electrorheological (ER) fluids, MR fluids exhibit much higher yield strengths for the applied magnetic fields than ER fluids for the applied electric fields. The devices using MR fluids have an ability to provide high-torque, low-inertia, a safe device and simple interface. In this study, we report on an actuator developed using the MR fluid, which consists of an input part, an output part and an MR fluid clutch between them. First, the basic experiments are examined to investigate the characteristics of the actuator. Next, the torque control system of the MR-fluid actuator is proposed. Finally, the closed-loop control experiments were carried out and it is confirmed that the torque-feedback control is effective for improving the response properties of MR actuators.
Schedulability-driven performance analysis of multiple mode embedded real-time systems
Providing multiple modes to support dynamically changing environments, standards, and new services is prevalent in embedded systems, especially in mobile radio systems. Because such a system frequently contains time-constrained tasks, it is important to analyze the temporal requirements as well as the functional correctness. This paper presents a method to analyze temporal requirements imposed on an embedded real-time system supporting multiple modes. While most performance analysis methods focus only on testing the feasibility of a task or a system, our method goes further by addressing the problem of locating hot spots of a system thereby helping the designer to choose among alternative designs or architectures. We formally define the analysis problem and show that it is very unlikely to be solved efficiently. We present a heuristic algorithm, which is accurate and fast enough to be used in iterative processes in system-level analysis and design. The analysis problem is extended to accommodate probabilistic behavior exhibited by soft real-time tasks.
Performance analysis of cognitive coexistence systems with sensing errors
In this paper, we focus on the performance analysis of the cognitive coexistence between Bluetooth and WLAN systems with sensing errors. The packet transmission rate and packet error probability are derived corresponding to the sensor operating points (consisting of the false alarm probability and the miss detection probability). Then, the optimization problem is established as maximizing the throughput of cognitive user with the constraint of the colliding probability with primary user. Simulation results illustrate that the proposed error analysis approach can obtain the optimal performance when considering the sensing errors. Moreover, the influence on the throughput of cognitive user caused by different sensor operating points is investigated by simulation.
The CHiC Interactive Task (CHiCi) at CLEF2013
The interactive task in Cultural Heritage in CLEF 2013 used a standardised interactive protocol, information retrieval system and interface to observe a set of participants remotely via the web as well as in the lab access an English language collection from the Europeana Digital Library. Both user response and log data were collected from the 208 participants.
Advanced fault tolerant bus for multicore system implemented in FPGA
In the paper, a technique for design of highly dependable communication structure in SRAM-based FPGA is presented. The architecture of the multicore system and the structure of fault tolerant bus with cache memories are demonstrated. The fault tolerant properties are achieved by the replication and utilization of the self checking techniques together with partial dynamic reconfiguration. The experimental results show that presented system has small overhead if the high number of function units are used. All experiments were done on the Virtex5 and Virtex6 platform.
Performance analysis of maximal ratio combining in the presence of multiple equal-power cochannel interferers in a Nakagami fading channel
The effect of cochannel interference on the performance of digital mobile radio systems in a Nakagami (1960) fading channel is studied. The performance of maximal ratio combining (MRC) diversity is analyzed in the presence of multiple equal-power cochannel interferers and additive white Gaussian noise. Closed-form expressions are derived for the average probability of error as well as outage probability of both coherent and noncoherent (differentially coherent) binary frequency-shift keying and binary phase-shift keying schemes in an environment with cochannel interference and noise. The results are expressed in terms of the confluent hypergeometric function of the second kind, a function that can be easily evaluated numerically. The analysis assumes an arbitrary number of independent and identically distributed Nakagami interferers.
Proof search for propositional abstract separation logics via labelled sequents
Abstract separation logics are a family of extensions of Hoare logic for reasoning about programs that mutate memory. These logics are "abstract" because they are independent of any particular concrete memory model. Their assertion languages, called propositional abstract separation logics, extend the logic of (Boolean) Bunched Implications (BBI) in various ways.   We develop a modular proof theory for various propositional abstract separation logics using cut-free labelled sequent calculi. We first extend the cut-fee labelled sequent calculus for BBI of Hou et al to handle Calcagno et al's original logic of separation algebras by adding sound rules for partial-determinism and cancellativity, while preserving cut-elimination. We prove the completeness of our calculus via a sound intermediate calculus that enables us to construct counter-models from the failure to find a proof. We then capture other propositional abstract separation logics by adding sound rules for indivisible unit and disjointness, while maintaining completeness and cut-elimination. We present a theorem prover based on our labelled calculus for these logics.
Storyboarding: an empirical determination of best practices and effective guidelines
Storyboarding is a common technique in HCI and design for demonstrating system interfaces and contexts of use. Despite its recognized benefits, novice designers still encounter challenges in the creation of storyboards. Furthermore, as computing becomes increasingly integrated into the environment, blurring the distinction between the system and its surrounding context, it is imperative to depict context explicitly in storyboards. In this paper, we present two formative studies designed to uncover the important elements of storyboards. These elements include the use of text, inclusion of people, level of detail, number of panels, and representation of the passage of time. We further present an empirical study to assess the effects of these elements on the understanding and enjoyment of storyboard consumers. Finally, we demonstrate how these guidelines were successfully used in an undergraduate HCI class.
Constraint Based Automated Synthesis of Nonmasking and Stabilizing Fault-Tolerance
We focus on constraint-based automated addition of nonmasking and stabilizing fault-tolerance to hierarchical programs. We specify legitimate states of the program in terms of constraints that should be satisfied in those states. To deal with faults that may violate these constraints, we add recovery actions while ensuring interference freedom among the recovery actions added for satisfying different constraints. Since the constraint-based{\em manual} design of fault-tolerance is well-known to be applicable in the manual design of nonmasking fault-tolerance, we expect our approach to have a significant benefit in automation of fault-tolerant programs. We illustrate our algorithms with three case studies:stabilizing mutual exclusion, stabilizing diffusing computation, and a data dissemination problem in sensor networks. With experimental results,we show that the complexity of synthesis is reasonable and that it can be reduced using the {\em structure} of the hierarchical systems. To our knowledge, this is the first instance where automated synthesis has been successfully used in synthesizing programs that are correct under fairness assumptions. Moreover, in two of the case studies considered in this paper, the structure of the recovery paths is too complex to permit existing heuristic based approaches for adding recovery.
Automatic Histogram Threshold Using Fuzzy Measures
In this paper, an automatic histogram threshold approach based on a fuzziness measure is presented. This work is an improvement of an existing method. Using fuzzy logic concepts, the problems involved in finding the minimum of a criterion function are avoided. Similarity between gray levels is the key to find an optimal threshold. Two initial regions of gray levels, located at the boundaries of the histogram, are defined. Then, using an index of fuzziness, a similarity process is started to find the threshold point. A significant contrast between objects and background is assumed. Previous histogram equalization is used in small contrast images. No prior knowledge of the image is required.
Ksensor: Multithreaded kernel-level probe for passive QoS monitoring
Traffic monitoring is an increasingly important discipline for nowadays networking, as Accounting, Security and Traffic Engineering lay on it. Besides, traffic bandwidth has increased exponentially in the last few years, and high-speed network monitoring has become a challenging task. Performance requirements are highly relevant for passive QoS monitoring systems. A low-level study of the capturing and processing stages on a traffic analysis system (TAS) has shown room for improvement. We provide an architecture able to cope with high-speed traffic monitoring using commodity hardware. Our system is intended to exploit the parallelism available in up-to- date workstations, which also introduces constraints for multithreaded QoS analysis. This paper presents a kernel-level framework (ksensor) that, keeping the previous requirements, removes some issues from user-level processing and effectively integrates QoS algorithms, improving the overall performance.
Chaos-Modulated Ramp IC for EMI Reduction in PWM Buck Converters Design and Analysis of Critical Issues
Various non-conventional methods have been employed in the past, to reduce the cost and weight of traditional conducted EMI filters and radiation screens for EMI suppression in switching power electronic converters. This paper points out various shortcomings of these methods which are mainly frequency modulation based, and describes the design of a ramp-generator IC based on a modified modulation scheme. This IC can be used on any voltage mode controlled converter and has a feature that enables the user to tune the same converter to various EMC norms. Test results from a prototype showing significant reduction in harmonic power level have been presented. Moreover, this paper discusses a theoretical formulation for calculating the output capacitor size to maintain ripple specifications, when operating under chaotic modulation.
Simultaneous placement and assignment for exploration in mobile backbone networks
This paper presents new algorithms for conducting cooperative sensing using a mobile backbone network. This hierarchical sensing approach combines backbone nodes, which have superior mobility and communication capability, with regular nodes, which are constrained in mobility and communication capability but which can sense the environment. In the framework of a cooperative exploration problem, a technique is developed for simultaneous placement and assignment of regular and mobile backbone nodes. This method, a generalization of existing techniques that only consider stationary regular nodes, optimally solves the simultaneous placement and assignment problem in computationally tractable time for problems of moderate size. For large-scale instances of this problem, a polynomial-time approximation algorithm is developed. This algorithm carries the benefit of a theoretical performance guarantee and also performs well in practice. Finally, the simultaneous placement and assignment technique is incorporated into a cooperative exploration algorithm, and its performance is shown to compare favorably with that of a benchmark based on existing assignment algorithms for mobile backbone networks.
The mystique of numbers: belief in quantitative approaches to segmentation and persona development
Quantitative market research and qualitative user-centered design research have long had an uneasy and complex relationship. A trend toward increasingly complex statistical segmentations and associated personas will once again increase the urgency of addressing paradigm differences to allow the two disciplines to collaborate effectively.   We present an instructive case in which qualitative field research helped contribute to abandoning a "state of the art" quantitative user segmentation that was used in an attempt to unify both marketing and user experience planning around a shared model of users. This case exposes risks in quantitative segmentation research, common fallacies in the evolving practice of segmentation and use of personas, and the dangers of excessive deference to quantitative research generally.
Analyzing the effects of disk-pointer corruption
The long-term availability of data stored in a file system depends on how well it safeguards on-disk pointers used to access the data. Ideally, a system would correct all pointer errors. In this paper, we examine how well corruption-handling techniques work in reality. We develop a new technique called type-aware pointer corruption to systematically explore how a file system reacts to corrupt pointers. This approach reduces the exploration space for corruption experiments and works without source code. We use type-aware pointer corruption to examine Windows NTFS and Linux ext3. We find that they rely on type and sanity checks to detect corruption, and NTFS recovers using replication in some instances. However, NTFS and ext3 do not recover from most corruptions, including many scenarios for which they possess sufficient redundant information, leading to further corruption, crashes, and unmountable file systems. We use our study to identify important lessons for handling corrupt pointers.
The nature of dialog: structural and lexical markers of dialogic teacher/learner interactions
In this paper I argue that dialog facilitates learning. As a consequence teaching and learning should be dialogic in principal. Against this background the question arises what dialog actually is and how it can be implemented for teaching and learning. A pilot corpus study of a dialog and a monolog corpus of selected teacher/learner interactions sheds light on some of the structural and lexical characteristics of dialogic teaching and learning. The analysis of the data discloses five communicative functions of selected keywords of the dialog corpus, which indicate how speakers are affiliated in dialogic interaction.
Evaluating the enjoyability of the ghosts in Ms Pac-Man
The video games industry is one of the fastest-growing industries in the world, bolstered by sophisticated technology in gaming consoles and modern trends such as mobile and social gaming. The goal of most video games is to entertain the gamer and in most games this stems from the interaction between the gamer and the non-player characters (NPCs): it is no longer sufficient for a game to be visually appealing but instead, the gamer must be challenged at the right level of difficulty to be engaged by the game. It is thus necessary to develop suitable NPCs that are fun to play against and the realm of computational intelligence offers a variety of techniques to do so. However, the perception of fun is clearly subjective and indeed, difficult to quantify. In this paper we make use of the Ms Pac-Man vs Ghosts gaming competition to gather and analyse data from human gamers regarding their preference of opponent: each gamer plays two games against different ghost teams, indicating their preference at the end. We subsequently use this data to establish which ghost teams are generally preferred and demonstrate that there are measurable differences between these ghost teams. These differences are sufficient to group the ghosts into different categories with a good degree of accuracy. This work is a first step in better understanding the attributes required by NPCs for players to be engaged.
Learning human multimodal dialogue strategies
We investigate the use of different machine learning methods in combination with feature selection techniques to explore human multimodal dialogue strategies and the use of those strategies for automated dialogue systems. We learn policies from data collected in a Wizard-of-Oz study where different human ‘wizards’ decide whether to ask a clarification request in a multimodal manner or else to use speech alone. We first describe the data collection, the coding scheme and annotated corpus, and the validation of the multimodal annotations. We then show that there is a uniform multimodal dialogue strategy across wizards, which is based on multiple features in the dialogue context. These are generic features, available at runtime, which can be implemented in dialogue systems. Our prediction models (for human wizard behaviour) achieve a weighted f-score of 88.6 per cent (which is a 25.6 per cent improvement over the majority baseline). We interpret and discuss the learned strategy. We conclude that human wizard behaviour is not optimal for automatic dialogue systems, and argue for the use of automatic optimization methods, such as Reinforcement Learning. Throughout the investigation we also discuss the issues arising from using small initial Wizard-of-Oz data sets, and we show that feature engineering is an essential step when learning dialogue strategies from such limited data.
Multi-objective hierarchical genetic algorithm for interpretable fuzzy rule-based knowledge extraction
A new scheme based on multi-objective hierarchical genetic algorithm (MOHGA) is proposed to extract interpretable rule-based knowledge from data. The approach is derived from the use of multiple objective genetic algorithm (MOGA), where the genes of the chromosome are arranged into control genes and parameter genes. These genes are in a hierarchical form so that the control genes can manipulate the parameter genes in a more effective manner. The effectiveness of this chromosome formulation enables the fuzzy sets and rules to be optimally reduced. Some important concepts about the interpretability are introduced and the fitness function in the MOGA will consider both the accuracy and interpretability of the fuzzy model. In order to remove the redundancy of the rule base proactively, we further apply an interpretability-driven simplification method to newborn individuals. In our approach, we first apply the fuzzy clustering to generate an initial rule-based model. Then the multi-objective hierarchical genetic algorithm and the recursive least square method are used to obtain the optimized fuzzy models. The accuracy and the interpretability of fuzzy models derived by this approach are studied and presented in this paper. We compare our work with other methods reported in the literature on four examples: a synthetic nonlinear dynamic system, a nonlinear static system, the Lorenz system and the Mackey-Glass system. Simulation results show that the proposed approach is effective and practical in knowledge extraction.
Cognitive Dust: Linking CSCW Theories to Creative Design Processes
Results from empirical research in Requirements Engineering lead us to characterise work processes, not as evolutionary and systematic, but as semi structured, emergent and creative. These same properties are found in Situated Action models where work is described as emergent and self defining. At another level within these processes, we use the term "cognitive dust" to describe the External and Distributed cognitive representations suspended in a small group workspace. These cognitive representations are components of communicative actions between humans, or between humans and their technological infrastructures, and are part of these creative processes. We use a multi modal sensor infrastructure, which is situated in a ubiquitous workspace, to observe and capture this cognitive dust. We hypothesise that, if an infrastructure can capture enough of these representations and extract enough semantic meaning to "understand" the communicative intentions, the infrastructure can dynamically support creative, unstructured activities. This paper sets out the conceptual, theoretical framework for this research by linking known CSCW theories with the emergent, unstructured or semi structured nature of creative work processes within small groups such as designers.
Error control using retransmission schemes in multicast transport protocols for real-time media
We analyze different retransmission (ARQ) schemes for error control in multicast protocols geared toward real-time, multimedia applications. We discuss why retransmission schemes are not inappropriate for such applications, but in fact can be quite effective. We present a quantitative analysis of such schemes, as well as simulation results, taking into account four different parameters (and not just the source throughput): (1) the probability of dropping a packet due to limited time for retransmissions; (2) the average time required to deliver a packet correctly to end receivers; (3) the number of times a packet will be retransmitted; and (4) the cost to the network, in terms of packet duplications, of retransmitting a packet. We reach the counter-intuitive conclusion that the optimum scheme, in terms of all four of the above parameters, in the most general scenarios (where several hosts with widely varying propagation delays and 'quality of connections' are participating in the session) is to immediately retransmit packets-preferably multicast-upon reception of a NACK from any receiver. We also demonstrate, again through quantitative analysis, the circumstances under which it would be beneficial (as well as those under which it would be counter-productive) to multicast control messages in the hope of suppressing duplicates and preventing the source from being overwhelmed by control messages.
Outer-loop vectorization: revisited for short SIMD architectures
Vectorization has been an important method of using data-level parallelism to accelerate scientific workloads on vector machines such as Cray for the past three decades. In the last decade it has also proven useful for accelerating multi-media and embedded applications on short SIMD architectures such as MMX, SSE and AltiVec. Most of the focus has been directed at innermost loops, effectively executing their iterations concurrently as much as possible. Outer loop vectorization refers to vectorizing a level of a loop nest other than the innermost, which can be beneficial if the outer loop exhibits greater data-level parallelism and locality than the innermost loop. Outer loop vectorization has traditionally been performed by interchanging an outer-loop with the innermost loop, followed by vectorizing it at the innermost position. A more direct unroll-and-jam approach can be used to vectorize an outer-loop without involving loop interchange, which can be especially suitable for short SIMD architectures.   In this paper we revisit the method of outer loop vectorization, paying special attention to properties of modern short SIMD architectures. We show that even though current optimizing compilers for such targets do not apply outer-loop vectorization in general, it can provide significant performance improvements over innermost loop vectorization. Our implementation of direct outer-loop vectorization, available in GCC 4.3, achieves speedup factors of 3.13 and 2.77 on average across a set of benchmarks, compared to 1.53 and 1.39 achieved by innermost loop vectorization, when running on a Cell BE SPU and PowerPC970 processors respectively. Moreover, outer-loop vectorization provides new reuse opportunities that can be vital for such short SIMD architectures, including efficient handling of alignment. We present an optimization tapping such opportunities, capable of further boosting the performance obtained by outer-loop vectorization to achieve average speedup factors of 5.26 and 3.64.
Designing gestures for hands and feet in daily life
In wearable computing environments, people handles various information anytime and anywhere with a wearing computer. In such situation, a gesture is one of powerful methods as input method because it needs no physical devices to touch and a user can input quickly. However, there are various restrictions for gesture input in daily life; gestures must be socially acceptable because a user has to gesture with unusual movements in a crowd, gestures must be flexible because a user cannot gesture when he/she has a bag with his hand that is used for a gesture. In this paper, we clarify the restrictions on gesture interfaces in daily life, then propose practical gestures for selecting simple menu items with hands and feet.
Modeling of heart systolic murmurs based on multivariate matching pursuit for diagnosis of valvular disorders
Heart murmurs are pathological sounds produced by turbulent blood flow due to certain cardiac defects such as valves disorders. Detection of murmurs via auscultation is a task that depends on the proficiency of physician. There are many cases in which the accuracy of detection is questionable. The purpose of this study is development of a new mathematical model of systolic murmurs to extract their crucial features for identifying the heart diseases. A high resolution algorithm, multivariate matching pursuit, was used to model the murmurs by decomposing them into a series of parametric time-frequency atoms. Then, a novel model-based feature extraction method which uses the model parameters was performed to identify the cardiac sound signals. The proposed framework was applied to a database of 70 heart sound signals containing 35 normal and 35 abnormal samples. We achieved 92.5% accuracy in distinguishing subjects with valvular diseases using a MLP classifier, as compared to the matching pursuit-based features with an accuracy of 77.5%.
Parallel implementation of the sparse-matrix/canonical grid method for the analysis of two-dimensional random rough surfaces (three-dimensional scattering problem) on a Beowulf system
Wave scattering from two-dimensional (2-D) random rough surfaces [three-dimensional (3-D) scattering problem] has been previously analyzed using the sparse-matrix/canonical grid (SM/CG) method. The computational complexity and memory requirement of the SM/CG method are O(N log N) per iteration and O(N), respectively, where N is the number of surface unknowns. Furthermore, the SM/CG method is FFT based, which facilitates the implementation on parallel processors. In this paper, we present a cost-effective solution by implementing the SM/CG method on a Beowulf system consisting of PCs (processors) connected by a 100 Base TX Ethernet switch. The workloads of computing the sparse-matrix-vector multiplication corresponding to the near interactions and the fast Fourier transform (FFT) operations corresponding to the far interactions in the SM/CG method can be easily distributed among all the processors. Both perfectly conducting and lossy dielectric surfaces of Gaussian spectrum and ocean spectrum are analyzed thereafter. When possible, speedup factors against a single processor are given. It is shown that the SM/CG method for a single realization of rough surface scattering can be efficiently adapted for parallel implementation. The largest number of surface unknowns solved in this paper is over 1.5 million. On the other hand, a problem of 131072 surface unknowns for a PEC random rough surface of 1024 square wavelengths only requires a CPU time of less than 20 min. We demonstrate that analysis of a large-scale 2-D random rough surface feasible for a single realization and for one incident angle is possible using the low-cost Beowulf system.
Active noise cancellation using aggressor-aware clamping circuit for robust on-chip communication
As the IC process technology scales the on-chip wiring network becomes denser. Increasing aspect ratios of the on-chip interconnects lead to higher coupling capacitances and ultimately higher cross-talk noise, which degrades signal integrity. In this paper we propose a clamping circuit for on-chip busses, which clamps a victim wire in an on-chip bus based on the states of its immediate aggressors. These clampers help the driver of the victim wire in draining the charge, which is induced due to cross-talk between aggressors and victim wires. This helps in decreasing the cross-talk peak noise and also the delay variability (referred to as delay noise). Simulation results for a 10 mm long communication bus (parallel wires) laid at minimum pitch in 0.13 /spl mu/m CMOS technology show that a reduction of 30%(17.6%), 37%(27.2%) and 26%(65.8%) in cross-talk peak noise amplitude (delay noise) is observed for point to point, parallel repeater inserted and staggered repeater inserted respectively when only immediate neighbours are considered (1/sup st/ order). Furthermore the aggressor-aware clamper is very effective in avoiding glitches, which may occur when more aggressors, in addition to the immediate ones are also switching simultaneously in the same.
High resolution two-dimensional ARMA spectral estimation
The authors present a practical algorithm for estimating the power spectrum of a 2-D homogeneous random field based on 2-D autoregressive moving average (ARMA) modeling. This algorithm is a two-step approach: first, the AR parameters are estimated by solving a version of the 2-D modified Yule-Walker equation, for which some existing efficient algorithms are available; then the MA spectrum parameters are obtained by simple computations. The potential capability and the high-resolution performance of the algorithm are demonstrated by using some numerical examples. >
Compiler verification in LF
A methodology for the verification of compiler correctness based on the LF logical framework as realized within the Elf programming language is presented. This technique is used to specify, implement, and verify a compiler from a simple functional programming language to a variant of the Categorical Abstract Machine (CAM). >
Blind Amplify-and-Forward Relaying in Multiple-Antenna Relay Networks
In this paper, we investigate the performance of a single-relay cooperative scenario where the source, relay, and destination terminals are equipped with multiple transmit/receive antennas. We particularly focus on the so-called blind amplify-and-forward relaying in which the availability of channel state information at the relay terminal is not required. Through the derivation of pairwise error probability, we quantify analytically the impact of multiple antenna deployment assuming various scenarios which involve relay location and power allocation assumptions imposed on the cooperating nodes.
Coupling of two 2-link robots with a passive joint for reconfigurable planar parallel robot
This paper proposes a reconfigurable planar parallel robot by coupling two 2R open kinematic chains (or 2-link robots), the first joints of which are passive. We show that they can reconfigure to a 5R closed kinematic chain which has the same number of actuators as its degrees of freedom. They can also reconfigure to a 4R closed kinematic chain plus one actuated link. The parallel robot has only two actuators but can have multiple functions by reconfigurations. Due to the passive joints, whether or not the 2R open kinematic chains can couple with each other is a non-trivial problem. We propose coupling sequences for forming the 4R and 5R closed kinematic chains and verify those experimentally.
On the Fingerprinting Capacity Under the Marking Assumption
We address the maximum attainable rate of fingerprinting codes under the marking assumption, studying lower and upper bounds on the value of the rate for various sizes of the attacker coalition. Lower bounds are obtained by considering typical coalitions, which represents a new idea in the area of fingerprinting and enables us to improve the previously known lower bounds for coalitions of size two and three. For upper bounds, the fingerprinting problem is modeled as a communications problem. It is shown that the maximum code rate is bounded above by the capacity of a certain class of channels, which are similar to the multiple-access channel (MAC). Converse coding theorems proved in the paper provide new upper bounds on fingerprinting capacity. It is proved that capacity for fingerprinting against coalitions of size two and three over the binary alphabet satisfies and , respectively. For coalitions of an arbitrary fixed size , we derive an upper bound on fingerprinting capacity in the binary case. Finally, for general alphabets, we establish upper bounds on the fingerprinting capacity involving only single-letter mutual information quantities.
Methods for reducing events in sequential circuit fault simulation
Methods are investigated for reducing events in sequential circuit fault simulation by reducing the number of faults simulated for each test vector. Inactive faults, which are guaranteed to have no effect on the output or the next state, are identified using local information from the fault-free circuit in one technique. In a second technique, the Star-algorithm is extended to handle sequential circuits and provides global information about inactive faults, based on the fault-free circuit state. Both techniques are integrated into the PROOFS synchronous sequential circuit fault simulator. An average 28% reduction in faulty circuit gate evaluations is obtained for the 19 ISCAS-89 benchmark circuits studied using the first technique, and 33% reduction for the two techniques combined. Execution times decrease by an average of 17% when the first technique is used. For the largest circuits, further improvements in execution time are made when the Star-algorithm is included. >
Tracking performance of the coherent and noncoherent discriminators in strong multipath
Signal multipath leads to undesirable tracking errors and inaccurate ranging information for GPS receivers. The extent of the tracking error in compromising the receiver discriminator performance depends on the multipath amplitude, delay, and phase relative to the direct path. Compared with the rural area, the GPS receiver in the semi-enclosed area, such as city canyons and building shadows, is subject to much weaker line of sight propagation environment which further compromise its performance. In this paper, we derive analytical expressions of the multipath effect on the GPS tracking errors for both coherent discriminator and noncoherent early-minus-late power discriminators in strong multipath environment.
Designing hardware with dynamic memory abstraction
Recent progress in program analysis has produced tools that are able to compute upper bounds on the use of dynamic memory. This opens up a space for the use of dynamic memory abstraction in high-level synthesis. In this paper, we explain how to design hardware using C programs with  malloc () and  free (). A compilation process is outlined for transforming C programs with heap operations into a hardware description language. As demonstrated by our experiments, this approach is feasible. Further, automatic parallelization of the generated circuits improves by a factor up to 1.9 in terms of clock frequency and a factor up to 2.7 in terms of clock cycles over the previous work.
A New Dynamic OVSF Code Allocation Method based on Adaptive Simulated Annealing Genetic Algorithm (ASAGA)
Orthogonal variable spreading factor (OVSF) codes are widely used to provide variable data rates for supporting different bandwidth requirements in wideband code division multiple access (WCDMA) systems. Many works in the literature have intensively investigated to find an optimal dynamic code assignment scheme for OVSF codes. Unlike earlier studies, which assign OVSF codes using conventional (CCA) or dynamic (DCA) code allocation schemes, in this paper, adaptive simulated annealing genetic algorithm (ASAGA) was applied which population is adaptively constructed according to existing traffic density in the OVSF code-tree. Also, the influences of the ASAGA parameters (selection, crossover and mutation techniques and cooling schedules) were examined on the dynamic OVSF code allocation problem. The simulation results show that the ASAGA provides reduced code blocking probability and improved spectral efficiency in the system when compared to the CCA and DCA schemes. ASAGA is also tested with its components SA and GA.
An analysis of multicast forwarding state scalability
Scalability of multicast forwarding state is likely to be a major issue facing inter-domain multicast deployment. We present a comprehensive analysis of the multicast forwarding state problem. Our goal is to understand the scaling trends of multicast forwarding state in the Internet, and to explore the intuitions that have motivated state reduction research. We conducted simulation experiments on both real and generated network topologies, with a range of parameters driven by multicast application characteristics. We found that the increase in peering among Internet backbone networks has led to more multicast forwarding state at a handful of core domains, but less state in the rest of the domains. We observed that scalability of multicast forwarding state with respect to session size follows a power law. Our findings show that distribution and concentration of multicast forwarding state in the Internet is significantly, impacted by the application characteristics. We investigated the proposals on non-branching multicast forwarding state elimination, and found substantial reduction is attainable even with very dense multicast sessions.
Customized Interface Generation Model Based on Knowledge and Template for Web Service
With the development of Service-Oriented Architecture, more and more researches have provided automatic and semi-automatic approaches to end-user. Users can construct their own applications with web services. However, it is hard for most end-users to customize the interfaces of applications with current service composition methods. To address this issue, an interface generation model was proposed to provide customized user interface and interaction workflow. In this model, knowledge was involved to instruct the workflow of interaction. Templates were adopted to describe the user interface. Some significant points, such as user definition, data profile, user interaction workflow, interface description, were discussed in detail. A prototype system was implemented. Some demos have been shown to verify the customized interface generation model. With this model, end-users can define the interfaces and interaction workflows of web services with rules and templates. It supplies the gap of user interface in service composition. Compared with the current interface generation in service composition, the proposed model is more flexible and more effective for end-users
Petri Nets and Ontologies: Tools for the "Learning Player" Assessment in Serious Games
Serious games are now an increasingly used tool in business training. The question of the effectiveness of such devices on learning is a research issue. The indicators provided at the end of a video game are insufficient to understand and follow the path of a learning player. It is therefore necessary to track not only the player's actions but also to provide tools in order to analyze and diagnose the knowledge acquisition of the learner. We developed an approach based on Petri nets, used to model the accurate behavior of the player. We complete this tool with ontology to explain learner's mistakes.
Epileptic EEG classification based on kernel sparse representation.
The automatic identification of epileptic EEG signals is significant in both relieving heavy workload of visual inspection of EEG recordings and treatment of epilepsy. This paper presents a novel method based on the theory of sparse representation to identify epileptic EEGs. At first, the raw EEG epochs are preprocessed via Gaussian low pass filtering and differential operation. Then, in the scheme of sparse representation based classification (SRC), a test EEG sample is sparsely represented on the training set by solving l1-minimization problem, and the represented residuals associated with ictal and interictal training samples are computed. The test EEG sample is categorized as the class that yields the minimum represented residual. So unlike the conventional EEG classification methods, the choice and calculation of EEG features are avoided in the proposed framework. Moreover, the kernel trick is employed to generate a kernel version of the SRC method for improving the separability between ictal and interictal classes. The satisfactory recognition accuracy of 98.63% for ictal and interictal EEG classification and for ictal and normal EEG classification has been achieved by the kernel SRC. In addition, the fast speed makes the kernel SRC suit for the real-time seizure monitoring application in the near future.
Self-Immunity Technique to Improve Register File Integrity Against Soft Errors
Continuous shrinking in feature size, increasing power density etc. increase the vulnerability of microprocessors against soft errors even in terrestrial applications. The register file is one of the essential architectural components where soft errors can be very mischievous because errors may rapidly spread from there throughout the whole system. Thus, register files are recognized as one of the major concerns when it comes to reliability. This paper introduces Self-Immunity, a technique that improves the integrity of the register file with respect to soft errors. Based on the observation that a certain number of register bits are not always used to represent a value stored in a register. This paper deals with the difficulty to exploit this obvious observation to enhance the register file integrity against soft errors. We show that our technique can reduce the vulnerability of the register file considerably while exhibiting smaller overhead in terms of area and power consumption compared to state-of-the-art in register file protection.
Measuring Semantic Associaiton in Domain Ontology
In domain ontology, semantic association (SA) is used to depict the correlation between two concepts. In this paper, we define semantic association degree (SAD) for measuring SA in the domain ontology. We first present a method to measure SAD of two direct related concepts by evaluating the semantic relationship between them, and then give another method to measure SAD of two indirect related concepts though SAD of two directed neighboring concepts. A set of comparison experiments show the benefit of our approaches.
The $h$ -Index and the Number of Citations: Two Fuzzy Integrals
In this paper, we review two of the most well-known citation indexes and establish their connections with the Choquet and Sugeno integrals. In particular, we show that the recently established h-index is a particular case of the Sugeno integral, and that the number of citations corresponds to the Choquet integral. In both cases, they use the same fuzzy measure. The results presented here permit one to envision new indexes defined in terms of fuzzy integrals using other types of fuzzy measures. A few considerations in this respect are also included in this paper. Indexes for taking into account recent research and the publisher credibility are outlined.
The soft error problem: an architectural perspective
Radiation-induced soft errors have emerged as a key challenge in computer system design. If the industry is to continue to provide customers with the level of reliability they expect, microprocessor architects must address this challenge directly. This effort has two parts. First, architects must understand the impact of soft errors on their designs. Second, they must select judiciously from among available techniques to reduce this impact in order to meet their reliability targets with minimum overhead. To provide a foundation for these efforts, this paper gives a broad overview of the soft error problem from an architectural perspective. We start with basic definitions, followed by a description of techniques to compute the soft error rate. Then, we summarize techniques used to reduce the soft error rate. This paper also describes problems with double-bit errors. Finally, this paper outlines future directions for architecture research in soft errors.
A 5.5-GHz 1-mW Full-Modulus-Range Programmable Frequency Divider in 90-nm CMOS Process
Operating up to 5.5 GHz with 1-mW power consumption, a 90-nm CMOS programmable frequency divider with eight stages of new static D-flip-flop-based (2/1) divider cells is presented, where the supply voltage of 1.0 V is employed. The divider achieves a full modulus range from 1 to 256 and operates over a wide range maintaining up to 4 GHz with -30-dBm input power. The divider also accomplishes a power efficiency of 12.8 GHz/mW with 0.5-V supply voltage. It is favorable for advanced processes.
Space curve recognition based on the wavelet transform and string-matching techniques
A technique for representing and recognising 3-D or space curves is presented. In the proposed algorithm, the space curves are represented by a set of two zero-crossing representations which are constructed based on the dyadic wavelet transform. These representations are then described in the form of an ordered set of complex numbers which is referred to as the compact representation of the space curves. A string-matching technique is adapted for comparing two curves using their compact representations. Experimental results show that the proposed technique can be used for recognising space curves under similarity transformation with and without additive noise.
Five Weeks in the Robot House Exploratory Human-Robot Interaction Trials in a Domestic Setting
This paper presents five exploratory trials investigating scenarios likely to occur when a personal robot shares a home with a person. The scenarios are: a human and robot working on a collaborative task, a human and robot sharing a physical space in a domestic setting, a robot recording and revealing personal information, a robot interrupting a human in order to serve them, and finally, a robot seeking assistance from a human through various combinations of physical and verbal cues. Findings indicate that participants attribute more blame and less credit to a robot than compared to themselves when working together on a collaborative task. Safety is a main concern when determining participants' comfort when sharing living space with their robot. Findings suggest that the robot should keep its interruption of the user's activities to a minimum. Participants were happy for the robot to store information which is essential for the robot to improve its functionality. However, their main concerns were related to the storing of sensitive information and security measures to safeguard such information.
Workstation based parallel test generation
Generation of test vectors for the VLSI devices used in contemporary digital system is becoming much more difficult as these devices increase in size. Automatic Test Pattern Generation (ATPG) techniques are commonly used to generate these tests. Since ATPG is an NP complete problem with complexity exponential to circuit size, the application of parallel processing techniques to accelerate the process of finding test patterns is an active area of research. This paper presents an approach to parallelization of the test generation problem that is targeted to a network-of-workstations environment. The system is based upon partitioning of the fault list across multiple processors and includes enhancements designed to address the main drawbacks of this technique, namely unequal load balancing and generation of redundant vectors. The technique is generalized enough that it can be applied to any test generation system regardless of the ATPG or fault simulation algorithm employed. Results were gathered to determine the impact of workstation processing load and network communications load on the performance of the system. >
Classroom Presenter: Enhancing Interactive Education with Digital Ink
Classroom Presenter is a Tablet PC-based interaction system that supports the sharing of digital ink on slides between instructors and students. Initial deployments show that using the technology can achieve a wide range of educational goals and foster a more participatory classroom environment.
Two odds-radio-based text classification algorithms
Since 1990's, the exponential growth of theseWeb documents has led to a great deal of interestin developing efficient tools and software toassist users in finding relevant information. Textclassification has been proved to be useful inhelping organize and search text information onthe Web. Although there have been existed anumber of text classification algorithms, most ofthem are either inefficient or too complex. In thispaper we present two Odds-Radio-Based textclassification algorithms, which are called ORand TF*OR respectively. We have evaluated ouralgorithm on two text collections and compared itagainst k-NN and SVM. Experimental resultsshow that OR and TF*OR are competitive withk-NN and SVM. Furthermore, OR and TF*OR ismuch simpler and faster than them. The resultsalso indicate that it is not TF but relevancefactors derived from Odds Radio that play thedecisive role in document categorization.
Congestion Adaptive Routing in Mobile Ad Hoc Networks
Mobility, channel error, and congestion are the main causes for packet loss in mobile ad hoc networks. Reducing packet loss typically involves congestion control operating on top of a mobility and failure adaptive routing protocol at the network layer. In the current designs, routing is not congestion-adaptive. Routing may let a congestion happen which is detected by congestion control, but dealing with congestion in this reactive manner results in longer delay and unnecessary packet loss and requires significant overhead if a new route is needed. This problem becomes more visible especially in large-scale transmission of heavy traffic such as multimedia data, where congestion is more probable and the negative impact of packet loss on the service quality is of more significance. We argue that routing should not only be aware of, but also be adaptive to, network congestion. Hence, we propose a routing protocol (CRP) with such properties. Our ns-2 simulation results confirm that CRP improves the packet loss rate and end-to-end delay while enjoying significantly smaller protocol overhead and higher energy efficiency as compared to AODV and DSR
Implementation and evaluation for dependable bus control using CPLD
Bus systems are used in computers as essential architecture, and dependability of bus systems should be accomplished reasonably for various applications. In this paper, we will present dependable bus operations with actual implementation and evaluation by CPLD. Most of the bus systems control transition of some classified phases with synchronous clock or guard time to avoid incorrect phase transition. However, these phase control methods may degrade system performance or cause incorrect operations. We design an asynchronous sequential circuit for bus phase control without clock or guard time. This circuit prevents incorrect phase transition at the time when large input delay or erroneous input occurs. We estimate probability of incorrect phase transition with single stuck-at fault on input signals. From the result of estimation, we also design checking system verifying outputs of initiator and target devices. Incorrect phase transition with single stuck-at fault occurred between both sequential circuits is inhibited completely by implementation of the system.
Portscan Detection with Sampled NetFlow
Sampling techniques are often used for traffic monitoring in high-speed links in order to avoid saturation of network resources. Although there is a wide existing research dealing with anomaly detection, few studies analyzed the impact of sampling on the performance of portscan detection algorithms. In this paper, we performed several experiments on two already existing portscan detection mechanisms to test whether they are robust enough to different sampling techniques. Unlike previous works, we found that flow sampling is not always better than packet sampling to continue detecting portscans reliably.
Morpheme-based chinese nested named entity recognition
Named entity recognition plays an important role in many natural language processing applications. While considerable attention has been pain in the past to research issues related to named entity recognition, few studies have been reported on the recognition of nested named entities. This paper presents a morpheme-based method to Chinese nested named entity recognition. To approach this task, we first employ the logistic regression model to extract multi-level entity morphemes from an entity-tagged corpus, and thus explore a variety of lexical features under the framework of conditional random fields to perform Chinese nested named entity recognition. Our experimental results on different data set show that our system is effective for most nested named entities under evaluation.
Voice Priority Queue Scheduling System Models for VoIP over WLANs
The Voice over Internet Protocol (VoIP) is a delay sensitive traffic due to real-time applications on networks. The assessment of voice flow quality in the VoIP is an essential requirement for technical and commercial motivation. The packets of VoIP streaming may experience drops because of the competition among the different kinds of traffic flow over the network. A VoIP application is also sensitive to delay and requires the voice packets to arrive on time from the sender to the receiver side without any delay over WLAN. The scheduling system model for VoIP traffic is an unresolved problem. In this research paper, the author proposes a new Voice Priority Queue (VPQ) scheduling system models and algorithms for the VoIP over WLANs to solve scheduling issues over IP-based networks. They present new contributions, through the three stages of the VPQ. The VPQ scheduling algorithm is provided as an essential technique in the VoIP communication networks to guarantee the QoS requirements. The design of the VPQ is managed by the limited bandwidth utilization and has been proven to have an efficient performance over WLANs.
Harmonic retrieval using higher order statistics: a deterministic formulation
Given a single record, the authors consider the problem of estimating the parameters of a harmonic signal buried in noise. The observed data are modeled as a sinusoidal signal plus additive Gaussian noise of unknown covariance. The authors define novel higher order statistics-referred to as "mixed" cumulants-that can be consistently estimated using a single record and are insensitive to colored Gaussian noise. Employing fourth-order mixed cumulants, they estimate the sinusoid parameters using a consistent, nonlinear matching approach. The algorithm requires an initial estimate that is obtained from a consistent, linear estimator. Finally, the authors examine the performance of the proposed method via simulations. >
A comparison between strand spaces and multiset rewriting for security protocol analysis
Formal analysis of security protocols is largely based on a set of assumptions commonly referred to as the Dolev-Yao model. Two formalisms that state the basic assumptions of this model are related here: strand spaces and multiset rewriting with existential quantification. Strand spaces provide a simple and economical approach to analysis of completed protocol runs by emphasizing causal interactions among protocol participants. The multiset rewriting formalism provides a very precise way of specifying finite-length protocols with unboundedly many instances of each protocol role, such as client, server, initiator, or responder. A number of modifications to each system are required to produce a meaningful comparison. In particular, we extend the strand formalism with a way of incrementally growing bundles in order to emulate an execution of a protocol with parametric strands. The correspondence between the modified formalisms directly relates the intruder theory from the multiset rewriting formalism to the penetrator strands. The relationship we illustrate here between multiset rewriting specifications and strand spaces thus suggests refinements to both frameworks, and deepens our understanding of the Dolev-Yao model.
Off-the-path flow handling mechanism forhigh-speed and programmable traffic management
In this paper, we propose a high-speed and programmable traffic management mechanism to enable easy and timely innovations. A control framework introduced by 4D, Tesseract, or OpenFlow, separates control functions from the switch nodes to a control server so that a variety of network control policies can be implemented outside of the switches. Within this framework, we propose a mechanism to enable flexible flow-based traffic management so that a variety of innovative traffic management schemes can be realized. Per-flow traffic management, however, requires packet-by-packet state updates, which can spoil this control framework. The proposed mechanism consists of a control server that monitors traffic conditions using sampled packets sent from the switches and calculates per-flow packet discarding rate, and switches that discard incoming packets according to the discarding rate. Packet sampling and discarding do not require packet-by-packet state handling at the switches and thus allows controls from a control server. We also propose a mechanism to compress the discarding information using a time series of bloom filters, so that frequent control updates are allowed. We tested the mechanism with per-flow WFQ emulation and the simulation results showed very good per-flow fairness. Furthermore, we found that the flow table is compressed 600 times smaller and that the processing cost at the server and the switches is small enough for use with 10 Gbps links.
PLDA: Parallel Latent Dirichlet Allocation for Large-Scale Applications
This paper presents PLDA, our parallel implementation of Latent Dirichlet Allocation on MPI and MapReduce. PLDA smooths out storage and computation bottlenecks and provides fault recovery for lengthy distributed computations. We show that PLDA can be applied to large, real-world applications and achieves good scalability. We have released  MPI-PLDA  to open source at http://code.google.com/p/plda under the Apache License.
A fast thresholded linear convolution representation of morphological operations
In this correspondence, we present a fast thresholded linear convolution representation of morphological operations. The thresholded linear convolution representation of dilation and erosion is first proposed. A comparison of the efficiency of the direct implementation of morphological operations and the thresholded linear convolution representation of morphological operations is subsequently conducted. Mathematical morphology has emerged as a powerful new tool for image processing. >
Facebook users have become much more private: A large-scale study
We investigate whether Facebook users have become more private in recent years. Specifically, we examine if there have been any important trends in the information Facebook users reveal about themselves on their public profile pages since early 2010. To this end, we have crawled the public profile pages of 1.4 million New York City (NYC) Facebook users in March 2010 and again in June 2011. We have found that NYC users in our sample have become dramatically more private during this period. For example, in March 2010 only 17.2% of users in our sample hid their friend lists, whereas in June 2011, just 15 months later, 52.6% of the users hid their friend lists. We explore privacy trends for several personal attributes including friend list, networks, relationship, high school name and graduation year, gender, and hometown. We find that privacy trends have become more pronounced for certain demographics. Finally, we attempt to determine the primary causes behind the dramatic decrease in the amount of information Facebook users reveal about themselves to the general public.
Improved compression by coupling of coding techniques and redundant transform
The techniques commonly used in image coding (JPEG, MPEG, ...) have as the main objective to compress as much as possible while retaining most of the information. These methods are often based on the use of the discrete cosine transform (DCT) and the wavelet transform (WT). Our purpose is to consider the necessary redundancy to achieve a good reception in the case of heavy interruptions of bits transmission. There is however a contradiction between an optimal compression and the redundancy required. It is thus necessary to master and compress the information to be transmitted as much as possible and to withstand the noise on the transmission channel. This article makes a contribution to this difficult problem: its originality resides in the coupling of orthogonal transforms and a redundant transform. Simulation results are provided using our method and the results are compared with that of DCT and WT based methods for Lena and Mountain images.
Principal Component Analysis for Large Scale Problems with Lots of Missing Values
Principal component analysis (PCA) is a well-known classical data analysis technique. There are a number of algorithms for solving the problem, some scaling better than others to problems with high dimensionality. They also differ in their ability to handle missing values in the data. We study a case where the data are high-dimensional and a majority of the values are missing. In case of very sparse data, overfitting becomes a severe problem even in simple linear models such as PCA. We propose an algorithm based on speeding up a simple principal subspace rule, and extend it to use regularization and variational Bayesian (VB) learning. The experiments with Netflix data confirm that the proposed algorithm is much faster than any of the compared methods, and that VB-PCA method provides more accurate predictions for new data than traditional PCA or regularized PCA.
On the guaranteed error correction capability of LDPC codes
We investigate the relation between the girth and the guaranteed error correction capability of gamma-left regular LDPC codes when decoded using the bit flipping (serial and parallel) algorithms. A lower bound on the number of variable nodes which expand by a factor of at least 3gamma/4 is found based on the Moore bound. An upper bound on the guaranteed correction capability is established by studying the sizes of smallest possible trapping sets.
Image recovery using a new nonlinear adaptive filter based on neural networks
This work defines a new nonlinear adaptive filter based on a feed-forward neural network with the capacity of significantly reducing the additive noise of an image. Even though measurements have been carried out using X-ray images with additive white Gaussian noise, it is possible to extend the results to other type of images. Comparisons have been carried out with the Weiner filter because it is the most effective option for reducing Gaussian noise. In most of the cases, image reconstruction using the proposed method has produced satisfactory results. Finally, some conclusions and future work lines are presented
Blob Analysis of the Head and Hands: A Method for Deception Detection
Behavioral indicators of deception and behavioral state are extremely difficult for humans to analyze. Blob analysis, a method for analyzing the movement of the head and hands based on the identification of skin color is presented. This method is validated with numerous skin tones. A proof-of-concept study is presented that uses blob analysis to explore behavioral state identification in the detection of deception.
Constrained Angular Motion Estimation in a Gyro-Free IMU
In this paper, we present an extended Kalman filter (EKF)-based solution for the estimation of the angular motion using a gyro-free inertial measurement unit (GF-IMU) built of twelve separate mono-axial accelerometers. Using such a GF-IMU produces a vector, which we call the angular information vector (AIV) that consists of 3D angular acceleration terms and six quadratic terms of angular velocities. We consider the multiple distributed orthogonal triads of accelerometers that consist of three nonplanar distributed triads equally spaced from a central triad as a specific case to solve. During research for the possible filter schemes, we derived equality constraints. Hence we incorporate the constraints in the filter to improve the accuracy of the angular motion estimation, which in turn improves the attitude accuracy (direction cosine matrix (DCM) or quaternion vector).
Subject searching in online catalogs: metaknowledge used by experienced searchers
This paper begins to identify and characterize the knowledge used by experienced librarians while searching for subject information in online catalogs. Ten experienced librarians performed the same set of six subject searches in an online catalog. Investigated was the knowledge used to solve retrieval problems. This knowledge represents expertise in the use of the catalog. Data were collected through the use of think-aloud protocols, transaction logs, and structured interviews. Knowledge was defined as knowledge of objects (factual knowledge), knowledge of events (experiential knowledge), knowledge of performance (process knowledge), and metaknowledge. Metaknowledge is the sense of whole derived from the integration of factual, process, and experiential knowledge about the search and the conditions under which it is performed. The focus of this paper is on metaknowledge. For evidence of metaknowledge the data were examined for explanations that participants gave for their actions and observations, and for ways that participants evaluated their own progress during the process of searching. Reasons and explanations given by searchers were related to all phases of the library information retrieval process from the user's receipt of material to policies for collection development, and not just events directly related to the performance of a particular search task.
Injection-Locked Clocking: A Low-Power Clock Distribution Scheme for High-Performance Microprocessors
We propose injection-locked clocking (ILC) to combat deteriorating clock skew and jitter, and reduce power consumption in high-performance microprocessors. In the new clocking scheme, injection-locked oscillators are used as local clock receivers. Compared to conventional clocking with buffered trees or grids, ILC can achieve better power efficiency, lower jitter, and much simpler skew compensation thanks to its built-in deskewing capability. Unlike other alternatives, ILC is fully compatible with conventional clock distribution networks. In this paper, a quantitative study based on circuit and microarchitectural-level simulations is performed. Alpha21264 is used as the baseline processor, and is scaled to 0.13 m and 3 GHz. Simulations show 20- and 23-ps jitter reduction, 10.1% and 17% power savings in two ILC configurations. A test chip distributing 5-GHz clock is implemented in a standard 0.18- m CMOS technology and achieved excellent jitter performance and a deskew range up to 80 ps.
State of the Art in Modeling and Deployment of Electronic Contracts
Modeling and deployment of e-contracts is a challenging task because of the involvement of both technological and business aspects. There are several frameworks and systems available in the literature. Some works mainly deal with the automatic handling of paper contracts and others provide monitoring and enactment of contracts. Because contracts evolve, it is useful to have a system that models and enacts the evolution of e-contracts.
CODEX: Exploration of semantic changes between ontology versions
Summary: Life science ontologies substantially change over time to meet the requirements of their users and to include the newest domain knowledge. Thus, an important task is to know what has been modified between two versions of an ontology (diff ). This diff should contain all performed changes as compact and understandable as possible. We present CODEX (Complex Ontology Diff Explorer), a tool that allows determining semantic changes between two versions of an ontology which users can interactively analyze in multiple ways. Availability and Implementation: CODEX is available under http: //www.izbi.de/codex and is supported by all major browsers. It is implemented in Java based on Google Web Toolkit technology. Additionally, users can access a web service interface to use the diff functionality in their applications and analyses.
Reliability Analysis in the Early Development of Real-Time Reactive Systems
The increasing trend toward complex software systems has highlighted the need to incorporate quality requirements earlier in the development process. Reliability is one of the important quality indicators of such systems. This paper proposes a reliability analysis approach to measure reliability in the early development of real-time reactive systems (RTRS). The goal is to provide decision support and detect the first signs of low or decreasing reliability as the system design evolves. The analysis is conducted in a formal development environment for RTRS, formalized mathematically and illustrated using a train-gate-controller case study.
Tools for Creating Documents in 'Preferred Format' for Visually Impaired People
A suite of tools is described that allow a word-processor operator to produce documents in large print, Braille and on cassette tape so that the documents can subsequently be read by visually impaired or blind users. The tools integrate with Microsoft Word, are written in Visual Basic for Applications and make use of the Word Object Model.
NCAC: Network Congestion Analyzer and Controller
The stability of the current Internet architecture depends mostly on end-to-end TCP congestion control mechanisms. The Network Congestion Analyzer and Controller (NCAC) is an effort to build a complete user interface application to NS-2 that provides graphical access to most of NS2's functionalities. Besides, the application provides powerful visual tools for monitoring and displaying network performance metrics calculated by the simulation. The application is developed in Java to benefit from Java's portability and platform independence. There are three main parts in NCAC: (1) NS-2 interface, (2) network animation and alarm, and (3) graph plotting application. The functionality of each part is discussed.
On the comparison of bilinear, cubic spline, and fuzzy interpolation techniques for robotic position measurements
This paper describes a novel technique for position error compensations of robots based on a fuzzy error interpolation method. A traditional robot calibration implements either model or modelless methods. The compensation of position error in a model-less method is to move the robot's end-effector to a target position in the robot workspace, and to find the target position error online based on the measured neighboring four-point errors around the target position. For this purpose, a stereo camera or other measurement device can be used to measure offline the position errors of the robot's end-effector at predefined grid points. By using the proposed fuzzy error interpolation technique, the accuracy of the position error compensation can be greatly improved, which is confirmed by the simulation results given in this paper. A comparison study among various interpolation methods, such as bilinear, cubic spline, and the fuzzy error interpolation technique is also made via simulation. The simulation results show that more accurate compensation results can be achieved using the fuzzy error interpolation technique compared with its bilinear and cubic spline counterparts.
Locally Discriminative Coclustering
Different from traditional one-sided clustering techniques, coclustering makes use of the duality between samples and features to partition them simultaneously. Most of the existing co-clustering algorithms focus on modeling the relationship between samples and features, whereas the intersample and interfeature relationships are ignored. In this paper, we propose a novel coclustering algorithm named Locally Discriminative Coclustering (LDCC) to explore the relationship between samples and features as well as the intersample and interfeature relationships. Specifically, the sample-feature relationship is modeled by a bipartite graph between samples and features. And we apply local linear regression to discovering the intrinsic discriminative structures of both sample space and feature space. For each local patch in the sample and feature spaces, a local linear function is estimated to predict the labels of the points in this patch. The intersample and interfeature relationships are thus captured by minimizing the fitting errors of all the local linear functions. In this way, LDCC groups strongly associated samples and features together, while respecting the local structures of both sample and feature spaces. Our experimental results on several benchmark data sets have demonstrated the effectiveness of the proposed method.
Automatic annotation of drosophila developmental stages using association classification and information integration
In current developmental research, one of the challenging tasks is to understand the spatio-temporal gene expression patterns and the relationships among different genes. In situ hybridization (ISH) assay which shows mRNA spatio-temporal expression patterns in cells and tissues directly is currently widely utilized in the bench work. With the increasing of available ISH images, automatic annotation systems are highly demanded. In this paper, an automatic classification system is proposed for annotating the in situ hybridization images with respect to the developmental stages. The embryo is first segmented from the original image, registered and normalized. The segmented embryo image is then divided into 100 blocks from which the pixel intensity and texture features are extracted and discretized. The multiple correspondence analysis (MCA) based association classification approach is proposed to generate classification rules for different stages based on the training data set. The testing instance is classified by applying the rules generated in the training process and a classification coordination module is incorporated to resolve the conflicts utilizing the weights derived from angle values in the MCA procedure. Experimental results show that our proposed method achieves promising results and outperforms other state-of-the-art algorithms.
Theoretical calculations of homoconjugation equilibrium constants in systems modeling acid-base interactions in side chains of biomolecules using the potential of mean force.
The potentials of mean force (PMFs) were determined for systems forming cationic and anionic homocomplexes composed of acetic acid, phenol, isopropylamine, n-butylamine, imidazole, and 4(5)-methylimidazole, and their conjugated bases or acids, respectively, in three solvents with different polarity and hydrogen-bonding propensity: acetonitrile (AN), dimethyl sulfoxide (DMSO), and water (H 2 O). For each pair and each solvent a series of umbrella-sampling molecular dynamics simulations with the AMBER force field, explicit solvent, and counterions added to maintain a zero net charge of a system were carried out and the PMF was calculated by using the Weighted Histogram Analysis Method (WHAM). Subsequently, homoconjugation-equilibrium constants were calculated by numerical integration of the respective PMF profiles. In all cases but imidazole stable homocomplexes were found to form in solution, which was manifested as the presence of contact minima corresponding to hydrogen-bonded species in the PMF curves. The calculated homoconjugation constants were found to be greater for complexes with the OHO bridge (acetic acid and phenol) than with the NHN bridge and they were found to decrease with increasing polarity and hydrogen-bonding propensity of the solvent (i.e., in the series AN > DMSO > H 2 O), both facts being in agreement with the available experimental data. It was also found that interactions with counterions are manifested as the broadening of the contact minimum or appearance of additional minima in the PMF profiles of the acetic acid-acetate, phenol/ phenolate system in acetonitrile, and the 4(5)-methylimidazole/4(5)-methylimidzole cation conjugated base system in dimethyl sulfoxide.
Cost-Effective Computer-Aided Manufacturing of Prototype Parts
Computer-aided manufacturing (CAM) can be made cost effective, even for one-of-a-kind jobs. To do so requires reevaluation of Computerized Numerical Control (CNC) from the point of view of a computer system, rather than a glorified machine shop, Good systematization, useful software and cooperative CNC shop staff can improve the productivity of a typical CNC shop by a factor of five.
Data-driven design of HMM topology for online handwriting recognition
Although HMM is widely used for online handwriting recognition, there is no simple and well-established method of designing the HMM topology. We propose a data-driven systematic method to design HMM topology. Data samples in a single pattern class are structurally simplified into a sequence of straight-line segments. Then the resulting multiple models of the class are combined to form an architecture of a multiple parallel-path HMM, which behaves as single HMM. To avoid excessive growing of the number of the states, parameter trying is applied such that structural similarity among patterns is reflected. Experiments on online Hangul recognition showed about 19% of error reductions, compared to the intuitive deisgn method.
Fast block size prediction for MPEG-2 to H.264/AVC transcoding
One objective in MPEG-2 to H.264 transcoding is to improve the H.264 compression ratio by using more accurate H.264 motion vectors. Motion re-estimation is by far the most time consuming process in video transcoding, and improving the searching speed is a challenging problem. We introduce a new transcoding scheme that uses the MPEG-2 DCT coefficients to predict the block size partitioning for H.264. Performance evaluations have shown that, for the same rate-distortion performance, our proposed scheme achieves an impressive reduction in the computational complexity of more than 82% compared to the full range motion estimation used by H.264.
A curvature-based approach to contour motion estimation
We present a novel method of velocity field estimation for points on moving contours in an image sequence. The method determines the corresponding point in the next image frame by considering curvature changes at each point on a contour. In previous methods, there are errors in estimation for the points which have low curvature variations since those methods compute the solutions by approximating the normal component of optical flow. The proposed method computes optical flow vectors of contour points by minimizing the curvature changes. As a first step, snakes are used to locate smooth curves in 2D imagery. Then, the extracted curves are tracked continuously. We excluded the rearranging process in snakes and allowed the snaxel distance to vary. Each point on a contour has a unique corresponding point in the nest frame. Experimental results showed that the proposed method computes accurate optical flow vectors for various moving contours.
The Case for Timing-Centric Distributed Software Invited Paper
This paper makes the case that the time is right to introduce temporal semantics into programming models for cyber-physical systems. Specifically, we argue for a programming model called PTIDES that provides a coordination language rooted in discreteevent semantics, supported by a lightweight runtime framework and tools for verifying concurrent software components. PTIDES leverages recent innovations in network time synchronization to deliver distributed real-time systems with determinate concurrent semantics, decentralized and robust control, and the potential for rigorous schedulability analysis.
Balancing Resource Utilization to Mitigate Power Density in Processor Pipelines
Power density is a growing problem in high-performance processors in which small, high-activity resources overheat. Two categories of techniques, temporal and spatial, can address power density in a processor. Temporal solutions slow computation and heating either through frequency and voltage scaling or through stopping computation long enough to allow the processor to cool; both degrade performance. Spatial solutions reduce heat by moving computation from a hot resource to an alternate resource (e.g., a spare ALU) to allow cooling. Spatial solutions are appealing because they have negligible impact on performance, but they require availability of spatial slack in the form of spare or underutilized resource copies. Previous work focusing on spatial slack within a pipeline has proposed adding extra resource copies to the pipeline, which adds substantial complexity because the resources that overheat, issue logic, register files, and ALUs, are the resources in some of the tightest critical paths in the pipeline. Previous work has not considered exploiting the spatial slack already existing within pipeline resource copies. Utilization can be quite asymmetric across resource copies, leaving some copies substantially cooler than others. We observe that asymmetric utilization within copies of three key back-end resources, the issue queue, register files, and ALUs, creates spatial slack opportunities. By balancing asymmetry in their utilization, we can reduce power density. Scheduling policies for these resources were designed for maximum simplicity before power density was a concern; our challenge is to address asymmetric heating while keeping the pipeline simple. Balancing asymmetric utilization reduces the need for other performance-degrading temporal power-density techniques. While our techniques do not obviate temporal techniques in high-resource-utilization applications, we greatly reduce their use, improving overall performance.
Efficient Vertical/Horizontal-Space 1D-DCT Processing Based on Massive-Parallel Matrix-Processing Engine
This paper reports an efficient discrete cosine transform (DCT) processing for the JPEG algorithm using a massive-parallel memory-embedded SIMD matrix processor. The matrix-processing engine has 2,048 2-bit processing elements, which are connected by a flexible switching network, and supports 2-bit 2,048-way bit-serial and word-parallel operations with a single command. For compatibility with this matrix-processing architecture, the conventional DCT algorithm has been improved in arithmetic order and the vertical/horizontal-space 1 dimensional (1D)-DCT processing has been further developed. Evaluation results of the matrix-engine-based DCT processing show that the necessary clock cycles per image blocks can be reduced by 87% in comparison to a conventional DSP architecture. The determined performances in MOPS and MOPS/mm are factors 8 and 5.6 better than with a conventional DSP, respectively. Moreover, the matrix-processing engine can reduce the number of total clock cycles for JPEG application about 49% in comparison to a conventional DSP architecture.
Parallel Online Ranking of Web Pages
Modern search engines use link structure of the World Wide Web in order to gain better results for ranking the results of users' queries. One of the most popular ranking algorithms which is based on link analysis is HITS. It generates very accurate outputs but because of huge amount of online computations, this algorithm is relatively slow. In this paper we introduce PHITS, a parallelized version of the HITS algorithm that is suitable for working with huge web graphs in a reason- able time. For implementing this algorithm, we use WebGraph framework and we focus on parallelizing access to web graph as the main bottleneck in the HITS algorithm. I. INTRODUCTION Search technology is one of the most important reasons for success of the web. The huge amount of information available on the web, its high growth rate, and its unstructured nature, all increase the need for search engines with high performance and accurate results. One of the major components of each search engine is its ranking algorithm. Traditional Information Retrieval (IR) systems usually use some models like VMS (4) and compute rank of results using content similarity measures between user's query and retrieved documents. But in the context of the web, there are some problems with these approaches. For example, spamming may lead to inefficient ranking. Some methods have been proposed to encounter these problems most of which uses some implicit information which is embedded in the web graph. These methods are known as Link-Analysis based algorithms. PageRank (5) and HITS (Hyperlink Induced Topic Search) (1) are the most well known algorithms in this category. PageRank, which is used by Google for ranking its results, is an offline and query-independent ranking algorithm. This means that the ranking is independent of the specific queries of users and therefore can be done once and used for all of the upcoming queries. On the other hand, HITS is an online and query-dependent algorithm. Being query dependent makes HITS more precise but it has some disadvantages too. In fact, required online computations for this algorithm is too much and the response time of the search engine after submitting queries by users is not acceptable. To overcome this problem, in this paper we will exploit the parallel processing methods to improve the execution performance of the algorithm. The rest of this paper is organized as follows. In section II, link-analysis based algorithms in general and HITS as a special case are discussed. At the end of this section, some of the variations and improvements for the HITS algorithm that are suggested in the literature are also described. Implementing the HITS algorithm and its parallel version, PHITS, are discussed in sections III and IV respectively. Finally, last section of this paper contains conclusion and some ideas for future work in this topic.
Authentication in distributed systems: theory and practice
We describe a theory of authentication and a system that implements it. Our theory is based on the notion of principal and a "speaks for" relation between principals. A simple principal either has a name or is a communication channel; a compound principal can express an adopted role or delegation of authority. The theory explains how to reason about a principal's authority by deducing the other principals that it can speak for; authenticating a channel is one important application. We use the theory to explain many existing and proposed mechanisms for security. In particular, we describe the system we have built. It passes principals efficiently as arguments or results of remote procedure calls, and it handles public and shared key encryption, name lookup in a large name space, groups of principals, loading programs, delegation, access control, and revocation.
Human Gait Recognition With Matrix Representation
Human gait is an important biometric feature. It can be perceived from a great distance and has recently attracted greater attention in video-surveillance-related applications, such as closed-circuit television. We explore gait recognition based on a matrix representation in this paper. First, binary silhouettes over one gait cycle are averaged. As a result, each gait video sequence, containing a number of gait cycles, is represented by a series of gray-level averaged images. Then, a matrix-based unsupervised algorithm, namely coupled subspace analysis (CSA), is employed as a preprocessing step to remove noise and retain the most representative information. Finally, a supervised algorithm, namely discriminant analysis with tensor representation, is applied to further improve classification ability. This matrix-based scheme demonstrates a much better gait recognition performance than state-of-the-art algorithms on the standard USF HumanID Gait database
Design for Testability Based on Single-Port-Change Delay Testing for Data Paths
This paper introduces a new concept of hierarchical testability called Single-Port-Change (SPC) two-pattern testability. We propose a non-scan design-for-testability (DFT) method which makes each path that needs to be tested in a data path SPC two-pattern testable. An SPC two-pattern test guarantees robust (resp. non-robust) test if the path is robust (resp. non-robust) testable. Since it is easy to find justification paths for SPC two-pattern tests at register-transfer level, the proposed DFT method can reduce hardware overhead compared to that of our previous DFT method for arbitrary two-pattern tests. Furthermore, we propose a method to reduce test generation effort by removing a subset of sequentially untestable paths from targets of test generation. Experimental results show that the proposed method can reduce hardware overhead without losing the quality of test.
DAIDS: An Architecture for Modular Mobile IDS
The popularity of mobile devices and the enormous number of third party mobile applications in the market have naturally lead to several vulnerabilities being identified and abused. This is coupled with the immaturity of intrusion detection system (IDS) technology targeting mobile devices. In this paper we propose a modular host-based IDS framework for mobile devices that uses behavior analysis to profile applications on the Android platform. Anomaly detection can then be used to categorize malicious behavior and alert users. The proposed system accommodates different detection algorithms, and is being tested at a major telecom operator in North America. This paper highlights the architecture, findings, and lessons learned.
An integrated data mining system to automate discovery of measures of association
Many data analysts require tools which can integrate their database management packages (e.g. Microsoft Access) with their data analysis ones (e.g. SAS, SPSS), and provide guidance for the selection of appropriate mining algorithms. In addition, the analysts need to extract and validate statistical results to facilitate data mining. In this paper, we describe an integrated data mining system called the Linear Correlation Discovery System (LCDS) that meets the above requirement. LCDS consists of four major sub-components, two of which, the selection assistant and the statistics coupler, we discussed in this paper. The former examines the scheme and instances to determine appropriate association measurement functions (e.g, chi-square, linear regression, ANOVA). The latter involves the appropriate statistical function on a sample data set, and extracts relevant statistical output such as /spl eta//sup 2/, and R/sup 2/ for effective mining of data. We also describe a new validation algorithm based on measuring the consistency of mining results applied to multiple test sets.
Configuring sensors by user learning for a locomotion aid interface
This article presents a study about a mobility aid system, which integrates adaptive control interface for a robotic walker. The objective is to give to the patient more flexibility in the choice of a technical aid. The specificity of our system is based on an auto-adaptive interface, which improves the configuration choice of the patient's driving commands. The results obtained from experimentations show the capabilities of our method for a technical aid. The learning process is applied on line to a neural network controller. The experiment is focused on sensors configuration and command of a powered walker interface.
Hermite normality tests
Nous presentons dans cet article une etude complete du test de normalite d'Hermite. Ce test utilise les proprietes des polynomes d'Hermite et une statistique de sphericite modifiee pour decider si un echantillon monodimensionnel, standardise et blanc est gaussien ou non. L'avantage majeur de cette approche est de definir une famille de statistiques qui va permettre d'adapter le choix d'un test particulier aux donnees. Nous avons etabli la distribution asymptotique du test d'Hermite sous l'hypothese nulle et sous l'hypothese alternative et etudie en details le cas particulier de tests a deux polynomes. Nous avons determine les tests asymptotiquement les plus puissants pour quelques distributions alternatives et effectue un grand nombre de simulations afin de comparer le test d'Hermite a trois autres tests. Les bons resultats obtenus nous encouragent a generaliser le test d'Hermite aux cas de donnees colorees et multivariees.
Modeling and Simulation of a Fuzzy Supervisory Controller for an Industrial Boiler
In this paper we compare and discuss the performance of a boiler evaporator system when the system is controlled by a traditional PID-type strategy and when the system is enhanced by using fuzzy logic blocks to provide set-points for the system. The strategy used in fuzzy logic controllers (FLCs) is called fuzzy supervisory control and it generates set-points for the conventional controllers. The boiler under test is a VU-60 industrial system that produces 180,000 pounds of steam per hour. The mathematical model of the plant is a scaled version model of that obtained for a thermoelectric unit. The new model simplifies the large-scale thermoelectric boiler model to an industrial small-scale type VU-60 boiler model based upon first principle mass and energy balance equations. The main change consists of representing only the behavior of the drum—evaporator system, having a partial model of the combustion process, with a simplified combustion control system and a three-element boiler feed-water controller. The control system for combustion and boiler feed-water receives a supervisory signal (or set-point tracking signal) that comes from the FLC to improve the performance of the overall control system. The behavior of the supervisory controller brings some advantages to the system performance, compared with the traditional control schemes. The comparison reflects fuel improvements from 2.5% to 6.5% depending upon the steam load ramp regime. The simulations are performed using the SIMULINK® shell running under the MATLAB ® platform.
Partial Functional Manipulation Based Wirelength Minimization
In-place flipping of rectangular blocks/cells can potentially reduce the wirelength of a floorplan/placement solution without changing the chip area, In a recent work [Hao 05], the flipping optimization is solved through a binary decision diagram (BDD) based approach. However, the BDD-based approach is not scalable for large SOC designs with many blocks due to memory and runtime blow-up. This paper presents a new approach using the partitioned ordered partial decision diagrams (POPDD) for wirelength minimization. POPDD is based on a novel compact partial functional representation between flip configurations and corresponding wirelengths. By controlling the number of nodes allowed per POPDD and the iterations, easy trade-off between runtime/memory and accuracy/optimality can be achieved. Experimental results clearly demonstrate the efficiency of the proposed approach.
Assessment of an Operational System for Crop Type Map Production Using High Temporal and Spatial Resolution Satellite Optical Imagery
Crop area extent estimates and crop type maps provide crucial information for agricultural monitoring and management. Remote sensing imagery in general and, more specifically, high temporal and high spatial resolution data as the ones which will be available with upcoming systems, such as Sentinel-2, constitute a major asset for this kind of application. The goal of this paper is to assess to what extent state-of-the-art supervised classification methods can be applied to high resolution multi-temporal optical imagery to produce accurate crop type maps at the global scale. Five concurrent strategies for automatic crop type map production have been selected and benchmarked using SPOT4 (Take5) and Landsat 8 data over 12 test sites spread all over the globe (four in Europe, four in Africa, two in America and two in Asia). This variety of tests sites allows one to draw conclusions applicable to a wide variety of landscapes and crop systems. The results show that a random forest classifier operating on linearly temporally gap-filled images can achieve overall accuracies above 80% for most sites. Only two sites showed low performances: Madagascar due to the presence of fields smaller than the pixel size and Burkina Faso due to a mix of trees and crops in the fields. The approach is based on supervised machine learning techniques, which need in situ data collection for the training step, but the map production is fully automatic.
Data driven frequency mapping for computationally scalable object detection
Nonlinear kernel Support Vector Machines achieve better generalizations, yet their training and evaluation speeds are prohibitively slow for real-time object detection tasks where the number of data points in training and the number of hypotheses to be tested in evaluation are in the order of millions. To accelerate the training and particularly testing of such nonlinear kernel machines, we map the input data onto a low-dimensional spectral (Fourier) feature space using a cosine transform, design a kernel that approximates the classification objective in a supervised setting, and apply a fast linear classifier instead of the conventional radial basis functions. We present a data driven hypotheses generation technique and a LogistBoost feature selection. Our experimental results demonstrate the computational improvements 20∼100× while maintaining a high classification accuracy in comparison to SVM linear and radial kernel basis function classifiers.
Towards computer-assisted photo-identification of humpback whales
This paper describes current work on a photo-id system for humpback whales. Individuals of this species can be uniquely identified by the light and dark pigmentation patches on their tails (flukes). We developed an interface that assists the user in segmenting the animal's tail from the sea and fitting an affine invariant coordinate grid to it. A numerical feature vector capturing the patch-distribution with respect to the grid is then automatically extracted and used to match the individual against a database of similarly processed images.
A novel fast approach for estimating error propagation in decision feedback detectors
The study of error-burst statistics is important for all detection systems, and more so for the decision feedback class. In data storage applications, many detection systems use decision feedback in one form or another. Fixed-delay tree search with decision feedback (FDTS/DF) and decision feedback equalization (DFE) are the direct forms, whereas the partial response detectors such as the reduced state sequence estimator (RSSE) and noise predictive maximum likelihood (NPML) detectors are the other forms. Although DF reduces the system complexity, it is inevitably linked with error propagation (EP), which can be quantified using error-burst statistics. Analytical evaluation of these statistics is difficult, if not impossible, because of the complexity of the problem. Hence, the usual practice is to use computer simulations. However, the computational time in traditional bit-by-bit simulations can be prohibitive at meaningful signal-to-noise ratios. In this paper, we propose a novel approach for fast estimation of error-burst statistics in FDTS/DF detectors, which is also applicable to other detection systems. In this approach, error events are initiated more frequently than natural by artificially injecting noise samples. These noise samples are generated using a transformation that results in significant reduction in computational complexity. Simulation studies show that the EP performance obtained by the proposed method matches closely with those obtained by bit-by-bit simulations, while saving as much as 99% of simulation time.
Incrementally maintaining classification using an RDBMS
The proliferation of imprecise data has motivated both researchers and the database industry to push statistical techniques into relational database management systems (RDBMSes). We study strategies to maintain model-based views for a popular statistical technique, classification, inside an RDBMS in the presence of updates (to the set of training examples). We make three technical contributions: (1) A strategy that incrementally maintains classification inside an RDBMS. (2) An analysis of the above algorithm that shows that our algorithm is optimal among all deterministic algorithms (and asymptotically within a factor of 2 of a non-deterministic optimal strategy). (3) A novel hybrid-architecture based on the technical ideas that underlie the above algorithm which allows us to store only a fraction of the entities in memory. We apply our techniques to text processing, and we demonstrate that our algorithms provide an order of magnitude improvement over non-incremental approaches to classification on a variety of data sets, such as the Citeseer and DBLife.
Optimal band selection for future satellite sensor dedicated to soil science
Hyperspectral imaging systems could be used for identifying the different soil types from the satellites. However, detecting the reflectance of the soils in all the wavelengths involves the use of a large number of sensors with high accuracy and also creates a problem in transmitting the data to earth stations for processing. The current sensors can reach a bandwidth of 20 nm and hence, the reflectance obtained using the sensors are the integration of reflectance obtained in each of the wavelength present in the spectral band. Moreover, not all spectral bands contribute equally to classification and hence, identifying the bands necessary to have a good classification is necessary to reduce sensor cost and problem in data transmission from the satellite. The work presents the spectral bands selected using a PCA-Based Forward Sequential band selection algorithm.
Positive Macroscopic Approximation for Fast Attribute Reduction
Attribute reduction is one of the challenging problems facing the effective application of computational intelligence technology for artificial intelligence. Its task is to eliminate dispensable attributes and search for a feature subset that possesses the same classification capacity as that of the original attribute set. To accomplish efficient attribute reduction, many heuristic search algorithms have been developed. Most of them are based on the model that the approximation of all the target concepts associated with a decision system is dividable into that of a single target concept represented by a pair of definable concepts known as lower and upper approximations. This paper proposes a novel model called macroscopic approximation, considering all the target concepts as an indivisible whole to be approximated by rough set boundary region derived from inconsistent tolerance blocks, as well as an efficient approximation framework called positive macroscopic approximation (PMA), addressing macroscopic approximations with respect to a series of attribute subsets. Based on PMA, a fast heuristic search algorithm for attribute reduction in incomplete decision systems is designed and achieves obviously better computational efficiency than other available algorithms, which is also demonstrated by the experimental results.
Managing multiple engineering projects in a manufacturing support environment
Business trends require front-line managers to integrate multiproject concepts with those of traditional single-project management since very rarely can one find major organizations managing just one project. A typical situation entails a limited pool of resources which is applied to the management of several projects, with people moving back and forth among different assignments in different projects. Yet, few studies on project management have started to explore the issue of how to manage an organization with multiple inter- or intradepartmental projects. Using a case study method, our exploratory research investigates the specific problems associated with the management of multiple engineering projects in a manufacturing support environment, with the intent to identify common factors of success. Knowing the factors of success is but the first step toward improving multi-project management. Our findings provide insight into how the most important multiple-project success factors in this environment differ from factors of success in traditional single-project management, and are consistent with other emerging research in product development environments. The differences center on resource allocation and flexibility. Some factors, such as ownership, staff experience, and communication, take on additional dimensions when considered in a multiple-versus a single-project environment. Division and assignment of resources, prioritization, and customized management style, which have little relevance in relation to single projects, are shown to play a major role in the success of multiproject management.
Performance analysis of a recursive fractional super-exponential algorithm
The super-exponential algorithm is a block-based technique for blind channel equalization and system identification. Due to its fast convergence rate, and no a priori parameterization other than the block length, it is a useful tool for linear equalization of moderately distortive channels. This paper presents a recursive implementation of the super-exponential algorithm for fractionally-sampled PAM signals. Although the resulting algorithm is still block-based, recursive propagation of several key variables allows the block length to be significantly reduced without compromising the algorithm's accuracy or speed, thereby enhancing its ability to track channel variations. The convergence rate is only mildly influenced by specific channel responses, and oversampling provides smaller output variance and almost perfect tolerance to sampling errors. Simulation results demonstrate the effectiveness of the proposed technique.
Free-viewpoint image generation using different focal length camera array
The availability of multi-view images of a scene makes new and exciting applications possible, including Free-Viewpoint TV (FTV). FTV allows us to change viewpoint freely in a 3D world, where the virtual viewpoint images are synthesized by Image-Based Rendering (IBR). In this paper, we introduce a FTV depth estimation method for forward virtual viewpoints. Moreover, we introduce a view generation method by using a zoom camera in our camera setup to improve virtual viewpoint-ts' image quality. Simulation results confirm reduced error during depth estimation using our proposed method in comparison with conventional stereo matching scheme. We have demonstrated the improvement in image resolution of virtually moved forward camera using a zoom camera setup.
Automatic location and tracking of the facial region in color video sequences
A novel technique is introduced to locate and track the facial area in videophone-type sequences. The proposed method essentially consists of two components: (i) a color processing unit, and (ii) a knowledge-based shape and color analysis module. The color processing component utilizes the distribution of skin-tones in the HSV color space to obtain an initial set of candidate regions or objects. The second component in the segmentation scheme, that is, the shape and color analysis module is used to correctly identify and select the facial region in the case where more than one object has been extracted. A number of fuzzy membership functions are devised to provide information about each object’s shape, orientation, location and average hue. An aggregation operator finally combines these measures and correctly selects the facial area. The suggested approach is robust with regard to di⁄erent skin types, and various types of object or background motion within the scene. Furthermore, the algorithm can be implemented at a low computational complexity due to the binary nature of the operations involved. Experimental results are presented for a series of CIF and QCIF video sequences. ( 1999 Elsevier Science B.V. All rights reserved.
On deriving the second-stage training set for trainable combiners
Unlike fixed combining rules, the trainable combiner is applicable to ensembles of diverse base classifier architectures with incomparable outputs. The trainable combiner, however, requires the additional step of deriving a second-stage training dataset from the base classifier outputs. Although several strategies have been devised, it is thus far unclear which is superior for a given situation. In this paper we investigate three principal training techniques, namely the re-use of the training dataset for both stages, an independent validation set, and the stacked generalization. On experiments with several datasets we have observed that the stacked generalization outperforms the other techniques in most situations, with the exception of very small sample sizes, in which the re-using strategy behaves better. We illustrate that the stacked generalization introduces additional noise to the second-stage training dataset, and should therefore be bundled with simple combiners that are insensitive to the noise. We propose an extension of the stacked generalization approach which significantly improves the combiner robustness.
Automated Service Composition with Adaptive Planning
Service-Oriented Computing is a cornerstone for the realization of user needs through the automatic composition of services from service descriptions and user tasks, i.e., high-level descriptions of the user needs. Yet, automatic service composition processes commonly assume that service descriptions and user tasks share the same abstraction level, and that services have been pre-designed to integrate. To release these strong assumptions and to augment the possibilities of composition, we add adaptation features into the service composition process using semantic descriptions and adaptive extensions to graph planning.
Base station scheduling of requests with fixed deadlines
We consider a packet switched wireless network in which a base station serves the mobiles. The packets for the mobiles arrive at the base station and have to be scheduled for transmission to the mobiles. The capacity of the channel from the base station to the mobiles is varying with time due to fading. We assume the mobiles can obtain different types of service based on the prices they are willing to pay. The objective of the base station is to schedule packets for transmission to mobiles to maximize the revenue earned. Our main result is that a simple greedy algorithm does at least 1/2 as good as the optimal offline algorithm that knows the complete future request pattern and channel conditions. We also show that no other online algorithm can do better.
THE SRI NIST 2008 speaker recognition evaluation system
The SRI speaker recognition system for the 2010 NIST speaker recognition evaluation (SRE) incorporates multiple subsystems with a variety of features and modeling techniques. We describe our strategy for this year's evaluation, from the use of speech recognition and speech segmentation to the individual system descriptions as well as the final combination. Our results show that under most conditions, the cepstral systems tend to perform the best, but that other, non-cepstral systems have the most complementarity. The combination of several subsystems with the use of adequate side information gives a 35% improvement on the standard telephone condition. We also show that a constrained cepstral system based on nasal syllables tends to be more robust to vocal effort variabilities.
Rehabilitation of handwriting skills in stroke patients using interactive games: a pilot study
This paper describes an interactive application that aims to support the rehabilitation of handwriting skills in people that suffer from paralysis after a stroke. The purpose of the application is to make the rehabilitation of handwriting skills fun and engaging. Four platform-independent games with adjustable levels of difficulty were created in order to target varying levels of skills. The application also features a performance history, audio-visual feedback, and posture reminders. It was evaluated with medical staff and patients from the Hoensbroeck Rehabilitation Centre in the Netherlands. The initial results indicated that the games are more motivating and fun than traditional pen and paper exercises. The feedback received from therapists supports our claim that the games are a useful addition to the rehabilitation of handwriting.
A Digital Switching Demodulator for Electrical Capacitance Tomography
In this paper, a digital switching demodulator is presented for use in ac-based electrical capacitance tomography systems. Implementing a switching phase-sensitive demodulator (PSD) digitally offers the following advantages: 1) Demodulation can be implemented using a programmable digital device, and hence, CMOS switches, which are used in a conventional switching PSD, are no longer needed; 2) compared with the widely used digital quadrature PSD, this proposed demodulator is simple in configuration because neither a reference signal nor multiplication is required; 3) according to the specific requirements, the new demodulator can be implemented in two operation modes, i.e., the amplitude mode and the phase-sensitive mode; and 4) because only subtractions and accumulations are needed, the proposed demodulator can be easily implemented with low-cost logic devices, e.g., a complex programmable logic device (CPLD). By simulation, the feasibility and effectiveness of the proposed demodulator have been confirmed. CPLD-based and field-programmable-gate-array-based capacitance measurement circuits are constructed, and the performances of different demodulation methods are compared. Both simulation and experiment show that the proposed demodulator can provide demodulation results with high signal-to-noise ratio. The system design can be simplified using the digital switching demodulator.
Psychophysical evaluation of in-situ ultrasound visualization
We present a novel psychophysical method for evaluating ultrasonography based on real-time tomographic reflection (RTTR), in comparison to conventional ultrasound (CUS). The method measures the user's perception of the location of an ultrasound-imaged target independently from assessing the action employed to reach it. Three experiments were conducted with the sonic flashlight (SF), an RTTR device, and CUS. The first two experiments determined subjects' perception of target location with a triangulation-by-pointing task. Depth perception with the SF was comparable to direct vision, while CUS caused considerable underestimation of target depth. Binocular depth information in the SF was shown to significantly contribute to its superiority. The third experiment tested subjects in an ultrasound-guided needle insertion task. Because the SF provides visualization of the target at its actual location, subjects performed insertions faster and more accurately by using the SF rather than CUS. Furthermore, the trajectory analysis showed that insertions with the SF generally went directly to the target along the desired path, while CUS often led to a large deviation from the correct path consistent with the observed underestimation of target depth. These findings lend great promise to the use of RTTR-based imaging in clinical practice and provide precise means of assessing efficacy.
Probabilistic model checking of complex biological pathways
Probabilistic model checking is a formal verification technique that has been successfully applied to the analysis of systems from a broad range of domains, including security and communication protocols, distributed algorithms and power management. In this paper we illustrate its applicability to a complex biological system: the FGF (Fibroblast Growth Factor) signalling pathway. We give a detailed description of how this case study can be modelled in the probabilistic model checker PRISM, discussing some of the issues that arise in doing so, and show how we can thus examine a rich selection of quantitative properties of this model. We present experimental results for the case study under several different scenarios and provide a detailed analysis, illustrating how this approach can be used to yield a better understanding of the dynamics of the pathway. Finally, we outline a number of exact and approximate techniques to enable the verification of larger and more complex pathways and apply several of them to the FGF case study.
A resistance-based approach to consensus algorithm performance analysis
We study the well known linear consensus algorithm by means of a LQ-type performance cost. We want to understand how the communication topology influences this algorithm. In order to do this, we recall the analogy between Markov Chains and electrical resistive networks. By exploiting this analogy, we are able to rewrite the performance cost as the average effective resistance on a suitable network. We use this result to show that if the communication graph fulfills some local properties, then its behavior can be approximated with that of a suitable grid, over which the behavior of the cost is known.
On the Use of the Source Reconstruction Method for Estimating Radiated EMI in Electronic Circuits
Electromagnetic interference (EMI) regulations are a very important issue in the design of almost any electronic circuit. Over the years, “cut-and-try” procedures have been adopted by electronic designers to make circuits comply with these regulations, mainly due to the lack of reliable theoretical models of radiated noise and clear design rules. To gain new insight into this field, a novel approach is presented in this paper based on a well-known technique in the field of antenna design, i.e., the source reconstruction method (SRM). Its application allows a set of equivalent currents to be obtained that behave exactly like the circuit under consideration with regard to radiated noise. From these currents, magnetic and electric radiated fields can be obtained at any point in space, even at 3 or 10 m away from the circuit where the regulations must be met. Moreover, the equivalent currents accurately represent noise sources in the circuit, thus permitting the elements responsible for generating radiated noise to be located. The aforementioned method would enable designers to reduce the use of anechoic-chamber facilities when testing their designs, thereby leaving the chamber only for final certification purposes.
MIMO Cooperative Diversity in a Transmit Power Limited Environment
This paper considers a fading relay channel where the total transmit power used is constrained to be equal to that of the standard single-hop channel. The relay channel used operates in what is termed as MIMO cooperative diversity mode, where the source transmits to both relay and destination terminals in the first instance. Both the source and relay then transmit to the destination in the second instance. Initially the cooperative diversity framework is introduced to consider system constraints so a direct and fair comparison with the single-hop case can be made. In-particular a power constraint is placed on the system and the optimal transmit power levels are derived and presented. The derived technique for finding the optimal transmit power levels is then used to demonstrate the advantages of using cooperative diversity in a wireless network. The results presented show that MIMO cooperative diversity offers a 3.4 dB increase in spectral efficiency at 5 % outage, with no additional cost incurred in transmit time, power or bandwidth.
An exact solution procedure for multi-item two-echelon spare parts inventory control problem with batch ordering in the central warehouse
We consider a multi-item two-echelon inventory system in which the central warehouse operates under a (Q,R) policy, and the local warehouses implement basestock policy. An exact solution procedure is proposed to find the inventory control policy parameters that minimize the system-wide inventory holding and fixed ordering cost subject to an aggregate mean response time constraint at each facility.
Utility-based admission control for mobile WiMAX networks
This paper presents a novel utility-based connection admission control (CAC) scheme for IEEE 802.16e broadband wireless access networks. We develop specific utility functions for real-time and non-real-time services coupled with a handover process. Given these utility functions we characterize the network utility with respect to the allocated bandwidth, and further propose a CAC algorithm which admits a connection that conducts to the greatest utility so as to maximize the total resource utilization. The simulation results demonstrate the effectiveness of the proposed CAC algorithm in terms of network utility.
Planetary-Scale Terrain Composition
Many interrelated planetary height map and surface image map data sets exist, and more data are collected each day. Broad communities of scientists require tools to compose these data interactively and explore them via real-time visualization. While related, these data sets are often unregistered with one another, having different projection, resolution, format, and type. We present a GPU-centric approach to the real-time composition and display of unregistered-but-related planetary-scale data. This approach employs a GPGPU process to tessellate spherical height fields. It uses a render-to-vertex-buffer technique to operate upon polygonal surface meshes in image space, allowing geometry processes to be expressed in terms of image processing. With height and surface map data processing unified in this fashion, a number of powerful composition operations may be uniformly applied to both. Examples include adaptation to nonuniform sampling due to projection, seamless blending of data of disparate resolution or transformation regardless of boundary, and the smooth interpolation of levels of detail in both geometry and imagery. Issues of scalability and precision are addressed, giving out-of-core access to giga-pixel data sources, and correct rendering at scales approaching one meter.
Continuous signature monitoring: low-cost concurrent detection of processor control errors
A low-cost approach to concurrent detection of processor control errors is presented that uses a simple hardware monitor and signatures embedded into the executing program. Existing signature-monitoring techniques detect a large portion of processor control errors at a fraction of the cost of duplication. Analytical methods developed in this study show that the new approach, continuous signature monitoring (CSM), makes major advances beyond existing techniques. CSM reduces the fraction of undetected control-flow errors by orders of magnitude, to less than 10/sup -6/, while the number of signatures reaches a theoretical minimum, being lowered by as much as three times to a range of 4-11%. Signature cost is reduced by placing CSM signatures at locations that minimize performance loss and (for some architectures) memory overhead. CSM exploits the program memory's SEC/DED code to decrease error-detection latency by as much as 1000 times, to 0.016 program memory cycles, without increasing memory overhead. This short latency allows transient faults to be tolerated. >
Traffic monitoring techniques for measurement based flow acceptance control
In this paper we describe the development of a measurement based flow acceptance control mechanism that will support guaranteed services in multiservice packet switched networks. Using simulation models we present simulation experiments with two monitoring techniques. One that monitors the percentile occupancy of queue and the other monitors the mean and variance of packet inter-arrival times and packet lengths of a continuous media packet stream. Both these techniques are considered in the development of the measurement based flow acceptance mechanism.
Using Fibonacci Compression Codes as Alternatives to Dense Codes
Recent publications advocate the use of various variable length codes for which each codeword consists of an integral number of bytes in compression applications using large alphabets. This paper shows that another tradeoff with similar properties can be obtained by Fibonacci codes. These are fixed codeword sets, using binary representations of integers based on Fibonacci numbers of order m ges 2. Fibonacci codes have been used before, and this paper extends previous work presenting several novel features. In particular, they compress better and are more robust, at the price of being slower.
Inter-finger coordination and postural synergies in robot hands via mechanical implementation of principal components analysis
Human hands employ characteristic patterns of actuation, or synergies, that contain much of the information required to describe an entire hand shape. In some cases, 80% or more of the total information can be described with only two scalar component values. Robotic hands, however, commonly only couple intra-finger joints, and rarely take advantage of this inter-finger coordination. In this paper, real-world data on a variety of human hand postures was collected using a data glove, and principal components analysis was used to calculate these synergies, resulting in what we call eigenpostures. A novel mechanism design is presented to combine the eigenpostures and drive a 17-degree-of-freedom 5-fingered robot hand. The hand uses only 2 DC motors to accurately recreate a wide range of hand shapes. We also present a design improvement that allows us to distinguish between high-precision and low-precision tasks, as well as greatly reduce overall error.
Developing Process Mediator for Web Service Interactions
Web service interactions lie in the core of SOA. Due to the autonomy, heterogeneity and continuous evolution of Web services, mediators are usually needed to support service interactions to overcome possible mismatches that may exist among business processes. In this paper, we introduce a space-based architecture for process mediator which considers both control-flow and data-flow, present possible mismatch patterns, and suggest how they can be automatically mediated. Our work can be used to perform runtime mediation and thus to facilitate service interactions.
Multiple-view object recognition in band-limited distributed camera networks
In this paper, we study the classical problem of object recognition in low-power, low-bandwidth distributed camera networks. The ability to perform robust object recognition is crucial for applications such as visual surveillance to track and identify objects of interest, and compensate visual nuisances such as occlusion and pose variation between multiple camera views. We propose an effective framework to perform distributed object recognition using a network of smart cameras and a computer as the base station. Due to the limited bandwidth between the cameras and the computer, the method utilizes the available computational power on the smart sensors to locally extract and compress SIFT-type image features to represent individual camera views. In particular, we show that between a network of cameras, high-dimensional SIFT histograms share a joint sparse pattern corresponding to a set of common features in 3-D. Such joint sparse patterns can be explicitly exploited to accurately encode the distributed signal via random projection, which is unsupervised and independent to the sensor modality. On the base station, we study multiple decoding schemes to simultaneously recover the multiple-view object features based on the distributed compressive sensing theory. The system has been implemented on the Berkeley CITRIC smart camera platform. The efficacy of the algorithm is validated through extensive simulation and experiments.
Change Detection for Bridges over Water in Airborne and Spaceborne SAR Data
The main advantages of SAR are the capability of imaging large areas in short time and delivering data at any day time and under nearly all weather conditions. This is especially important for disaster management and continuous long term monitoring applications. Key elements of man-made infrastructure are bridges. Especially for bridges over water, the SAR specific side looking imaging geometry can lead to special characteristics in the image. In this paper, the possibilities of extracting bridge features like width and height from SAR data, especially for bridges over water, are discussed. The feature extraction is based on the segmentation of parallel lines in an image. An approach is presented to exploit this feature extraction for change detection. The investigations are supported by SAR simulations, and real airborne and spaceborne data are presented.
Variational-based speckle noise removal of SAR imagery
In this paper we present a variational method for synthetic aperture radar (SAR) speckle removal. Variational method is a newly developed technique for the removal of SAR's multiplicative noise. For an image, we could define an energy functional. The energy evolves as the original image changes, and the minimum energy corresponds to the speckle reduced result. Partial differential equation (PDE) technique is used to get the minimal solution. Our energy functional makes use of the statistical information of the multiplicative noise since it follows a Gamma law with mean mu = 1 and variance sigma 2  = 1/M for M-look SAR. Our energy is a regularization term with two constraints. The regularization term is the integral for the norm of image gradient; two constraints are the mean of noise should be 1 and the variance of noise should be 1/M. We use the method of Lagrange multipliers, Euler-Lagrange equation and heat flow method to obtain the minimizer of the energy. ERS Precision Image (PRI) data are to demonstrate our algorithm. Numerical result shows that the speckle reduced image preserves edges and point targets while smoothes homogenous regions in the original image. The algorithm is computationally efficient and easy to implement.
Cluster-based distributed consensus
In this paper, we incorporate clustering techniques into distributed consensus algorithms for faster convergence and better energy efficiency. Together with a simple distributed clustering algorithm, we design cluster-based distributed consensus algorithms in forms of both fixed linear iteration and randomized gossip. The time complexity of the proposed algorithms is presented in terms of metrics of the original and induced graphs, through which the advantage of clustering is revealed. Our cluster-based algorithms are also shown to achieve an Omega(log n) gain in message complexity over the standard ones.
XIS-UML Profile for eXtreme Modeling Interactive Systems
The first version of the XIS profile addressed the development of interactive systems by defining models oriented only towards how the system should perform tasks. However, issues such as user-interface layouts, or the capture of interaction patterns, were not addressed by the profile, but only by the source-code generation process. This originated systems that, although functional, were considered by end-users as "difficult to use". In this paper we present the second version of the XIS UML profile, which is now a crucial component of the ProjectIT research project. This profile follows the "separation of concerns" principle by proposing an integrated set of views that address the various issues detected with the previous version of XIS. In addition, this profile also promotes the usage of extreme modeling, by relying on the extensive use of model-to-model transformation templates that are defined to accelerate the model development tasks
Forward acknowledgement: refining TCP congestion control
We have developed a Forward Acknowledgment (FACK) congestion control algorithm which addresses many of the performance problems recently observed in the Internet. The FACK algorithm is based on first principles of congestion control and is designed to be used with the proposed TCP SACK option. By decoupling congestion control from other algorithms such as data recovery, it attains more precise control over the data flow in the network. We introduce two additional algorithms to improve the behavior in specific situations. Through simulations we compare FACK to both Reno and Reno with SACK. Finally, we consider the potential performance and impact of FACK in the Internet.
In-service signal quality estimation for TDMA cellular systems
In-service interference plus noise power (I+N) and signal-to-interference plus noise power SI(I+N) estimation methods are examined for TDMA cellular systems. A simple (I+N) estimator is developed whose accuracy depends on the channel and symbol estimate error statistics. Improved (I+N) and S/(I+N) estimators are developed whose accuracy depends only on the symbol error statistics. The proposed estimators are evaluated through software simulation with an IS-54 frame structure. For high speed mobiles, it is demonstrated that S/(I+N) can be estimated to within 2 dB in less than a second.
Embedding agents within the intruder to detect parallel attacks
We carry forward the work described in our previous papers [5,18,20] on the application of data independence to the model checking of security protocols using CSP [19] and FDR [10]. In particular, we showed how techniques based on data independence [12,19] could be used to justify, by means of a finite FDR check, systems where agents can perform an unbounded number of protocol runs. Whilst this allows for a more complete analysis, there was one significant incompleteness in the results we obtained: while each individual identity could perform an unlimited number of protocol runs sequentially, the degree of parallelism remained bounded (and small to avoid state space explosion). In this paper, we report significant progress towards the solution of this problem, by means anticipated in [5], namely by “internalising” protocol roles within the “intruder” process. The internalisation of protocol roles (initially only server-type roles) was introduced in [20] as a state-space reduction technique (for which it is usually spectacularly successful). It was quickly noticed that this had the beneficial side-effect of making the internalised server arbitrarily parallel, at least in cases where it did not generate any new values of data independent type. We now consider the case where internal roles do introduce fresh values and address the issue of capturing their state of mind (for the purposes of analysis).
Can the Web Be Made Accessible for People with Intellectual Disabilities
This article presents the findings of a research project that aimed to contribute to the social inclusion of people with intellectual disabilities (ID) in the World Wide Web (the Web). The Inclusive New Media Design (INMD) project brought together thirty-one Web designers and developers with twenty-nine people with intellectual disabilities to explore the best practice for building Web sites accessible to the ID community. Specifically, the project took accessibility techniques identified in ID accessibility research, and investigated what would (or would not) make it possible for Web professionals to implement them. This article suggests some tentative answers to the question of whether a fully accessible Web can be built, one that includes people with ID. While the article outlines simple steps that can be taken to facilitate accessibility for people at the mild end of the ID spectrum, it also highlights a number of barriers that exist to implementing ID accessibility guidance, most notably the power holders and decision makers with whom Web designers work, who may not share the designers' commitment to accessibility.
A parallel implementation of a fast multipole based 3-D capacitance extraction program on distributed memory multicomputers
Very fast and accurate 3-D capacitance extraction is essential for interconnect optimization in ultra deep sub-micro designs (UDSM). Parallel processing provides an approach to reducing the simulation turn-around time. This paper examines the parallelization of the well known fast multipole based 3-D capacitance extraction program FASTCAP, which employs new preconditioning and adaptive techniques. To account for the complicated data dependencies in the unstructured problems, we propose a generalized cost function model, which can be used to accurately measure the workload associated with each cube in the hierarchy. We then present two adaptive partitioning schemes, combined with efficient communication mechanisms with bounded buffer size, to reduce the parallel processing overhead. The overall load balance is achieved through balancing the load at each level of the multipole computation. We report detailed performance results using a variety of standard benchmarks on 3-D capacitance extraction, on an IBM SP2.
Camera parameters auto-adjusting technique for robust robot vision
How to make vision system work robustly under dynamic light conditions is still a challenging research focus in computer/robot vision community. In this paper, a novel camera parameters auto-adjusting technique based on image entropy is proposed. Firstly image entropy is defined and its relationship with camera parameters is verified by experiments. Then how to optimize the camera parameters based on image entropy is proposed to make robot vision adaptive to the different light conditions. The algorithm is tested by using the omnidirectional vision in indoor RoboCup Middle Size League environment and the perspective camera in outdoor ordinary environment, and the results show that the method is effective and color constancy to some extent can be achieved.
New but not improved : a critical examination of revisions to the Regulation of Investigatory Powers Act 2000 encryption provisions
Considering the criminal uses of encryption, it has been asserted that national security and law enforcement endeavours must not be frustrated by potential evidence being hidden through digital encryption while the encryption key is withheld. The facilitation of state access to encryption keys through the Regulation of Investigatory Powers Act 2000 (‘RIPA 2000’) was intended to address precisely such a danger. However, since this statute’s enactment there have been significant shifts in law and policy relating to terrorism and child pornography as well as important technological developments. This article critically examines changes to the RIPA 2000 encryption provisions made by the Terrorism Act 2006 and the Policing and Crime Act 2009. It also considers emerging statistical data and cases including R v S(F) [2008]. It concludes that the underlying premises of the revised provisions are flawed, the provisions themselves ineffectual in practice and in the longer term potentially open to ‘mission creep’ to cover lesser offences.
The metaDESK: models and prototypes for tangible user interfaces
The metaDESK is a user interface platform demonstrating new interaction techniques we call "tangible user inter- faces." We explore the physical instantiation of interface elements from the graphical user interface paradigm, giving physical form to windows, icons, handles, menus, and controls. The design and implementation of the metaDESK display, sensor, and software architectures is discussed. A prototype application driving an interaction with geographi- cal space, Tangible Geospace, is presented to demonstrate these concepts.
A Motivation for "Ubuntu" to Enhance e-Learning Social Network Services in South Africa
This paper acknowledges the move from the content-centric to the network-centric approach to teaching and learning using Social Network Services (SNSs) in Higher Education. The South African cultural context is explained with a view to reconciling this with current SNSs. Finally, a way forward is proposed for developing SNSs particular to the Southern African cultural context.
Recent experiences utilizing TerraSAR-X for the monitoring of natural disasters in different parts of the world
PASCO in approximately 4 years, after the launch of TerraSAR-X (TSX), has successfully carried out a total 22 studies of disaster response for the worldwide and domestic cases. The outline and a part of the conducted cases are explained in detailed in this paper.
ÆTHER: an Authorization Management Architecture for Ubiquitous Computing ±
The ubiquitous computing paradigm suggests that we are going to be surrounded by countless wireless devices capable of providing services trans- parently. By definition, the nature of ubiquitous computing environments is open and extremely dynamic, making difficult the establishment of predefined security relationships between all of the participating entities. Authentication mechanisms can be employed to establish the identity of a pervasive computing entity but they suffer from scalability problems and have limited value in defin- ing authorization decisions among strangers. In this paper we propose AETHER, an authorization management architecture designed specifically to address trust establishment and access control in ubiquitous computing environments. Own- ers define attribute authority sets and access control policy entries that are em- bedded into their devices. Members of the attribute authority sets are trusted to issue credentials for the corresponding attributes that can then be used in order to gain access to protected resources. Our architecture supports dynamic mem- bership in these sets facilitating distributed administration, which is required in the context of the volatile nature of ubiquitous security relationships, and at- tribute mapping to allow roaming among authority domains. Moreover, we pre- sent the foundation of a logic model for our proposed architecture that is used to prove access control decisions.
Utilizing semantic caching in ubiquitous environment
Semantic caching is a dynamic caching strategy which deals with not only exact but also inexact similar queries. In this manner, each query will be carefully analyzed by the cache manager to identify the part that can be found in the cache from the part that needs to be retrieved from the server. This trimming process not only speeds up information retrieval but also saves on communication cost especially for mobile and wireless devices. Therefore, query trimming is a key problem in mobile and wireless environment, and devices in this environment have limited connection time, bandwidth, and battery power. However, the existing methods for query trimming have a number of limitations such as, inefficiency in time, space and the complexity of the algorithm used for trimming. These factors restrict the applicability of semantic caching for many applications. In this paper we investigate the shortcomings of query trimming process and propose a new solution to improve this process.
A study on using hierarchical basis error estimates in anisotropic mesh adaptation for the finite element method
A common approach for generating an anisotropic mesh is the M-uniform mesh approach where an adaptive mesh is generated as a uniform one in the metric specified by a given tensor M. A key component is the determination of an appropriate metric, which is often based on some type of Hessian recovery. Recently, the use of a global hierarchical basis error estimator was proposed for the development of an anisotropic metric tensor for the adaptive finite element solution. This study discusses the use of this method for a selection of different applications. Numerical results show that the method performs well and is comparable with existing metric tensors based on Hessian recovery. Also, it can provide even better adaptation to the solution if applied to problems with gradient jumps and steep boundary layers. For the Poisson problem in a domain with a corner singularity, the new method provides meshes that are fully comparable to the theoretically optimal meshes.
Believable judge bot that learns to select tactics and judge opponents
This paper describes our believable judge bot ICE-CIG2011 that has an ability to learn tactics from a judge player and an ability to judge an opponent character as a human or a bot. We conjecture that a bot with these two abilities should be considered human-like in a competition environment, such as BotPrize, where human players participate to compete not only for being the most human-like player but also the best judge. Main contributions of this work lie in our mechanisms for achieving these two abilities. To achieve the former ability, we develop a system and GUI that allow a selected judge player — whose role is to train ICE-CIG2011 — to control his or her character by only deciding which tactic to use under a given situation. We then obtain the judge's tactic log and use it for training tactic selection of ICE-CIG2011 with neuro evolution of augmenting topologies. To achieve the latter ability, we acquire additional logs when the judge character interacts with other opponent characters. In order to represent the play of a known (bot or human) character, we train a neural gas — a kind of self-organizing neural network — from its log. For an unknown character, once its neural gas is trained after a certain period of observation, ICE-CIG2011 decides if it is a human or bot by using the if-nearest-neighbor algorithm; this algorithm considers the majority in the labels of the if-nearest neural gases, of known characters, to the neural gas of that unknown character. Experimental results are given and discussed concerning these two abilities of ICE-CIG2011.
Type-based parametric analysis of program families
Previous research on static analysis for program families has focused on lifting analyses for single, plain programs to program families by employing idiosyncratic representations. The lifting effort typically involves a significant amount of work for proving the correctness of the lifted algorithm and demonstrating its scalability. In this paper, we propose a parameterized static analysis framework for program families that can automatically lift a class of type-based static analyses for plain programs to program families. The framework consists of a parametric logical specification and a parametric variational constraint solver. We prove that a lifted algorithm is correct provided that the underlying analysis algorithm is correct. An evaluation of our framework has revealed an error in a previous manually lifted analysis. Moreover, performance tests indicate that the overhead incurred by the general framework is bounded by a factor of 2.
Polar write once memory codes
A coding scheme for write once memory (WOM) using polar codes is presented. It is shown that the scheme achieves the capacity region of noiseless WOMs when an arbitrary number of multiple writes is permitted. The encoding and decoding complexities scale as O(N log N) where N is the blocklength. For N sufficiently large, the error probability decreases sub-exponentially in N. Some simulation results with finite length codes are presented.
Tolerance towards sensor failures: an application to a double inverted pendulum
In this paper we present a sensor fault tolerance control scheme that is applied to a double inverted pendulum. Sensor faults will affect the system when it is used in closed-loop feedback. The scheme uses a linear observer reconstruct the sensor fault and to subtract the reconstruction from the faulty sensor. The net result is then used for the closed-loop feedback. It was found that the scheme restored the performance to the fault-free scenario.
On the Expressiveness of Coordination Models
A number of different coordination models for specifying inter-process communication and synchronisation rely on a notion of shared dataspace. Many of these models are extensions of the Linda coordination model, which includes operations for adding, deleting and testing the presence/absence of data in a shared dataspace.#R##N##R##N#We compare the expressive power of three classes of coordination models based on shared dataspaces. The first class relies on Linda's communication primitives, while a second class relies on the more general notion of multi-set rewriting (e.g., like Bauhaus Linda or Gamma). Finally, we consider a third class of models featuring communication transactions that consist of sequences of Linda-like operations to be executed atomically (e.g., like in Shared Prolog or PoliS).
Manifold alignment for multitemporal hyperspectral image classification
While spectral and temporal advantages of multitemporal hyperspectral images provide opportunities for advancing classification of time varying phenomena, significant challenges are associated with high dimensionality and nonstationary signatures. While manifold learning retains critical geometry and develops a low dimension space where class clusters are recovered, spectral changes in temporal imagery impact the fidelity of the geometric representation of class dependent data. In this paper, we investigate a manifold alignment framework that exploits prior information while exploring similar local structures. The aim is to make use of common underlying geometries of two multitemporal images and embed the resemblances in a joint data manifold for classification tasks. Promising results support the advantages of the proposed manifold alignment approach.
Incremental rule learning based on example nearness from numerical data streams
Mining data streams is a challenging task that requires online systems based on incremental learning approaches. This paper describes a classification system based on decision rules that may store up-to-date border examples to avoid unnecessary revisions when virtual drifts are present in data. Consistent rules classify new test examples by covering and inconsistent rules classify them by distance as the nearest neighbor algorithm. In addition, the system provides an implicit forgetting heuristic so that positive and negative examples are removed from a rule when they are not near one another.
Elements of prescribed order, prescribed traces and systems of rational functions over finite fields
Let k ≥ 1 and f1 .... , fr ∈ Fqk (x) be a system of rational functions forming a strongly linearly independent set over a finite field Fq. Let γ1 ..... γr ∈ Fq be arbitrarily prescribed elements. We prove that for all sufficiently large extensions Fqkm, there is an element ξ ∈ Fqkm of prescribed order such that TrFqkm/Fq (fi (ξ))= γi for i = 1, ..., r, where TrFqkm/Fq is the relative trace map from Fqkm onto Fq. We give some applications to BCH codes, finite field arithmetic and ordered orthogonal arrays. We also solve a question of Helleseth et al. (Hypercubic 4 and 5-designs from Double-Error-Correcting codes, Des. Codes. Cryptgr. 28(2003). pp. 265-282) completely.
The experimental evaluation of knowledge acquisition techniques and methods: history, problems, and new directions
The special problems of experimentally evaluating knowledge acquisition and knowledge engineering tools, techniques and methods are outlined, and illustrated in detail with reference to two series of studies. The first is a series of experiments undertaken at Nottingham University under the aegis of the UK Alvey initiative and the ESPRIT project ACKnowledge. The second is the series of Sisyphus benchmark studies. A suggested programme of experimental evaluation is outlined which is informed by the problems with using Sisyphus for evaluation.
A domain-specific modeling language for scientific data composition and interoperability
Domain-Specific Modeling Languages (DSMLs) can offer assistance to domain experts, who may not be computer scientists, by providing notations and semantic constructs that align with abstractions from a particular domain. In this paper, we describe our design and application of a DSML in the area of data composition and interoperability. In particular, we introduce our recent effort to design a DSML to assist with interoperability issues across scientific software applications (e.g., composing scientific data in different file structures and integrating scientific data with data gathering devices). Currently, several different scientific data file specifications have been proposed (e.g., CID, netCDF, and HDF). Each file specification is optimized to manage a specific data type efficiently. Thus, each file specification has evolved with slightly different notions and implementation technologies. These differences led to the need for an environment that provides interoperability among the different specification formats. In this paper, we introduce our framework, supported by a DSML, that provides functionality to visually model the data composition and integration concepts independent from a particular data file specification.
Migrating Autonomic Self-Testing to the Cloud
The cloud computing model continues to gain much attention from software industry practitioners. As such, leading companies are investing in the development, packaging and delivery of cloud services over the Internet. However, although much work is being done to model and build cloud applications and services, there is significantly less research devoted to testing them. In this paper, we describe our research-in-progress towards migrating autonomic self-testing (AST) to the cloud. Our approach combines the development of an automated test harness for a cloud service, with the delivery of test support as-a-service (TSaaS). Both AST and TSaaS are supported by a virtual test environment, which utilizes the power of the cloud to enhance the self-testing process.
A CMOS fifth-order low-pass current-mode filter using a linear transconductor
In this paper, the design and analysis of a CMOS fifth-order low-pass GM-C filter are presented. It has a cutoff frequency of 4.3 MHz to accommodate the wideband CDMA standard. The transconductor used in this filter is based on a four-transistor cell operating in triode or saturation mode. It achieves high linearity range of /spl plusmn/ 1 V at /spl plusmn/ 1.5 V supply voltages. PSpice simulations show that total harmonic distortion at 1 Vpp and 1 MHZ is equal to 0.1% with 1.234 mW standby power dissipation. The proposed filter and the transconductor are simulated using 0.35 /spl mu/m technology.
Inverse problems theory and application: analysis of the two-temperature method for land-surface temperature and emissivity estimation
The two-temperature method (TTM) allows the separation of land-surface temperature and land-surface emissivity information from radiance measurements, and therefore, the solution can be uniquely determined by the data. However, the inverse problem is still an ill-posed problem, since the solution does not depend continuously on the data. Accordingly, we have used some mathematical tools, which are suited for analyses of ill-posed problems in order to show TTM properties, evaluate it, and optimize its estimations. Related to this last point, we have shown that it is necessary to constrain the problem, either by defining a region of physically admissible solutions and/or by using regularization methods, in order to obtain stable results. Besides, the results may be improved by using TTM with systems that possess a high temporal resolution, as well as by acquiring observations near the maximum and minimum of the diurnal temperature range.
Operating Rules Classification System of Water Supply Reservoir Based on LCS
Genetic algorithm-based learning classifier system (LCS) is a massively parallel, message-passing and rule-based machine learning system. But its potential self-adaptive learning capability has not been paid enough attention in reservoir operation research. In this paper, an operating rule classification system based on LCS , which learns through credit assignment (the bucket brigade algorithm) and rule discovery (the genetic algorithm), is established to extract water-supply reservoir operating rules. The proposed system acquires the online identification rate 95% for training samples and offline rate 85% for testing samples in a case study, and further discussions are made about the impacts on the performances or behaviors of the rule classification system from three aspects of obtained rules, training or testing samples and the comparisons between the rule classification system and the artificial neural network (ANN). The results indicate the learning classifier system is feasible and effective for the system to obtain the reservoir supply operating rules.
Assembling 2D blocks into 3D chips
Three-dimensional ICs promise to significantly extend the scale of system integration and facilitate new-generation electronics. However, progress in commercial 3D ICs has been slow. In addition to technology-related difficulties, industry experts cite the lack of a commercial 3D EDA tool-chain and design standards, high risk associated with a new technology, and high cost of transition from 2D to 3D ICs. To streamline the transition, we explore design styles that reuse existing 2D Intellectual Property (IP) blocks in 3D ICs. Currently, these design styles severely limit the placement of Through-Silicon Vias (TSVs) and constrain the reuse of existing 2D IP blocks in 3D ICs. To overcome this problem, we develop a methodology for using TSV islands and novel techniques for clustering nets to connect 2D IP blocks through TSV islands. Our empirical validation demonstrates 3D integration of traditional 2D circuit blocks without modifying their layout for this context.
Software engineering issues for small-scale parallelism
The availability of low-cost commodity multiprocessor machines change the nature of mainstream programming. This discipline is required to include small-scale, dual and quadruple processor machines, to remain competitive. These small-scale parallel systems require software engineering principles capable of encapsulating the complex parallel programming issues. This paper discusses a technique that provides a simple model for incorporating parallel programming in a scheduler. This model can dynamically adjust to single and small-scale multiple processor environments.
Pilotless Frame Synchronization for LDPC-Coded Transmission Systems
We present a pilotless frame synchronization approach that exploits feedback from a low-density parity-check (LDPC) code decoder. The synchronizer is based on syndrome checks using hard decisions from the channel observations. The bandwidth overhead associated with pilot symbols in conventional receiver architectures is eliminated while providing sufficient synchronization performance. An LDPC decoder coupled with our synchronizer exhibits negligible frame error rate degradation over a system with perfect synchronization. The complexity of the frame synchronizer is kept relatively low due to its XOR-based approach.
Third-octave analysis of multichannel amplitude compressed speech
