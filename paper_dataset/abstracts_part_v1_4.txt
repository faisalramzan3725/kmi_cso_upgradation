Multiband amplitude compression has been studied as a compensation for the reduced auditory dynamic range often associated with sensorineural hearing loss, since it is capable of altering the dynamic range of speech as a function of frequency. Compression systems are generally characterized by their response to steady state tones and to simple dynamic stimuli, such as tone bursts. Level distributions of unprocessed and compressed materials are presented to demonstrate that these descriptions are inadequate to predict the processing of speech. A simple analysis of multiband compression which incorporates the interactions of static and dynamic properties is presented.
Depth image post-processing method by diffusion
Three-dimensional (3D) movies in theaters have become a massive commercial success during recent years, and it is likely that, with the advancement of display technologies and the production of 3D contents, TV broadcasting in 3D will play an important role in home entertainments in the not too distant future. 3D video contents contain at least two views from different perspectives for the left and the right eye of viewers. The amount of coded information is doubled if these views are encoded separately. Moreover, for multi-view displays (i.e. different perspectives of a scene in 3D are presented to the viewer at the same time through different angles), either video streams of all the required views must be transmitted to the receiver, or the displays must synthesize the missing views with a subset of the views. The latter approach has been widely proposed to reduce the amount of data being transmitted. The virtual views can be synthesized by the Depth Image Based Rendering (DIBR) approach from textures and associated depth images. However it is still the case that the amount of information for the textures plus the depths presents a significant challenge for the network transmission capacity. An efficient compression will, therefore, increase the availability of content access and provide a better video quality under the same network capacity constraints.In this thesis, the compression of depth images is addressed. These depth images can be assumed as being piece-wise smooth. Starting from the properties of depth images, a novel depth image model based on edges and sparse samples is presented, which may also be utilized for depth image post-processing. Based on this model, a depth image coding scheme that explicitly encodes the locations of depth edges is proposed, and the coding scheme has a scalable structure. Furthermore, a compression scheme for block-based 3D-HEVC is also devised, in which diffusion is used for intra prediction. In addition to the proposed schemes, the thesis illustrates several evaluation methodologies, especially, the subjective test of the stimulus-comparison method. It is suitable for evaluating the quality of two impaired images, as the objective metrics are inaccurate with respect to synthesized views.The MPEG test sequences were used for the evaluation. The results showed that virtual views synthesized from post-processed depth images by using the proposed model are better than those synthesized from original depth images. More importantly, the proposed coding schemes using such a model produced better synthesized views than the state of the art schemes. As a result, the outcome of the thesis can lead to a better quality of 3DTV experience.
Memory in the small: an application to provide task-based organizational memory for a scientific community
Many forms of organizational memory must exist embedded within the organizational processes and tasks. This paper argues that "memory-in-the small", memory utilized in the performance of an organizational task, can serve as an effective performance support mechanism. By basing organizational memory upon organizational tasks (and basing task support upon organizational memory), organizational memory systems can provide additional and necessary support services for organizations and communities. As an example of memory-in-the-small, this paper describes a software application called the ASSIST, that combines organizational memory with task performance for a scientific community. The ASSIST utilizes and stores the collective memory of astrophysicists about data analysis and is used world-wide by astrophysicists. The paper also considers the theoretical and architectural issues involved when combining organizational memory with task performance. >
CyloFold: secondary structure prediction including pseudoknots
Computational RNA secondary structure prediction approaches differ by the way RNA pseudoknot interactions are handled. For reasons of computational efficiency, most approaches only allow a limited class of pseudoknot interactions or are not considering them at all. Here we present a computational method for RNA secondary structure prediction that is not restricted in terms of pseudoknot complexity. The approach is based on simulating a folding process in a coarse-grained manner by choosing helices based on established energy rules. The steric feasibility of the chosen set of helices is checked during the folding process using a highly coarse-grained 3D model of the RNA structures. Using two data sets of 26 and 241 RNA sequences we find that this approach is competitive compared to the existing RNA secondary structure prediction programs pknotsRG, HotKnots and UnaFold. The key advantages of the new method are that there is no algorithmic restriction in terms of pseudoknot complexity and a test is made for steric feasibility. Availability: The program is available as web server at the site: http://cylofold.abcc.ncifcrf.gov.
Topological analysis of the power grid and mitigation strategies against cascading failures
This paper presents a complex systems overview of a power grid network. In recent years, concerns about the robustness of the power grid have grown because of several cascading outages in different parts of the world. In this paper, cascading effect has been simulated on three different networks, the IEEE 300 bus test system, the IEEE 118 bus test system, and the WSCC 179 bus equivalent model. Power Degradation has been discussed as a measure to estimate the damage to the network, in terms of load loss and node loss. A network generator has been developed to generate graphs with characteristics similar to the IEEE standard networks and the generated graphs are then compared with the standard networks to show the effect of topology in determining the robustness of a power grid. Three mitigation strategies, Homogeneous Load Reduction, Targeted Range-Based Load Reduction, and Use of Distributed Renewable Sources in combination with Islanding, have been suggested. The Homogeneous Load Reduction is the simplest to implement but the Targeted Range-Based Load Reduction is the most effective strategy.
Development of architecture and software technologies in high-performance low-power SoC design
For the success of an SoC design, a design platform and some key design technologies are needed. For this purpose, a research project in "Technology Development Program for Academia" was granted recently in Taiwan to develop the following technologies toward a high-performance low-power SoC design platform. First, a soft intellectual property (soft IP) and related RTOS, compiler, and integrated design environment (IDE) software of an Advanced Taiwan VLTW DSP Processor Core, which can be used as the Star IP of an SoC design platform, will be developed. Second, low-power and low-voltage digital and mixed-signal circuit design technologies will be developed based on the advanced MTCMOS process. Third, some key multimedia and communication soft or hard IPs will be developed. The developed advanced key technologies can be used as the technology driver to facilitate the design of SoC-based products, and they will also help to enhance the design capability of the Taiwan SoC industry. In this paper, we introduce the research project and focus on architecture and software technologies developing in one of the subitems.
Using information systems to structurally map workplace injury
Previous authors have identified mechanical, human and work organisation factors that may contribute to accident occurrences and have noted the value of site-specific data in accident analysis. However, major criticisms of site specific safety information systems have focused on the difficulties using traditional approaches for the identification of critical paths involving processes related to injury and non-injury incidents. This paper presents an idiographic approach to the study of accidents and through contextual analysis develops maps of work processes related to injury incidents in mining. The information used in the contextual maps includes data related to work area, activity and work role being performed, and equipment being used at the time of the injury incident. The computer algorithm consists of a series of contextual conditionals where the incidents and lost days recorded in the final category of each pathway meet all of the conditionals of the previous categories. The order of selection indicates the criticality of the path. The analysis resolves the processes related to the incident into their constitutive components, and then redescribes these processes as paths in order to reveal associations. These contextual paths illustrate the processes which are part of an established pattern of recurring regularities associated with injury incidents.
A PRAM-NUMA model of computation for addressing low-TLP workloads
It is possible to implement the parallel random access machine (PRAM) on a chip multiprocessor (CMP) efficiently with an emulated shared memory (ESM) architecture to gain easy parallel programmability crucial to wider penetration of CMPs to general purpose computing. This implementation relies on exploitation of the slack of parallel applications to hide the latency of the memory system instead of caches, sufficient bisection bandwidth to guarantee high throughput, and hashing to avoid hot spots in intercommunication. Unfortunately this solution can not handle workloads with low thread-level parallelism (TLP) efficiently because then there is not enough parallel slackness available for hiding the latency. In this paper we show that integrating nonuniform memory access (NUMA) support to the PRAM implementation architecture can solve this problem. The obtained PRAM-NUMA hybrid model is described and architectural implementation of it is outlined on our Eclipse ESM CMP framework.
DAVIM: Adaptable Middleware for Sensor Networks
Middleware services facilitate sensor-network application development. DAVIM is adaptable middleware that enables dynamic management of services and isolation between simultaneously running applications.
A survey on optimization metaheuristics
Metaheuristics are widely recognized as efficient approaches for many hard optimization problems. This paper provides a survey of some of the main metaheuristics. It outlines the components and concepts that are used in various metaheuristics in order to analyze their similarities and differences. The classification adopted in this paper differentiates between single solution based metaheuristics and population based metaheuristics. The literature survey is accompanied by the presentation of references for further details, including applications. Recent trends are also briefly discussed.
Mining formal concepts with a bounded number of exceptions from transactional data
We are designing new data mining techniques on boolean contexts to identify a priori interesting bi-sets (i.e., sets of objects or transactions associated to sets of attributes or items). A typical important case concerns formal concept mining (i.e., maximal rectangles of true values or associated closed sets by means of the so-called Galois connection). It has been applied with some success to, e.g., gene expression data analysis where objects denote biological situations and attributes denote gene expression properties. However in such real-life application domains, it turns out that the Galois association is a too strong one when considering intrinsically noisy data. It is clear that strong associations that would however accept a bounded number of exceptions would be extremely useful. We study the new pattern domain of α/β concepts, i.e., consistent maximal bi-sets with less than α false values per row and less than β false values per column. We provide a complete algorithm that computes all the α/β concepts based on the generation of concept unions pruned thanks to anti-monotonic constraints. An experimental validation on synthetic data is given. It illustrates that more relevant associations can be discovered in noisy data. We also discuss a practical application in molecular biology that illustrates an incomplete but quite useful extraction when all the concepts that are needed beforehand can not be discovered.
EpiToolKit—a web server for computational immunomics
Predicting the T-cell-mediated immune response is an important task in vaccine design and thus one of the key problems in computational immunomics. Various methods have been developed during the last decade and are available online. We present EpiToolKit, a web server that has been specifically designed to offer a problem-solving environment for computational immunomics. EpiToolKit offers a variety of different prediction methods for major histocompatibility complex class I and II ligands as well as minor histocompatibility antigens. These predictions are embedded in a user-friendly interface allowing refining, editing and constraining the searches conveniently. We illustrate the value of the approach with a set of novel tumor-associated peptides. EpiToolKit is available online at www. epitoolkit.org.
A Novel Breath Analysis System Based on Electronic Olfaction
Certain gases in the breath are known to be indicators of the presence of diseases and clinical conditions. These gases have been identified as biomarkers using equipments, such as gas chromatography and electronic nose (e-nose). GC is very accurate but is expensive, time consuming, and nonportable. E-nose has the advantages of low cost and easy operation, but is not particular for analyzing breath odor, and hence, has a limited application in diseases diagnosis. This paper proposes a novel system that is special for breath analysis. We selected chemical sensors that are sensitive to the biomarkers and compositions in human breath, developed the system, and introduced the odor signal preprocessing and classification method. To evaluate the system performance, we captured breath samples from healthy persons and patients known to be afflicted with diabetes, renal disease, and airway inflammation, respectively, and conducted experiments on medical treatment evaluation and disease identification. The results show that the system is not only able to distinguish between breath samples from subjects suffering from various diseases or conditions (diabetes, renal disease, and airway inflammation) and breath samples from healthy subjects, but in the case of renal failure is also helpful in evaluating the efficacy of hemodialysis (treatment for renal failure).
Improving business processes with business process modelling notation and business process execution language: an action research approach
The first purpose of this paper is to improve business processes in an industrial firm with business process modelling notation (BPMN) and business process execution language (BPEL). The second purpose is to provide a scientific approach for bringing business analysts and IT professionals together in a framework of an action research (AR). The research is conducted in one of the biggest distribution companies in Iran. BPMN is applied to model as-is and to-be situations of sale and distribution process. Both as-is and to-be models were simulated to compare their performance indexes. Moreover, models were implemented using BPEL, so that the model can be used for automating and improving the process. In this study, AR methodology was used to find a resolution of an organisational issue with those who experience this issue directly and to improve scientific knowledge and real-life contexts simultaneously.
Virtual marionettes: a system and paradigm for real-time 3D animation
This paper describes a computer graphics system that enables users to define virtual marionette puppets, operate them using relatively simple hardware input devices, and display the scene from a given viewpoint on the computer screen. This computerized marionette theater has the potential to become a computer game for children, an interaction tool over the Internet, enabling the creation of simultaneously viewed and operated marionette show by users on the World Wide Web, and, most importantly, a versatile and efficient professional animation system.
A Call Admission Control Algorithm Based on Utility Fairness for Low Earth Orbit Satellite Networks
A fair and adaptive call admission control algorithm for multimedia low Earth orbit (LEO) satellite networks was proposed. Based on current call dropping probability of destination cell, this algorithm reserves bandwidth for handoff calls using double threshold method. To avoid the discrimination of quality of service (QoS) caused by the allocation based on fair bandwidth, this algorithm adopts the bandwidth allocation rule based on fair QoS. Simulation results show that the proposed algorithm can accurately and adaptively reserve bandwidth, present satisfactory call blocking probability and greatly reduce handoff call dropping probability, while guarantees the high bandwidth utilization.
Co-Training for Domain Adaptation
Domain adaptation algorithms seek to generalize a model trained in a source domain to a new target domain. In many practical cases, the source and target distributions can differ substantially, and in some cases crucial target features may not have support in the source domain. In this paper we introduce an algorithm that bridges the gap between source and target domains by slowly adding to the training set both the target features and instances in which the current algorithm is the most confident. Our algorithm is a variant of co-training [7], and we name it CODA (Co-training for domain adaptation). Unlike the original co-training work, we do not assume a particular feature split. Instead, for each iteration of co-training, we formulate a single optimization problem which simultaneously learns a target predictor, a split of the feature space into views, and a subset of source and target features to include in the predictor. CODA significantly out-performs the state-of-the-art on the 12-domain benchmark data set of Blitzer et al. [4]. Indeed, over a wide range (65 of 84 comparisons) of target supervision CODA achieves the best performance.
Multi-agent Resource Allocation Algorithm Based on the XSufferage Heuristic for Distributed Systems
Distributed computing systems provide a highly dynamic behavior which originates from heterogeneous computing and storage resources, heterogeneous users and the variety of submitted applications and finally from the heterogeneous communication that takes part among the systems entities. As such applying global optima oriented allocation algorithms usually produces poor results and heuristics are used instead. We concentrated our experiments around the Sufferage heuristic and its adaptive cluster-aware version XSufferage. Both Sufferage and XSufferage use a centralized design and produce good results for low levels of dynamism and deterministic environments. In real life distributed environments, both heuristics produce poor results. We expose the Sufferage heuristic through a distributed architecture based on a cooperative set of entities, which form a Multi-Agent System, such that the results could be improved. We implemented a new algorithm, based on this architecture, called Distributed XSufferage. In order to test the new algorithm, a series of experiments were developed by simulating two real life Grid environments. A complex set of performance metrics were collected -- flow time, make span, throughput -- both resource and cluster level, utilization -- both resource and cluster level and resources and clusters mean loads. Algorithms produce their allocation solution based on estimates and modeling of system's resources and as such are sensitive to estimation errors. Throughout our experiments DX Sufferage was more robust to such errors compared to the original Sufferage and, respectively, XSufferage heuristics.
Hierarchical A* Parsing with Bridge Outside Scores
Hierarchical A* (HA*) uses of a hierarchy of coarse grammars to speed up parsing without sacrificing optimality. HA* prioritizes search in refined grammars using Viterbi outside costs computed in coarser grammars. We present Bridge Hierarchical A* (BHA*), a modified Hierarchial A* algorithm which computes a novel outside cost called a bridge outside cost. These bridge costs mix finer outside scores with coarser inside scores, and thus constitute tighter heuristics than entirely coarse scores. We show that BHA* substantially outperforms HA* when the hierarchy contains only very coarse grammars, while achieving comparable performance on more refined hierarchies.
Strings of congruent primes in short intervals
Fix �> 0, and let p1 =2 ,p 2 =3 ,... be the sequence of all primes. We prove that if (q, a )=1 , then there are infinitely many pairs pr ,p r+1 such that pr ≡ pr+1 ≡ a mod q and pr+1 − pr <� log pr. � � � �
Leveraging speculative architectures for runtime program validation
Program execution can be tampered with by malicious attackers through exploiting software vulnerabilities. Changing the program behavior by compromising control data and decision data has become the most serious threat in computer system security. Although several hardware approaches have been presented to validate program execution, they either incur great hardware overhead or introduce false alarms. We propose a new hardware-based approach by leveraging the existing speculative architectures for runtime program validation. The on-chip branch target buffer (BTB) is utilized as a cache of the legitimate control flow transfers stored in a secure memory region. In addition, the BTB is extended to store the correct program path information. At each indirect branch site, the BTB is used to validate the decision history of previous conditional branches and monitor the following execution path at runtime. Implementation of this approach is transparent to the upper operating system and programs. Thus, it is applicable to legacy code. Because of good code locality of the executable programs and effectiveness of branch prediction, the frequency of control-flow validations against the secure off-chip memory is low. Our experimental results show a negligible performance penalty and small storage overhead.
A proposal of cell selection algorithm for LTE handover optimization
A large number of cells will be deployed to provide high speed services in any places using the Long-Term Evolution (LTE) system. The management of such a large number of cells increases the operating expenditure (OPEX). Self-organizing networks (SON) attracted interest as an effective way to reduce OPEX. One of the main targets in SON is the self-optimization of handover (HO) that realizes mobility robustness. HO optimization algorithms adjust HO parameters between the serving and the reconnected cells based on the HO failure logs and cell selection, which is the procedure used to select a suitable reconnected cell, is very important for HO optimization algorithms. In this paper, we propose a cell selection scheme to enhance the performance of HO optimization. In the proposed scheme, both the uplink and downlink channel quality is considered when selecting a suitable reconnected cell. Through the computer simulation, we can see that the proposed scheme reduces the HO failure rate and the number of HO failures by 3 percentage points and 38%, respectively, compared to the conventional scheme based solely on downlink channel quality.
Clustering Large Optical Networks for Distributed and Dynamic Multicast
For a large-scale mesh network with dynamic traffic, maintaining the global state information in a centralized fashion is impractical. Hence, distributed schemes are needed to organize nodes and to manage state information in a more localized manner. One such effective scheme for organizing nodes is to cluster the nodes into a hierarchical structure. In this paper, we address the problem of determining the appropriate clustering of nodes for providing scalability in wavelength division multiplexed (WDM) optical networks with dynamic traffic. We present an on-line load-based (or bandwidth availability based) clustering technique that determines clusters adaptively in response to current network conditions. We also consider the problem of dynamic multicast on clustered networks with wavelength conversion capability. We introduce a heuristic using an auxiliary graph model to address routing, wavelength assignment, and traffic grooming jointly. Simulation results demonstrate the feasibility of our approach.
Detecting patterns of change using enhanced parallel coordinates visualization
Analyzing data to find trends, correlations, and stable patterns is an important problem for many industrial applications. We propose a new technique based on parallel coordinates visualization. Previous work on parallel coordinates method has shown that they are effective only when variables that are correlated and/or show similar patterns are displayed adjacently. Although current parallel coordinates tools allow the user to manually rearrange the order of variables, this process is very time-consuming when the number of variables is large. Automated assistance is needed. We propose an edit-distance based technique to rearrange variables so that interesting patterns can be easily detected. Our system, V-Miner, includes both automated methods for visualizing common patterns and a query tool that enables the user to describe specific target patterns to be mined/displayed by the system. Following an overview of the system, a case study is presented to explain how Motorola engineers have used V-Miner to identify significant patterns in their product test and design data.
Phase Diversity for an Antenna-Array System With a Short Interelement Separation
This paper presents a simple procedure of obtaining a diversity gain in an antenna-array system with a short interelement separation, typically less than the carrier wavelength. The new technique provides a diversity gain through a noncoherent combination of received signals at each antenna element. The diversity gain arises because, as the number of signal components of the received signal at each antenna element becomes large enough and as the arrival angle of each signal component is distinct from one another, which is a general signal circumstance in most practical code division multiple access (CDMA) signal environments, the amplitudes of the received signals become nearly independent due to the phase difference among the received signals. The diversity gain was referred to as ldquophase diversityrdquo in this paper. The proposed technique is first theoretically analyzed to estimate the performance in terms of pseudorandom-noise-code acquisition, which is verified through extensive computer simulations. Then, through the experimental results that are obtained from a CDMA array-antenna base station system, it has been shown that the performance of noncoherent detection is proportionally improved to the number of antenna elements.
Evaluating Usability and Efficaciousness of an E-learning System: A Quantitative, Model-Driven Approach
Using an e-learning system as a case example of complex information systems, we tested a conceptual framework that expands upon the Technology Acceptance Model (TAM) and incorporate measures of self-efficacy. A survey instrument was developed to gather large samples of users' perceptions on system usability and usefulness. Advanced statistical tests such as structural equation modeling were carried out. The paper concludes with a discussion on efficacy of evaluation techniques and design implications for e-learning systems.
Conception D'un Environnement Sous Matlab Pour le Marquage D'images en Jpeg2000
Le standard de compression JPEG2000 se caracterise par la diversite des options d'encodage menant a de bons compromis compression/qualite. Cependant, une extension integrant des elements de securite est fortement attendue. Le marquage numerique (watermarking, de l'expression anglaise) est une technologie prometteuse pour securiser la circulation des contenus multimedias. L'incorporation d'un marquage dans la chaine de compression presente l'avantage de prise en compte de la distorsion introduite par la marque dans le processus global de compression, en plus de la possibilite de reduire la complexite d'implementation en exploitant les traitements deja effectues pour la compression. Le travail realise consiste a la conception et au developpement d'un environnement sous MATLAB offrant differentes ouvertures sur l'architecture JPEG2000 et permettant d'incorporer une ou plusieurs methodes de marquage. L'environnement a ete developpe a partir de JJ2000, implementation du standard en langage JAVA. Comme premiere experimentation, un marquage robuste a ete incorpore au module de transformee en ondelettes du codeur et du decodeur JPEG2000.
Analyzing the Energy-Time Trade-Off in High-Performance Computing Applications
Although users of high-performance computing are most interested in raw performance both energy and power consumption has become critical concerns. One approach to lowering energy and power is to use high-performance cluster nodes that have several power-performance states so that the energy-time trade-off can be dynamically adjusted. This paper analyzes the energy-time trade-off of a wide range of applications-serial and parallel-on a power-scalable cluster. We use a cluster of frequency and voltage-scalable AMD-64 nodes, each equipped with a power meter. We study the effects of memory and communication bottlenecks via direct measurement of time and energy. We also investigate metrics that can, at runtime, predict when each type of bottleneck occurs. Our results show that, for programs that have a memory or communication bottleneck, a power-scalable cluster can save significant energy with only a small time penalty. Furthermore, we find that, for some programs, it is possible to both consume less energy and execute in less time by increasing the number of nodes while reducing the frequency-voltage setting of each node
Using Evolutionary Algorithms for Signal Integrity Checks of High-Speed Data Buses
Today's high performance computer systems must have fast, reliable access to memory and I/O devices. Unfortunately, inter-symbol interference, transmission line effects and other noise sources can distort data transfers. Engineers must therefore determine if bus designs have signal integrity  . e., the bus can transfer data with minimal amplitude or timing distortion. One method of determining signal quality on buses is to conduct a set of data transfers and measure various signal parameters at the receiver end. But the tests must be conducted with stressful test patterns that maximize inter-symbol interference to help identify any potential problems. In this paper we describe how an evolutionary algorithm was used to evolve such test patterns. All test results were obtained intrinsically
A Binary Communication Channel With Memory Based on a Finite Queue
A model for a binary additive noise communication channel with memory is introduced. The channel noise process, which is generated according to a ball sampling mechanism involving a queue of finite length M, is a stationary ergodic Mth-order Markov source. The channel properties are analyzed and several of its statistical and information-theoretical quantities (e.g., block transition distribution, autocorrelation function (ACF), capacity, and error exponent) are derived in either closed or easily computable form in terms of its four parameters. The capacity of the queue-based channel (QBC) is also analytically and numerically compared for a variety of channel conditions with the capacity of other binary models, such as the well-known Gilbert-Elliott channel (GEC), the Fritchman channel, and the finite-memory contagion channel. We also investigate the modeling of the traditional GEC using this QBC model. The QBC parameters are estimated by minimizing the Kullback-Leibler divergence rate between the probability of noise sequences generated by the GEC and the QBC, while maintaining identical bit-error rates (BER) and correlation coefficients. The accuracy of fitting the GEC via the QBC is evaluated in terms of ACF, channel capacity, and error exponent. Numerical results indicate that the QBC provides a good approximation of the GEC for various channel conditions; it thus offers an interesting alternative to the GEC while remaining mathematically tractable.
A new approach for a topographic feature-based characterization of digital elevation data
Triangular Irregular Network (TIN) and Regular Square Grid (RSG) are widely used for representing 2.5 dimensional spatial data. However, these models are not defined from the topographic properties of the terrain (i.e., ridge lines, valley lines, saddle points, etc.). This paper introduces a three-step feature-based approach for topographic properties extraction on scattered elevation data modeled by a TIN. Firstly, a segmentation process extracts homogeneous morphological areas bounded by critical lines and points. Secondly, these lines and points are displaced using a deformable process in order to derive the terrain feature points, lines and areas. Thirdly, a classification process labels any topographic feature. This three-step approach relies on the definition of an adapted model of representation (SPIN) and data structure (DCFL2). The proposed approach is validated on a real case study (Seolak mountain in South Korea). Consistent results with the morphology of terrain are displayed.
Ultrahigh Dimensional Feature Selection: Beyond The Linear Model
Variable selection in high-dimensional space characterizes many contemporary problems in scientific discovery and decision making. Many frequently-used techniques are based on independence screening; examples include correlation ranking (Fan & Lv, 2008) or feature selection using a two-sample t-test in high-dimensional classification (Tibshirani et al., 2003). Within the context of the linear model, Fan & Lv (2008) showed that this simple correlation ranking possesses a sure independence screening property under certain conditions and that its revision, called iteratively sure independent screening (ISIS), is needed when the features are marginally unrelated but jointly related to the response variable. In this paper, we extend ISIS, without explicit definition of residuals, to a general pseudo-likelihood framework, which includes generalized linear models as a special case. Even in the least-squares setting, the new method improves ISIS by allowing feature deletion in the iterative process. Our technique allows us to select important features in high-dimensional classification where the popularly used two-sample t-method fails. A new technique is introduced to reduce the false selection rate in the feature screening stage. Several simulated and two real data examples are presented to illustrate the methodology.
DNS Resource Record Analysis of URLs in E-Mail Messages for Improving Spam Filtering
In recent years, spam mails intending for ``One-click fraud" or ``Phishing" have become increasing. As one anti-spam technology, DNSBL based on the URLs or their corresponding IP addresses in the messages is well used. However, some spam mails that cannot be filtered by conventional DNSBLs get appearing since the spammers create websites using various techniques such as botnet, fast-flux and Wildcard DNS record. To improve the accuracy of filtering spam mails using these techniques, we analyzed DNS record features corresponding to the domain name from the URLs in actual spam mails. According to the result of this analysis, we confirmed that abuse of Wildcard DNS record is one effective criterion for spam filtering.
Recovery of Digital Information Using Bacterial Foraging Optimization Based Nonlinear Channel Equalizers
Transmission and storing of high density digital information plays an important role in the present age of information technology. These binary data are distorted while reading out of the recording medium or arriving at the receiver end due to inter symbol interference in the channel. The adaptive channel equalizer alleviates this distortion and reconstructs the transmitted data faithfully. The bacterial foraging optimization (BFO) is a recently developed efficient and derivative free evolutionary computing tool used for optimization purpose. In the present paper we propose a novel nonlinear channel equalizer using BFO algorithm. The recovery performance of the new equalizer is obtained through computer simulation study using nonlinear channels. It is shown that the proposed equalizer offers superior performance both in terms of bit-error-rate and convergence speed compared to the GA based equalizers. In addition it requires substantially less computation during training.
A fault-tolerant approach to secure information retrieval
Several private information retrieval (PIR) schemes were proposed to protect users' privacy when sensitive information stored in database servers is retrieved. However, existing PIR schemes assume that any attack to the servers does not change the information stored and any computational results. We present a novel fault-tolerant PIR scheme (called FT-PIR) that protects users' privacy and at the same time ensures service availability in the presence of malicious server faults. Our scheme neither relies on any unproven cryptographic assumptions nor the availability of tamper-proof hardware. A probabilistic verification function is introduced into the scheme to detect corrupted results. Unlike previous PIR research that attempted mainly to demonstrate the theoretical feasibility of PIR, we have actually implemented both a PIR scheme and our FT-PIR scheme in a distributed database environment. The experimental and analytical results show that only modest performance overhead is introduced by FT-PIR while comparing with PIR in the fault-free cases. The FT-PIR scheme tolerates a variety of server faults effectively. In certain fail-stop fault scenarios, FT-PIR performs even better than PIR. It was observed that 35.82% less processing time was actually needed for FT-PIR to tolerate one server fault.
Autonomous land vehicle navigation using millimeter wave radar
This paper discusses the use of a 77 GHz millimeter wave radar as a guidance sensor for autonomous land vehicle navigation. A test vehicle has been fitted with a radar and encoders that give steer angle and velocity. An extended Kalman filter optimally fuses the radar range and bearing measurements with vehicle control signals to give estimated position and variance as the vehicle moves around a test site. The effectiveness of this data fusion is compared with encoders alone and with a satellite positioning system. Consecutive scans have been combined to give a radar image of the surrounding environment. Data in this format are invaluable for future work on collision detection and map building navigation.
WiBro Net.-Based Five Senses Multimedia Technology Using Mobile Mash-Up
In this paper, we suggest and implement an enhanced Wireless Broadband (WiBro) Net.-based five senses multimedia technology using Web-map-oriented mobile Mash-up. The WiBro Net. in this applicative technology supports and allows Web 2.0-oriented various issues such as user-centric multimedia, individual multimedia message exchange between multi-users, and a new media-based information and knowledge sharing / participation without spatiotemporal-dependency. To inspect applicability and usability of the technology, we accomplish various experiments, which include 1) WiBro Net.-based real-time field tests and 2) ISO 9241/11 and /10-based surveys on the user satisfaction by relative experience in comparison with the AP-based commercialized mobile service. As a result, this application provides higher data rate transmission in UP-DOWN air-link and wider coverage region. Also, its average System Usability Scale (SUS) scores estimated at 83.58%, and it relatively held competitive advantage in the specific item scales such as system integration, individualization, and conformity with user expectations.
Generalizing Analytic Shrinkage for Arbitrary Covariance Structures
Analytic shrinkage is a statistical technique that offers a fast alternative to cross-validation for the regularization of covariance matrices and has appealing consistency properties. We show that the proof of consistency requires bounds on the growth rates of eigenvalues and their dispersion, which are often violated in data. We prove consistency under assumptions which do not restrict the covariance structure and therefore better match real world data. In addition, we propose an extension of analytic shrinkage -orthogonal complement shrinkage- which adapts to the covariance structure. Finally we demonstrate the superior performance of our novel approach on data from the domains of finance, spoken letter and optical character recognition, and neuroscience.
ChariTime — Concepts of Analysis and Design of an Agent-Oriented System for Appointment Management
Conceptual modeling is an important basis for developing general purpose systems for information and service management. In this paper, we present concepts for the analysis and design of a system of agents in which agents represent interests of individuals or groups. In particular, we develop a semantic model of agent domain knowledge by introducing analytical concepts for service scheduling which is the envisioned application of our prototypical system ChariTime.
Monitoring cross-channel correlation solar scan measurements using the Iowa X-band polarimetric radars
The sun is a convenient and frequently employed external radiation source for calibrating weather radar antenna and receiver characteristics. However, changes in solar activity can be a major source of error in interpreting the results of solar calibration for lower frequency bands. The cross-correlation of horizontal and vertical polarization signals, which is zero for perfectly unpolarized electromagnetic radiation, could give non-zero estimates if a quiet sun is not observed by the radar. In this paper, solar scan measurements are made at X-band to detect the effect of solar activity on this cross-correlation coefficient. To facilitate mitigation of instrument-wide errors, we employ multiple XPOL radars to simultaneously observe the sun. Though our experiments during a limited period show that the cross-channel correlation estimates obtained by X-band weather radars remain relatively unaffected by the variations in solar flux, the paper makes suggestions on improving the results.
A Context-based Ontological Structure for Knowledge Sharing and Customization
Several research projects have recently surfaced to automate the discovery and acquisition of knowledge from information and to facilitate information sharing and reusability in the global network. However, some of these research attempts are limited in scope with respect to proper identification and delivery of customized information. This paper addresses some of these challenges by proposing a high-level ontological and context-based architecture that enables information customization and knowledge organization. The proposed model aims at managing knowledge items through the use of stand-alone computational layers. Ontology is used in this research to describe knowledge representation and structure, whereas context reflects knowledge adaptability to its hosting environment.
Verification of security properties of payment protocol using AVISPA
Emerging e-commerce activity is giving scope for the design of many new protocols, and to gain confidence, these protocol need to be verified for its designed properties. Specifically protocol used in ecommerce transactions needs to be verified for their security properties. Verification of these protocols is done using the formal verification tools. AVISPA is one of the evolving tools used mainly for verifying security properties. A newly designed electronic payment protocol is verified for its correctness and security properties. This paper presents the use of AVISPA for verifying the security properties of the newly evolved electronic transaction protocol.
On Kalman Filtering for Detectable Systems With Intermittent Observations
We consider the problem of Kalman filtering when observations are available according to a Bernoulli process. It is known that there exists a critical probability  pc  such that, if measurements are available with probability greater than  pc , then the expected prediction covariance is bounded for all initial conditions; otherwise, it is unbounded for some initial conditions. We show that, when the system observation matrix restricted to the observable subspace is invertible, the known lower bound on  pc  is, in fact, the exact critical probability. This result is based on a novel decomposition of positive semidefinite matrices.
Probabilistic temporal databases, I: algebra
Dyreson and Snodgrass have drawn attention to the fact that, in many temporal database applications, there is often uncertainty about the start time of events, the end time of events, and the duration of events.  When the granularity of time is small (e.g., milliseconds), a statement such as “Packet  p  was shipped sometime during the first 5 days of January, 1998” leads to a massive amount of uncertainty (5×24×60×60×1000) possibilities.  As noted in Zaniolo et al. [1997], past attempts to deal with uncertainty in databases have been restricted to relatively small amounts of uncertainty in attributes.  Dyreson and Snodgrass have taken an important first step towards solving this problem.  In this article, we first introduce the   syntax of Temporal-Probabilistic (TP) relations and then show how they can be converted to an explicit, significantly more space-consuming form, called Annotated Relations.  We then present a  theoretical annotated temporal algebra  (TATA).  Being explicit, TATA is convenient for specifying how the algebraic operations should behave, but is impractical to use because annotated relations are overwhelmingly large.  Next, we present a  temporal probabilistic algebra  (TPA).  We show that our definition of the TP-algebra provides a correct implementation of TATA despite the fact that it operates on implicit, succinct TP-relations instead of overwhemingly large annotated relations. Finally, we report on timings for an implementation of the TP-Algebra built on   top of ODBC.
A Conceptual Design of an Adaptive and Collaborative E-Work Environment
Emerging work models increasingly take the form of loosely structured, often self-organising networks of nimble and virtual knowledge work teams within and between organisations. To model such work patterns requires a different approach from that of traditional workflow management systems. This paper presents the conceptual design of a prototype adaptive and collaborative e-Work environment - e-Workbench, which we are currently developing to enable future collaborative workspaces to adapt to emerging knowledge work models. We argue that with appropriate knowledge of tasks, workspaces will be able to adapt to work, and automatically retrieve contextually relevant knowledge elements from the Web in order to contribute creatively to problem solving and semantically manage shared information among collaborating workers. Our goal is to enable e-Workbench to become, not only a working environment but also, a collaborator and a co-worker as a result of its knowledge of work and creative participation in problem solving
Statistical distributions of DCT coefficients and their application to an interframe compression algorithm for 3-D medical images
Displacement estimated interframe (DEI) coding, a coding scheme for 3-D medical image data sets such as X-ray computed tomography (CT) or magnetic resonance (MR) images, is presented. To take advantage of the correlation between contiguous slices, a displacement-compensated difference image based on the previous image is encoded. The best fitting distribution functions for the discrete cosine transform (DCT) coefficients obtained from displacement compensated difference images are determined and used in allocating bits and optimizing quantizers for the coefficients. The DEI scheme is compared with 2-D block discrete cosine transform (DCT) as well as a full-frame DCT using the bit allocation technique of S. Lo and H.K. Huang (1985). For X-ray CT head images, the present bit allocation and quantizer design, using an appropriate distribution model, resulted in a 13-dB improvement in the SNR compared to the full-frame DCT using the bit allocation technique. For an image set with 5-mm slice thickness, the DEI method gave about 5% improvement in the compression ratio on average and less blockiness at the same distortion. The performance gain increases to about 10% when the slice thickness decreases to 3 mm. >
CodeRank: a new family of software metrics
The concept of pagerank has proved successful in allowing search engines to identify important pages in the World Wide Web. In this paper, we describe the application of the pagerank concept to the domain of software in order to derive a new family of metrics, CodeRank, which captures aspects of software not readily obtainable from other metrics. We have implemented a tool, CODERANKER, to compute values of CodeRank metrics using a full semantic model which we have developed. We present some results and discuss the use of CodeRank metrics in their interpretation.
New multiparty authentication services and key agreement protocols
Many modern computing environments involve dynamic peer groups. Distributed simulation, multiuser games, conferencing applications, and replicated servers are just a few examples. Given the openness of today's networks, communication among peers (group members) must be secure and, at the same time, efficient. This paper studies the problem of authenticated key agreement in dynamic peer groups with the emphasis on efficient and provably secure key authentication, key confirmation, and integrity. It begins by considering two-party authenticated key agreement and extends the results to group Diffie-Hellman (1976) key agreement. In the process, some new security properties (unique to groups) are encountered and discussed.
Fusing Asynchronous Feature Streams for On-line Writer Identification
In this paper, we present a new approach to improving the performance of a writer identification system by fusing asynchronous feature streams. Different feature streams are extracted from on-line handwritten text acquired from a whiteboard. The feature streams are used to train a text and language independent writer identification system based on Gaussian mixture models (GMMs). From a stroke consisting of n points, n point-based feature vectors and one stroke-based feature vector are extracted. The resulting feature streams thus have an unequal number of feature vectors. We evaluate different methods to directly fuse the feature streams and show that, by means of feature fusion, we can improve the performance of the writer identification system on a data set produced by 200 different writers.
Scalable BGP QoS Extension with Multiple Metrics
Being an important and yet a challenging problem, the QoS-based routing in the converging Internet has received significant attention from the research community. However, most of the QoS-based routing research is conducted in the context of intra-domain routing, leaving QoS-based inter-domain routing relatively open. In this paper, we specifically investigate how to extend the current inter-domain routing protocol (BGP) to support multiple QoS metrics. With multiple metrics, a BGP speaker has to advertise multiple routes for each destination to its peer. This will increase the routing message overhead and make QoS-aware BGP unscalable to large networks. Therefore, our focus in this paper is to bring QoS extensions to the original BGP while simultaneously providing high performance and scalability. In response to this, we propose path reduction algorithms, i.e., Contribution Based Reduction (CBR) algorithms, to reduce the number of routes advertised by BGP speakers while maximizing the routing success ratio. Extensive simulations show that our schemes achieve high performance with low complexity in terms of message overhead and computation, making the QoS extension to BGP scalable.
Optimized submerged batch fermentation strategy for systems scale studies of metabolic switching in Streptomyces coelicolor A3(2)
Background: Systems biology approaches to study metabolic switching in Streptomyces coelicolor A3(2) depend on cultivation conditions ensuring high reproducibility and distinct phases of culture growth and secondary metabolite production. In addition, biomass concentrations must be sufficiently high to allow for extensive time-series sampling before occurrence of a given nutrient depletion for transition triggering. The present study describes for the first time the development of a dedicated optimized submerged batch fermentation strategy as the basis for highly time-resolved systems biology studies of metabolic switching in S. coelicolor A3(2). Results: By a step-wise approach, cultivation conditions and two fully defined cultivation media were developed and evaluated using strain M145 of S. coelicolor A3(2), providing a high degree of cultivation reproducibility and enabling reliable studies of the effect of phosphate depletion and L-glutamate depletion on the metabolic transition to antibiotic production phase. Interestingly, both of the two carbon sources provided, D-glucose and L-glutamate, were found to be necessary in order to maintain high growth rates and prevent secondary metabolite production before nutrient depletion. Comparative analysis of batch cultivations with (i) both L-glutamate and D-glucose in excess, (ii) L-glutamate depletion and D-glucose in excess, (iii) L-glutamate as the sole source of carbon and (iv) D-glucose as the sole source of carbon, reveal a complex interplay of the two carbon sources in the bacterium's central carbon metabolism. Conclusions: The present study presents for the first time a dedicated cultivation strategy fulfilling the requirements for systems biology studies of metabolic switching in S. coelicolor A3(2). Key results from labelling and cultivation experiments on either or both of the two carbon sources provided indicate that in the presence of D-glucose, L-glutamate was the preferred carbon source, while D-glucose alone appeared incapable of maintaining culture growth, likely due to a metabolic bottleneck at the oxidation of pyruvate to acetyl-CoA.
Scalable, absolute position recovery for omni-directional image networks
We describe a linear-time algorithm that recovers absolute camera positions for networks of thousands of terrestrial images spanning hundreds of meters in outdoor urban scenes, under uncontrolled lighting. The algorithm requires no human input or interaction. For real data, it recovers camera pose globally consistent on average to roughly five centimeters, or about four pixels of epipolar alignment. The paper's principal contributions include an extension of Markov chain Monte Carlo estimation techniques to the case of unknown numbers of feature points, unknown occlusion and deocclusion, large scale (thousands of images, and hundreds of thousands of point features), and large dimensional extent (tens of meters of inter-camera baseline, and hundreds of meters of baseline overall). Also, a principled method is given to manage uncertainty on the sphere; a new use of the Hough transform is proposed; and a method for aggregating local baseline constraints into a globally consistent pose set is described.
Last mile problem in overlay design
Performance of overlay networks is dependent on last-mile connections, since they require that data traverse these last-mile bottlenecks at each forwarding step. This requires several times more upstream bandwidth than downstream, further exaggerating the asymmetry between down-stream and upstream bandwidth in last-mile technologies. This imbalance can cause packet queuing at the outgoing network interface of forwarding nodes, increasing latency and causing packet losses. We describe a model of a last-mile constrained overlay network and formulate and use it to solve a simplified latency- and bandwidth-bounded overlay construction problem. We observe that queueing delay may be a significant component of the end-to-end delay and approaches ignoring this may potentially result in an overlay network violating the delay and/or loss bounds. We observe that allowing a small amount of loss, it is possible to support a significantly large number of nodes. For a given end to end delay and loss bound we identify feasible degree (fan out) of each nodes. Our study sheds insights which provide engineering guidelines for designing overlays accounting for last mile problem in the Internet.
A learning adaptive Bollinger band system
This paper introduces a novel forecasting algorithm that is a blend of micro and macro modelling perspectives when using Artificial Intelligence (AI) techniques. The micro component concerns the fine-tuning of technical indicators with population based optimization algorithms. This entails learning a set of parameters that optimize some economically desirable fitness function as to create a dynamic signal processor which adapts to changing market environments. The macro component concerns combining the heterogeneous set of signals produced from a population of optimized technical indicators. The combined signal is derived from a Learning Classifier System (LCS) framework that combines population based optimization and reinforcement learning (RL). This research is motivated by two factors, that of non-stationarity and cyclical profitability (as implied by the adaptive market hypothesis [10]). These two properties are not necessarily in contradiction but they do highlight the need for adaptation and creation of new models, while synchronously being able to consult others which were previously effective. The results demonstrate that the proposed system is effective at combining the signals into a coherent profitable trading system but that the performance of the system is bounded by the quality of the solutions in the population.
Pitch Estimation using Models of Voiced Speech on Three Levels
We present an algorithm for estimating the fundamental frequency in speech signals. Our approach incorporates models of voiced speech on three levels. First, we estimate the pitch for each time frame based on its harmonic structure using non-negative matrix factorization. The second level utilizes temporal pitch continuity to extract partial pitch contours. Thirdly, we incorporate statistics of the succession of voiced segments to aggregate partial contours to the final contour of an utterance. We evaluate our approach on the Keele database. The experimental results show the robustness of our method for noisy speech, and the good performance for clean speech in comparison with state-of-the-art algorithms.
On the family of ML spectral estimates for mixed spectrum identification
A recently developed point spectrum identification procedure based on a family of AR and ML spectral estimates is exploited to arrive at a mixed spectrum identification procedure. To this end, a variety of properties of the AR and ML estimates as a function of model order are described. These properties relate to amplitude convergence, resolution and a characterization of the AR spectral artifact which is used to arrive at improved continuous spectral estimates. A variety of examples are presented. >
Using Linda for supercomputing on a local area network
A distributed parallel processing system based on the LINDA programming constructs has been implemented on a local area network of computers. This system allows a single application program to utilize many machines on the network simultaneously. Several applications have been implemented on the network at Sandia National Laboratories and have achieved performances considerably faster than that of a Cray-1S. Several collections of machines have been used including up to eleven DEC VAXes, three Sun/3 workstations, and a PC.
Fault modeling and pattern-sensitivity testing for a multilevel DRAM
Multilevel dynamic random-access memory (MLDRAM) attempts to increase the storage density of semiconductor memory without further reducing the lithographic dimensions. It does so by using more than two possible signal voltages on each cell capacitor thus permitting more than one bit to be stored in each cell. Birk's MLDRAM scheme has several promising properties, including robust locally-generated data signal and reference signal generation, and fast flash-conversion sensing. This paper describes a fault model for Birk's MLDRAM that was developed by considering the behaviors produced by likely defects at the schematic level. The resulting behaviors include faults that are detectable as observable logical errors, faults that can be detected by current measurements, and faults that, in the worst case, can only be detected by testing for degraded noise margins. All Boolean faults in the fault model can be detected by an efficient test whose length grows linearly in the number of cells. The narrower noise margins in MLDRAM will make it more vulnerable to pattern sensitivities. We also developed a linear test that evaluates worst-case sensing conditions.
Bit error rate analysis in IEEE 802.15.3a UWB channels
In this paper, we present a computable bit error rate (BER) expression for the binary signals in the IEEE 802.15.3a ultra-wideband (UWB) channel. In the literature, the impacts of the RAKE receiver's finger numbers and lognormal shadowing on the BER performance have not been reported yet. We propose a characteristic function (CF) based BER formula to overcome the convergence problem of the existing moment generating function (MGF) approach when the BER calculation takes account of the shadowing effect. Simulation results demonstrate that the proposed CF-based computable BER formula can accurately estimate the complete effects of the cluster and ray arrival processes, the lognormal fading as well as shadowing, and the finger numbers at RAKE receivers.
Application-Driven Voltage-Island Partitioning for Low-Power System-on-Chip Design
Among the different methods of reducing power for core-based system-on-chip (SoC) designs, the  voltage-island   technique  has gained in popularity. Assigning cores to the different supply voltages and floorplanning to create contiguous voltage islands are two important steps in the design process. We propose a new application-driven approach to voltage partitioning and island creation with the objective of reducing overall SoC power, area, and floorplanner runtime. Given an application power-state machine (PSM), we first identify the suitable range of supply voltages for each core. Then, we generate the discrete voltage assignment table using a heuristic technique. Next, we describe a methodology of reducing the large number of available choices from the voltage assignment table down to a useful set using the application PSM. We partition the cores into islands, using a cost function that gradually shifts from a power-based assignment to a connectivity-based one. Compared with previously reported techniques, a 9.4% reduction in power and 8.7% reduction in area are achieved using our approach, with an average runtime improvement of 2.4 times.
Query scheduling in multi query optimization
Complex queries are becoming commonplace, with the growing use of decision support systems. Decision support queries often have a lot of common sub-expressions within each query, and queries are often run as a batch. Multi query optimization aims at exploiting common sub-expressions, to reduce the evaluation cost of queries, by computing them once and then caching them for future use, both within individual queries and across queries in a batch. In case cache space is limited, the total size of sub-expressions that are worth caching may exceed available cache space. Prior work in multi query optimization involves choosing a set of common sub-expressions that fit in available cache space, and once computed, retaining their results across the execution of all queries in a batch. Such optimization algorithms do not consider the possibility of dynamically changing the cache contents. This may lead to sub-expressions occupying cache space even if they are not used by subsequent queries. The available cache space can be best utilized by evaluating the queries in an appropriate order and changing the cache contents as queries are executed. We present several algorithms that consider these factors, in order to reduce the cost of query evaluation.
DDoS detection and traceback with decision tree and grey relational analysis
In Distributed Denial-of-Service (DDoS) Attack, an attacker breaks into many innocent computers (called zombies). Then, the attacker sends a large number of packets from zombies to a server, to prevent the server from conducting normal business operations. We design a DDoS-detection system based on a decision-tree technique and, after detecting an attack, to trace back to the attacker's locations with a traffic-flow pattern-matching technique. Our system could detect DDoS attacks with the false positive ratio about 1.2-2.4%, false negative ratio about 2-10%, and find the attack paths in traceback with the false negative rate 8-12% and false positive rate 12-14%.
Comparative interactomics analysis of protein family interaction networks using PSIMAP (protein structural interactome map)
Motivation: Many genomes have been completely sequenced. However, detecting and analyzing their protein--protein interactions by experimental methods such as co-immunoprecipitation, tandem affinity purification and Y2H is not as fast as genome sequencing. Therefore, a computational prediction method based on the known protein structural interactions will be useful to analyze large-scale protein--protein interaction rules within and among complete genomes.#R##N##R##N#Results: We confirmed that all the predicted protein family interactomes (the full set of protein family interactions within a proteome) of 146 species are scale-free networks, and they share a small core network comprising 36 protein families related to indispensable cellular functions. We found two fundamental differences among prokaryotic and eukaryotic interactomes: (1) eukarya had significantly more hub families than archaea and bacteria and (2) certain special hub families determined the topology of the eukaryotic interactomes. Our comparative analysis suggests that a very small number of expansive protein families led to the evolution of interactomes and seemed tohave played a key role in species diversification.#R##N##R##N#Contact: jong@kribb.re.kr#R##N##R##N#Supplementary information: http://interactomics.org
Exploring board game design using digital technologies
This talk presents results of a case study from a course called "History of Games" offered at the School of Art + Design at New Jersey Institute of Technology. After analyzing various traditional board games and their mechanics, students explore the possibility of producing their own original board games by altering various existing game structures through application of new technologies such as digital prototyping, including laser cutting and 3-D printing, and microcontroller technologies. In principle, we can fully emulate the playing of a board game such as Monopoly inside a computer, digitally. However, there is a certain quality of physicality with traditional board games that cannot be experienced through games in fully digital environments. The existence of tangible game pieces, boards, and real human players can produce cooperation, engagement, and tensions unlike those in video games and AR-based applications. Through this project-based course, students further explore how new technologies can help in developing new types of games.
Using Call-Context to Prevent the Emergence of Chaotic Workflow Behaviors in Overload Situations
By defining workflows, existing services are put into novel contexts and exposed to different workloads, which in turn can result in unexpected behaviors. This paper examines the chaotic behavior of sequential workflows in overload situations and discusses the use of call-contexts as a means of avoiding them.
An analytical and experimental investigation of flexible manipulator performance
A critical modeling of a flexible manipulator will involve a thorough understanding of the issues specific to flexible manipulator performance and an experimental investigation of a physical system. A comprehensive dynamic model of a flexible manipulator has been presented in this paper together with a preliminary experimental investigation that examines the performance of this comprehensive model. The analytical model recognizes in particular, the coupling characteristics of the deformations in a flexible manipulator configuration and realizes an efficient and dedicated finite element scheme to model general spatial manipulator configurations with revolute and prismatic pairs. The experimental results indicate a favorable level of performance for the application of the special finite element for a practical manipulator.
BIDDLE: a bidirectional data driven Lisp engine
The authors propose BIDDLE, a direct execution architecture for Lisp based on both data- and demand-driven principles. A priority queuing mechanism is used to control the parallelism and the workload of the processing elements. Also introduced is a novel mechanism whereby the sequential semantics of Lisp can be enforced in such a way as not to reduce the parallelism too drastically. BIDDLE is, therefore, aimed at achieving a balance between eager and lazy evaluation, sequential semantics, and parallelism. At present, BIDDLE exists only on paper and is a long way from hard ware implementation. >
Ant algorithms for discrete optimization
This article presents an overview of recent work on ant algorithms, that is, algorithms for discrete optimization that took inspiration from the observation of ant colonies' foraging behavior, and introduces the ant colony optimization (ACO) metaheuristic. In the first part of the article the basic biological findings on real ants are reviewed and their artificial counterparts as well as the ACO metaheuristic are defined. In the second part of the article a number of applications of ACO algorithms to combinatorial optimization and routing in communications networks are described. We conclude with a discussion of related work and of some of the most important aspects of the ACO metaheuristic.
Vehicular networking for intelligent and autonomous traffic management
Traffic congestion has become a daily problem that most people suffer. This not only impacts the productivity of the population but also poses a safety risk. Most of the technologies for intelligent highways focus on safety measures and increased driver awareness, and expect a centralized management for the traffic flow. This paper presents a new approach for enabling autonomous and adaptive traffic management through vehicular networks. By allowing data exchange between vehicles about route choices, congestions and traffic alerts, a vehicle makes a decision on the best course of action. Unlike centralized schemes that provide recommendations, our VANET-based Autonomous Management (VAM) approach factors in the destination and routes of nearby vehicles in deciding on whether rerouting is advisable. In addition, VAM leverages the presence of smart traffic lights and enables coordination between vehicles and lightcontrollers in order to ease congestion. The collective effect of all vehicles will be an autonomous reshape of the traffic pattern based on their destinations and road conditions. The simulation results demonstrate the advantage of VAM.
Peer-to-peer support for low-latency Massively Multiplayer Online Games in the cloud
Cloud gaming has recently been proposed as an alternative to traditional video game distribution. With this approach, the entire game is stored, run and rendered on a remote server. Player input is forwarded to the server via the Internet and the game's output is returned as a video stream. This adds network delay, which can negatively impact the gameplay. The delay is acceptable as long as the user is located geographically close to the cloud servers. However, for Massively Multiplayer Online Games (MMOGs), this delay is added on top of the existing delay between MMOG client and server. As MMOGs are highly delay-sensitive, this can significantly degrade their playability. To deal with this issue, we propose to use peer-to-peer techniques to distribute the MMOG server functionality and place it at the cloud server centers. This allows us to reduce the additional delay introduced by running the MMOG clients in the cloud.
Estimating crowd densities and pedestrian flows using wi-fi and bluetooth
The rapid deployment of smartphones as all-purpose mobile computing systems has led to a wide adoption of wireless communication systems such as Wi-Fi and Bluetooth in mobile scenarios. Both communication systems leak information to the surroundings during operation. This information has been used for tracking and crowd density estimations in literature. However, an estimation of pedestrian flows has not yet been evaluated with respect to a known ground truth and, thus, a reliable adoption in real world scenarios is rather difficult. With this paper, we fill in this gap. Using ground truth provided by the security check process at a major German airport, we discuss the quality and feasibility of pedestrian flow estimations for both Wi-Fi and Bluetooth captures. We present and evaluate three approaches in order to improve the accuracy in comparison to a naive count of captured MAC addresses. Such counts only showed an impractical Pearson correlation of 0.53 for Bluetooth and 0.61 for Wi-Fi compared to ground truth. The presented extended approaches yield a superior correlation of 0.75 in best case. This indicates a strong correlation and an improvement of accuracy. Given these results, the presented approaches allow for a practical estimation of pedestrian flows.
Alleviating the Constant Stochastic Variance Assumption in Decision Research: Theory, Measurement, and Experimental Test
Analysts often rely on methods that presume constant stochastic variance, even though its degree can differ markedly across experimental and field settings. This reliance can lead to misestimation of effect sizes or unjustified theoretical or behavioral inferences. Classic utility-based discrete-choice theory makes sharp, testable predictions about how observed choice patterns should change when stochastic variance differs across items, brands, or conditions. We derive and examine the implications of assuming constant stochastic variance for choices made under different conditions or at different times, in particular, whether substantive effects can arise purely as artifacts. These implications are tested via an experiment designed to isolate the effects of stochastic variation in choice behavior. Results strongly suggest that the stochastic component should be carefully modeled to differ across both available brands and temporal conditions, and that its variance may be relatively greater for choices made for the future. The experimental design controls for several alternative mechanisms (e.g., flexibility seeking), and a series of related models suggest that several econometrically detectable explanations like correlated error, state dependence, and variety seeking add no explanatory power. A series of simulations argues for appropriate flexibility in discrete-choice specification when attempting to detect temporal stochastic inflation effects.
The Design of a Low-Power Asynchronous DES Coprocessor for Sensor Network Encryption
Sensor network nodes have a very tight power budget and the power efficiency is the biggest design concern in sensor network circuits. A general-purpose processor (e.g. an ARM processor) is not efficient to execute encryption algorithms because it has no special instructions to support encryption operations, for example very often-used permutation operations. In the paper, we propose a low-power ASIC encryption coprocessor for sensor network nodes. A DES algorithm is used because the algorithm does not include power-hungry and complex mathematic operations, such as multiplication, division and addition. An asynchronous logic style is used to design the coprocessor. With an asynchronous controller, a global clock is not necessary when idle, resulting in zero standby dynamic power. Using the DES coprocessor, the power consumed by encryption can be saved by 4 orders of magnitude than a pure software calculation.
The effect of a stand-alone ethics course in Chilean engineering students' attitudes
Engineering ethics education is taking on increasing importance worldwide, but in Chile the percentage of universities that have a mandatory course concerning ethics is still small. Traditionally, Chilean universities with existing ethics courses teach them using a philosophical or theological perspective, limited to occidental theories, and usually from a Christian point of view. This article studies the impact of a new methodology and technique to teach ethics in Chile: case-based, non-normative, and with a critical-descriptive approach. An empirical study is conducted to assess the relative impact of an ethics class on students individual and inherent moral values and attitudes, and understand the factors that contribute to this impact. Results indicate that even though the importance of religion in Chile is decreasing, it is still a major source of students� ethical principles and moral values. In addition, results suggest that a change in moral values develops when discussions among groups with different points of view occur.
Functional validation of fault-tolerant asynchronous algorithms
The paper presents an alternative approach to the formal specification and validation of distributed asynchronous algorithms. It begins with a syntactically correct description of the algorithm whose correctness is then to be validated. The validation of the algorithm is based on the process-oriented discrete simulation and permits a partial correctness validation of the algorithm implemented by a program. The suggested method enables to model independent activity of several processors (using pseudo-parallel processes) in simulation time and to model communication channels with defined time behavior and failure semantics. Using the approach it is easy to add other processes like model of system's environment, fault injector and state observer. The method is described with the aid of a simple C-based validation tool called C-Sim. The utilization of C-Sim requires only slight changes in C-coded implementation of the verified algorithm. An example of validation of distributed election algorithm with the presence of faults is presented.
An event-driven SIR model for topic diffusion in web forums
Social media is being increasingly used as a communication channel. Among social media, web forums, where people in online communities disseminate and receive information by interaction, provide a good environment to examine information diffusion. In this research, we aim to understand the mechanisms and properties of the information diffusion in the web forum. For that, we model topic-level information diffusion in web forums using the baseline epidemic model, the SIR(Susceptible, Infective, and Recovered) model, frequently used in previous research to analyze disease outbreaks and knowledge diffusion. In addition, we propose an event-driven SIR model that reflects the event effect on information diffusion in the web forum. The proposed model incorporates the effect of news postings on the web forum. We evaluate two models using a large longitudinal dataset from the web forum of a major company. The event-SIR model outperforms the SIR model in fitting on major spikey topics that have peaks of author participation.
An intersection algorithm based on Delaunay triangulation
A robust method for finding points of intersection of line segments in a 2-D plane is presented. The plane is subdivided by Delaunay triangulation to localize areas where points of intersection exist and to guarantee the topological consistency of the resulting arrangement. The subdivision is refined by inserting midpoints recursively until the areas containing points of intersection are sufficiently localized. The method is robust in the sense that it does not miss points of intersection that are easily detectable when costly line-pair checking is performed. The algorithm is adaptive in the sense that most of the computational cost is incurred for the areas where finding points of intersection is difficult. >
Earth System Science Workbench: a data management infrastructure for earth science products
The Earth System Science Workbench (ESSW) is a non-intrusive data management infrastructure for researchers who are also data publishers. An implementation of ESSW to track the processing of locally received satellite imagery is presented, demonstrating the Workbench's transparent and robust support for archiving and publishing data products. ESSW features a Lab Notebook metadata service, an ND-WORM (No Duplicate-Write Once Read Many) storage service, and Web user interface tools. The Lab Notebook logs processes (experiments) and their relationships via a custom API to XML documents stored in a relational database. The ND-WORM provides a managed storage archive for the Lab Notebook by keeping unique file digests and name-space meta-data, also in a relational database. ESSW Notebook tools allow project searching and ordering, and file and meta-data management.
Highly-available services using the primary-backup approach
The authors derive lower bounds and the corresponding optimal protocols for three parameters for synchronous primary-backup systems. They compare their results with similar results for active replication in order to determine whether the common folklore on the virtues of the two approaches can be shown formally. They also extend some of their results to asynchronous primary-backup systems. They implement an important subclass of primary-backup protocols that they call 0-blocking. These protocols are interesting because they introduce no additional protocol related delay into a failure-free service request. Through implementing these protocols the authors hope to determine the appropriateness of their theoretical system model and uncover other practical advantages or limitations of the primary-backup approach. >
Efficient composition of media object for multimedia scene rendering
In an audiovisual scene consisting of high capacity multimedia objects, when a scene change, which an object is inserted, deleted, or replaced in real time by user interaction, is required, the object priority order compositor for an multimedia player according to the present invention performs rendering of objects based on priority order of objects. The compositor searches priority order of objects, and makes it possible to arbitrarily change order of objects and further to perform rendering of only objects requiring reconstruction. Thus, the object priority order compositor provides re-usability and availability of multimedia data. Further, the object priority order compositor renders efficient multimedia data processing possible by providing a user with a dynamic scene.
Percolation based synthesis
A new approach called Percolation Based Synthesis for the scheduling phase of High Level Synthesis (HLS) is presented. We discuss some new techniques (which are implemented in our tools) for compaction of flow graphs beyond basic blocks limits, which can produce order of magnitude speed ups versus serial execution. Our algorithm applies to programs with  conditional jumps, loops  and  multicycle pipelined operations . In order to schedule under resource constraints we start by first finding the  optimal schedule  (without constraints) and then add heuristics to map the optimal schedule onto the given system. We argue that starting from an optimal schedule is one of the most important factors in scheduling because it offers the user flexibility to tune the heuristics and gives him a good bound for the resource constrained schedule. This scheduling algorithm is integrated with synthesis tool which uses VHDL as input description and produces a  structural netlist  of generic register-transfer components and a  unit based control table  as output. We show that our algorithm obtains better results than previously published algorithms.
Using SDL for implementing a wireless medium access control protocol
Specification and Description Language (SDL) is a high abstraction level system design language with a clear graphical notation. Because of formal presentation, an SDL model can be automatically converted into source C code for implementation. However, the high abstraction level creates a conceptual gap between a general SDL model and its implementation in a final operational platform. The paper studies the SDL development of an embedded Medium Access Control (MAC) protocol for a wireless local area network (WLAN) demonstrator. The SDL design flow for the protocol is first started by architectural design without target platform dependencies. Functionality is added to the model using the top-down design approach. Functional simulations are used for verifying the operation of the protocol. Next, the performance is estimated using performance simulations in a workstation environment. Performance improvements can be achieved by optimising the SDL model.
AGUIA: Agents Guidance for Intelligence Amplification in Goal Oriented Tasks
All I know is that I know nothing (Socratic Ignorance). The world is an increasingly complex with problems that require swift resolution. Although knowledge is widely available, be it stored in companies’ databases or spread over the Internet, humans have intrinsic limitations for handling very large volumes of information or keeping track of frequent updates in a constantly changing world. Moreover, human reasoning is highly intuitive and potentially biased. Decision-making is often based on rules of thumb instead of systematic analysis with full understanding of decisions’ context. Computer systems that manage knowledge to thoroughly explore the context and the range of alternatives may improve human decision-making by making people aware of possible misconceptions and biases. Computer systems are also limited in their potential usage due to the frame problem. Systems are not aware of their ignorance, thus they cannot surplus human intelligence, but they may be useful complement to human’s intelligence. The objective of this paper is to present the AGUIA model for amplifying human intelligence, based on agent’s technology, for task-oriented contexts. AGUIA uses domain ontology and task scripts for handling formal and semiformal knowledge bases, thereby helping to systematically (1) explore the range of alternatives, (2) interpret the problem and the context, and (3) maintain “awareness” of the problem. As for humans, knowledge is a fundamental resource for AGUIA performance. AGUIA’s knowledge base keeps updating its content, in the background, during interaction with humans, either through identified individuals or through anonymous mass contribution. The feasibility and benefits of AGUIA were demonstrated in many different fields, such as engineering design, fault diagnosis, accident investigation and online interaction with the government. The experiments considered a set of criteria including: product cost, number of explored alternatives, users’ problem understanding and users’ awareness of problem context changes. Results indicate AGUIA can actually improve human problem solving capacity in many different areas.
Globally asymptotically stable filters for source localization and navigation aided by direction measurements
This paper presents a set of filters with globally asymptotically stable error dynamics for source localization and navigation, in 3-D, based on direction measurements from the agent (or vehicle) to the source, in addition to relative velocity readings of the agent. Both the source and the agent are allowed to have constant unknown drift velocities and the relative drift velocity is also explicitly estimated. The observability of the system is studied and realistic simulation results are presented, in the presence of measurement noise, that illustrate the performance of the achieved solutions. Comparison results with the Extended Kalman Filter are also provided and similar performances are achieved.
Learning and Leveraging the Relationship between Architecture-Level Measurements and Individual User Satisfaction
The ultimate goal of computer design is to satisfy the end-user. In particular computing domains, such as interactive applications, there exists a variation in user expectations and user satisfaction relative to the performance of existing computer systems. In this work, we leverage this variation to develop more efficient architectures that are customized to end-users. We first investigate the relationship between microarchitectural parameters and user satisfaction. Specifically, we analyze the relationship between hardware performance counter (HPC) readings and individual satisfaction levels reported by users for representative applications. Our results show that the satisfaction of the user is strongly correlated to the performance of the underlying hardware. More importantly, the results show that user satisfaction is highly user-dependent. To take advantage of these observations, we develop a framework called Individualized Dynamic Voltage and Frequency Scaling (iDVFS). We study a group of users to characterize the relationship between the HPCs and individual user satisfaction levels. Based on this analysis, we use artificial neural networks to model the function from HPCs to user satisfaction for individual users. This model is then used online to predict user satisfaction and set the frequency level accordingly. A second set of user studies demonstrates that iDVFS reduces the CPU power consumption by over 25% in representative applications as compared to the Windows XP DVFS algorithm.
Performance of Different Mobile Payment Service Concepts Compared with a NFC-Based Solution
The paper compares the performance of different traditional mobile payment service concepts with a state of the art NFC-based mobile payment solution. The goal is to evaluate the different mobile payment concepts, not their software implementation, from a performance and end-to-end service duration time point of view. Overall, there have been five different mobile payment services developed, implemented and benchmarked for the concept comparison, namely: Interactive Voice Response, Short Message Service, Wireless Application Protocol, One Time Password Generator as well as a solution based on Near Field Communication.
Electrical Power Monitoring System Using Thermochron Sensor and 1-Wire Communication Protocol
Maintaining quality and reliability of operation of remotely located electrical machines as well as power supply equipment by monitoring their load is an important task in modern practical electrical engineering. This paper presents a low-cost efficient, robust and relatively simple load monitoring electronic system based on the use of special encapsulated temperature sensors-data loggers (so-called Thermochron iButtons) combined with application of 1-wire data communications implemented on the power supply line, dedicated data communication line with off-line data transfer.
Undergraduate students' gender differences in IT skills and attitudes
The worldwide concern about the gender gap in information technology and the lack of woman participation in computer science has been attributed to the different cultural influences to which boys and girls are subject. In The University of Hong Kong, girls achieved greater improvements in their computer skills than their male counterparts after completing one year of studies. Recognising their own progress has, in turn, boosted their confidence in using IT. The young women's estimates of their skill levels have doubled over the years from 1998 to 2000. Despite this recorded acceleration at the end of the academic years, girls were less confident of their abilities and possessed lower IT skill levels than boys before starting their university education, as found in surveys of freshmen's computer skills. This study compares the responses of student participants of the HKU/IBM Notebook Computer Programme, which started in 1998, in the self-reported IT skills and attitudes of male and female students, in surveys conducted both at the beginning and again at the end of the freshman year. It also examines the achievement scores of the IT Proficiency Tests and the 'Foundations to Information Technology' courses administered for the student IT requirement for graduation.
Construction and completion of flux balance models from pathway databases
Motivation: Flux balance analysis (FBA) is a well-known technique for genome-scale modeling of metabolic flux. Typically, an FBA formulation requires the accurate specification of four sets: biochemical reactions, biomass metabolites, nutrients and secreted metabolites. The development of FBA models can be time consuming and tedious because of the difficulty in assembling completely accurate descriptions of these sets, and in identifying errors in the composition of these sets. For example, the presence of a single non-producible metabolite in the biomass will make the entire model infeasible. Other difficulties in FBA modeling are that model distributions, and predicted fluxes, can be cryptic and difficult to understand.#R##N##R##N#Results: We present a multiple gap-filling method to accelerate the development of FBA models using a new tool, called MetaFlux, based on mixed integer linear programming (MILP). The method suggests corrections to the sets of reactions, biomass metabolites, nutrients and secretions. The method generates FBA models directly from Pathway/Genome Databases. Thus, FBA models developed in this framework are easily queried and visualized using the Pathway Tools software. Predicted fluxes are more easily comprehended by visualizing them on diagrams of individual metabolic pathways or of metabolic maps. MetaFlux can also remove redundant high-flux loops, solve FBA models once they are generated and model the effects of gene knockouts. MetaFlux has been validated through construction of FBA models for Escherichia coli and Homo sapiens.#R##N##R##N#Availability: Pathway Tools with MetaFlux is freely available to academic users, and for a fee to commercial users. Download from: biocyc.org/download.shtml.#R##N##R##N#Contact: mario.latendresse@sri.com#R##N##R##N#Supplementary information:Supplementary data are available at Bioinformatics online.
Practice Prize Report---The Power of CLV: Managing Customer Lifetime Value at IBM
Customer management activities at firms involve making consistent decisions over time, about: a which customers to select for targeting, b determining the level of resources to be allocated to the selected customers, and c selecting customers to be nurtured to increase future profitability. Measurement of customer profitability and a deep understanding of the link between firm actions and customer profitability are critical for ensuring the success of the above decisions. We present the case study of how IBM used customer lifetime value CLV as an indicator of customer profitability and allocated marketing resources based on CLV. CLV was used as a criterion for determining the level of marketing contacts through direct mail, telesales, e-mail, and catalogs for each customer. In a pilot study implemented for about 35,000 customers, this approach led to reallocation of resources for about 14% of the customers as compared to the allocation rules used previously which were based on past spending history. The CLV-based resource reallocation led to an increase in revenue of about $20 million a tenfold increase without any changes in the level of marketing investment. Overall, the successful implementation of the CLV-based approach resulted in increased productivity from marketing investments. We also discuss the organizational and implementation challenges that surrounded the adoption of CLV in this firm.
Multi-User Joint Tx/Iterative Rx MMSE-FDE and Successive MUI Cancellation for Uplink DS-CDMA
Uplink multi-user direct sequence-code division multi-access (DS-CDMA) suffers from strong multi-user interference (MUI) and self inter-chip interference (ICI) caused by severe frequency-selective fading. In this paper, we propose a joint Tx/iterative Rx frequency-domain equalization (FDE) based on minimum mean square error (MMSE) criterion and successive MUI cancellation (MUIC) for DS-CDMA uplink. In the proposed scheme, each user applies one-tap Tx FDE before transmitting signal. At the base station, joint one-tap Rx FDE and successive MUIC is iteratively performed. The FDE weights of users and base station are jointly optimized based on the MMSE criterion in order to reduce MUI and ICI while exploiting channel frequency-selectivity. Computer simulation results show that the proposed scheme provides much improved bit error rate (BER) performance than the conventional iterative Rx MMSE-FDE with successive MUIC.
Basic primitives for molecular diagram sketching
A collection of primitive operations for molecular diagram sketching has been developed. These primitives compose a concise set of operations which can be used to construct publication-quality 2 D coordinates for molecular structures using a bare minimum of input bandwidth. The input requirements for each primitive consist of a small number of discrete choices, which means that these primitives can be used to form the basis of a user interface which does not require an accurate pointing device. This is particularly relevant to software designed for contemporary mobile platforms. The reduction of input bandwidth is accomplished by using algorithmic methods for anticipating probable geometries during the sketching process, and by intelligent use of template grafting. The algorithms and their uses are described in detail.
Efficient and complete remote authentication scheme with smart cards
A complete remote authentication scheme should provide the following security properties: (1) mutual authentication, (2) session key exchange, (3) protection of user anonymity, (4) support of immediate revocation capability, (5) low communication and computation cost, (6) resistance to various kinds of attacks, (7) freely choosing and securely changing passwords by users, and (8) without storing password or verification tables in servers. However, none of the existing schemes meets all the requirements. In this paper, along the line of cost effective approach using hash functions for authentication, we propose an efficient and practical remote user authentication scheme with smart cards to support the above complete security properties.
Multimonostatic Shape Reconstruction of Two-Dimensional Dielectric Cylinders by a Kirchhoff-Based Approach
The inverse problem of reconstructing the shape of dielectric cylinders by aspect-limited multimonostatic multifrequency electromagnetic scattering data is dealt with. The problem is formulated as a linear one by means of the physical-optics approximation distributional approach. The difference with respect to the case of perfectly electrical conducting scatterers is pointed out, since the penetrability of the scatterers is taken into account by considering the contribution of the “shadowed” side to the local reflection coefficient. The adopted model allows one to predict that both the “illuminated” and “shadowed” sides of the scatterer provide contribution to the reconstructed image but with a delocalization depending on the relative dielectric permittivity. The numerical results confirm this expectation and show the effectiveness of the approach.
Robust and low complexity packet detector design for MB-OFDM UWB
Multiband orthogonal frequency division multiplexing (MB-OFDM) ultra wideband (UWB) systems have drawn much attention for its high spectrum efficiency and multiple access capability. However, its large throughput requirement and low power spectral density result in high hardware complexity and high power consumption, which are challenges of designing the packet detector. In this paper, a novel detection method is proposed with very good detection performance in low SNR. Low cost and low power schemes are also introduced in circuit design to save 70% area and 71% power. The proposed packet detector is synthesized with UMC 0.13µm library at 132MHz clock frequency. The hardware cost is 56.9K gates and the power consumption is only 11.7mW.
Microfluidic device for continuous magnetophoretic separation of red blood cells
This paper presents a microfluidic device for magnetophoretic separation red blood cells from blood under continuous flow. The separation method consist of continuous flow of a blood sample (diluted in PBS) through a microfluidic channel which presents on the bottom ldquodotsrdquo of ferromagnetic layer. By applying a magnetic field perpendicular on the flowing direction, the ferromagnetic ldquodotsrdquo generate a gradient of magnetic field which amplifies the magnetic force. As a result, the red blood cells are captured on the bottom of the microfluidic channel while the rest of the blood is collected at the outlet. Experimental results show that an average of 95 % of red blood cells is trapped in the device.
Polynomial Time Construction Algorithm of BCNC for Network Coding in Cyclic Networks
Network coding in cyclic networks meets more problems than in acyclic networks. Recently, S.-Y.R.Li et al.proposed a framework of convolutional network coding (CNC) as well as its four properties for cyclic networks with theoretic fundamentals of discrete valuation ring (DVR). The four properties, Convolutional Multicast (CM), Convolutional Broadcast (CB), Convolutional Dispersion (CD) and Basic Convolutional NetworkCode (BCNC), are notions of increasing strength in this order with regard to linear independence among the global encoding kernels. The existence of a BCNC then implies the existence ofthe rest. That is, BCNC is the best convolutional network code in terms of linear independence. However, the code construction algorithm of BCNC was not presented explicitly. To the best of our knowledge, this is the first paper to propose a polynomial time construction algorithm of BCNC for network coding in cyclic networks, which can deal with different characteristics of cycles in terms of topology, including link cycles but flow acyclic, simpleflow cycles and knots. Finally, polynomial time complexity of the algorithm was proved as well as its effectiveness.
Using Context Ontologies for Addressing and Routing in Mobile Ad Hoc Networks
This paper presents a new way of addressing and routing for mobile ad hoc networks on the basis of contextual information such as air pressure, brightness, wind direction and strength, or GPS position. The most common use case of context-based addressing is group communication: A participant sends a message to an a priori unspecified set of recipients, but indicates the context in which the message could be useful for a potential receiver. In contrast to infrastructure networks the sender no longer designates the receiver of its message with a distinct identifier. Instead, each recipient using his local context decides by himself, whether the message is useful for him and whether it should be sent out again. The modeling of the necessary application knowledge is done as ontologies in OWL (Web Ontology Language). As an example scenario, a wind gust warning on highways using a vehicular ad hoc network (VANET) is described: the warning message should be sent to all vehicles on the same route containing the place where the wind was detected. The models are applied in a prototypical example scenario in order to show the performance of the approach through a simulation, using the JiST/SWANS simulator for mobile ad hoc networks. The results show that the number of messages that are necessary to warn all vehicles in a given environment of the wind danger can be reduced by half – as opposed to a simple flooding of the network.
Local histogram of figure/ground segmentations for dynamic background subtraction
We propose a novel feature, local histogram of figure/ground segmentations, for robust and efficient background subtraction (BGS) in dynamic scenes (e.g., waving trees, ripples in water, illumination changes, camera jitters, etc.). We represent each pixel as a local histogram of figure/ground segmentations, which aims at combining several candidate solutions that are produced by simple BGS algorithms to get a more reliable and robust feature for BGS. The background model of each pixel is constructed as a group of weighted adaptive local histograms of figure/ground segmentations, which describe the structure properties of the surrounding region. This is a natural fusion because multiple complementary BGS algorithms can be used to build background models for scenes. Moreover, the correlation of image variations at neighboring pixels is explicitly utilized to achieve robust detection performance since neighboring pixels tend to be similarly affected by environmental effects (e.g., dynamic scenes). Experimental results demonstrate the robustness and effectiveness of the proposedmethod by comparing with four representatives of the state of the art in BGS.
Search space boundary extension method in real-coded genetic algorithms
In real-coded genetic algorithms (GAs), some crossover operators do not work well on functions which have their optimum at the corner of the search space. To cope with this problem, we have proposed a boundary extension method which allows individuals to be located within a limited space beyond the boundary of the search space. In this paper, we give an analysis of the boundary extension methods from the viewpoint of sampling bias and perform a comparative study on the effect of applying two boundary extension methods, namely the boundary extension by mirroring (BEM) and the boundary extension with extended selection (BES). We were able to confirm that to use sampling methods which have smaller sampling bias had good performance on both functions which have their optimum at or near the boundaries of the search space, and functions which have their optimum at the center of the search space. The BES/SD/A (BES by shortest distance selection with aging) had good performance on functions which have their optimum at or near the boundaries of the search space. We also confirmed that applying the BES/SD/A did not cause any performance degradation on functions which have their optimum at the center of the search space.
A New Implementation of a Half-Duplex Decode-and-Forward Cooperative Algorithm Using Complete Complementary Code Sets.
This paper introduces an implementation of a halfduplex decode-and-forward cooperative algorithm using the complete complementary code (CCC) sets. These code sets have an impulsive autocorrelation sum among each set and a cross correlation sum along the set size that vanishes for all shifts. Each user is assigned a set of spreading codes to spread his data and send each of the resulting signals on a different sub-band. The decode-and-forward cooperative procedure is applied and a receiver consisting of parallel branches matched filter followed by a λ-MRC combiner is used for detection. It is demonstrated that the probability of error performance of the proposed algorithm is very close to the performance of the analytical closed-form probability of error in similar transmission conditions. The algorithm is also compared with a noncooperative multiband direct-sequence code division multiple access (MB DS-CDMA) algorithm using the complete complementary sets. The simulation results illustrate the enhanced performance of the proposed algorithm under different channel assumptions.
Efficient lower bounds and heuristics for the variable cost and size bin packing problem
We consider the variable cost and size bin packing problem, a generalization of the well-known bin packing problem, where a set of items must be packed into a set of heterogeneous bins characterized by possibly different volumes and fixed selection costs. The objective of the problem is to select bins to pack all items at minimum total bin-selection cost. The paper introduces lower bounds and heuristics for the problem, the latter integrating lower and upper bound techniques. Extensive numerical tests conducted on instances with up to 1000 items show the effectiveness of these methods in terms of computational effort and solution quality. We also provide a systematic numerical analysis of the impact on solution quality of the bin selection costs and the correlations between these and the bin volumes. The results show that these correlations matter and that solution methods that are un-biased toward particular correlation values perform better.
An efficiently revised sustain driver for AC plasma display
A new sustain driver employing a step-up function is presented to achieve the faster rise-time of sustain voltage, which Ls suitable for widely used address-and-display-period-separated (ADS) driving method, and most of all for cost saving. The proposed sustain driver can reduce the number of switching devices by 25 (%) compared to the prior approach improving the overall system efficiency about 10 (%). And brightness decrease problem resulted from the lack of the number of the sustain pulse can be solved without a limitation to sustain pulse width. Operational principle and its features are illustrated comparing with conventional approaches. The validity of the proposed sustain driver is verified through experiments using a prototype equipped with a 7.5 (inch) diagonal panel, which is operated at 200 (kHz) switching frequency.
Short note: An automated system for the statistical analysis of sediment texture and structure at the micro scale
A macro has been developed that allows for automated statistical analyses of particles. It can be used with binary images from any source, and is developed for use with lacustrine, marine, Aeolian, and marine sediments. The macro code is freely available, and runs on open-source software. It has been specially designed to accommodate very small regions of interest relative to particle size, and to produce continuous downcore stratigraphies. The macro runs quickly, requires no user-interaction, is easily modifiable, saves 'raw data' for algorithm checking and further analyses, and can run in a mode that quantifies cracks and other disturbances on images. The macro allows for rapid analyses of relatively long sedimentary sequences, and provides relevant sedimentary interpretations of transport and depositional processes. Examples of output from two lacustrine sediment records and one marine record are shown.
Development of A Hierarchical Saving Power System for Campus
Saving power is the major goal in every area either in industry or in campus. However, there are more uncertainties in campus. For instances, the usages of air conditioners in the classroom are irregular events. In the other hand, the lighting in the parking area is the regular event. For the regular event, we use the unstable green energy like wind or solar to save the electric cost. For the irregular event, we use the stable electricity but managed by dynamic power dispatch system to efficiently handle usages of air conditioners in the classroom in order to reduce cost. This dispatch system is the integration of Internet, network, coded radio frequency, database, interfacing technology, and campus administration computerization servers. By using the existing media and devices, the set-up of this system is without extra expensive cost. This hybrid strategy is simulated in a University with about 15000 students and more than 100 classrooms equipped with air conditioners. We can estimate to save more than 10% power than usual.
Architectural Adaptation Addressing the Criteria of Multiple Quality Attributes in Mission-Critical Systems
Mission-critical software claims safe and robust adaptations that comply with rigorous criteria of multiple critical quality attributes. Existing adaptation approaches pay little attention to comprehensively capture mission goals and explicitly specify adaptation requirements. We propose an approach to using scenario-based analysis to elicit and specify the criteria of multiple quality attributes as adaptation invariants, and design corresponding architecture variants as facilities implementing adaptations. We also present how to make adaptation decisions at runtime.
Simulation-based ATPG for low power testing of crosstalk delay faults in asynchronous circuits
A new multi-objective genetic algorithm has been proposed for testing crosstalk delay faults in asynchronous sequential circuits that reduces average power dissipation during test application. The proposed Elitist Non-dominated Sorting Genetic Algorithm ENGA-based Automatic Test Pattern Generation ATPG for crosstalk induced delay faults generates test pattern set that has high fault coverage and low switching activity. Redundancy is introduced in ENGA-based ATPG by modifying the fault dropping phase and hence a very good reduction in transition activity is achieved. Tests are generated for several asynchronous SIS benchmark circuits. Experimental results demonstrate that ENGA gives higher fault coverage, reduced transitions and compact test vectors for most of the asynchronous benchmark circuits when compared with those generated by Weighted Sum Genetic Algorithm WSGA.
Field Measurements of a Hybrid DVB-SH Single Frequency Network With an Inclined Satellite Orbit
Field measurements of a DVB-SH network with both satellite and terrestrial transmitters are presented. The system is a Single Frequency Network transmitting video streams to vehicular terminals with up to 4 branches of receiver antenna diversity. The signal is transmitted in the S-band at 2.1859 GHz with both time and frequency synchronization of the terrestrial repeaters with the satellite's signal. The time and frequency variations are cancelled out at the satellite gateway, but because these can not be canceled at all locations, and because the satellite's position in space is variable (in an inclined orbit) the terrestrial repeaters are made to cancel the residual time and frequency variation. The field measurements include data taken in late 2008 through 2009 with multiple repeaters located in several cities including Las Vegas, NV, Raleigh and Durham, NC, as well as various morphologies, and with elevation angles to the satellite ranging from 25 °  to 52 °  . This paper presents coverage data for these various morphologies, modulation and code rates, as well as various MPE-iFEC interleaver settings used to ameliorate the effects of long shadowed intervals such as when the vehicle goes under highways or bridges where there signal is obscured for several seconds. We observed excellent performance in hybrid mode with better than 99% of the measured seconds error free. In satellite-only mode, the MPE-iFEC interleaver raised the performance from 81% to 91% averaged over all environments, including dense urban.
ASSESSING THE FEASIBILITY OF SELF-ORGANIZING MAPS FOR DATA MINING FINANCIAL INFORMATION
Analyzing financial performance in today’s information-rich society can be a daunting task. With the evolution of the Internet, access to massive amounts of financial data, typically in the form of financial statements, is widespread. Managers and stakeholders are in need of a data-mining tool allowing them to quickly and accurately analyze this data. An emerging technique that may be suited for this application is the self-organizing map. The purpose of this study was to evaluate the performance of self-organizing maps for analyzing financial performance of international pulp and paper companies. For the study, financial data, in the form of seven financial ratios, was collected, using the Internet as the primary source of information. A total of 77 companies, and six regional averages, were included in the study. The time frame of the study was the period 1995-00. An example analysis was performed, and the results analyzed based on information contained in the annual reports. The results of the study indicate that self-organizing maps can be feasible tools for the financial analysis of large amounts of financial data.
An Energy-Aware Video Streaming System for Portable Computing Devices
In this demonstration, we show an energy-aware video streaming system which allows users to play back video for the specified duration within the remaining battery amount. In the system, we execute a proxy server on an intermediate node in the network. It receives the video stream from a content server, transcodes it to the videos with appropriate quality, and forwards it to a PDA or a laptop PC. Here, suitable parameter values of the video (such as picture size, frame rate and bitrate) which enable playback for the specified duration are automatically calculated on the proxy using our battery consumption model. The system also allows users to play back video segments with different qualities based on the importance specified to each video segment.
A TOOL KIT FOR LEXICON BUILDING
This paper describes a set of interactive routines that can be used to create, maintain, and update a computer lexicon. The routines are available to the user as a set of commands resembling a simple operating system. The lexicon produced by this system is based on lexical-semantic relations, but is compatible with a variety of other models of lexicon structure. The lexicon builder is suitable for the generation of moderate-sized vocabularies and has been used to construct a lexicon for a small medical expert system. A future version of the lexicon builder will create a much larger lexicon by parsing definitions from machine-readable dictionaries.
Exact phase transition of backtrack-free search with implications on the power of greedy algorithms
Backtracking is a basic strategy to solve constraint satisfaction problems (CSPs). A satisfiable CSP instance is backtrack-free if a solution can be found without encountering any dead-end during a backtracking search, implying that the instance is easy to solve. We prove an exact phase transition of backtrack-free search in some random CSPs, namely in Model RB and in Model RD. This is the first time an exact phase transition of backtrack-free search can be identified on some random CSPs. Our technical results also have interesting implications on the power of greedy algorithms, on the width of superlinear dense random hypergraphs and on the exact satisfiability threshold of random CSPs.
Autonomous climbing of spiral staircases with humanoids
In this paper, we present an approach to enable a humanoid robot to autonomously climb up spiral staircases. This task is substantially more challenging than climbing straight stairs since careful repositioning is needed. Our system globally estimates the pose of the robot, which is subsequently refined by integrating visual observations. In this way, the robot can accurately determine its relative position with respect to the next step. We use a 3D model of the environment to project edges corresponding to stair contours into monocular camera images. By detecting edges in the images and associating them to projected model edges, the robot is able to accurately locate itself towards the stairs and to climb them. We present experiments carried out with a Nao humanoid equipped with a 2D laser range finder for global localization and a low-cost monocular camera for short-range sensing. As we show in the experiments, the robot reliably climbs up the steps of a spiral staircase.
Lightweight map matching for indoor localisation using conditional random fields
Indoor tracking and navigation is a fundamental need for pervasive and context-aware smartphone applications. Although indoor maps are becoming increasingly available, there is no practical and reliable indoor map matching solution available at present. We present MapCraft, a novel, robust and responsive technique that is extremely computationally efficient (running in under 10 ms on an Android smartphone), does not require training in different sites, and tracks well even when presented with very noisy sensor data. Key to our approach is expressing the tracking problem as a conditional random field (CRF), a technique which has had great success in areas such as natural language processing, but has yet to be considered for indoor tracking. Unlike directed graphical models like Hidden Markov Models, CRFs capture arbitrary constraints that express how well observations support state transitions, given map constraints. Extensive experiments in multiple sites show how MapCraft outperforms state-of-the art approaches, demonstrating excellent tracking error and accurate reconstruction of tortuous trajectories with zero training effort. As proof of its robustness, we also demonstrate how it is able to accurately track the position of a user from accelerometer and magnetometer measurements only (i.e. gyro- and WiFi-free). We believe that such an energy-efficient approach will enable always-on background localisation, enabling a new era of location-aware applications to be developed.
Medial-Guided Fuzzy Segmentation
Segmentation is generally regarded as partitioning space at the boundary of an object so as to represent the object's shape, pose, size, and topology. Some images, however, contain so much noise that distinct boundaries are not forthcoming even after the object has been identified. We have used statistical methods based on medial features in Real Time 3D echocardiography to locate the left ventricular axis, even though the precise boundaries of the ventricle are simply not visible in the data. We then produce a fuzzy labeling of ventricular voxels to represent the shape of the ventricle without any explicit boundary. The fuzzy segmentation permits calculation of total ventricular volume as well as determination of local boundary equivalencies, both of which are validated against manual tracings on 155 left ventricles. The method uses a medial-based compartmentalization of the object that is generalizable to any shape.
Cryptographically secure identity certificates
We present FACECERTS, a simple, inexpensive, and cryptographically secure identity certification system. A FACECERT is a printout of person's portrait photo, an arbitrary textual message, and a 2D color bar-code which encodes an RSA signature of the message hash and the compressed representation of the face encompassed by the photo. The signature is created using the private key of the party issuing the ID. Verification is performed by a simple, intelligent, and off-line scanning device that contains the public key of the issuer. The system does not require smart cards. More interestingly, the ID does not need to be printed by a high-end printer, it can be printed anywhere. We present a novel algorithm for compressing faces and investigate the reliability of the crucial components of the system.
Use of communication technologies in South Korean universities
South Korean universities are currently at various stages of advancement in their use of electronic communication. Networking provision for universities is expected to improve considerably over the next few years. There is therefore a need to examine their present usage of communication technologies, how this depends on the level of facilities available and what the implications are for their future development. The study described here investigates usage at a stratified sample of South Korean universities via a questionnaire survey. It also examines the factors affecting use via a series of interviews. The results suggest that, though infrastructural limitations are important, optimal deployment of communication technologies will require organisational changes within the university system.
A 62.5 ns holographic reconfiguration of an optically differential reconfigurable gate array
To date, holographic configuration speeds have remained limited to 16 mus because of issues related to the architecture of optically reconfigurable gate array VLSIs (ORGA-VLSIs). Therefore, to improve the issue, optically differential reconfigurable gate array VLSIs (ODRGA-VLSIs) have been developed and have achieved zero-overhead and nanosecond optical reconfiguration. Moreover, the architecture of an ODRGA-VLSI has the advantage of accelerating the reconfiguration speed compared to that of other ORGAs. However, nanosecond holographic configurations and, in particular, rapid holographic reconfiguration exploiting the advantages of ODRGA-VLSIs have not been reported. Therefore, this paper presents results of the world's fastest 62.5 ns holographic reconfiguration, exploiting advantages of the ODRGA-VLSI.
Performance of coherent ASK lightwave systems with finite intermediate frequency
The impact of finite intermediate frequency (IF) on the performance of heterodyne ASK lightwave systems is examined and quantified in the presence of laser phase noise and shot noise. For negligible linewidths, it is shown that certain finite choices of IF (R/sub b/,3R/sub b//2,2R/sub b/,5R/sub b//2, etc.) lead to the same ideal bit-error-rate (BER) performance as infinite choices of IF. Results indicate that for negligible linewidths the worst case sensitivity penalty is 0.9 dB for proper heterodyne detection and occurs when f/sub IF/=1.25 R/sub b/. For nonnegligible linewidths (e.g., when /spl Delta//spl nu/T/spl ges/0.04) the sensitivity penalty is always less than 0.9 dB for finite choices of IF. The analysis presented does lead to a closed-form signal-to-noise ratio (SNR) expression at the decision gate of the receiver which can readily be used for BER and sensitivity penalty computations. The SNR expression provided includes all the key system parameters of interest such as system bit rate (R/sub b/), the peak IF SNR (/spl xi/), laser linewidth (/spl Delta//spl nu/), and the IF filter expansion factor (/spl alpha/). The findings of this work suggest that the number of channels in a multichannel heterodyne ASK lightwave system can be increased substantially by properly choosing a small value for the IF at the expense of a small penalty <1 dB. On the negative side, IF frequency stabilization becomes a more critical requirement in multichannel systems employing small values of IF.
Simulation and Measurement of Narrow-Band Antennas for Small Terminals
In this paper both normal-band and narrow-band PIFA antennas for small terminals are compared through numerical simulations and measurements for different UMTS bandwidths. It is found that using different antennas for the transmitting and receiving regions of the frequency duplex it is possible to achieve a significant improvement in terms of isolation. Measurement results show also that ohmic losses lower the total efficiency in the narrow-band case especially at low frequencies.
An extension to the ordered subcarrier selection algorithm (OSSA)
In this letter, we propose an extension to the ordered subcarrier selection algorithm (OSSA) for orthogonal frequency division multiplexing (OFDM) systems. The result is a simple algorithm for minimizing the bit error rate of the OFDM system at a fixed throughput. The proposed algorithm employs multiple modulations (non-uniform bit loading) within an OFDM symbol. However, unlike existing bit loading algorithms that have a very high computational complexity, the proposed algorithm is based only on the ordered statistics of the subcarrier gains and is consequently very simple. After ordering the subcarriers based on their gains, progressively higher order modulations are used with increasing gains. The key aspect here that greatly simplifies the algorithm is that the modulation used on a subcarrier depends only on the position of its gain in the ordered set and not on the actual values of the gains. We show an analytical approach for determining the parameters of the algorithm.
Classifiers for Motion
In this paper, we present a supervised learning based approach for sub-pixel motion estimation. The novelty of this work is the learning based method itself which tries to learn the shifts from a large training database. Integer pixel shift is sub-divided and discretized to levels in both the horizontal and vertical direction. We pose the problem of motion estimation in a polar coordinate system. Shift estimation in the x and y direction has been posed as a problem of estimating r and thetas. The ordinal property of r has been used, and consequently, we employ a ranking based approach for estimating r. For thetas estimation we employ multi-class classification techniques. We demonstrate how very simplistic features can be used to differentiate between different sub-pixel shifts
Performance analysis of a novel traffic scheduling algorithm in slotted optical networks
This paper considers the scheduling problem in a new slotted optical network called Time-Domain Wavelength Interleaved network (TWIN). The TWIN architecture possesses interesting properties, which may offer solutions for next-generation optical networks. Besides, better Quality of Service (QoS) could be achieved in TWIN by minimizing two parameters: queueing delay and delay variance. However, to the best of our knowledge, most of the existing scheduling algorithms in TWIN ignored consideration of QoS and focused mainly on maximizing the throughput. In this paper, we formulate the scheduling problem into an Integer Linear Programming (ILP) problem and propose a novel heuristic - Destination Slot Set (DSS) algorithm to solve it fast and efficiently. Besides, we derive an analytical model for TWIN and investigate the performance of DSS in it. By means of simulations, we demonstrate that our analytical model approximates the TWIN network very well; moreover, DSS incurs smaller queueing delay and delay variance, which ensures better QoS.
Motion filter vector quantization
Motion-compensated prediction of video is formulated as a novel vector quantization scheme called motion filter vector quantization (MFVQ). In MFVQ, the motion vector and the pixel-intensity interpolation filter are combined into a motion filter and the entire filter is vector quantized. A codebook design algorithm is proposed for designing unit gain and entropy constrained MFVQ codebooks. The algorithm is tested under two application configurations, MFVQ with static codebook and MFVQ with forward-adaptive codebook, and is shown to furnish up to a dB of PSNR gain.
Towards a real-time navigation strategy for a mobile robot
Describes the design of a real-time obstacle avoidance and navigation strategy for a mobile robot, using simulated time of flight infrared data. Algorithms have been developed in order to overcome the undesirable effect of potential traps within the field, and a new approach for dealing with sensing whilst moving is demonstrated. The authors show simulated results of a vehicle moving under artificial potential force equations, which is designed to achieve modification of behaviour at a speed close to the normal operational speed of a real mobile. >
Development of a telepresence controlled ambidextrous robot for space applications
A master-slave system is developed to evaluate the effectiveness of telepresence in space telerobotics applications. An operator uses the master's telepresence and virtual reality equipment to control the slave. The slave is a dual-arm, dual-hand robot equipped with a stereo camera platform designed to provide an operator-centered perspective of the remote environment. The initial integration and tests of the system presented several operational issues that are resolved through a variety of shared control techniques and optimized algorithms. The resulting system provides a very flexible capability and is used to perform a range of tasks: grasping and handling toots, manipulating electronics controls, manipulating soft flexible material, and performing planetary geology tasks that involve a variety of manipulation and tool-use skills. Using this system an operator is able to complete most of these tasks in less than 2 minutes.
Natural hand posture recognition based on Zernike moments and hierarchical classifier
View-independence and user-independence are two fundamental requirements for hand posture recognition during natural human-robot interaction. However only a few research concerns on the two issues simultaneously. The difficulty for natural gesture-based human-robot interaction lies in that appearances of the same hand posture vary with different users from different viewing directions. In this paper, we propose a systematic feature selection approach based on Zernike moments and Isomap dimensionality reduction. A hierarchical classifier based on multivariate decision tree and piecewise linearization is developed to deal with the irregular distribution of the same hand postures. The proposed method is compared with other commonly used ones in hand posture recognition. Experimental results indicate that the proposed method can effectively identify different hand postures, irrespective of viewing directions and users.
Unsupervised Feature Learning for Visual Sign Language Identification
Prior research on language identification focused primarily on text and speech. In this paper, we focus on the visual modality and present a method for identifying sign languages solely from short video samples. The method is trained on unlabelled video data (unsupervised feature learning) and using these features, it is trained to discriminate between six sign languages (supervised learning). We ran experiments on short video samples involving 30 signers (about 6 hours in total). Using leave-one-signer-out cross-validation, our evaluation shows an average best accuracy of 84%. Given that sign languages are underresourced, unsupervised feature learning techniques are the right tools and our results indicate that this is realistic for sign language identification.
Association Control in Mobile Wireless Networks
As mobile nodes roam in a wireless network, they continuously associate with different access points and perform handoff operations. However, frequent handoffs can potentially incur unacceptable delays and even interruptions for interactive applications. To alleviate these negative impacts, we present novel association control algorithms that can minimize the frequency of handoffs occurred to mobile devices. Specifically, we show that a greedy LookAhead algorithm is optimal in the offline setting, where the user's future mobility is known. Inspired by such optimality, we further propose two online algorithms, namely LookBack and Track, that operate without any future mobility information. Instead, they seek to predict the lifetime of an association using randomization and statistical approaches, respectively. We evaluate the performance of these algorithms using both analysis and trace-driven simulations. The results show that the simple LookBack algorithm has surprisingly a competitive ratio .of (log k + 2), where k is the maximum number of APs that a user can hear at any time, and the Track algorithm can achieve near-optimal performance in practical scenarios.
Debugging the Internet of Things: The Case of Wireless Sensor Networks
The Internet of Things (IoT) has the strong potential to support a human society interacting more symbiotically with its physical environment. Indeed, the emergence of tiny devices that sense environmental cues and trigger actuators after consulting logic and human preferences promises a more environmentally aware and less wasteful society. However, the IoT inherently challenges software development processes, particularly techniques for ensuring software reliability. Researchers have developed debugging tools for wireless sensor networks (WSNs), which can be viewed as the enablers of perception in the IoT. These tools gather run-time information on individual sensor node executions and node interactions and then compress that information.
Two empirical studies of computer-supported collaborative learning in science: methodological and affective implications
In this paper the results and implications of two studies of computer-supported collaborative learning are presented and implications discussed. The first study was an experimental study in a British secondary school, while the second study followed a group of primary school children in a naturalistic context. Assessing learning situations is discussed with an emphasis on the affective factors. The differences between the products, the interactions and the outcomes of learning situations are discussed along with the research methodology. There is an emphasis on pre- and post-testing, naturalistic and experimental studies and time-based analyses.
Acquisition of Project-Specific Assets with Bayesian Updating
We study the impact of learning on the optimal policy and the time-to-decision in an infinite-horizon Bayesian sequential decision model with two irreversible alternatives: exit and expansion. In our model, a firm undertakes a small-scale pilot project to learn, via Bayesian updating, about the project's profitability, which is known to be in one of two possible states. The firm continuously observes the project's cumulative profit, but the true state of the profitability is not immediately revealed because of the inherent noise in the profit stream. The firm bases its exit or expansion decision on the posterior probability distribution of the profitability. The optimal policy is characterized by a pair of thresholds for the posterior probability. We find that the time-to-decision does not necessarily have a monotonic relation with the arrival rate of new information.
A RULE REPOSITORY FOR ACTIVE DATABASE SYSTEMS
Active Database Systems (ADBSs) provides a good infrastructure to define and execute active rules. Nevertheless, this infrastructure offered by ADBSs does not completely satisfy the necessities of rules management that demands current business applications. Rules also need to be stored in appropriate structures to facilitate their management, as the existing structures for data in these systems. This work proposes a rule repository, composed by structures that allow the storage and organization of rules, in order to facilitate their management. For this purpose, a rule classification with the main rule types existing in the literature is presented, and then, it represents the characteristics and anatomy of each type in a meta-model, with the goal of analyzing the data that must be stored about rules. The rule repository, proposed in this paper, has been built based on this meta-model.
Decentralized group formation
Imagine a network of entities, being it replica servers aiming to minimize the probability of data loss, players of online team-based games and tournaments, or companies that look into co-branding opportunities. The objective of each entity in any of these scenarios is to find a few suitable partners to help them achieve a shared goal: replication of the data for fault tolerance, winning the game, successful marketing campaign. All information attainable by the entities is limited to the profiles of other entities that can be used to assess the pairwise fitness. How can they create teams without help of any centralized component and without going into each other’s way? We propose a decentralized algorithm that helps nodes in the network to form groups of a specific size. The protocol works by finding an approximation of a weighted k-clique matching of the underlying graph of assessments. We discuss the basic version of the protocol, and explain how dissemination via gossiping helps in improving its scalability. We evaluate its performance through extensive simulations.
Efficient Blind Compressed Sensing Using Sparsifying Transforms with Convergence Guarantees and Application to MRI
Natural signals and images are well-known to be approximately sparse in transform domains such as Wavelets and DCT. This property has been heavily exploited in various applications in image processing and medical imaging. Compressed sensing exploits the sparsity of images or image patches in a transform domain or synthesis dictionary to reconstruct images from undersampled measurements. In this work, we focus on blind compressed sensing, where the underlying sparsifying transform is a priori unknown, and propose a framework to simultaneously reconstruct the underlying image as well as the sparsifying transform from highly undersampled measurements. The proposed block coordinate descent type algorithms involve highly efficient optimal updates. Importantly, we prove that although the proposed blind compressed sensing formulations are highly nonconvex, our algorithms are globally convergent (i.e., they converge from any initialization) to the set of critical points of the objectives defining the formulations. These critical points are guaranteed to be at least partial global and partial local minimizers. The exact point(s) of convergence may depend on initialization. We illustrate the usefulness of the proposed framework for magnetic resonance image reconstruction from highly undersampled k-space measurements. As compared to previous methods involving the synthesis dictionary model, our approach is much faster, while also providing promising reconstruction quality.
Optimizing local pickup and delivery with uncertain loads
The local pickup and delivery problem (LPDP) is an essential operational problem in intermodal industry. While the problem with deterministic settings is already difficult to solve, in reality, there exist a set of loads, called uncertain loads, which are unknown at the beginning of the day. But customers may call in during the day to materialize these loads. In this paper, we call the LPDP considering these uncertain loads as the stochastic LPDP. The problem description and the mathematical modeling of stochastic LPDP are discussed. Then, a simulation-based optimization approach is proposed to solve the problem, which features in a fast solution generation procedure and an intelligent simulation budget allocation framework. The numerical examples show the best strategy to consider the stochastic loads in the planning process and validate the benefits compared to its deterministic counterpart.
Object-Based Coding for Plenoptic Videos
A new object-based coding system for a class of dynamic image-based representations called plenoptic videos (PVs) is proposed. PVs are simplified dynamic light fields, where the videos are taken at regularly spaced locations along line segments instead of a 2-D plane. In the proposed object-based approach, objects at different depth values are segmented to improve the rendering quality. By encoding PVs at the object level, desirable functionalities such as scalability of contents, error resilience, and interactivity with an individual image-based rendering (IBR) object can be achieved. Besides supporting the coding of texture and binary shape maps for IBR objects with arbitrary shapes, the proposed system also supports the coding of grayscale alpha maps as well as depth maps (geometry information) to respectively facilitate the matting and rendering of the IBR objects. Both temporal and spatial redundancies among the streams in the PV are exploited to improve the coding performance, while avoiding excessive complexity in selective decoding of PVs to support fast rendering speed. Advanced spatial/temporal prediction methods such as global disparity-compensated prediction, as well as direct prediction and its extensions are developed. The bit allocation and rate control scheme employing a new convex optimization-based approach are also introduced. Experimental results show that considerable improvements in coding performance are obtained for both synthetic and real scenes, while supporting the stated object-based functionalities.
Adaptive reconstruction method of missing textures based on inverse projection via sparse representation
This paper presents an adaptive reconstruction method of missing textures based on an inverse projection via sparse representation. The proposed method approximates original and corrupted textures in lower-dimensional subspaces by using the sparse representation technique. Then, this approach effectively solves problems of not being able to directly estimate an inverse projection for reconstructing missing textures. Furthermore, even if target textures contain missing areas, the proposed method enables adaptive generation of the subspaces by monitoring errors caused in their known neighboring textures by the estimated inverse projection. Consequently, since the optimal inverse projection is adaptively estimated for each texture, successful reconstruction of the missing areas can be expected. Experimental results show impressive improvement of the proposed reconstruction technique over previously reported reconstruction techniques.
Large margin non-linear embedding
It is common in classification methods to first place data in a vector space and then learn decision boundaries. We propose reversing that process: for fixed decision boundaries, we "learn" the location of the data. This way we (i) do not need a metric (or even stronger structure) - pairwise dissimilarities suffice; and additionally (ii) produce low-dimensional embeddings that can be analyzed visually. We achieve this by combining an entropy-based embedding method with an entropy-based version of semi-supervised logistic regression. We present results for clustering and semi-supervised classification.
Scientific Misconduct: Three Forms that Directly Harm Others as the Modus Operandi of Mill’s Tyranny of the Prevailing Opinion
Scientific misconduct is usually assumed to be self-serving. This paper, however, proposes to distinguish between two types of scientific misconduct: ‘type one scientific misconduct’ is self-serving and leads to falsely positive conclusions about one’s own work, while ‘type two scientific misconduct’ is other-harming and leads to falsely negative conclusions about someone else’s work. The focus is then on the latter type, and three known issues are identified as specific forms of such scientific misconduct: biased quality assessment, smear, and officially condoning scientific misconduct. These concern the improper ways how challenges of the prevailing opinion are thwarted in the modern world. The central issue is pseudoskepticism: uttering negative conclusions about someone else’s work that are downright false. It is argued that this may be an emotional response, rather than a calculated strategic action. Recommendations for educative and punitive measures are given to prevent and to deal with these three forms of scientific misconduct.
Yield Curve Shapes and the Asymptotic Short Rate Distribution in Affine One-Factor Models
We consider a model for interest rates where the short rate is given under the risk-neutral measure by a time-homogeneous one-dimensional affine process in the sense of Duffie, Filipovic, and Schachermayer. We show that in such a model yield curves can only be normal, inverse, or humped (i.e., endowed with a single local maximum). Each case can be characterized by simple conditions on the present short rate rt. We give conditions under which the short rate process converges to a limit distribution and describe the risk-neutral limit distribution in terms of its cumulant generating function. We apply our results to the Vasicek model, the CIR model, a CIR model with added jumps, and a model of Ornstein–Uhlenbeck type.
Multimedia descriptions based on MPEG-7: extraction and applications
The amount of digital multimedia content available to consumers is growing because of the existence of digital capturing devices such as digital cameras, camcorders and the advent of digital video broadcast. With this increase in content, it becomes important for users to be able to browse and search for content in a timely manner. Descriptions and annotations of the content are needed to enable searching and browsing of content. MPEG-7 is a recent ISO standard for multimedia content description. In this paper, we present three descriptors, which we had proposed to MPEG-7, and have been accepted to the standard. In addition, we describe algorithms for automatically extracting these descriptors. We also present the indexing and retrieval algorithms that we developed for these descriptors; these algorithms are fast and scalable for large databases. The results presented in the paper for image and video segment matching using these descriptors show their usefulness in real applications.
3D shape reconstruction using volume intersection techniques
Volume intersection algorithms are used to reconstruct incomplete objects from their silhouettes. An imagined light source is moved about the data and the cumulative amount of "light" seen at each point an space is interpreted as indicating the likelihood that the point is inside the object. The object data need not be uniformly distributed nor exclusively surface data. Explicit distinction between noise, surface and interior data is avoided. The novel concept of a localised viewing region is introduced to overcome the inherent inability of volume intersection algorithms to reconstruct concave surfaces. Algorithms for 2D pixel and 3D voxel data are described and applied to 3D ultrasound data.
Characterizing and Mitigating Inter-domain Policy Violations in Overlay Routes
The Internet is a complex structure arising from the interconnection of numerous autonomous systems (AS), each exercising its own administrative policies to reflect the commercial agreements behind the interconnection. However, routing in service overlay networks is quite capable of violating these policies to its advantage. To prevent these violations, we see an impending drive in the current Internet to detect and filter overlay traffic. In this paper, we first present results from a case study overlay network, constructed on top of Planetlab, that helps us gain insights into the frequency and characteristics of the different inter-domain policy violations. We further investigate the impact of two types of overlay traffic filtering that aim to prevent these routing policy violations: blind filtering and policy- aware filtering. We show that such filtering can be detrimental to the performance of overlay routing. We next consider two approaches that allow the overlay network to realize the full advantage of overlay routing in this context. In the first approach, overlay nodes are added so that good overlay paths do not represent inter-domain policy violations. In the second approach, the overlay acquires transit permits from certain ASes that allow certain policy violations to occur. We develop a single cost-sharing framework that allows the incorporation of both approaches into a single strategy. We formulate and solve an optimization problem that aims to determine how the overlay network should allocate a given budget between paying for additional overlay nodes and paying for transit permits to ASes. We illustrate the use of this approach on our case study overlay network and evaluate its performance under varying network characteristics.
A Sensor Fusion Framework Using Multiple Particle Filters for Video-Based Navigation
This paper presents a sensor-fusion framework for video-based navigation. Video-based navigation offers the advantages over existing approaches. With this type of navigation, road signs are directly superimposed onto the video of the road scene, as opposed to those superimposed onto a 2-D map, as is the case with conventional navigation systems. Drivers can then follow the virtual signs in the video to travel to the destination. The challenges of video-based navigation require the use of multiple sensors. The sensor-fusion framework that we propose has two major components: (1) a computer vision module for accurately detecting and tracking the road by using partition sampling and auxiliary variables and (2) a sensor-fusion module using multiple particle filters to integrate vision, Global Positioning Systems (GPSs), and Geographical Information Systems (GISs). GPS and GIS provide prior knowledge about the road for the vision module, and the vision module, in turn, corrects GPS errors.
An evolution programme for the resource-constrained project scheduling problem
This paper describes an implementation of an evolution programme for the resource-constrained project scheduling problem. In essentials, the problem consists of two issues; (a) to determine the order of activities without violating precedence constraints and (b) subsequently to determine earliest start time for each activity according to available resources. How to determine the order of activation is critical to the problem, because if the order is determined, a schedule can be easily constructed with some determining procedures. The basic ideas of the proposed approach are; (a) using an evolution programme to evolve an appropriate order of activities and (b) using a fit-in-best procedure to calculate the earliest start times of activities. A new approach is addressed to guide how to design genetic operators; one operator is designed to perform a wide spread search to try to explore the area beyond local optima, whereas the other is designed to perform an intensive search to try to find an improved solut...
A methodology for evaluating the accuracy of wave field rendering techniques
In this paper we propose a methodology for assessing the accuracy of techniques of wave field rendering through loudspeaker arrays. In order to measure the rendered wave field we adopt a solution based on a circular harmonic analysis of the sound field captured by a virtual microphone array. As a result of this analysis stage, we are able to compare the target, the theoretical and the measured wave fields, which may differ due to the non-ideality in the loudspeaker array or in the environment that generates some spurious reverberations. Moreover, in order to quantify the error between target, theoretical and measured wave fields, we define some evaluation metrics, based on RMSE and modal analysis of the acquired wave fields. We show some experimental results on real data.
Rectangular v-Splines
This article describes and presents examples of some techniques for the representation and interactive design of surfaces based on a parametric surface representation that user v-spline curves. These v-spline curves, similar in mathematical structure to v-splines, were developed as a more computationally efficient alternative to splines in tension. Although splines in tension can be modified to allow tension to be applied at each control point, the procedure is computationally expensive. The v-spline curve, however, uses more computationally tractable piecewise cubic curves segments, resulting in curves that are just as smoothly joined as those of a standard cubic spline. After presenting a review of v-splines and some new properties, this article extends their application to a rectangular grid of control points. Three techniques and some application examples are presented.
Downlink Joint Base-station Assignment and Packet Scheduling Algorithm for Cellular CDMA/TDMA Networks
In this paper using a utility-based approach, down-link packet transmission in a CDMA/TDMA cellular network is formulated as an optimization problem. A utility function corresponds to each packet served by a base-station that is an increasing function of the packet experienced delay and the channel gain, and a decreasing function of the base-station load. Unlike previous works, in this paper, the optimization objective is to maximize the total network utility instead of the base-station utility. We show that this optimization results in joint base-station assignment and packet scheduling. Therefore, in addition to multi-user diversity, the proposed method also exploits multi-access-point diversity and soft capacity. A polynomial time heuristic algorithm is then proposed to solve the optimization problem. Simulation results indicate a significant performance improvement in terms of packet-drop-ratio and achieved throughput.
Enhanced routing-aware adaptive MAC with traffic differentiation and smoothed contention window in wireless ad-hoc networks
In wireless mobile ad-hoc networks, data packets have to be relayed hop by hop from a given source node to a destination node. This means that some or all of the mobile nodes must accept to forward information for the benefit of other nodes. It has been shown by F. Nait-Abdesselam, et al., (2003) that this ability of forwarding packets leads to a new unfairness problem in wireless ad-hoc networks, where a node which is forwarding other node's packets gets less bandwidth, for its own use, than a node which is not participating to the routing service. The proposed RAMAC scheme by F. Nait-Abdesselam, et al., (2003), has shown all its effectiveness to cope with this unfairness problem. However, an extra bandwidth is sometimes allowed to the routing node's own traffic comparing to other nonrouting node's own traffics, and the routing node's routed traffic gets much less bandwidth. This is explained by the fact that at the upper layer (for instance the IP layer) does not differentiate between the routing and the own traffics, and that the multiplicative factor used to compute the new contention window is too aggressive. In this paper, an enhanced RAMAC scheme is proposed, by taking into account the differentiation on top of the MAC layer, between the and routed traffics within a routing node, and by smoothing the multiplicative factor used to compute the new contention window. The simulation results obtained showed a good improvement of the original RAMAC scheme, leading to better approximate equal bandwidth share among all the mobile nodes in the wireless ad-hoc network.
Meaning of Pearson Residuals Linear Algebra View
Marginal distributions play an central role in statistical analysis of a contingency table. However, when the number of partition becomes large, the contribution from marginal distributions decreases. This paper focuses on a formal analysis of marginal distributions in a contingency table. The main approach is to take the difference between two matrices with the same sample size and the same marginal distributions, which we call difference matrix. The important nature of the difference matrix is that the determinant is equal to 0: when the rank of a matrix is r, the difference between a original matrix and the expected matrix will become r - 1 at most. Since the sum of rows or columns of the will become zero, which means that the information of one rank corresponds to information on the frequency of a contingency matrix. Interestingly, if we take an expected matrix whose elements are the expected values based on marginal distributions, the difference between an original matrix and expected matrix can be represented by linear combination of determinants of 2 times 2 submatrices.
An experimental study of Quartets MaxCut and other supertree methods
Background#R##N#Supertree methods represent one of the major ways by which the Tree of Life can be estimated, but despite many recent algorithmic innovations, matrix representation with parsimony (MRP) remains the main algorithmic supertree method.
On-line knowledge-based simulation for FMS: a state of the art survey
Simulation modeling has been widely used to study many aspects of FMS design, planing and control. Yet, simulation modeling still offers other capabilities that are proven to be effective for the on-line control of FMS. An example of it is the training of neural nets off-line, so that it will later control the decision making process when the FMS is in operation. if the neural net runs out of knowledge, it turns back to a simulation model to learn new situations. In this paper, we review the various ways in which on-line knowledge-based simulation for FMS has been approached. This paper represents the early stages of on-going research efforts at Florida International University.
Study on the Management Models of Urban Multi-Ethnic Community in Northwestern Cities of China - The Case of Lanzhou
Urban multi-ethnic community is a special type of community which has a high degree of heterogeneity. Thus a study on the management of these communities will be of theoretical and practical significance. This paper took an urban multi-ethnic community of Lanzhou as example, investigating on the management of multi-ethnic communities in northwestern cities. Based on the theory of social relationship, using qualitative and case study methods, this paper has probed into the management models, causes, further development and other issues s in multi-ethnic communities in the Northwestern cities of China.
A Cross-Layer Adaptation for VoIP over Infrastructure Mesh Network
The deployment of wireless mesh paradigm was meant to extend Internet access without a consideration of delay sensitive applications. None the less, since voice over IP (VoIP) services are rapidly increasing in popularity, IEEE 802.11 based wireless mesh networks are challenged with the provision of guaranteed quality VoIP calls. In this paper, the disquiet on VoIP systems caused by physical (PHY) and medium access control (MAC) anomaly in the current wireless mesh deployment is addressed through a cross-layer scheme. The scheme is aimed at enhancing VoIP call capacity by mitigating PHY and MAC overheads through aggregation of packets of the same next hop. Through simulations, it is shown that the proposed scheme has significant performance improvements while leaving the IEEE 802.11 standard intact.
Combining variants of iterative flattening search
Iterative flattening search (ifs) is an iterative improvement heuristic schema for makespan minimization in scheduling problems. Given an initial solution, ifsiteratively interleaves a relaxation-step, which randomly retracts some search decisions, and an incremental solving step (or flattening-step) to recompute a new solution. The process continues until a stop condition is met and the best solution found is returned. In recent work we have created a uniform software framework to analyze component techniques that have been proposed in ifsapproaches. In this paper we combine basic components to obtain hybrid variants and perform a detailed experimental evaluation of their performance. Specifically, we examine the utility of: (1) operating with different relaxation strategies and (2) using different searching strategies to built a new solution. We present a two-step experimental evaluation: (a) an extensive explorative evaluation with a spectrum of parameter combination; (b) a time-intensive evaluation of the best ifscombinations emerged from the previous. The experimental results shed light on weaknesses and strengths of the different variants improving the current understanding of this family of meta-heuristics.
A Multielement Tactile Feedback System for Robot-Assisted Minimally Invasive Surgery
A multi-element tactile feedback (MTF) system has been developed to translate the force distribution, in magnitude and position, from 3times2 sensor arrays on surgical robotic end-effectors to the fingers via 3times2 balloon tactile displays. High detection accuracies from perceptual tests (> 96%) suggest that MTF may be an effective means to improve robotic control.
Can Brand Extension Signal Product Quality
This paper asks whether brand extension can serve as a signal of product quality given that it costs less than a new brand. (Existing literature has assumed either that brand extension is cost-neutral or that it costs more.) I show that it can as a perfect Bayesian equilibrium, but the argument is unconvincing. For one thing, the separating equilibrium is not unique; a pooling equilibrium also exists in which brand extension signals nothing. For another, the separating equilibrium relies on off-equilibrium beliefs that are poorly motivated in the model. I propose a refinement of the perfect Bayesian equilibrium that resolves both issues. Empirical off-equilibrium beliefs require that consumers' off-equilibrium beliefs be justifiable on the basis of their prior beliefs and product performance observations. With empirical off-equilibrium beliefs, two necessary conditions for brand extension to signal product quality are identified: (i) consumers must perceive old and new products of the firm to be positively correlated in quality, and (ii) at least some consumers must identify with brands and not the firm behind the brands. Even with these conditions in place, the signaling argument is fragile: firm observability of past performance diminishes brand extension's signaling capability; an arbitrarily small probability of failure for good products eliminates it. My results suggest that, going forward, the case for brand extension must rest on foundations other than signaling product quality.
Community detection using a neighborhood strength driven Label Propagation Algorithm
Studies of community structure and evolution in large social networks require a fast and accurate algorithm for community detection. As the size of analyzed communities grows, complexity of the community detection algorithm needs to be kept close to linear. The Label Propagation Algorithm (LPA) has the benefits of nearly-linear running time and easy implementation, thus it forms a good basis for efficient community detection methods. In this paper, we propose new update rule and label propagation criterion in LPA to improve both its computational efficiency and the quality of communities that it detects. The speed is optimized by avoiding unnecessary updates performed by the original algorithm. This change reduces significantly (by order of magnitude for large networks) the number of iterations that the algorithm executes. We also evaluate our generalization of the LPA update rule that takes into account, with varying strength, connections to the neighborhood of a node considering a new label. Experiments on computer generated networks and a wide range of social networks show that our new rule improves the quality of the detected communities compared to those found by the original LPA. The benefit of considering positive neighborhood strength is pronounced especially on real-world networks containing sufficiently large fraction of nodes with high clustering coefficient.
Semidefinite Programming Based Algorithms for the Sparsest Cut Problem
In this paper we analyze a known relaxation for the Spars- est Cut problem based on positive semidefinite constraints, and we present a branch and bound algorithm and heuristics based on this relaxation. The relaxed formulation and the algorithms were tested on small and moderate sized instances. It leads to values very close to the optimum solution values. The exact algorithm could obtain solutions for small and moderate sized instances, and the best heuristics obtained optimum or near optimum solutions for all tested instances. The semidefinite relaxation gives a lower bound C/W and each heuristic produces a cut S with a ratio c S /w S , where either c S  is at most a factor of C or w S  is at least a factor of W. We solved the semidefinite relaxation using a semi-infinite cut generation with a commercial linear programming package adapted to the sparsest cut problem. We showed that the proposed strategy leads to a better performance compared to the use of a known semidefinite programming solver.
Asynchronous comparison-based decoders for delay-insensitive codes
A comparison-based decoder detects the arrival of a code word by comparing the received checkbits with the checkbits computed using the received data. Implementation issues underlying comparison-based decoders for systematic delay-insensitive (DI) or unordered codes is the subject of this paper. We show that if the decoder is to be implemented using asynchronous logic, i.e., if the gate and wire delays are arbitrary (unbounded but finite), then it is impossible to design a comparison-based decoder for any code that is more efficient than a dual-rail code. In other words, the encoded word must contain at least twice as many bits as the data. In addition, the codes should satisfy two other properties, called the initial condition and the all-zero lower triangle (AZLT) property, for the realization of a delay-insensitive comparison-based decoder. The paper shows that comparison-based decoders for codes that have the requisite level of redundancy and that satisfy the two properties can be implemented using asynchronous logic.
Analysis of an adaptive SIC for near-far resistant DS-CDMA
A new multiuser detection scheme is proposed which employs adaptive minimum mean square error (MMSE) detection in combination with successive interference cancellation (SIC). Through theoretical analysis and numerical examples, it is shown that the proposed detector provides superior performance to existing ones in terms of asymptotic multiuser efficiency (AME) and bit error rate (BER).
Rex: A randomized EXclusive region based scheduling scheme for mmWave WPANs with directional antenna
Millimeter-wave (mmWave) transmissions are promising technologies for high data rate (multi-Gbps) Wireless Personal Area Networks (WPANs). In this paper, we first introduce the concept of exclusive region (ER) to allow concurrent transmissions to explore the spatial multiplexing gain of wireless networks. Considering the unique characteristics of mmWave communications and the use of omni-directional or directional antennae, we derive the ER conditions which ensure that concurrent transmissions can always outperform serial TDMA transmissions in a mmWave WPAN. We then propose REX, a randomized ER based scheduling scheme, to decide a set of senders that can transmit simultaneously. In addition, the expected number of flows that can be scheduled for concurrent transmissions is obtained analytically. Extensive simulations are conducted to validate the analysis and demonstrate the effectiveness and efficiency of the proposed REX scheduling scheme. The results should provide important guidelines for future deployment of mmWave based WPANs.
Year 5 pupils reading an interactive storybook on CD-ROM: losing the plot?
The use of “interactive storybooks” in the primary classroom may facilitate small group and individual reading with minimal teacher intervention. This small-scale study examines whether small groups of Year 5 pupils, without teacher supervision, progress linearly through an “interactive storybook” and whether such diversions as cued animations affect pupil comprehension. The study finds that more intensive choice of diversions affects some pupils' comprehension.
Using texture mapping with mipmapping to render a VLSI layout
This paper presents a method of using texture mapping with mipmapping to render a VLSI layout.  Texture mapping is used to save already rasterized areas of the layout from frame to frame, and to take advantage of any hardware accelerated capabilities of the host platform. Mipmapping is used to select which textures to display so that the amount of information sent to the display is bounded, and the image rendered on the display is filtered correctly.  Additionally, two caching schemes are employed.  The first, used to bound memory consumption, is a general purpose cache that holds textures spatially close to the user's current viewpoint. The second, used to speed up the rendering process, is a cache of heavily used sub-designs that are precomputed so rasterization on the fly is not necessary.  An experimental implementation shows that real-time navigation can be achieved on arbitrarily large designs. Results also show how this technique ensures that image quality does not degrade as the number of polygons drawn increases, avoiding the aliasing artifacts common in other layout systems.
Matrix inversion on CPU–GPU platforms with applications in control theory
In this paper we tackle the inversion of large-scale dense matrices via conventional matrix factorizations (LU, Cholesky, LDL T ) and the Gauss-Jordan method on hybrid platforms consisting of a multi-core CPU and a many-core graphics processor (GPU). Specifically, we introduce the different matrix inversion algorithms using a unified framework based on the notation from the FLAME project; we develop hybrid implementations for those matrix operations underlying the algorithms, alternative to those in existing libraries for singleGPU systems; and we perform an extensive experimental study on a platform equipped with state-of-the-art general-purpose architectures from Intel and a “Fermi” GPU from NVIDIA that exposes the efficiency of the different inversion approaches. Our study and experimental results show the simplicity and performance advantage of the GJE-based inversion methods, and the difficulties associated with the symmetric indefinite case.
Opportunistic Relaying in In-Home PLC Networks
We consider the use of a relay to provide capacity improvements and range extension for in-home power line communication networks. In particular, we focus on opportunistic relaying where the relay is exploited only if it provides improved capacity w.r.t. the use of direct transmission between the source and the destination. The relay applies a decode and forward scheme and the channel is shared in a time division multiple access mode. The performance is studied in statistically representative in-home power line communication (PLC) networks via the use of a statistical topology model together with the application of transmission line theory for the computation of the channel transfer function among network nodes. The statistical topology model allows determining the capacity improvements as a function of the relay position. Furthermore, we determine the optimal time slot duration for each considered relay configuration, as well as we propose the use of a globally optimal time slot duration that maximizes the average network capacity. The numerical results show that significant capacity improvement can be obtained via opportunistic relaying in in-home PLC networks. The gains are more significant for low SNR scenarios and for networks composed by sub-networks each connected to the main panel via a circuit breaker that introduces signal attenuation.
SPEECH OGLE: Indexing Uncertainty for Spoken Document Search
The paper presents the Position Specific Posterior Lattice (PSPL), a novel lossy representation of automatic speech recognition lattices that naturally lends itself to efficient indexing and subsequent relevance ranking of spoken documents.In experiments performed on a collection of lecture recordings --- MIT iCampus data --- the spoken document ranking accuracy was improved by 20% relative over the commonly used baseline of indexing the 1-best output from an automatic speech recognizer.The inverted index built from PSPL lattices is compact --- about 20% of the size of 3-gram ASR lattices and 3% of the size of the uncompressed speech --- and it allows for extremely fast retrieval. Furthermore, little degradation in performance is observed when pruning PSPL lattices, resulting in even smaller indexes --- 5% of the size of 3-gram ASR lattices.
Performance evaluation for different suggested wireless ATM systems deployed at a wireless channel with abrupt fading conditions
In this paper we suggest and compare the performance of different systems that can be used for wireless asynchronous transfer mode (WATM) to improve performance in terms of cell loss ratio and throughput under a certain fading wireless channel for a certain period of time. The type of fading considered in this paper is a result of a sudden and a sharp change in the signal level due to a sudden change in the weather condition (e.g. lightning) or due to a multipath effect caused by a mobile obstacle. It was found that compressing the ATM cell header and then applying the Bose-Chaudhuri-Hocquenghem BCH (i+18,i,3) code to the compressed header during the fading period, when the bit error rate ranges between 10/sup -3/ and 10/sup -1/, results in an optimum performance in terms of cell loss ratio and throughput.
i-Miner: a Web usage mining framework using hierarchical intelligent systems
Recently Web mining has become a hot research topic, which combines two of the prominent research areas comprising of data mining and the World Wide Web (WWW). Web usage mining attempts to discover useful knowledge from the secondary data obtained from the interactions of the users with the Web. Web usage mining has become very critical for effective Web site management, business and support services, personalization, network traffic flow analysis and so on. Our previous study on Web usage mining using a concurrent neuro-fuzzy approach has shown that the usage trend analysis very much depends on the performance of the clustering of the number of requests. In this paper, a novel approach 'intelligent-miner' (i-Miner) is introduced to optimize the concurrent architecture of a fuzzy clustering algorithm (to discover data clusters) and a fuzzy inference system to analyze the trends. In the concurrent neuro-fuzzy approach, self-organizing maps were used to cluster the Web user requests. A hybrid evolutionary FCM approach is proposed in this paper to optimally segregate similar user interests. The clustered data is then used to analyze the trends using a Takagi-Sugeno fuzzy inference system learned using a combination of evolutionary algorithm and neural network learning. Empirical results clearly shows that the proposed technique is efficient with lesser number of if-then rules and improved accuracy at the expense of complicated algorithms and extra computational cost.
Preceding car tracking using belief functions and a particle filter
This article presents a preceding car rear view tracking algorithm which utilizes a particle filter and belief function data fusion. Most of tracking applications resort to only one source of information, making the system dependent on the source reliability. To achieve more robust and longer tracking, multiple source data fusion is a solution. Belief functions are a powerful tool for data fusion. Using bridges between probability theory and belief function theory, data fusion information can be incorporated inside a particle filter. The efficiency of the proposed method is demonstrated on natural on-road sequences.
Performance Analysis for BICM Transmission over Gaussian Mixture Noise Fading Channels
Bit-interleaved coded modulation (BICM) has been adopted in many systems and standards for spectrally efficient coded transmission. The analytical evaluation of BICM performance parameters, in particular bit-error rate (BER), has received considerable attention in the recent past. In this paper, we derive BER approximations for BICM transmission over general fading channels impaired by Gaussian mixture noise (GMN). To this end, we build upon the saddlepoint approximation of the pairwise error probability (PEP) and a recently established approximation for the probability density function (PDF) of bit-wise reliability metrics for nonfading additive white Gaussian noise (AWGN) channels. We extend this PDF approximation to the case of GMN, and obtain closed-form expressions for its Laplace transform for fading GMN channels. The latter allows us to express the PEP and thus BER via the saddlepoint approximation. For the special case of fading AWGN channels the presented approximations are closed form, since the saddlepoint is well approximated by 1/2 for BICM decoding. Furthermore, we derive closed-form PEP expressions also for GMN channels in the high signal-to-noise ratio regime and establish the diversity and coding gain for BICM transmission over fading GMN channels. Selected numerical results for the BER of convolutional coded BICM highlight the usefulness of the proposed approximations and the differences between AWGN and GMN channels.
A Staggered FEC System for Seamless Handoff in Wireless LANs: Implementation Experience and Experimental Study
We report the implementation experience and experimental evaluation of a staggered adaptive forward error correction (FEC) system for video multicast over wireless LANs. In the system, the parity packets generated by a cross-packet FEC code are transmitted at a time delay from the original video packets, i.e. staggercasting video stream and FEC stream in different multicast groups. The delay provides temporal diversity to improve the robustness of video multicast, especially to enable the clients to correct burst packet loss using FEC and to achieve seamless handoff. A wireless client dynamically joins the FEC multicast groups based upon its channel conditions and handoff events. We have implemented the system including the streaming server and client proxy. A novel software architecture is designed to integrate the FEC functionality in the clients without requirement for changing the existing video player software. We conduct extensive experiments to investigate the impact of FEC overhead and the delay between the video stream and FEC stream to the video quality under different interference levels and mobile handoff durations. The efficacy of staggered adaptive FEC system on improving video multicast quality is demonstrated in real system implementation.
An adaptive feedforward compensation algorithm for active vibration control
Adaptive feedforward broadband vibration (or noise) compensation is currently used when an image of the disturbance is available. However in most of the systems there is a “positive” feedback coupling between the compensator system and the measurement of the image of the disturbances. The paper proposes a new algorithm taking in account this coupling effect and the corresponding analysis. The algorithm has been applied to an active vibration control (AVC) system and real time results are presented.
Explaining robust additive utility models by sequences of preference swaps
As decision-aiding tools become more popular everyday—but at the same time more sophisticated—it is of utmost importance to develop their explanatory capabilities. Some decisions require careful explanations, which can be challenging to provide when the underlying mathematical model is complex. This is the case when recommendations are based on incomplete expression of preferences, as the decision-aiding tool has to infer despite this scarcity of information. This step is key in the process but hardly intelligible for the user. The robust additive utility model is a necessary preference relation which makes minimal assumptions, at the price of handling a collection of compatible utility functions, virtually impossible to exhibit to the user. This strength for the model is a challenge for the explanation. In this paper, we come up with an explanation engine based on sequences of preference swaps, that is, pairwise comparison of alternatives. The intuition is to confront the decision maker with “elementary” comparisons, thus building incremental explanations. Elementary here means that alternatives compared may only differ on two criteria. Technically, our explanation engine exploits some properties of the necessary preference relation that we unveil in the paper. Equipped with this, we explore the issues of the existence and length of the resulting sequences. We show in particular that in the general case, no bound can be given on the length of explanations, but that in binary domains, the sequences remain short.
Extending the Applicability of Recommender Systems: A Multilayer Framework for Matching Human Resources
Recommender systems (RS) so far have been applied to many fields of e-commerce in order to assist users in finding the products that best meet their preferences. However, while the application of RS to the search for objects is well established, this is not the case for the search for subjects. This is astonishing as a growing number of people make personal and professional information digitally available to others by managing profiles in CV databases, social networking platforms and other online services. In order to address this new field of application for RS, we integrate own prior research into a unified multilayer framework supporting the matching of individuals for recruitment and team staffing processes. By this means we enhance RS research and make a next step towards the development of empirically and theoretically grounded decision support for the human resources function
Multicast for small conferences
This paper describes a concept to support scalable multicast communications for small audio/video conferencing groups on the Internet. The solution presented in this paper is based on extensions of IPv6 and the session description protocol (SDP). A goal of the concept called multicast for small conferences (MSC) is the smooth deployment in the Internet.
Transformational Construction of Correct Pointer Algorithms
This paper shows how to use the transformation of Paterson and Hewitt to improve the memory and operations used in a pointer algorithm. That transformation scheme normally is only of theoretical interest because of the inefficient performance of the transformed function. However we present a method how it can be used to decrease the amount of selective updates in memory while preserving the original runtime performance. This leads to a general transformation framework for the derivation of a class of pointer algorithms.
Network and content aware information management
The presented approach addresses the problem of query and persistent query (subscription) resolution, taking into consideration distribution across multi-domains, network infrastructure and content management. This approach is particularly suitable for information-centric and cloud computing applications based around a mobile-device infrastructure.
Outsourcing Decisions of Small and Medium-Sized Enterprises: A Multiple-Case Study Approach in the German Software Industry
Outsourcing of software development tasks has become a major issue for large software enterprises over the last decades. Nowadays, small and medium-sized enterprises (SMEs) follow this trend and outsource parts of their software development as well. However, most of the existing literature deals with large enterprises whereas the situation of SMEs is being neglected. Especially sourcing decisions and the organizational as well as operational setup may differ between large enterprises and SMEs. We choose an exploratory multiple-case study approach focusing on the German software market in order to shed light on these aspects of the sourcing behavior of SMEs. This paper addresses three complementing research questions. Besides the question of which phases of the software development process qualify for outsourcing, we explore the organizational setup of SMEs' outsourcing scenarios. In addition, we seek to find out which characteristics a software component has to fulfill in order to qualify for outsourcing.
Efficient incremental analysis of on-chip power grid via sparse approximation
In this paper, a new sparse approximation technique is proposed for incremental power grid analysis. Our proposed method is motivated by the observation that when a power grid network is locally updated during circuit design, its response changes locally and, hence, the incremental "change" of the power grid voltage is almost zero at many internal nodes, resulting in a unique sparse pattern. An efficient Orthogonal Matching Pursuit (OMP) algorithm is adopted to solve the proposed sparse approximation problem. In addition, several numerical techniques are proposed to improve the numerical stability of the proposed solver, while simultaneously maintaining its high efficiency. Several industrial circuit examples demonstrate that when applied to incremental power grid analysis, our proposed approach achieves up to 130× runtime speed-up over the traditional Algebraic Multi-Grid (AMG) method, without surrendering any accuracy.
LCARS: the next generation programming context
In this paper, we present a high-level graphical language to develop pervasive applications based on a unique interface design. The language supports a wide range of programming constructs. Its graphical notation is based on the LCARS design, which is appealing to different target groups, based on their specific interests and requirements. We show that users can easily create pervasive applications using an LCARS-based user interface. The first step is to describe the technical context in which the application will execute. Based on this technical context, the UI offers a context-specific set of visual primitives. By composing these visual primitives on the screen, the user can specify the behavior of the application.
Hierarchical Part-Template Matching for Human Detection and Segmentation
Local part-based human detectors are capable of handling partial occlusions efficiently and modeling shape articulations flexibly, while global shape template-based human detectors are capable of detecting and segmenting human shapes simultaneously. We describe a Bayesian approach to human detection and segmentation combining local part-based and global template-based schemes. The approach relies on the key ideas of matching a part-template tree to images hierarchically to generate a reliable set of detection hypotheses and optimizing it under a Bayesian MAP framework through global likelihood re-evaluation and fine occlusion analysis. In addition to detection, our approach is able to obtain human shapes and poses simultaneously. We applied the approach to human detection and segmentation in crowded scenes with and without background subtraction. Experimental results show that our approach achieves good performance on images and video sequences with severe occlusion.
Using Google Drive to facilitate a blended approach to authentic learning
Abstract#R##N##R##N#While technology has the potential to create opportunities for transformative learning in higher education, it is often used to merely reinforce didactic teaching that aims to control access to expert knowledge. Instead, educators should consider using technology to enhance communication and provide richer, more meaningful platforms for the social construction of knowledge. By using technology to engage in shared learning experiences that extend beyond the walls of the classroom, we can create opportunities to develop the patterns of thinking that students need to participate in complex, real world situations. We used authentic learning as a framework to guide the implementation of a case-based, blended module in a South African physiotherapy department. Google Drive was used as a collaborative online authoring environment in which small groups of students used clinical cases to create their own content, guided by a team of facilitators. This paper describes an innovative approach to clinical education using authentic learning as a guiding framework, and Google Drive as an implementation platform. We believe that this approach led to the transformation of student learning practices, altered power relationships in the classroom and facilitated the development of critical attitudes towards knowledge and authority.
Quantitative Comparison of the Error-Containment Capabilities of a Bus and a Star Topology in CAN Networks
There has been an increasing interest in using star topologies in field-bus communications, e.g., in Time Triggered Protocol for SAE classC applications (TTP/C), FlexRay, or controller area network (CAN), due to increased fault resilience and potential error-containment advantages. In this context, an innovative CAN-compliant star topology, CANcentrate, has been developed, whose hub includes enhanced fault-treatment mechanisms. However, despite this interest toward stars, it is still necessary to quantify their real dependability benefits. For this purpose and for the particular case of CAN, this paper presents models for the dependability features of CAN and CANcentrate using Stochastic Activity Networks (SANs). It quantitatively compares their reliability and error-containment capabilities under permanent hardware faults. These models rely on assumptions that ensure that results are not biased toward CANcentrate, which, in some cases, is too detrimental for it. Thus, despite not reflecting the full CANcentrate potential, results quantitatively confirm the improvement of error-containment it achieves over CAN. Additionally, the way in which the nodes' ability to contain their own errors affects the relevance of using a star topology has been quantified. Although this paper refers to the case of CAN, conclusions regarding the justification of using a star topology depending on this ability can be extrapolated to other field-bus technologies.
Analysis and Design of Single-Stage AC/DC $LLC$ Resonant Converter
Analysis and design of a single-stage  LLC  resonant converter are proposed. A single-stage converter uses only one control signal to drive two power converters, a power factor corrector (PFC) converter and a dc/dc converter, for reducing the cost of the system. However, this simplicity induces power imbalance between two converters, and then, the bus voltage between two converters drifts and becomes unpredictable. To ensure that the bus capacitor voltage can be kept in a tolerable region, the characteristics of a PFC converter and an  LLC  tank are investigated, and then, a design procedure is proposed correspondingly. Finally, a single-stage  LLC  resonant converter is implemented to verify the analysis.
Agent-Based Petroleum Offshore Monitoring Using Sensor Networks
This paper investigates the architecture and design of agent-based sensor networks for petroleum offshore monitoring. A few challenges to monitor the reservoir, wellbore and wellhead are identified. Moreover, the necessary components for a reliable, precise, and accurate monitoring are suggested. The paper describes the architecture of the routing agent and discusses the cross layer optimization issues for query processing. The paper also provides the software design and components for a web-based continuous monitoring application.
The Study of 3D Reconstruction Method Based on Dynamic Threshold Method and Improved Ray Casting Algorithm
This article carries on a new method which can improve quality and speed of 3D visualization. This method, first based on dynamic threshold method, divides region of interest ROI from the original image; then extends bounding box algorithm to 3D space, and combines it with ray casting algorithm. We validate the validity and robustness of this method in 3D reconstruction, experimenting with 3D lung parenchyma's image, vascular image and bones' image, which used to be 2D medical images about chest CT.
CAE-L: An Ontology Modelling Cultural Behaviour in Adaptive Education
The presentation of learning materials in Adaptive Education Hypermedia is influenced by several factors such as learning style, background knowledge and cultural background, to name a few. In this paper, we introduce the notion of the CAE-L Ontology for modelling stereotype cultural artefacts in adaptive education. The Ontology design is based on the user study gathered from the respondents to the CAE questionnaire which determines the cultural artefacts that influence a learner?s behaviour within an educational environment. We present a brief overview of the implementation and discuss the stereotype presentation styles from three different countries, namely China, Ireland and UK.
Solar powered unmanned aerial vehicle for continuous flight: Conceptual overview and optimization
An aircraft that is capable of continuous flight offers a new level of autonomous capacity for unmanned aerial vehicles. We present an overview of the components and concepts of a small scale unmanned aircraft that is capable of sustaining powered flight without a theoretical time limit. We then propose metrics that quantify the robustness of continuous flight achieved and optimization criteria to maximize these metrics. Finally, the criteria are applied to a fabricated and flight tested small scale high efficiency aircraft prototype to determine the optimal battery and photovoltaic array mass for robust continuous flight.
Measuring Oscillating Walking Paths with a LIDAR
This work describes the analysis of different walking paths registered using a Light Detection And Ranging (LIDAR) laser range sensor in order to measure oscillating trajectories during unsupervised walking. The estimate of the gait and trajectory parameters were obtained with a terrestrial LIDAR placed 100 mm above the ground with the scanning plane parallel to the floor to measure the trajectory of the legs without attaching any markers or modifying the floor. Three different large walking experiments were performed to test the proposed measurement system with straight and oscillating trajectories. The main advantages of the proposed system are the possibility to measure several steps and obtain average gait parameters and the minimum infrastructure required. This measurement system enables the development of new ambulatory applications based on the analysis of the gait and the trajectory during a walk.
Identification of Normalised Coprime Plant Factors from Closed-loop Experimental Data
Recently introduced methods of iterative identification and control design are directed towards the design of high performing and robust control systems. These methods show the necessity of identifying approximate models from closed loop plant experiments. In this paper a method is proposed to approximately identify normalized coprime plant factors from closed loop data. The fact that normalized plant factors are estimated gives specific advantages both from an identification and from a robust control design point of view. It will be shown that the proposed method leads to identified models that are specifically accurate around the bandwidth of the closed loop system. The identification procedure fits very naturally into a recently developed the iterative identification/control design scheme based on H∞ robustness optimization.
Current Situation of Digitalized Ship Navigation System for Safety
Automatic Identification System (AIS) has to be on board in every ship over 500 gross tonnages (GT) in Japan from July 1, 2008. This is a response of amendment of maritime relating laws in accordance with SOLAS (International Convention for the Safety of Life at Sea) Convention and IMO (International Maritime Organization) requires all ships over 500 GT or upward are fitted with ship borne AIS. AIS is a system which makes it possible to get precise on-line information from a large area about ships and their movements. AIS is based on a ship borne radio wave of VHF, it continuously and automatically transmits fixed, dynamic and voyage-related information and receives corresponding information from other ships near by. VTS (Vessel Transport Service) is provided by collection of AIS information from ships. Most of the VTS stations around the area of Gulf of Finland have equipped with coastal surveillance system with radar for early warnings to endangering ships. Combinations of AIS and VTS, in addition, practical lane-separation system have also been activated under international agreement of Russia, Estonia and Finland. These three systems are well combined and organised then significant effects of reducing risks of ship borne accidents have been observed. Practices of Gulf of Finland should be introduced other congested sea lanes.
Inference of gene predictor set using Boolean satisfiability
The inference of gene predictors in the gene regulatory network (GRN) has become an important research area in the genomics and medical disciplines. Accurate predicators are necessary for constructing the GRN model and to enable targeted biological experiments that attempt to validate or control the regulation process. In this paper, we implement a SAT-based algorithm to determine the gene predictor set from steady state gene expression data (attractor states). Using the attractor states as input, the states are ordered into attractor cycles. For each attractor cycle ordering, all possible predictors are enumerated and a conjunctive normal form (CNF) expression is generated which encodes these predictors and their biological constraints. Each CNF is solved using a SAT solver to find candidate predictor sets. Statistical analysis of the resulting predictor sets selects the most likely predictor set of the GRN, corresponding to the attractor data. We demonstrate our algorithm on attractor state data from a melanoma study [1] and present our predictor set results.
Integration of Chemical and Visual Sensors for Identifying an Odor Source in Near Shore Ocean Conditions
This paper develops new multiple sensor-based algorithms for identifying a chemical odor source in a near-shore and ocean environment via an autonomous underwater vehicle (AUV). Those algorithms implement two modules: source declaration and source verification, which are embedded in a subsumption architecture for chemical plume tracing (CPT). The source declaration module is based on chemical events detected by a chemical sensor, in combination with measured vehicle locations and fluid flow directions, while the source verification module uses a fuzzy color segmentation algorithm to process an image taken when the odor source is declared.
HIVSetSubtype: software for subtype classification of HIV-1 sequences
An automated web based tool for assigning HIV-1 pure and recombinant subtypes within unaligned sequences is presented. The system combines the BLAST search algorithm and the recombination identification program for genetic subtyping of HIV-1. The software was validated through combined analysis of simulated and other HIV-1 real data.
Robust intensity-based 3D-2D registration of ct and X-ray images for precise estimation of cup alignment after total hip arthroplasty
The widely used procedure of evaluation of cup orientation following THA using single standard anteroposterior radiograph is known inaccurate, largely due to the wide variability in individual pelvic position relative to X-ray plate. 3D-2D image registration methods have been introduced to estimate the transformation between a CT volume and the radiograph for an accurate estimation of the cup orientation relative to an anatomical reference extracted from the CT data. However, the robustness of these methods is questionable. This paper presents a robust image similarity measure which is derived from a variational approximation to mutual information and allows for incorporation of spatial information via energy minimization. Experimental results on estimating cup alignment from single X-ray radiograph with gonadal shielding demonstrate the robustness and the accuracy of the present approach.
Image Denoising with Shrinkage and Redundant Representations
Shrinkage is a well known and appealing denoising technique. The use of shrinkage is known to be optimal for Gaussian white noise, provided that the sparsity on the signals representation is enforced using a unitary transform. Still, shrinkage is also practiced successfully with nonunitary, and even redundant representations. In this paper we shed some light on this behavior. We show that simple shrinkage could be interpreted as the first iteration of an algorithm that solves the basis pursuit denoising (BPDN) problem. Thus, this work leads to a novel iterative shrinkage algorithm that can be considered as an effective pursuit method. We demonstrate this algorithm, both on synthetic data, and for the image denoising problem, where we learn the image prior parameters directly from the given image. The results in both cases are superior to several popular alternatives.
Global tracking for robot manipulators using a simple causal pd controller plus feedforward
This paper shows that a well-known causal PD controller plus feedforward solves the global output feedback tracking control problem of robot manipulators, by requiring only the existence of the robot natural damping, no matter how small. To this end, we first demonstrate that a robot controlled by a causal PD is globally input-to-state stable (ISS) with respect to a bounded input disturbance. Then, we prove that the addition of a feedforward compensation renders the error system uniformly globally asymptotically stable. Furthermore, we present a possible extension to more general nonlinear systems and also to uncertain systems.
Prototype-oriented development of high-performance systems
We discuss the problem of developing performance-oriented software and the need for methodologies. We then present the EDPEPPS (Environment for Design and Performance Evaluation of Portable Parallel Software) approach to the problem of designing and evaluating high-performance (parallel) applications. The EDPEPPS toolset is based on a rapid prototyping philosophy, where the designer synthesises a model of the intended software which may be simulated, and the performance is subsequently analysed using visualisation. The toolset combines a graphical design tool, a simulation facility, and a visualisation tool. The same design is used to produce a code suitable for simulation and real execution.
A Strategy for Vision-Based Controlled Pushing of Microparticles
In this paper, a strategy for controlled pushing is presented for microassembly of 4.5 mum polystyrene particles on a flat glass substrate using an atomic force microscope probe tip. Real-time vision based feedback from a CCD camera mounted to a high resolution optical microscope is used to track particle positions relative to the tip and target position. Tip-particle system is modeled in 2D as a nonholonomic differential drive robot. Effectiveness of the controller is demonstrated through experiments performed using a single goal position as well as linking a series of target positions to form a single complex trajectory. Cell decomposition and wavefront expansion algorithms are implemented to autonomously locate a navigable path to a specified target position. Control strategy alleviates problem of slipping and spinning during pushing.
Crowd character complexity on Big Hero 6
On Disney's  Big Hero 6 , we needed to create the city of  San Fransokyo  with unparalleled levels of visual complexity. The cityscape has more buildings and more geometry than any prior Disney film. Inhabiting this city are hundreds of unique characters, each performing a high caliber of animation individually and as a group. These challenges prompted a major upgrade to our existing crowd pipeline and the development of several new technologies in authoring crowd characters, generating crowd animation cycles, and instancing crowds for rendering.
Evaluating real-time Java features and performance for real-time embedded systems
This paper provides two contributions to the study of programming languages and middleware for real-time and embedded applications. First, we present the empirical results from applying the RTJPerf benchmarking suite to evaluate the efficiency and predictability of several implementations of the real-time specification for Java (RTSJ). Second, we describe some of the techniques used to develop jRate, which is an open-source ahead-of-time-compiled implementation of RTSJ we are developing. Our results indicate that RTSJ implementations are maturing to the point where they can be applied to a variety of real-time embedded applications.
Biologically inspired design principles for Scalable, Robust, Adaptive, Decentralized search and automated response (RADAR)
Distributed search problems are ubiquitous in Artificial Life (ALife). Many distributed search problems require identifying a rare and previously unseen event and producing a rapid response. This challenge amounts to finding and removing an unknown needle in a very large haystack. Traditional computational search models are unlikely to find, nonetheless, appropriately respond to, novel events, particularly given data distributed across multiple platforms in a variety of formats and sources with variable and unknown reliability. Biological systems have evolved solutions to distributed search and response under uncertainty. Immune systems and ant colonies efficiently scale up massively parallel search with automated response in highly dynamic environments, and both do so using distributed coordination without centralized control. These properties are relevant to ALife, where distributed, autonomous, robust and adaptive control is needed to design robot swarms, mobile computing networks, computer security systems and other distributed intelligent systems. They are also relevant for searching, tracking the spread of ideas, and understanding the impact of innovations in online social networks. We review design principles for Scalable Robust, Adaptive, Decentralized search with Automated Response (Scalable RADAR) in biology. We discuss how biological RADAR scales up efficiently, and then discuss in detail how modular search in the immune system can be mimicked or built upon in ALife. Such search mechanisms are particularly useful when components have limited capacity to communicate and when physical distance makes communication more costly.
Combining Grid and Cloud Resources by Use of Middleware for SPMD Applications
Distributed computing environments have evolved from in-house clusters to Grids and now Cloud platforms. We, as others, provide HPC benchmarks results over Amazon EC2 that show a lower performance of Cloud resources compared to private resources., So, it is not yet clear how much of impact Clouds will have in high performance computing (HPC). But hybrid Grid/Cloud computing may offer opportunities to increase overall applications performance, while benefiting from in-house computational resources extending them by Cloud ones only whenever needed. In this paper, we advocate the usage of Proactive, a well established middleware in the grid community, for mixed Grid/Cloud computing, extended with features to address Grid/Cloud issues with little or, no effort for application developers. We also introduce a framework, developed in the context of the Disco Grid project, based upon the Proactive middleware to couple HPC domain-decomposition SPMD applications in heterogeneous multi-domain environments. Performance results, coupling Grid and Cloud resources for the execution of such, kind of highly communicating and processing intensive applications, have shown an overhead of about 15%, which is a non-negligible value, but lower enough to consider using such environments to achieve a better cost-performance trade-off than using exclusively Cloud resources.
The evolution of a hierarchical partitioning algorithm for large-scale scientific data: three steps of increasing complexity
As scientific data sets grow exponentially in size, the need for scalable algorithms that heuristically partition the data increases. In this paper, we describe the three-step evolution of a hierarchical partitioning algorithm for large-scale spatio-temporal scientific data sets generated by massive simulations. The first version of our algorithm uses a simple top-down partitioning technique, which divides the data by using a four-way bisection of the spatio-temporal space. The shortcomings of this algorithm lead to the second version of our partitioning algorithm, which uses a bottom-up approach. In this version, a partition hierarchy is constructed by systematically agglomerating the underlying Cartesian grid that is placed on the data. Finally, the third version of our algorithm utilizes the intrinsic topology of the data given in the original scientific problem to build the partition hierarchy in a bottom-up fashion. Specifically, the topology is used to heuristically agglomerate the data at each level of the partition hierarchy. Despite the growing complexity in our algorithms, the third version of our algorithm builds partition hierarchies in less time and is able to build trees for larger size data sets as compared to the previous two versions.
Determinantal equations for secant varieties and the Eisenbud-Koh-Stillman conjecture
We address special cases of a question of Eisenbud on the ideals of secant varieties of Veronese re-embeddings of arbitrary varieties. Eisenbud's question generalizes a conjecture of Eisenbud, Koh and Stillman (EKS) for curves. We prove that set-theoretic equations of small secant varieties to a high degree Veronese re-embedding of a smooth variety are determined by equations of the ambient Veronese variety and linear equations. However this is false for singular varieties, and we give explicit counter-examples to the EKS conjecture for singular curves. The techniques we use also allow us to prove a gap and uniqueness theorem for symmetric tensor rank. We put Eisenbud's question in a more general context about the behaviour of border rank under specialisation to a linear subspace, and provide an overview of conjectures coming from signal processing and complexity theory in this context.
An Approach to Supply Simulations of the Functional Environment of ECUs for Hardware-in-the-Loop Test Systems Based on EE-architectures Conform to AUTOSAR
Today’s vehicles include a complex network of programmableelectronic control units with software components. Avehicle’s electric and electronic (EE) architecture has to bemodeled in an early design phase to evaluate design alternatives.The tool PREEvision offers possibilities to modelEE-architectures considering feature function networks,function networks, component networks as well as wiringharness and the respective mappings.The software architecture specified by AUTOSAR separateshardware dependent and hardware independent softwaremodules. This allows the mapping of hardware independentsoftware applications to different hardware platforms.Hardware-in-the-Loop (HiL) is an established technologyfor testing electronic control units (ECU) and to assurequality. HiL-test-systems (HiL-TS) simulate the ECU’sfunctional environment (car, driver, road, tires, etc.) andadditionally offers the possibility to insert logical faults aswell as electrical faults (short circuit, open load, etc.).Mostly, this HiL-simulation is individually engineered forevery single ECU.This paper introduces a concept for the automated supportof such simulations. This includes the derivation of relevantinformation from the model of the EE-architecture as wellas the portation of the AUTOSAR software architecture tothe HiL-TS. Following this concept, engineering costs canbe reduced and the quality and correctness of the simulationincreased.
Constriction of Mutual Information Based Matching-Suitable Features for SAR Image Aided Navigation
Mutual information based matching-suitable features are studied in this paper, for the selection of good matching areas with high success probability in SAR matching aided navigation system. Based on the analysis of SAR imaging and multi-source matching, four feature constructing guide lines are proposed first. Then ten candidate features are designed under mutual information measurement. Effectiveness of these features is tested and compared through experiments under real SAR data. And features, such as reference image complexity and absolute value roughness et.al., which show stable monotony and good convergence, can be use as good matching-suitable features in practices.
Blockwise zero mapping image coding
A coding algorithm of low addressing and implementation complexity is proposed. It is based on partitioning uniformly quantized wavelet coefficients into multiscale blocks with each block classified as either an all-zero block or a non-zero block. The non-zero blocks are coded by a new method called zero-mapping whose outputs are further compressed by a first-order arithmetic coder. The performance of the proposed coding algorithm compares favorably with that of some well-known coding algorithms, particularly for images with considerable high frequency components.
Demo: Distributed video coding applications in wireless multimedia sensor networks
Novel distributed video coding (DVC) architectures developed by the IBBT DVC group realize state-of-the-art video coding efficiency under stringent energy restrictions, while supporting error-resilience and scalability. Therefore, these architectures are particularly attractive for application scenarios involving low-complexity energy-constrained wireless visual sensors. This demo presents the scenarios, which are considered to be the most promising areas of integration for IBBT's DVC systems, considering feasibility and commercial applicability.
HPC-Europa: towards uniform access to European HPC infrastructures
One of the goals of the HPC-Europa project is to provide users with a Single Point of Access (SPA) to the resources of HPC centers in Europe. To this end, the HPC-Europa Portal is being built to provide transparent, uniform, flexible and intuitive user access to HPC-Europa resources. In this paper, we present a mechanism that enables end-users to transparently access the diverse services available in the HPC-Europa environment. The uniform job submission interface that uses this mechanism, utilizing the job specification description language (JSDL), is described. We also present the architecture of the SPA, based on the GridSphere portal framework. Finally, we discuss the various interoperability problems encountered, in particular those concerning job submission, security and accounting.
Modeling the faulty behaviour of digital designs using a feed forward neural network approach
Cosmic rays lead to soft errors and faulty behavior in electronic circuits. Knowing about their faulty behavior before fabrication would be helpful. This research proposes an approach for modeling the faulty behaviour of digital circuits. It could be applied in a design flow before circuit fabrication. This is achieved by extracting information about faulty behaviour of circuits from low-level models expressed in the VHDL language. Afterwards the extracted information is used to train high-level artificial neural networks models expressed in C/C++ or MATLAB TM  languages. The trained neural network models are able to replicate the behaviour of circuits in presence of faults. The methodology is based on experiments done with two benchmarks, the ISCAS-C17 and a 4-bit multiplier. Results show that the neural network approach leads to models that are more accurate than a previously reported signature generation method. For the C17, using only 30% of the dataset generated with the LIFTING fault simulator, the neural network is able to replicate the output of the circuit in presence of faults with a mean absolute modeling error below 6%.
Optimal/Near-Optimal Dimensionality Reduction for Distributed Estimation in Homogeneous and Certain Inhomogeneous Scenarios
We consider distributed estimation of a deterministic vector parameter from noisy sensor observations in a wireless sensor network (WSN). The observation noise is assumed uncorrelated across sensors. To meet stringent power and bandwidth budgets inherent in WSNs, local data dimensionality reduction is performed at each sensor to reduce the number of messages sent to a fusion center (FC). The problem of interest is to jointly design the compression matrices associated with those sensors, aiming at minimizing the estimation error at the FC. Such a dimensionality reduction problem is investigated in this paper. Specifically, we study a homogeneous environment where all sensors have identical noise covariance matrices and an inhomogeneous environment where the noise covariance matrices across the sensors have the same correlation structure but with different scaling factors. Given a total number of messages sent to the FC, theoretical lower bounds on the estimation error of any compression strategy are derived for both cases. Compression strategies are developed to approach or even attain the corresponding theoretical lower bounds. Performance analysis and simulations are carried out to illustrate the optimality and effectiveness of the proposed compression strategies.
Incentive Compatible Configuration for Wireless Multicast: A Game Theoretic Approach
Multicast and broadcast service (MBS) is a service offered by the base station (BS) to multiple receivers requesting the same information. Such a BS must be kept updated with the receivers' feedback information (e.g., packet loss rates) to configure the MBS. We propose MBS operation schemes that are dominant-strategy incentive compatible in game theory, i.e., the schemes induce dominant-strategy equilibria, where all selfish receivers reveal their true information. Moreover, the induced equilibria are Pareto efficient and max-min fair. To conclude, the proposed schemes can elicit true feedback information from the receivers, avoiding any manipulation and, thereby, ensuring an efficient and fair system operation.
Quantization, channel compensation, and energy allocation for estimation in wireless sensor networks
In clustered networks of wireless sensor motes, each mote collects noisy observations of the environment, quantizes these observations into a local estimate of finite length, and forwards them through one or more noisy wireless channels to the Cluster Head (CH). The measurement noise is assumed to be zero-mean and have finite variance. Each wireless hop is assumed to be a Binary Symmetric Channel (BSC) with a known crossover probability. We propose a novel scheme that uses dithered quantization and channel compensation to ensure that each motes' local estimate received by the CH is unbiased. The CH then fuses these unbiased local estimates into a global one using a Best Linear Unbiased Estimator (BLUE). The energy allocation problem at each mote and among different sensor motes are also discussed. Simulation results show that the proposed scheme can achieve much smaller mean square error (MSE) than two other common schemes while using the same amount of energy. The sensitivity of the proposed scheme to errors in estimates of the crossover probability of the BSC channel is studied by both analysis and simulation.
A direct algorithm to compute rational solutions of first order linear q-difference systems
We present an algorithm to compute rational function solutions to a first order system of linear q-difference equations with rational coefficients. We make use of the fact that q-difference equations bear similarity to differential equations at the point 0 and to difference equations at other points. This allows the combining of known algorithms for the differential and the difference cases. This algorithm does not require preliminary uncoupling of the given system.
Product Model Derivation by Model Transformation in Software Product Lines
Product derivation is an essential part of the Software Product Line (SPL) development process. The paperproposes a model transformation for deriving automatically a UML model of a specific product from the UML model of a product line. This work is a part of a larger project aiming to integrate performance analysis in the SPL model-driven development. The SPL source model is expressed in UML extended with two separate profiles: a "product line" profile from literature for specifying the commonality and variability between products, and the MARTE profile recently standardized by OMG for performance annotations. The automatic derivation of a concrete product model based on a given feature configuration is enabled through the mapping between features from the feature model and their realizations in the design model. The paper proposes an efficient mapping technique that aims to minimize the amount of explicit feature annotations in the UML design model of SPL. Implicit feature mapping is inferred during product derivation from the relationships between annotated and non-annotated model elements as defined in the UML metamodel and well formedness rules. The transformation is realized in the Atlas Transformation Language (ATL) and illustrated with an ecommerce case study that models structural and behavioural SPL views.
Web Browsing, Mobile Computing and Academic Performance
Students in two different courses at a major research university (one a Communication course, the other a Computer Science course) were given laptop computers with wireless network access during the course of a semester. Students’ Web browsing on these laptops (including: URLs, dates, and times) was recorded 24 hours/day, 7 days/week in a log file by a proxy server during most of a semester (about 15 weeks). For each student, browsing behavior was quantified and then correlated with academic performance. The emergence of statistically significant results suggests that quantitative characteristics of browsing behavior—even prior to examining browsing content—can be useful predictors of meaningful behavioral outcomes. Variables such as Number of browsing sessions and Length of browsing sessions were found to correlate with students’ final grades; the valence and magnitude of these correlations were found to interact with Course (i.e., whether student was enrolled in the Communication or Computer Science course), Browsing Context (i.e., setting in which browsing took place: during class, on the wireless network between classes, or at home) and Gender. The implications of these findings in relation to previous studies of laptop use in education settings are discussed.
Optimization of transceivers with bit allocation to maximize bit rate for MIMO transmission
There have been many results on designing transceivers for MIMO channels. In early results, the transceiver is designed for a given bit allocation. In this paper we will jointly design the transceiver and bit allocation for maximizing bit rate. By using a high bit rate assumption, we will see that the optimal transceiver and bit allocation can be obtained in a closed form using simple Hadamard inequality and the Poincare separation theorem. In the simulation, we will demonstrate the usefulness of the joint design. Simulation results, in which a high bit rate assumption is not used in allocating bits, show that a higher bit rate can be achieved compared to previously reported methods.
MoRePriv: mobile OS support for application personalization and privacy
Privacy and personalization of mobile experiences are inherently in conflict: better personalization demands knowing more about the user, potentially violating user privacy. A promising approach to mitigate this tension is to migrate personalization to the client, an approach dubbed  client-side personalization . This paper advocates for  operating system support  for client-side personalization and describes MoRePriv, an operating system  service  implemented in the Windows Phone OS. We argue that personalization support should be as ubiquitous as location support, and should be provided by a unified system within the OS, instead of by individual apps.   We aim to provide a solution that will stoke innovation around mobile personalization. To enable easy application personalization, MoRePriv approximates users' interests using  personae  such as technophile or business executive. Using a number of case studies and crowd-sourced user studies, we illustrate how more complex personalization tasks can be achieved using MoRePriv.   For privacy protection, MoRePriv distills sensitive user information to a coarse-grained  profile , which limits the potential damage from information leaks. We see MoRePriv as a way to increase end-user privacy by enabling client-side computing, thus minimizing the need to share user data with the server. As such, MoRePriv shepherds the ecosystem towards a better privacy stance by  nudging  developers away from today's privacy-violating practices. Furthermore, MoRePriv can be  combined  with privacy-enhancing technologies and is complimentary to recent advances in data leak detection.
Hierarchical implicit deregistration with forced registrations in 3G wireless networks
Deregistration due to the departures of mobile users from their current visiting registration area may cause significant traffic in the wireless cellular networks. In this paper, we propose a hierarchical implicit deregistration scheme with forced registration in third-generation wireless cellular networks to reduce the remote/international roaming signaling traffic when home-location registers (HLRs), gateway-location registers (GLRs), and the visitor-location registers (VLRs) form a three-level database hierarchy. In this scheme, if a mobile phone arrives and the GLR/VLR is full, a random record is deleted and the reclaimed storage is reassigned to the new arriving mobile phone. When a call arrives and the callee's record is missing in the GLR/VLR, forced registration is executed to restore the GLR/VLR record before the call-setup operation proceeds. An analytic model is proposed to carry out the performance evaluation for the proposed scheme. Our results show that the proposed scheme not only reduces the local deregistration traffic between the GLR and the VLR, but also reduces the remote/international deregistration traffic between the HLR and the GLR, especially when the ratio of the cost of the remote/international traffic between GLR and HLR to the cost of local traffic between the VLR and the GLR is high.
Global optimization of extended hand-eye calibration
This paper introduces simultaneous global optimization of both camera orientation and vehicle wheel circumference without requiring any information about the translations in the system. The main contribution are new objective function bounds to integrate this problem into a branch-and-bound parameter space search. The presented method constitutes the first guaranteed globally optimal estimator for both components of the problem with respect to a cost function based on reprojection errors. The algorithm operates directly on image measurements and does not depend on any structure and motion preprocessing to estimate camera poses. The complete system is implemented and validated on both synthetic and real automotive datasets.
Blog Popularity Mining Using Social Interconnection Analysis
Analyzing interconnections among blog communities reveals blogger behaviors that could help in assessing blog quality. A new approach to ranking blogs uses a social blog network model based on blogs' interconnection structure and a popularity ranking method, called BRank. Experiments show that the proposed method can discriminate blogs with various degrees of popularity in the blogosphere.
Do changes in movements after tool use depend on body schema or motor learning
In a recent study, Cardinali et al. (2009) showed that training the use of a tool affected kinematic characteristics of subsequent free movements (i.e., movement were slower, for instance), which they interpreted as that the use of a tool affects the body schema. The current study examined whether these results can also be explained in terms of motor learning where movement characteristics during tool use persist in the free movements. Using a different tool we replicated parts of the study of Cardinali et al: As did Cardinali et al. we found that tool use after-effects can be found in subsequent free movements. Importantly, we showed that the tooling movement was very slow compared to the free hand movement. We concluded that it can not be ruled out yet that after-effects of tool use originate from a general slowing down of movement speed that persists in free hand movements.
Discovering golden nuggets: data mining in financial application
With the increase of economic globalization and evolution of information technology, financial data are being generated and accumulated at an unprecedented pace. As a result, there has been a critical need for automated approaches to effective and efficient utilization of massive amount of financial data to support companies and individuals in strategic planning and investment decision-making. Data mining techniques have been used to uncover hidden patterns and predict future trends and behaviors in financial markets. The competitive advantages achieved by data mining include increased revenue, reduced cost, and much improved marketplace responsiveness and awareness. There has been a large body of research and practice focusing on exploring data mining techniques to solve financial problems. In this paper, we describe data mining in the context of financial application from both technical and application perspectives. In addition, we compare different data mining techniques and discuss important data mining issues involved in specific financial applications. Finally, we highlight a number of challenges and trends for future research in this area.
Fault and simple power attack resistant RSA using Montgomery modular multiplication
Side channel attacks and more specifically fault, simple power attacks, constitute a pragmatic, potent mean of braking a cryptographic algorithm like RSA. For this reason, many researchers have proposed modifications on the arithmetic operation functions required for RSA in order to thwart those attacks. However, these modifications are applied on theoretic — algorithmic level and do not necessary result in high performance RSA designs. This paper constitute the first complete attempt for an efficient design approach on a fault and simple power attack resistant RSA based on the well known, for its high performance, Montgomery multiplication algorithm. To achieve this, a fault and simple power attack resistant modular exponentiation algorithm is proposed that is based on the Montgomery modular multiplication. In order to optimize this algorithm's performance we also propose a modified version of Montgomery modular multiplication algorithm that employs value precomputation and carry save logic in all input, output and intermediate values. We introduce a hardware architecture based on the proposed Montgomery modular multiplication algorithm and use it as a building block for the design of a fault and simple power attack resistant modular exponentiation unit. This unit is optimized by taking advantage of the inherit parallelism in the proposed fault and simple power attack resistant modular exponentiation algorithm. Realizing the proposed unit in FPGA technology very advantageous results are found when compared against other well known designs even though our design bears an extra computation cost due to its fault and simple power attack resistance characteristic.
A fast and efficient algorithm to map prerequisites of landslides in sensitive clays based on detailed soil and topographical information
We present an algorithm developed for GIS-applications in order to produce maps of landside susceptibility in postglacial and glacial sediments in Sweden. The algorithm operates on detailed topographic and Quaternary deposit data. We compare our algorithm to two similar computational schemes based on a global visibility operator and a shadow-casting algorithm. We find that our algorithm produces more reliable results in the vicinity of stable material than the global visibility algorithm. We also conclude that our algorithm is more computationally efficient than the other two methods, which is important when we may want to assess the effects of uncertainty in the data by evaluating many different models. Our method also provides the possibility to take other data into account. We show how different soil types with different geotechnical properties may be modelled. Our algorithm may also take depth information, i.e. the thicknesses of the deposits into account. We thus propose that our method may be used to provide more refined maps than the overview maps in areas where more detailed geotechnical/geological data have been acquired. The efficiency of our algorithm suggests that it may replace any global visibility operators used in other applications or processing schemes of gridded map data.
Assessing the Value of Enterprise Identity Management (EIdM) Towards a Generic Evaluation Approach
The introduction of enterprise identity management systems (EldMS) in organisations is a costly and challenging endeavour, where organisations have to face various costs for the planning, the implementation, and the operation of such systems. In the planning phase it is important that organisational aspects are incorporated into the development of an enterprise identity management (EldM) solution, instead of purely focussing on the technological or financial issues. Indeed, without a proper assessment of the costs and the organisational settings (such as stakeholders, processes), companies will not see the benefit for introducing EldM as additional layer into their IT infrastructure and their business processes. This paper proposes initial ideas for a generic approach, based on the balanced scorecard, for assessing the value of investing in the introduction of EldMS. In the decision process, such an instrument can be used for decision support purposes and the planning phase on a tactical level. Furthermore, the organisational aspects are discussed and possible solutions for integrating all relevant parties into the planning process are presented.
Reactive NUMA: a design for unifying S-COMA and CC-NUMA
This paper proposes and evaluates a new approach to directory-based cache coherence protocols called  Reactive NUMA  (R-NUMA). An R-NUMA system combines a conventional CC-NUMA coherence protocol with a more-recent Simple-COMA (S-COMA) protocol. What makes R-NUMA novel is the way it dynamically reacts to program and system behavior to switch between CC-NUMA and S-COMA and exploit the best aspects of both protocols. This reactive behavior allows each node in an R-NUMA system to independently choose the best protocol for a particular page, thus providing much greater performance stability than either CC-NUMA or S-COMA alone. Our evaluation is both qualitative and quantitative. We first show the theoretical result that R-NUMA's worst-case performance is bounded within a small constant factor (i.e., two to three times) of the best of CC-NUMA and S-COMA. We then use detailed execution-driven simulation to show that, in practice, R-NUMA usually performs better than either a pure CC-NUMA or pure S-COMA protocol, and no more than 57% worse than the best of CC-NUMA and S-COMA, for our benchmarks and base system assumptions.
Robust Exponential Stability of Markovian Jump Impulsive Stochastic Cohen-Grossberg Neural Networks With Mixed Time Delays
This paper is concerned with the problem of exponential stability for a class of Markovian jump impulsive stochastic Cohen-Grossberg neural networks with mixed time delays and known or unknown parameters. The jumping parameters are determined by a continuous-time, discrete-state Markov chain, and the mixed time delays under consideration comprise both time-varying delays and continuously distributed delays. To the best of the authors' knowledge, till now, the exponential stability problem for this class of generalized neural networks has not yet been solved since continuously distributed delays are considered in this paper. The main objective of this paper is to fill this gap. By constructing a novel Lyapunov-Krasovskii functional, and using some new approaches and techniques, several novel sufficient conditions are obtained to ensure the exponential stability of the trivial solution in the mean square. The results presented in this paper generalize and improve many known results. Finally, two numerical examples and their simulations are given to show the effectiveness of the theoretical results.
The type and effect discipline
The type and effect discipline, a framework for reconstructing the principal type and the minimal effect of expressions in implicitly typed polymorphic functional languages that support imperative constructs, is introduced. The type and effect discipline outperforms other polymorphic type systems. Just as types abstract collections of concrete values, effects denote imperative operations on regions. Regions abstract sets of possibly aliased memory locations. Effects are used to control type generalization in the presence of imperative constructs while regions delimit observable side effects. The observable effects of an expression range over the regions that are free in its type environment and its type; effects related to local data structures can be discarded during type reconstruction. The type of an expression can be generalized with respect to the variables that are not free in the type environment or in the observable effect. >
Acoustic-to-articulatory inversion using an episodic memory
This paper presents a new acoustic-to-articulatory inversion method-based on an episodic memory, which is an interesting model for two reasons. First, it does not rely on any assumptions about the mapping function but rather it relies on real synchronized acoustic and articulatory data streams. Second, the memory structurally embeds the naturalness of the articulatory dynamics. In addition, we introduce the concept of generative episodic memory, which enables the production of unseen articulatory trajectories according to the acoustic signals to be inverted. The proposed memory is evaluated on the MOCHA corpus. The results show its effectiveness and are very encouraging since they are comparable to those of recently proposed methods.
Home Network Framework Based on OSGi Service Platform Using SSL Component Bundle
It becomes increasingly common to use distributed services according to development of wire/wireless network. Home network area is also based on distributed environment which use a lot of services. Various technologies already have been used in home network. However, the research of information security part is insufficient. Thus, in this paper, we enhance our previous work by applying SSL component as a bundle format. This framework includes efficient user access control based on OSGi service platform from the former researches. This paper can cover the venerability between home gateway and authorization server in existing OSGi service platform. Therefore we provide the advantages of pre-studied framework and user information protection simultaneously.
Analyzing an embedded sensor with timed automata in uppaal
An infrared sensor is modeled and analyzed in Uppaal. The sensor typifies the sort of component that engineers regularly integrate into larger systems by writing interface hardware and software.   In all, three main models are developed. In the first model, the timing diagram of the sensor is interpreted and modeled as a timed safety automaton. This model serves as a specification for the complete system. A second model that emphasizes the separate roles of driver and sensor is then developed. It is validated against the timing diagram model using an existing construction that permits the verification of timed trace inclusion, for certain models, by reachability analysis (i.e., model checking). A transmission correctness property is also stated by means of an auxiliary automaton and shown to be satisfied by the model.   A third model is created from an assembly language driver program, using a direct translation from the instruction set of a processor with simple timing behavior. This model is validated against the driver component of the second timing diagram model using the timed trace inclusion validation technique. The approach and its limitations offer insight into the nature and challenges of programming in real time.
Social Motivations To Use Gamification: An Empirical Study Of Gamifying Exercise
This paper investigates how social factors predict attitude toward gamification and intention to continue using gamified services, as well as intention to recommend gamified services. The paper employs structural equation modelling for analyses of data (n=107) gathered through a survey that was conducted among users of one of the world’s largest gamification applications for physical exercise. The results indicate that social factors are strong predictors for attitudes towards gamification, and, further, continued use intentions and intentions to recommend the related service.
A global test for groups of genes: testing association with a clinical outcome
Motivation: This paper presents a global test to be used for the analysis of microarray data. Using this test it can be determined whether the global expression pattern of a group of genes is significantly related to some clinical outcome of interest. Groups of genes may be any size from a single gene to all genes on the chip (e.g. known pathways, specific areas of the genome or clusters from a cluster analysis).#R##N##R##N#Result: The test allows groups of genes of different size to be compared, because the test gives one p-value for the group, not a p-value for each gene. Researchers can use the test to investigate hypotheses based on theory or past research or to mine gene ontology databases for interesting pathways. Multiple testing problems do not occur unless many groups are tested. Special attention is given to visualizations of the test result, focussing on the associations between samples and showing the impact of individual genes on the test result.#R##N##R##N#Availability: An R-package globaltest is available from http://www.bioconductor.org
Information Security Governance control through comprehensive policy architectures
Information Security Governance has become one of the key focus areas of strategic management due to its importance in the overall protection of the organization's information assets. A properly implemented Information Security Governance framework should ideally facilitate the implementation of (directing), and compliance to (control), Strategic level management directives. These Strategic level management directives are normally interpreted, disseminated and implemented by means of a series of information security related policies. These policies should ideally be disseminated and implemented from the Strategic management level, through the Tactical level to the Operational level where eventual execution takes place. Control is normally exercised by capturing data at the lowest levels of execution and measuring compliance against the Operational level policies. Through statistical and summarized analyses of the Operational level data into higher levels of extraction, compliance at the Tactical and Strategic levels can be facilitated. This scenario of directing and controlling defines the basis of sound Information Security Governance. Unfortunately, information security policies are normally not disseminated onto the Operational level. As a result, proper controlling is difficult and therefore compliance measurement against all information security policies might be problematic. The objective of this paper is to argue towards a more complete information security policy architecture that will facilitate complete control, and therefore compliance, to ensure sound Information Security Governance.
Darwin: customizable resource management for value-added network services
The Internet is rapidly changing from a set of wires and switches that carry packets into a sophisticated infrastructure that delivers a set of complex value-added services to end users. Services can range from bit transport all the way up to distributed value-added services like video teleconferencing, data mining, and distributed interactive simulations. Before such services can be supported in a general and dynamic manner we have to develop appropriate resource management mechanisms. These resource management mechanisms must make it possible to identify and allocate resources that meet service or application requirements, support both isolation and controlled dynamic sharing of resources across organizations sharing physical resources, and be customizable so services and applications can tailor resource usage to optimize their performance. The Darwin project is developing a set of customizable resource management mechanisms that support value-added services, In this paper we present these mechanisms, describe their implementation in a prototype system, and describe the results of a series of proof-of-concept experiments.
Block-Quantized Support Vector Ordinal Regression
Support vector ordinal regression (SVOR) is a recently proposed ordinal regression (OR) algorithm. Despite its theoretical and empirical success, the method has one major bottleneck, which is the high computational complexity. In this brief, we propose a both practical and theoretical guaranteed algorithm, block-quantized support vector ordinal regression (BQSVOR), where we approximate the kernel matrix  K  with [( K )\tilde] that is composed of  k   2  constant blocks. We provide detailed theoretical justification on the approximation accuracy of BQSVOR. Moreover, we prove theoretically that the OR problem with the block-quantized kernel matrix [( K )\tilde] could be solved by first separating the data samples in the training set into  k  clusters with kernel  k -means and then performing SVOR on the  k  cluster representatives. Hence, the algorithm leads to an optimization problem that scales only with the number of clusters, instead of the data set size. Finally, experiments on several real-world data sets support the previous analysis and demonstrate that BQSVOR improves the speed of SVOR significantly with guaranteed accuracy.
Formality, Agility, Security, and Evolution in Software Development
Combining formal and agile techniques in software development has the potential to minimize change-related problems.
Non-negative pre-image in machine learning for pattern recognition
Moreover, in order to have a physical interpretation, some constraints should be incorporated in the signal or image processing technique, such as the non-negativity of the solution. This paper deals with the non-negative pre-image problem in kernel machines, for nonlinear pattern recognition. While kernel machines operate in a feature space, associated to the used kernel function, a pre-image technique is often required to map back features into the input space. We derive a gradient-based algorithm to solve the pre-image problem, and to guarantee the non-negativity of the solution. Its convergence speed is significantly improved due to a weighted stepsize approach. The relevance of the proposed method is demonstrated with experiments on real datasets, where only a couple of iterations are necessary.
